Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Log Work,Log Work,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Cloners),Outward issue link (Container),Outward issue link (Container),Outward issue link (Container),Outward issue link (Duplicate),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Parent Feature),Outward issue link (Problem/Incident),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Regression),Outward issue link (Regression),Outward issue link (Required),Outward issue link (Supercedes),Outward issue link (dependent),Outward issue link (dependent),Outward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (Hadoop Flags),Custom field (Hadoop Flags),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Tags),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Ability for having user's classes take precedence over the system classes for tasks' classpath,MAPREDUCE-1938,12469213,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Blocker,Fixed,ramach,ddas,ddas,14/Jul/10 00:37,26/Mar/13 08:06,12/Jan/21 09:52,30/Oct/11 20:17,,,,,,0.20.204.0,0.23.0,,job submission,task,tasktracker,,,,3,,,,,"It would be nice to have the ability in MapReduce to allow users to specify for their jobs alternate implementations of classes that are already defined in the MapReduce libraries. For example, an alternate implementation for CombineFileInputFormat. ",,aah,cutting,darose,eli,gwittel,kasha,kkambatl,larsfrancke,mahadev,matei,oae,omalley,pvoss,qwertymaniac,scott_carey,srivas,tomwhite,tucu00,vicaya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PIG-2484,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-6942,,,,,,,,,,,,,,,,,,,,,"19/Aug/10 02:18;ramach;mapred-1938-1;https://issues.apache.org/jira/secure/attachment/12452481/mapred-1938-1","19/Aug/10 23:06;ramach;mapred-1938-2.patch;https://issues.apache.org/jira/secure/attachment/12452591/mapred-1938-2.patch","04/Sep/10 02:07;ramach;mapred-1938-3.patch;https://issues.apache.org/jira/secure/attachment/12453856/mapred-1938-3.patch","14/Jul/10 20:35;ddas;mr-1938-bp20.1.patch;https://issues.apache.org/jira/secure/attachment/12449483/mr-1938-bp20.1.patch","14/Jul/10 00:49;ddas;mr-1938-bp20.patch;https://issues.apache.org/jira/secure/attachment/12449409/mr-1938-bp20.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,2010-07-14 16:56:58.76,,,false,,,,,,,,,,,,,,,,,,36998,,,,,Tue Mar 26 08:06:59 UTC 2013,,,,,,,"0|i02q2v:",13826,,,,,,,,,,,,,,,,,,,,,"14/Jul/10 00:49;ddas;Patch for Y20. Not for commit here.

The patch defines a configuration option using which users can specify that the system classpath be added last in the list of classpaths created before launching a task. It also introduces a HADOOP_CLIENT_CLASSPATH env variable in the hadoop shell script that should be set to point to the user's jar/classpath (containing his implementation of the system classes) before submitting a job over command line.","14/Jul/10 16:56;cutting;Two thoughts:
 1. In general, we need to better separate the kernel from the library.  CombineFileInputFormat is library code and should be easy to update without updating the cluster.  Long-term, only kernel code should be hardwired on the classpath of tasks, with library and user code both specified per job.  There should be no default version of library classes for a task: tasks should always specify their required libraries.  Is there a Jira for this?  I know Tom's expressed interest in working on this.
 2. We should permit user code to depend on different versions of things than the kernel does.  For example, user code might rely on a different version of HttpClient or Avro than that used by MapReduce.  This should be possible if instances of classes from these are not a passed between user and kernel code, e.g., as long as Avro and HttpClient classes are not a part of the MapReduce API.  In this case classloaders (probably via OSGI) could permit this.","14/Jul/10 17:01;omalley;I think that the default for this should be on.

Rather than add HADOOP_CLIENT_CLASSPATH, let's make a new variable HADOOP_USER_CLASSPATH_LAST. If it is defined, we add HADOOP_CLASSPATH to the tail like we currently do. Otherwise it is added to the front.","14/Jul/10 17:36;omalley;Doug,

I agree that the kernel code should be split out from libraries, however, that work is much more involved. I don't see a problem with putting the user's code first. It is not a security concern. The user's code is only run as the user. Furthermore, it doesn't actually stop them from loading system classes. They can exec a new jvm with a new class path of their own choosing.

Therefore, by putting the user's classes last all that we've done is make it harder for the user to implement hot fixes in their own jobs. That doesn't seem like a good goal.","14/Jul/10 18:46;cutting;Owen, I agree with your analysis.  I'm just trying to put this patch in context of these other related discussions.

This patch addresses some issues relevant to separation of kernel & library.  In common cases one can merely provide an alternate version of the library class in one's job.  Fully separating kernel & library with a well-defined, minimal kernel API is clearly aesthetically better.  Are there use cases that will that enable that this patch will not?  I think mostly it will just make it clear which classes are safe to replace with updated versions and which are not.  Does that sound right?

The issue of user versions of libraries that the kernel uses (like Avro, log4j, HttpClient, etc.) is not entirely addressed by this patch.  If the user's version is backwards compatible with the kernel's version then this patch is sufficient.  But if the user's version of a library makes incompatible changes then we'd need a classloader/OSGI solution.  Even then, I think it only works if user and kernel code do not interchange instances of classes defined by these libraries.  A minimal kernel API will help reduce that risk.  Does this analysis sound right?

I'm trying to understand how far this patch gets us towards those goals: what it solves and what it doesn't.","14/Jul/10 20:35;ddas;Addressing Owen's comment on the shell script part of the patch. 

Doug, this patch is a first step towards letting users use their own versions of library provided implementation for things like CombineFileInputFormat. The use case is to allow for specific implementations of library classes for certain classes of jobs. 

This doesn't aim to address the kernel/library separation in its entirety. So yes, if the user puts a class on the classpath that doesn't work with the kernel compatibly then tasks will fail, or produce obscure/inconsistent results, but that will affect only that job, and the user would notice that soon (hopefully). Did i understand your concern right?","14/Jul/10 20:55;cutting;> Did i understand your concern right?

I don't have specific concerns about this patch.  Sorry for any confusion in that regard.  I thought it worthwhile to discuss how this change relates to other changes that are contemplated.  It seems not inconsistent, provides some of the benefits, and is considerably simpler; in short, a good thing.","14/Jul/10 21:04;omalley;This patch basically puts the user in charge of their job. They can leave the safety switch set in which case they get the current behavior. But if they turn off the safety, their classes go ahead of the ones installed on the cluster. That means that they can break things, but all they can break is their own tasks.

After we do the split of core from library, you still need this switch. There will always be the possibility of needing to patch something in the core, because even MapTask has bugs. *smile* After splitting them apart, we can put the library code at the very end

safety on:  core, user, library
safety off: user, core, library

This patch is just about providing the safety switch.","13/Aug/10 14:46;darose;Just wondering:  does anyone know if Cloudera has plans to include this patch in the CDH any time soon?","19/Aug/10 02:18;ramach;We were going thro' major outstanding issues in the trunk
Thought will upload a preliminary port of Devaraj's patch to trunk + a simple test case
","19/Aug/10 18:20;vicaya;A couple of questions regarding the patch (filename should probably end with .patch):
# Are the new property a system/cluster property or a per job property? i.e, allowing user to to override the behavior (default to system jars first) per job. Owen's comments seem to imply it's a per job switch, which should be defined/documented in MRJobConfig (MAPREDUCE-1749)
# Why there are two boolean properties defined in the trunk patch ""mapreduce.user.classpath.first"" (defined in TaskRunner) and ""mapreduce.task.classpath.user.precedence"" (defined in JobContext)? The latter seems to have no effect and not tested.
# Why use Vector (in the test code) when there doesn't seem to be need for the list to be synchronized?
# Why do the test code use hard code paths and string literals when there are corresponding constants defined?
","19/Aug/10 23:06;ramach;Luke
Last one was a preliminary/quick port of what was there and lot of changes were redundant.  I have cleaned it up in this rev.

As for moving to MRJobConfig, based on discussions, my understanding is,  desired to have this flag semi-private (MRJobConfig i believe is very public)

  ","20/Aug/10 17:40;vicaya;Nits in the test code aside, +1","25/Aug/10 19:12;ramach;testpatch output

     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 5 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 Eclipse classpath. The patch causes the Eclipse classpath to differ from the contents of the lib directories.
","02/Sep/10 08:33;amareshwari;bq. As for moving to MRJobConfig, based on discussions, my understanding is, desired to have this flag semi-private (MRJobConfig i believe is very public).
MRJobConfig is the place to hold all job-level config paramaters. And it is not public. Annotation says ""private, evolving"". So, the configuration should be in MRJobConfig. Also, the name should say mapreduce.job.user.classpath.first saying it is a job level property.
Also, in your next patch, can you do the changes in test-case suggested by Luke?

Are you planning to raise a Hadoop-common jira for the changes in bin/hadoop script?","04/Sep/10 02:07;ramach;revised per previous comment 
+ Luke's test case review - vector is API requirement - can't modify

Yes there will be a separate jira for bin/hadoop client command line changes","16/Dec/10 09:36;tucu00;IMO this is not the right way of approaching this issue as it will lead to have multiple implementations of a library in the classpath and that sooner or later breaks things in very odd ways. Following an example of an issue I've run into few years ago:

 a-1.jar contains classes: A, B (with B using A)
 a-2.jar contains classes: A, C (with C using A)

A classes from a-1.jar and a-2.jar are different.
Kernel 'sometimes' uses class B from a-1.jar
user-app 'sometimes' uses class C from a-2.jar

All this will compile, but will create very odd errors at runtime.

Because of this, I wouldn't provide a mechanism to allow users to do this.

Instead, I think we should implement a task-classloader that only exposes to the application the MR API and the user is complete control of all the JARs in the app classpath, including input/output formats, etc, etc. 

I understand this can be a bit tricky because the line of API vs implementation classes is not that clear as in the servlet API (to bring an example of APIs with implementations doing this).

","16/Dec/10 17:49;mahadev;alejandro, good point. I think the premise here was to provide a simple way without making too many changes, to allow users to have there libraries in the classpath.  Also, I think the assumption is that the task code doesnt load that many libraries.I would think its more like a hack than a perfect solution but I'd be happy to see a solution like the one you proposed.  Are you working on such a soln? ","16/Dec/10 18:14;eli;This is a pretty serious bug that has ramifications for other jiras, let's make it a blocker for 22.","16/Dec/10 18:21;omalley;Alejandro, you are viewing this wrong. Of course the user shouldn't deliberately make conflicts without understanding the problem. This is about providing control to the user so that they can upgrade (or hotfix) a jar file for their job without installing it on the cluster.

I would suggest that the default should be user jars and then the framework jars with a compatibility switch for backwards compatibility.","17/Dec/10 09:28;tucu00;Mahadev,

I'm not working on a patch for this at the moment. I wouldn't mind (I'll have to allocate some time for it), but before doing so, I need a clear cut on the APIs that are interface, and they should be in their own JAR file, separate from the JAR file with the implementation classes.

Regarding your comment on 'task code does not load that many libraries', I'm not sure is a valid statement if you are doing Pig or Hive, just check how many more JARs they bring into the classpath, and then check of those JARs how many of them overlap with Hadoop JARs, and then check their versions.

Owen,

Users get in trouble without making conflicts in a deliberate way, they get into those conflicts because of the lack of isolation of the current model (refer to second paragraph of answer to Mahadev).

Having the use in control as the default is, IMO, the wrong way to go, all the testing you've done on a release becomes meaningless as the users classpath will override any library used by the cluster by default.



My take is that we should fix this via classloader isolation from the get go. 

If folks consider this to be a TOO big task (IMO, it will aways be) and the user&system classpath swap is a stop gap alternative, my take is that we should for a default of system-user classpath.

I personally see this as a big issue (using Hadoop from Oozie means that I have to deal with libraries from Hadoop/Pig/Hive/Sqoop/Hbase in the classpath).","17/Dec/10 18:24;mahadev;alejandro, I think your claim might be a little broken in seeing this as a big overhaul of the class loading. Users I have mostly interacted with usually dont do so. Other users (advanced) usually want this to try things out or in case of urgent fixes (not already installed) want to override that with a new jar ( the new jar has all the dependencies it needs). I have usually seens users wanting to override one of the jars in the classpath. If you think otherwise, please do make changes as you see fit.","17/Dec/10 18:27;tomwhite;BTW the classloader isolation issue is MAPREDUCE-1700.","17/Dec/10 18:41;darose;As per Mahadev's comment (""I have usually seens users wanting to override one of the jars in the classpath."")  That's my use case and, I'd think, the major use case re: this bug.

To clarify my issue:  the version of Jackson that ships with Hadoop is old.  But Avro, which I use in several of my M/R jobs, requires a newer version.  Even if I supply the newer Jackson jar using -libjars, the job still picks up the old one (which has priority on the classpath) and so my job fails.

There is no easy workaround for this.  What we wound up doing to fix this issue in our cluster was to go into every single node, rename the jackson.jar out of the way, and then restart the daemons.  Very manual and error prone process.  (Not to mention a very inelegant fix.)

So what's needed here is some way to tell Hadoop (without too much muss and fuss):  for execution of this M/R job only, *my* version of the jackson.jar takes precedence over the one that comes shipped.","17/Dec/10 23:08;tucu00;A lot of testing goes into Hadoop before doing a release. All this testing is done using a well defined set of libraries and sometimes a particular version is used because of a bug fix or because the absence of a bug.

By allowing users to override the libraries used by Hadoop (in the tasks) we are throwing overboard all the testing/certification done as part of a release as now pieces of the cluster (the tasks) will run with who-knows-what version of the different libraries.

IMO, this feature will be the nightmare of  the team in charge of operations/support of the Hadoop cluster.

If we introduce this feature, we should have a switch at cluster config level that turns it off so admins can switch it off. And by default it should be off.","19/Dec/10 18:44;omalley;{quote}
If we introduce this feature, we should have a switch at cluster config level that turns it off so admins can switch it off. And by default it should be off.
{quote}

This feature just makes it easier. It is already possible. The cluster operator can force the attribute in the configuration making it more difficult again.","10/Jan/11 21:07;nidaley;Too late for 0.22.  Moving Fix Version from 0.22 to 0.23.","13/Jan/11 18:52;scott_carey;{quote}
Users get in trouble without making conflicts in a deliberate way, they get into those conflicts because of the lack of isolation of the current model (refer to second paragraph of answer to Mahadev).

Having the use in control as the default is, IMO, the wrong way to go, all the testing you've done on a release becomes meaningless as the users classpath will override any library used by the cluster by default.

My take is that we should fix this via classloader isolation from the get go. 
{quote}

I agree and disagree.  If someone volunteered and fixed the whole situation with classloader and/or shade and/or OSGi then great, this would not be needed.  However, at this point I'd rather be given a crude tool with varioius risks than no tool at all.  No cluster since 0.18 has worked for me without classpath collisions requiring manual replacement of libraries.  

So all that precious testing on the release you mention is useless to me since such a cluster can't run my jobs!


{quote}
Too late for 0.22. Moving Fix Version from 0.22 to 0.23. 
{quote}

Considering this is in the Y! disro (and perhaps CDH3 ?) is there actually any veto for getting this in 0.22?    Its simplistic and does provide the user with a gun to shoot themselves in the foot, but the user interested in this already has a different gun, thats just harder to use and affects all tasks in the cluster instead of selected ones.","30/Oct/11 20:17;acmurthy;Fixed in 0.20.2xx. Not relevant for MR2.","30/Nov/11 09:12;larsfrancke;Is this documented somewhere? How would I find out about this possibility without having to use a search engine and stumbling across this and then having to read a patch or release notes?

Honest question. I never know where, how and if things are being documented in Hadoop.","06/Jan/12 14:42;qwertymaniac;This isn't present in 0.23. Removing from Fix Version. I'll take Arun's word that it is not needed there either (though it'd look like API breakage, hrm?)

It should be included in 0.22 though.","06/Jan/12 14:50;qwertymaniac;Sorry, my bad. I meant to target another JIRA. Adding back the Fix Version.","19/Mar/13 16:54;qwertymaniac;bq. Not relevant for MR2.

Apparently is still relevant for MR2:

./hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/util/MRApps.java utilizes it to setup proper classpath structures.

If your comment meant some other form of relevancy, please do clarify. In any case, this is present as ""mapreduce.job.user.classpath.first"" in MR2 :)","19/Mar/13 17:45;vicaya;This is option is no longer needed after MAPREDUCE-1700 (except for very rare cases where you want to implement your own classloader).","26/Mar/13 08:06;qwertymaniac;Thanks for clarifying Luke!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3A Group,MAPREDUCE-7316,13351389,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,wangzhengwei,wangzhengwei,11/Jan/21 07:43,11/Jan/21 07:43,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,,,wangzhengwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2021-01-11 07:43:14.0,,,,,,,"0|z0mfsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Speculative attempts should not run on the same node,MAPREDUCE-7169,13201904,New Feature,Patch Available,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,BilwaST,lichen1109,lichen1109,03/Dec/18 01:55,17/Dec/20 15:00,12/Jan/21 09:52,,2.7.2,,,,,,,,yarn,,,,,,1,,,,,"          I found in all versions of yarn, Speculative Execution may set the speculative task to the node of  original task.What i have read is only it will try to have one more task attempt. haven't seen any place mentioning not on same node.It is unreasonable.If the node have some problems lead to tasks execution will be very slow. and then placement the speculative  task to same node cannot help the  problematic task.
         In our cluster （version 2.7.2，2700 nodes），this phenomenon appear almost everyday.

 !image-2018-12-03-09-54-07-859.png! 
",,ahussein,bibinchundatt,BilwaST,epayne,hadoopci,hongyu.bi,jdonofrio,jeagles,jiwq,lichen1109,tangzhankun,uranus,xd_zhaodong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-6066,,,,,,,,,,,,,,,,,,,,,"21/Mar/19 12:56;BilwaST;MAPREDUCE-7169-001.patch;https://issues.apache.org/jira/secure/attachment/12963268/MAPREDUCE-7169-001.patch","16/Nov/19 19:13;BilwaST;MAPREDUCE-7169-002.patch;https://issues.apache.org/jira/secure/attachment/12986030/MAPREDUCE-7169-002.patch","27/Nov/19 05:50;BilwaST;MAPREDUCE-7169-003.patch;https://issues.apache.org/jira/secure/attachment/12986878/MAPREDUCE-7169-003.patch","11/Apr/20 15:02;BilwaST;MAPREDUCE-7169.004.patch;https://issues.apache.org/jira/secure/attachment/12999637/MAPREDUCE-7169.004.patch","11/Apr/20 18:33;BilwaST;MAPREDUCE-7169.005.patch;https://issues.apache.org/jira/secure/attachment/12999657/MAPREDUCE-7169.005.patch","31/Jul/20 18:32;BilwaST;MAPREDUCE-7169.006.patch;https://issues.apache.org/jira/secure/attachment/13008850/MAPREDUCE-7169.006.patch","01/Aug/20 06:34;BilwaST;MAPREDUCE-7169.007.patch;https://issues.apache.org/jira/secure/attachment/13008860/MAPREDUCE-7169.007.patch","03/Dec/18 01:54;lichen1109;image-2018-12-03-09-54-07-859.png;https://issues.apache.org/jira/secure/attachment/12950328/image-2018-12-03-09-54-07-859.png",,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,2018-12-03 02:33:41.431,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 17 15:00:11 UTC 2020,,,,,,,"0|s0139s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"03/Dec/18 02:33;xd_zhaodong;yeah, This issue is very important for us.","03/Dec/18 06:00;uranus;I will work for this.:D","04/Jan/19 11:10;bibinchundatt;[~uranus]

Looking at the code . Currently for the new taskAttempt container request, we use all {{datalocalHosts}}

TaskAttemptImpl#RequestContainerTransition
{code}
        taskAttempt.eventHandler.handle(new ContainerRequestEvent(
            taskAttempt.attemptId, taskAttempt.resourceCapability,
            taskAttempt.dataLocalHosts.toArray(
                new String[taskAttempt.dataLocalHosts.size()]),
            taskAttempt.dataLocalRacks.toArray(
                new String[taskAttempt.dataLocalRacks.size()])));
{code}

In async scheduling high probability for containers getting allocated to same node.
We should skip the nodes on which previous task attempt was lauched,when Avataar is *SPECULTIVE*


","12/Feb/19 03:41;lichen1109;[~uranus]Do you have some ideas?","12/Feb/19 04:06;uranus;[~bibinchundatt], [~lichen1109]. There are two ways to solve this problem.
 * *Before Container Request:* Ignore data locality container request for speculative task attempt. 
 * *After Container Allocated:* When get the same node with last attempt, release this container and request another. Also, we should add a limited number when retrying.

I think the way 2 is better, because we should still honor data locality even if in speculative task attempt.","17/Feb/19 02:33;uranus;After YARN-6592, we can solve this problem easily using new resource request class, SchedulingRequest.

Currently, maybe it's better just remove last attempt's node host in next resource request for speculation attempt. Just like, 
{code:java}
speculationTaskAttempt.dataLocalHosts = speculationTaskAttempt.dataLocalHosts.remove(lastAttempt.host);

//allocate resource for this speculation attempt.
// code...{code}
[~bibinchundatt], [~ajisakaa]. How do you think this?","16/Mar/19 05:28;BilwaST;Hi [~uranus]

are u still working on this issue? I have a solution for this already.","16/Mar/19 05:37;uranus;[~BilwaST], I am just testing for new method. Please feel free to take it if you have already have a solution.

BTW, Can you share your idea for this problem?","18/Mar/19 04:39;BilwaST;Hi [~uranus]

Same as what [~bibinchundatt] had suggested. We can skip nodes where previous attempt was launched. 
{quote} * In TaskImpl#addAndScheduleAttempt  whenever Avataar is SPECULATIVE  update previous attempts nodes and racks                                 
 *  In TaskAttemptImpl#RequestContainerTransition when Avataar is SPECULATIVE , we can skip previous attempts nodes and racks . Also we need to keep record of blacklisted  nodes                                                   
 *  During allocation for mapper RMContainerAllocator#assignMapsWithLocality ,we have three types of  resource requests                             
 *  Hosts - already we have updated datahosts for the task attempt                 
 *  Racks - this is also taken care in taskattemptimpl                                       
 *  Any - In this case we need blacklisted node record which we had upadated so that we can check if node is blacklisted for the task. If it is blacklisted then we skip allocating and it would retry for other container {quote}","18/Mar/19 04:52;uranus;[~BilwaST], thanks for your reply. I reassigned this issue to you. You can attach your patch if you have time. :D","21/Mar/19 13:01;BilwaST;cc [~bibinchundatt] [~jlowe]","21/Mar/19 14:01;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 22s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 21m 45s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 29s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 26s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 35s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 56s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 43s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 23s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 27s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 25s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 25s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 21s{color} | {color:orange} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app: The patch generated 15 new + 463 unchanged - 1 fixed = 478 total (was 464) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 29s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red}  0m  0s{color} | {color:red} The patch has 12 line(s) that end in whitespace. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 12s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 47s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 18s{color} | {color:red} hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  9m 39s{color} | {color:green} hadoop-mapreduce-client-app in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 23s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 59m 50s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8f97d6f |
| JIRA Issue | MAPREDUCE-7169 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12963268/MAPREDUCE-7169-001.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 7d98ec9b3b24 4.4.0-138-generic #164-Ubuntu SMP Tue Oct 2 17:16:02 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 9f1c017 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_191 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7605/artifact/out/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7605/artifact/out/whitespace-eol.txt |
| javadoc | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7605/artifact/out/diff-javadoc-javadoc-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7605/testReport/ |
| Max. process+thread count | 587 (vs. ulimit of 10000) |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7605/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.

","05/Jun/19 08:51;jiwq;Hi [~BilwaST], do you mind modify the issues gave by Yetus?","06/Nov/19 14:18;ahussein;[~BilwaST], what is the current state of the patch? Are you still working on this issue?","06/Nov/19 15:21;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 34s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m 55s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 33s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 38s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 55s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 44s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 27s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 25s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 25s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 29s{color} | {color:orange} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app: The patch generated 15 new + 463 unchanged - 1 fixed = 478 total (was 464) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 27s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red}  0m  0s{color} | {color:red} The patch has 12 line(s) that end in whitespace. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 32s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 46s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 24s{color} | {color:red} hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 10m 16s{color} | {color:green} hadoop-mapreduce-client-app in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 31s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 60m 58s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=19.03.4 Server=19.03.4 Image:yetus/hadoop:104ccca9169 |
| JIRA Issue | MAPREDUCE-7169 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12963268/MAPREDUCE-7169-001.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 262daed9877e 4.15.0-58-generic #64-Ubuntu SMP Tue Aug 6 11:12:41 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / c360141 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_222 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7686/artifact/out/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7686/artifact/out/whitespace-eol.txt |
| javadoc | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7686/artifact/out/diff-javadoc-javadoc-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7686/testReport/ |
| Max. process+thread count | 726 (vs. ulimit of 5500) |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7686/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.

","16/Nov/19 19:14;BilwaST;[~ahussein] I have updated patch.","16/Nov/19 19:16;BilwaST;[~jiwq] sorry i missed ur comment on patch. I have uploaded latest patch. Please check","16/Nov/19 20:01;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 24s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m  4s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 31s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 35s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 32s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 13m 33s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 42s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 24s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:red}-1{color} | {color:red} mvninstall {color} | {color:red}  0m 16s{color} | {color:red} hadoop-mapreduce-client-app in the patch failed. {color} |
| {color:red}-1{color} | {color:red} compile {color} | {color:red}  0m 17s{color} | {color:red} hadoop-mapreduce-client-app in the patch failed. {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red}  0m 17s{color} | {color:red} hadoop-mapreduce-client-app in the patch failed. {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 28s{color} | {color:orange} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app: The patch generated 5 new + 463 unchanged - 1 fixed = 468 total (was 464) {color} |
| {color:red}-1{color} | {color:red} mvnsite {color} | {color:red}  0m 18s{color} | {color:red} hadoop-mapreduce-client-app in the patch failed. {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red}  0m  0s{color} | {color:red} The patch has 5 line(s) that end in whitespace. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply {color} |
| {color:red}-1{color} | {color:red} shadedclient {color} | {color:red}  4m  8s{color} | {color:red} patch has errors when building and testing our client artifacts. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  0m 20s{color} | {color:red} hadoop-mapreduce-client-app in the patch failed. {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 21s{color} | {color:red} hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  0m 21s{color} | {color:red} hadoop-mapreduce-client-app in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 28s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 40m 46s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=19.03.5 Server=19.03.5 Image:yetus/hadoop:104ccca9169 |
| JIRA Issue | MAPREDUCE-7169 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12986030/MAPREDUCE-7169-002.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 158b42a0439b 4.15.0-58-generic #64-Ubuntu SMP Tue Aug 6 11:12:41 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / b3119b9 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_222 |
| findbugs | v3.1.0-RC1 |
| mvninstall | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7687/artifact/out/patch-mvninstall-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt |
| compile | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7687/artifact/out/patch-compile-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt |
| javac | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7687/artifact/out/patch-compile-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7687/artifact/out/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt |
| mvnsite | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7687/artifact/out/patch-mvnsite-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7687/artifact/out/whitespace-eol.txt |
| findbugs | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7687/artifact/out/patch-findbugs-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt |
| javadoc | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7687/artifact/out/diff-javadoc-javadoc-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt |
| unit | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7687/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7687/testReport/ |
| Max. process+thread count | 415 (vs. ulimit of 5500) |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7687/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.

","22/Nov/19 18:44;jeagles;[~BilwaST], the patch is failing with a cannot find symbol in RMContainerAllocator.java. Can you provide an updated patch?","26/Nov/19 17:05;jeagles;[~BilwaST], probably you missed my earlier comment. Will you be putting up a fixed version of the patch. I can help to review","27/Nov/19 05:37;BilwaST;HI [~jeagles]  I will update it by EOD","27/Nov/19 07:01;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 33s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 21m 14s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 31s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 32s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 32s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 14m 44s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 43s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 25s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 27s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 27s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 26s{color} | {color:orange} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app: The patch generated 11 new + 463 unchanged - 1 fixed = 474 total (was 464) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 28s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red}  0m  0s{color} | {color:red} The patch has 7 line(s) that end in whitespace. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 15m 20s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 22s{color} | {color:red} hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 11m  3s{color} | {color:green} hadoop-mapreduce-client-app in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 30s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 69m 33s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=19.03.5 Server=19.03.5 Image:yetus/hadoop:104ccca9169 |
| JIRA Issue | MAPREDUCE-7169 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12986878/MAPREDUCE-7169-003.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 81db03f6332e 4.15.0-58-generic #64-Ubuntu SMP Tue Aug 6 11:12:41 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / c8bef4d |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_222 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7689/artifact/out/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7689/artifact/out/whitespace-eol.txt |
| javadoc | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7689/artifact/out/diff-javadoc-javadoc-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7689/testReport/ |
| Max. process+thread count | 735 (vs. ulimit of 5500) |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7689/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.

","22/Dec/19 05:50;BilwaST;Hi [~jeagles]  can u please review the patch?","30/Dec/19 16:58;ahussein;[~BilwaST], I am sorry but the patch is not applicable anymore with the trunk. See YARN-9052 that causes the conflict. Can you please fix the compilation errors along with the java doc and checkstyle issues?

I see that you add the node hosting the original task to the blacklist of the speculative task. Wouldn't it be easier just to change the order of the dataLocalHosts so that the node will be picked last in the loop? In that case, the speculative task will run on the same node only if all other nodes cannot be assigned to the speculative task.","31/Dec/19 22:17;ahussein;[~BilwaST], can you also add a configuration to enable/disable your code changes?
My intuition is that changing the policy to pick the node for the speculative task will inherently change the efficiency of the speculation.
For example, picking a different node may increase the startup time of the speculative task. This implies change of the speculation efficiency compared to the legacy behavior. Thus, I suggest to give the option for the  user to enable/disable the new policy in case she prefers to evaluate the new behavior and revert back to the legacy one if necessary.","11/Apr/20 07:30;BilwaST;Hi [~ahussein] 

Sorry for late reply. 
{quote}I see that you add the node hosting the original task to the blacklist of the speculative task. Wouldn't it be easier just to change the order of the dataLocalHosts so that the node will be picked last in the loop? In that case, the speculative task will run on the same node only if all other nodes cannot be assigned to the speculative task.
{quote}
With change like this, there are chances that task attempt will get launched on same node where it was launched which wouldn't solve the problem. If we blacklist node then in next iteration of containers assigned it will be launched on other node.
{quote}My intuition is that changing the policy to pick the node for the speculative task will inherently change the efficiency of the speculation.
For example, picking a different node may increase the startup time of the speculative task. This implies change of the speculation efficiency compared to the legacy behavior. Thus, I suggest to give the option for the user to enable/disable the new policy in case she prefers to evaluate the new behavior and revert back to the legacy one if necessary.
{quote}

I agree with this. i will change code accordingly. Currently working on it","11/Apr/20 15:03;BilwaST;cc [~ahussein] [~jeagles] [~jiwq]","11/Apr/20 16:16;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 29s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 23s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m 52s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 42s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m  0s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  9s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 15m 17s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 35s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 48s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 12s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 59s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 34s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 54s{color} | {color:orange} hadoop-mapreduce-project/hadoop-mapreduce-client: The patch generated 11 new + 1001 unchanged - 1 fixed = 1012 total (was 1002) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 57s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 53s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  0m 47s{color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 23s{color} | {color:red} hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  5m 11s{color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m  4s{color} | {color:green} hadoop-mapreduce-client-app in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 29s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 73m 30s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app |
|  |  Write to static field org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.speculativeBlacklistEnabled from instance method new org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl(JobId, TaskType, int, EventHandler, Path, JobConf, TaskAttemptListener, Token, Credentials, Clock, int, MRAppMetrics, AppContext)  At TaskImpl.java:from instance method new org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl(JobId, TaskType, int, EventHandler, Path, JobConf, TaskAttemptListener, Token, Credentials, Clock, int, MRAppMetrics, AppContext)  At TaskImpl.java:[line 345] |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=19.03.8 Server=19.03.8 Image:yetus/hadoop:e6455cc864d |
| JIRA Issue | MAPREDUCE-7169 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12999637/MAPREDUCE-7169.004.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  xml  |
| uname | Linux 0381ece1eced 4.15.0-58-generic #64-Ubuntu SMP Tue Aug 6 11:12:41 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 275c478 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_242 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7761/artifact/out/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
| findbugs | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7761/artifact/out/new-findbugs-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.html |
| javadoc | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7761/artifact/out/diff-javadoc-javadoc-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7761/testReport/ |
| Max. process+thread count | 1610 (vs. ulimit of 5500) |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7761/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.

","11/Apr/20 20:22;hadoopqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 42s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m  5s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 31m 30s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  2m  2s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 59s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 19s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 18m 43s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 52s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 49s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 12s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  9s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  2m  0s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  2m  0s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 53s{color} | {color:orange} hadoop-mapreduce-project/hadoop-mapreduce-client: The patch generated 5 new + 1001 unchanged - 1 fixed = 1006 total (was 1002) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  7s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  2s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 17m  3s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  3s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 44s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  6m  0s{color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 45s{color} | {color:green} hadoop-mapreduce-client-app in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 31s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 98m 54s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=19.03.8 Server=19.03.8 Image:yetus/hadoop:e6455cc864d |
| JIRA Issue | MAPREDUCE-7169 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12999657/MAPREDUCE-7169.005.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  xml  |
| uname | Linux 1d821e979bca 4.15.0-60-generic #67-Ubuntu SMP Thu Aug 22 16:55:30 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 275c478 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_242 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7762/artifact/out/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7762/testReport/ |
| Max. process+thread count | 1583 (vs. ulimit of 5500) |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7762/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.

","15/Apr/20 11:48;BilwaST;Hi [~jeagles]

Could you please help to review?","28/Apr/20 15:20;BilwaST;Hi [~jeagles]

can you please review when you have free time?","29/Apr/20 14:13;ahussein;Thanks [~BilwaST] !
{quote}With change like this, there are chances that task attempt will get launched on same node where it was launched which wouldn't solve the problem. If we blacklist node then in next iteration of containers assigned it will be launched on other node.
{quote}
The same node will be picked if there are no other available nodes. In [^MAPREDUCE-7169.005.patch] , what is the expected behavior if the resources available to run the taskAttempt are only available on the same node? I do not see this case in the unit test.","29/Apr/20 14:37;BilwaST;Hi [~ahussein]
{code:java}
The same node will be picked if there are no other available nodes. In MAPREDUCE-7169.005.patch , what is the expected behavior if the resources available to run the taskAttempt are only available on the same node? I do not see this case in the unit test.{code}

Container is not assigned until resources are available on other node.  Task will wait until it gets container on other node. As per use case we do not want container to be launched on same node as node might have a problem","30/Apr/20 18:46;ahussein;Thanks [~BilwaST], I will take a look to the details of the implementation","07/May/20 15:43;ahussein;[~BilwaST], the speculation, taskAttempts, and allocations code set is not a straightforward module to tackle. You did a great good job!

I have the following points:

- Address checkstyle report.

*Corner Case scenario:*

* Assuming that a new speculative attempt is created. Following the implementation, the new attempt X will have blacklisted nodes and skipped racks relevant to the original taskAttempt Y
* Assuming taskAttempt Y is killed before attempt X gets assigned.
* The RMContainerAllocator would still assign a host to attemptX based on the dated blacklists.

Is this the expected behavior? or it is supposed to clear  attemptX' blacklisted nodes?

*TaskAttemptBlacklistManager*

*  Should that object be synchronized? I believe there are more than one thread reading/writing to that object. Perhaps changing {{taskAttemptToEventMapping}} to {{concurrentHashMap}} would be sufficient. What do you think?
*   In {{taskAttemptToEventMapping}}, the data is only removed when the taskAttempt is assigned. If taskAttempt is killed before being assigned, {{taskAttemptToEventMapping}} would still have the taskAttempt.

*{{TaskAttemptBlacklistManager}}*

* Should that object be synchronized? I believe there are more than one thread reading/writing to that object. Perhaps changing {{taskAttemptToEventMapping}} to concurrentHashMap would be sufficient. What do you think?
* In taskAttemptToEventMapping, the data is only removed when the taskAttempt is assigned. If taskAttempt is killed before being assigned, taskAttemptToEventMapping would still have the taskAttempt.

*{{TaskAttemptImpl}}*
* Racks are going to be black listed too. Not just nodes. I believe that the javadoc and description in default.xml should emphasize that enabling the flag also avoids the local rack unless no other rack is available for scheduling.

*{{TaskImpl}}*
* why do we need {{mapTaskAttemptToAvataar}} when each taskAttempt has a field called {{avataar}} ?

*{{ContainerRequestEvent}}*
  - That's a design issue. One would expect  that RequestEvent's lifetime should not survive {{handle()}} call. Therefore, the metadata should be consumed by the handlers. In the patch, {{ContainerRequestEvent.blacklistedNodes}} could be a field in taskAttempt. Then you won't need  {{TaskAttemptBlacklistManager}} class.
","09/May/20 17:20;BilwaST;Hi [~ahussein]

What we are trying to achieve here is speculative attempt shouldn't be launched on faulty node. So even if task gets killed there is no point launching it on that node as it will slow.This is expected behaviour
{quote} 
 * Assuming that a new speculative attempt is created. Following the implementation, the new attempt X will have blacklisted nodes and skipped racks relevant to the original taskAttempt Y
 * Assuming taskAttempt Y is killed before attempt X gets assigned.
 * The RMContainerAllocator would still assign a host to attemptX based on the dated blacklists.
 Is this the expected behavior? or it is supposed to clear attemptX' blacklisted nodes?{quote}
Yes i think these two cases should be handled
{quote} * Should that object be synchronized? I believe there are more than one thread reading/writing to that object. Perhaps changing {{taskAttemptToEventMapping}} to {{concurrentHashMap}} would be sufficient. What do you think?
{quote}* In {{taskAttemptToEventMapping}}, the data is only removed when the taskAttempt is assigned. If taskAttempt is killed before being assigned, {{taskAttemptToEventMapping}} would still have the taskAttempt.
{quote}{quote}
Will update this
{quote} * Racks are going to be black listed too. Not just nodes. I believe that the javadoc and description in default.xml should emphasize that enabling the flag also avoids the local rack unless no other rack is available for scheduling.{quote}
Actually when task attempt is killed by default Avataar is VIRGIN. this is defect which needs to be addressed. If speculative task attempt is killed it is launched as normal task attempt
{quote} * why do we need {{mapTaskAttemptToAvataar}} when each taskAttempt has a field called {{avataar}} ?{quote}
How do you get taskattempt details in RMContainerAllocator??
{quote} - That's a design issue. One would expect that RequestEvent's lifetime should not survive {{handle()}} call. Therefore, the metadata should be consumed by the handlers. In the patch, {{ContainerRequestEvent.blacklistedNodes}} could be a field in taskAttempt. Then you won't need {{TaskAttemptBlacklistManager}} class.{quote}

Thanks ","11/May/20 12:24;ahussein;{quote}Actually when task attempt is killed by default Avataar is VIRGIN. this is defect which needs to be addressed. If speculative task attempt is killed it is launched as normal task attempt
{quote}
That's interesting.
 If speculative task attempt is killed, a new task attempt is launched as normal. The new attempt will have a new ID I guess, right? in that case, the map entry would not be relevant and a new entry is created for the new attempt ID.
{quote}How do you get taskattempt details in RMContainerAllocator??
{quote}
I see your point. It is preferred that ""request"" object should only be alive if it is pending or not handled yet. In order to keep that concept, the simplest work around is to change the field in {{TaskAttemptBlacklistManager}}.
{code:java}
private Map<TaskAttemptId, Set<String>>
      taskAttemptToEventMapping =
          new HashMap<TaskAttemptId, Set<String>>();

  public void addToTaskAttemptBlacklist(ContainerRequestEvent event) {
    if (null != event.getBlacklistedNodes()
        && event.getBlacklistedNodes().size() > 0) {
      taskAttemptToEventMapping.put(event.getAttemptID(), event.getBlacklistedNodes());
    }
  }
{code}
One last thing:
 Since we are going to keep {{TaskAttemptBlacklistManager}}, then {{RMContainerAllocator.taskManager}} is not the best name. Perhaps it should be renamed to something more relevant to its functionality {{attemptBlacklistMgr}}, or {{speculativeLocalityMgr}}, .or..etc.","29/May/20 12:08;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  8s{color} | {color:red} MAPREDUCE-7169 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | MAPREDUCE-7169 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12999657/MAPREDUCE-7169.005.patch |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7787/console |
| versions | git=2.17.1 |
| Powered by | Apache Yetus 0.12.0 https://yetus.apache.org |


This message was automatically generated.

","01/Aug/20 03:15;hadoopci;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  1m  1s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} dupname {color} | {color:green}  0m  0s{color} | {color:green} No case conflicting files found. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  3m 26s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 26m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 56s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m  2s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 24s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 16m 50s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 49s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} spotbugs {color} | {color:blue}  1m 15s{color} | {color:blue} Used deprecated FindBugs config; considering switching to SpotBugs. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 14s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 29s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  4s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 51s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 51s{color} | {color:orange} hadoop-mapreduce-project/hadoop-mapreduce-client: The patch generated 8 new + 1001 unchanged - 1 fixed = 1009 total (was 1002) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  4s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  3s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 13m 36s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 41s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 16s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  6m 52s{color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 31s{color} | {color:green} hadoop-mapreduce-client-app in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 33s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 92m 16s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | ClientAPI=1.40 ServerAPI=1.40 base: https://ci-hadoop.apache.org/job/PreCommit-MAPREDUCE-Build/2/artifact/out/Dockerfile |
| JIRA Issue | MAPREDUCE-7169 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/13008850/MAPREDUCE-7169.006.patch |
| Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient findbugs checkstyle xml |
| uname | Linux 49bec53e6e6b 4.15.0-60-generic #67-Ubuntu SMP Thu Aug 22 16:55:30 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | personality/hadoop.sh |
| git revision | trunk / a7fda2e38f2 |
| Default Java | Private Build-1.8.0_252-8u252-b09-1~18.04-b09 |
| checkstyle | https://ci-hadoop.apache.org/job/PreCommit-MAPREDUCE-Build/2/artifact/out/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
|  Test Results | https://ci-hadoop.apache.org/job/PreCommit-MAPREDUCE-Build/2/testReport/ |
| Max. process+thread count | 1555 (vs. ulimit of 5500) |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://ci-hadoop.apache.org/job/PreCommit-MAPREDUCE-Build/2/console |
| versions | git=2.17.1 maven=3.6.0 findbugs=4.0.6 |
| Powered by | Apache Yetus 0.12.0 https://yetus.apache.org |


This message was automatically generated.

","01/Aug/20 06:35;BilwaST;Hi [~ahussein]

I have handled all review comments and fixed checkstyle issues which can be fixed.","01/Aug/20 11:02;hadoopci;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  1m  4s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} dupname {color} | {color:green}  0m  1s{color} | {color:green} No case conflicting files found. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  1s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  3m 22s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 29m 45s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  2m 21s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m  9s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 21s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 19m  4s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 48s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} spotbugs {color} | {color:blue}  1m 19s{color} | {color:blue} Used deprecated FindBugs config; considering switching to SpotBugs. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 22s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 28s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  7s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  2m  2s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  2m  2s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  1m  3s{color} | {color:orange} hadoop-mapreduce-project/hadoop-mapreduce-client: The patch generated 3 new + 1001 unchanged - 1 fixed = 1004 total (was 1002) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  8s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 16m 28s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 47s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 33s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  7m 35s{color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  9m 10s{color} | {color:green} hadoop-mapreduce-client-app in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 34s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}103m 23s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | ClientAPI=1.40 ServerAPI=1.40 base: https://ci-hadoop.apache.org/job/PreCommit-MAPREDUCE-Build/3/artifact/out/Dockerfile |
| JIRA Issue | MAPREDUCE-7169 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/13008860/MAPREDUCE-7169.007.patch |
| Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient findbugs checkstyle xml |
| uname | Linux 6ee8ef31e26d 4.15.0-58-generic #64-Ubuntu SMP Tue Aug 6 11:12:41 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | personality/hadoop.sh |
| git revision | trunk / 5323e83edfe |
| Default Java | Private Build-1.8.0_252-8u252-b09-1~18.04-b09 |
| checkstyle | https://ci-hadoop.apache.org/job/PreCommit-MAPREDUCE-Build/3/artifact/out/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
|  Test Results | https://ci-hadoop.apache.org/job/PreCommit-MAPREDUCE-Build/3/testReport/ |
| Max. process+thread count | 1598 (vs. ulimit of 5500) |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://ci-hadoop.apache.org/job/PreCommit-MAPREDUCE-Build/3/console |
| versions | git=2.17.1 maven=3.6.0 findbugs=4.0.6 |
| Powered by | Apache Yetus 0.12.0 https://yetus.apache.org |


This message was automatically generated.

","10/Aug/20 21:13;ahussein;[~BilwaST] sorry for my late response. I overlooked the notification.

(+1 non-binding) for MAPREDUCE-7169.007.patch","17/Dec/20 15:00;ahussein;Hey [~BilwaST], is there anything I can help with to get this change merged?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Specifying node labels when submitting MR jobs,MAPREDUCE-6304,12787518,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,Naganarasimha,john.jian.fang,john.jian.fang,01/Apr/15 23:12,11/Aug/20 06:50,12/Jan/21 09:52,18/May/17 07:31,,,,,,2.7.4,2.8.0,3.0.0-alpha1,job submission,,,,,,0,mapreduce,,,,"Per the discussion on YARN-796, we need a mechanism in MAPREDUCE to specify node labels when submitting MR jobs.",,antbofh,Cyl,dian.fu,dvillegas,elgoiri,erwaman,Feng Yuan,hammer,hudson,jhung,jianhe,john.jian.fang,junping_du,leftnoteasy,Naganarasimha,qwertymaniac,rohithsharma,shv,sunilg,varun_saxena,xinxianyin,Ying Zhang,yufeldman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,YARN-10394,,,,,,,,,,,,,,,,,,,,,YARN-796,YARN-3490,MAPREDUCE-6890,,,,,,,,,,,MAPREDUCE-6421,,,,,,,,"17/May/17 22:44;elgoiri;MAPREDUCE-6304-branch-2.7.patch;https://issues.apache.org/jira/secure/attachment/12868622/MAPREDUCE-6304-branch-2.7.patch","10/Apr/15 18:39;Naganarasimha;MAPREDUCE-6304.20150410-1.patch;https://issues.apache.org/jira/secure/attachment/12724623/MAPREDUCE-6304.20150410-1.patch","10/Apr/15 20:26;Naganarasimha;MAPREDUCE-6304.20150411-1.patch;https://issues.apache.org/jira/secure/attachment/12724652/MAPREDUCE-6304.20150411-1.patch","01/May/15 13:42;Naganarasimha;MAPREDUCE-6304.20150501-1.patch;https://issues.apache.org/jira/secure/attachment/12729764/MAPREDUCE-6304.20150501-1.patch","10/May/15 18:17;Naganarasimha;MAPREDUCE-6304.20150510-1.patch;https://issues.apache.org/jira/secure/attachment/12731811/MAPREDUCE-6304.20150510-1.patch","11/May/15 11:32;Naganarasimha;MAPREDUCE-6304.20150511-1.patch;https://issues.apache.org/jira/secure/attachment/12731906/MAPREDUCE-6304.20150511-1.patch","12/May/15 05:50;Naganarasimha;MAPREDUCE-6304.20150512-1.patch;https://issues.apache.org/jira/secure/attachment/12732135/MAPREDUCE-6304.20150512-1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,2015-04-08 05:49:12.0,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Aug 18 08:52:24 UTC 2017,,,,,,,"0|i27o93:",9223372036854775807,,,,,,,,,,,,,2.7.4,2.8.0,,,,,,,"01/Apr/15 23:16;john.jian.fang;Link related JIRAs","08/Apr/15 05:49;yufeldman;Should be as simple as:
YarnRunner sets label on ApplicationSubmissionContext for MapReduce applications based on additional configuration property like mapreduce.job.label.

It is described in https://issues.apache.org/jira/secure/attachment/12654148/LabelBasedScheduling.pdf even though it was not done as such.","08/Apr/15 16:57;john.jian.fang;mapreduce.job.label may not be enough. There should be at least another parameter such as mapreduce.job.am.label for the application master. For example, on EC2, we don't want to run an application master on a spot instance, but we do allow MR tasks to run on spot instances (otherwise, what is the purpose to use instances?). Furthermore, Application Master is a special Yarn container and MRAppMaster does not run as a YarnChild, right?","08/Apr/15 17:12;yufeldman;[~john.jian.fang] Definitely it depends whether you want separate label for AM and the rest of the tasks.","08/Apr/15 19:03;john.jian.fang;YarnRunner is the right place to set the labels for a MR job. However, there is one concern here. If I understand correctly, YarnRunner runs at the job client side, right? There should also be a way to hook in the labels on the server side, i.e., resource manager side. The reason is that many Hadoop users do not understand or set the labels by themselves and they simply rely on the Hadoop platform provider (or system admins for on-premise clusters) to set up the labels for them. I am not sure if this is a general use case, but it is definitely a feature that we need.","08/Apr/15 19:15;yufeldman;I do not think labels should be dictated by RM, since different types of jobs may require different types of resources. It is definitely true that cluster admins label nodes so those labels can be used while making a decision from the application prospective.
It is true that job is submitted from the client, but it is not like you submit different classes of jobs every time, so eventually job owner will know what resources he/she needs for that specific class of jobs. 
The case you described warrants just setting labels in mapred-site.xml on client side and be done. If you submit all sorts of jobs then may be you don't even care about labels.
Yet another choice is to utilize labels on Queues - this will be set on the cluster so for jobs submitted to a specific queue specific labels will be applied","08/Apr/15 19:47;john.jian.fang;I understand your point from on-premise cluster perspective. However, it is not very practical to manage mapred-site.xml or queue files for users if hadoop is a service in cloud. As a hadoop developer, you should consider both on-premise hadoop cluster and hadoop in cloud. 

There are many many users for a hadoop cloud service. Usually they launch their own hadoop clusters in cloud and control their own queue files or mapred-site.xml.  Some of them even run their hadoop jobs on their own gateways that the hadoop platform provider does not have access to. But the hadoop service provider may still want to have a mechanism to set up some global labels for all users to improve their user experiences. For example, a failure of an application master on a spot instance due to the termination of a spot instance will cause more trouble than a failure of one MR task. These types of settings most likely can only be done by hadoop cloud service providers based on their deep knowledge in their own cloud services.

Or could hadoop provide a mechanism for hadoop providers to extend so that you only need to specify the labels in YarnRunner in Vanilla hadoop?  
","08/Apr/15 19:55;Naganarasimha;Thanks for the comments [~yufeldman] & [~john.jian.fang],
bq. Definitely it depends whether you want separate label for AM and the rest of the tasks.
I have started working on this patch and had already considered the 2 kinds of labels as ApplicationSubmissionContext was supporting it and REST has already taken care of it .

bq. The reason is that many Hadoop users do not understand or set the labels by themselves and they simply rely on the Hadoop platform provider (or system admins for on-premise clusters) to set up the labels for them. I am not sure if this is a general use case, but it is definitely a feature that we need.
Even though labels should not be dictated by RM, i feel there might be cases as mentioned by [~john.jian.fang] where users might not be aware what each label stands for! or users might not want to understand the label as partition (which is more on administrative side in multi tenant scenarios) and they might just require some auto mapping based on user or group. So i think we can come up with some configurations of automapping based on user/group (similar to queues)   for labels too.","08/Apr/15 20:06;john.jian.fang;Thanks Naganarasimha for your understanding. However, user/group mapping may not work for us since we don't have control of that as a hadoop service provider. I would prefer a plugin mechanism rather than a solution here so that we can extend that for our service. But I think the change for YarnRunner is still needed for hadoop users anyway. ","08/Apr/15 20:13;Naganarasimha;bq. But I think the change for YarnRunner is still needed for hadoop users anyway.
yes that true, other discussion is more like an additional requirement, which i think needs attention from [~wangda] too.","09/Apr/15 18:43;leftnoteasy;Just found I'm not in the watcher list, missed some discussions.

I think [~john.jian.fang]'s use case is very unique but also interesting, platform provider has no control and knowledge of user's applications, and user doesn't understand about platform's details such as which machine is temporary or not.

Maybe a possible way to solve this problem is to add a global-am-resource-request setting, which can include node-label-expression, node/rack information. Because as YARN RM, it doesn't know what's the application's running model, it only knows which container is AM or not, which seems enough for your requirements.

But adding such global-am-resource-request setting can also be dangerous, such as how about a queue is not accessible node-label-expression of global-am-resource-request?","10/Apr/15 18:39;Naganarasimha;Attaching a patch for this issue","10/Apr/15 18:57;Naganarasimha;Hi [~wangda], As {{global-am-resource-request}} can have some caveats which you mention, what if we can have {{yarn.scheduler.capacity.\{queue_path\}.default-am-node-label-expression}} which we can ensure while loading if the labels configured for the queue path exists or not.  Or in much more generic way, can we have some {{ContainerPlacementPolicy}} interface \@ CS which needs to provide which queue and label to be used for a given container, user & group ?","10/Apr/15 19:16;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12724623/MAPREDUCE-6304.20150410-1.patch
  against trunk revision 7660da9.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5389//console

This message is automatically generated.","10/Apr/15 20:26;Naganarasimha;correcting the previous patch","10/Apr/15 21:03;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12724652/MAPREDUCE-6304.20150411-1.patch
  against trunk revision 7660da9.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:red}-1 eclipse:eclipse{color}.  The patch failed to build with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5390//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5390//console

This message is automatically generated.","13/Apr/15 16:37;john.jian.fang;A more generic way could be to add a decorator to ApplicationSubmissionContext in class ClientRMService so that people can change the ApplicationSubmissionContext in the method submitApplication(). The default decorator from Apache does nothing, but hadoop allows users to use a custom decorator from hadoop configuration. ","13/Apr/15 16:58;Naganarasimha;[~john.jian.fang], thanks for the comments but did not get your idea completely. Comment is for the approach taken in the patch or earlier discussion of supporting default label expression for am container ?
bq. The default decorator from Apache does nothing, but hadoop allows users to use a custom decorator from hadoop configuration.
Can you please specify which configuration you are referring to ?","13/Apr/15 18:40;john.jian.fang;I mean hadoop could provide a new mechanism such as a decorator for the ApplicationSubmissionContext. When the method submitApplication() in class ClientRMService is called, hadoop decorates the ApplicationSubmissionContext before it calls the following line. For example, manipulates the amLabelExpression. 

      rmAppManager.submitApplication(submissionContext,
          System.currentTimeMillis(), user);

Hadoop could provide a default decorator that does nothing. But users could override the default decorator in yarn-site.xml by a new configuration parameter, for example, ""yarn.app.submission.context.decorator.class"".

This new mechanism is not directly related to the change you are making, but it is more generic so that the platform providers could update the ApplicationSubmissionContext in their own ways. Once we have such a new mechanism in place, you do not really need to add anything new to your label code for my use case. Instead, the custom logic will be included in the custom decorator provided by the platform provider. For example, we could provide a decorator to update amLabelExpression in ApplicationSubmissionContext. Other fields of ApplicationSubmissionContext could be changed as well to meet user's needs.

","15/Apr/15 18:01;john.jian.fang;I created JIRA YARN-3490 for the application decorator proposal. ","18/Apr/15 00:58;Naganarasimha;[~wangda], if you have bandwidth can you take a look \@ this small patch ?
","25/Apr/15 00:39;Naganarasimha;Hi [~wangda],[~qwertymaniac],[~jianhe],
Can any one take a look at this patch ?","27/Apr/15 18:41;leftnoteasy;Hi [~Naganarasimha],
Just took a look at the patch, some comments about syntax:
- Beyond AM/Job node-labels setting, should we support Mapper/Reducer setting? 
- When job and AM (or mapper/reducer) set together, AM (or mapper/reducer)'s node-label-expression should overwrite job's setting.
- If we need support mapper/reducer's node-label-expression. We may need add some changes in MR AM side and some tests.

Any ideas? [~lohit]. ","27/Apr/15 23:21;Naganarasimha;Thanks for the review comments [~wangda],
bq. Beyond AM/Job node-labels setting, should we support Mapper/Reducer setting?
bq. If we need support mapper/reducer's node-label-expression. We may need add some changes in MR AM side and some tests.
Well my view should be to support them as i feel in some cases we might feel like mapper can run in any node but may be reducer requires more mem so may be clients might require it to be run in high mem nodes (may be some constraint on mem for the nodes)  or some other node constraints.

bq. When job and AM (or mapper/reducer) set together, AM (or mapper/reducer)'s node-label-expression should overwrite job's setting.
Well, IIUC then {{ApplicationSubmissionContext.getNodeLabelExpression()}} is for  mapper/reducer and AM's node label expression is set in {{ApplicationSubmissionContext.getAMContainerResourceRequest}}. I could cross verify the same with code in ApplicationMasterService (line number 495) & RMAppManager (line number 378) and so did not get what you meant by the above statement, correct me if my understanding is wrong.","27/Apr/15 23:37;leftnoteasy;bq. Well my view should be to support them as i feel in some cases we might feel like mapper can run in any node but may be reducer requires more mem so may be clients might require it to be run in high mem nodes (may be some constraint on mem for the nodes) or some other node constraints.
That's correct, so you agree to add mapper/reducer node-label-expression?

bq. Well, IIUC then ApplicationSubmissionContext.getNodeLabelExpression() is for mapper/reducer and AM's node label expression is set in ApplicationSubmissionContext.getAMContainerResourceRequest. I could cross verify the same with code in ApplicationMasterService (line number 495) & RMAppManager (line number 378) and so did not get what you meant by the above statement, correct me if my understanding is wrong.
What I meant is:
When job.label = x, and don't set am/mapper/reducer label, all containers should get allocated on x 
When job.label = x and am.label = y, don't set mapper/reducer label, am will get allocated on y (overwritten x) but mapper/reducer should get allocated on x.
When job.label = x, am.label=y, mapper.label=z, am will get allocated on y, mapper will get allocated on z (overwrite x), and reducer get allocated on x.
This should be existing behavior. And mapper/reducer's label should be added to ResourceRequest (may need modify {{RMContainerRequestor}}) instead of AplicationSubmissionContext.","28/Apr/15 00:35;Naganarasimha;Thanks for detailed clarification, got what you intended and was planning to do the same way, i.e. only if ""mapreduce.job.map.node-label-expression"" or  ""mapreduce.job.reduce.node-label-expression"" config are set then modify the ask in {{RMContainerRequestor}} as yarn takes care of setting in {{ApplicationMasterService}} when no labels specified in resource request for mapper and reducer and in {{RMAppManager}} for AM.","01/May/15 04:00;Naganarasimha;Hi [~wangda], have incorporated changes for {{add mapper/reducer node-label-expression}} also incorported limitations set by YARN-2694(Ensure only single node label specified in ResourceRequest) which limits labels to be supported on only ""ANY"" ResourceRequests. 
But one query what i have is : Isnt it a serious limitation if labels are supported for only ANY resource requests?","01/May/15 09:57;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 40s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 2 new or modified test files. |
| {color:green}+1{color} | javac |   7m 31s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 34s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   1m 42s | The applied patch generated  16 new checkstyle issues (total was 814, now 830). |
| {color:red}-1{color} | whitespace |   0m  2s | The patch has 10  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 35s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |
| {color:red}-1{color} | findbugs |   2m 57s | The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | mapreduce tests |   8m 40s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   1m 34s | Tests passed in hadoop-mapreduce-client-core. |
| {color:green}+1{color} | mapreduce tests | 105m 27s | Tests passed in hadoop-mapreduce-client-jobclient. |
| | | 154m 49s | |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-mapreduce-client-app |
|  |  Inconsistent synchronization of org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.reduceNodeLabelExpression; locked 66% of time  Unsynchronized access at RMContainerAllocator.java:66% of time  Unsynchronized access at RMContainerAllocator.java:[line 218] |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12729713/MAPREDUCE-6304.20150501-1.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 1b3b9e5 |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5482/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-core.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5482/artifact/patchprocess/whitespace.txt |
| Findbugs warnings | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5482/artifact/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-app.html |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5482/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5482/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| hadoop-mapreduce-client-jobclient test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5482/artifact/patchprocess/testrun_hadoop-mapreduce-client-jobclient.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5482/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf909.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5482/console |


This message was automatically generated.","01/May/15 13:42;Naganarasimha;hi [~Wangda] checkstyle has only valid white space issues, correcting it. Findbugs issues is not so relveant for the variables usage, resubmitting the patch with the white space issue","01/May/15 16:24;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 56s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 2 new or modified test files. |
| {color:green}+1{color} | javac |   7m 41s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 51s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   1m 32s | The applied patch generated  12 new checkstyle issues (total was 814, now 826). |
| {color:red}-1{color} | whitespace |   0m  2s | The patch has 1  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 39s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 35s | The patch built with eclipse:eclipse. |
| {color:red}-1{color} | findbugs |   3m 32s | The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | mapreduce tests |  10m 48s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   2m 17s | Tests passed in hadoop-mapreduce-client-core. |
| {color:red}-1{color} | mapreduce tests | 100m 51s | Tests failed in hadoop-mapreduce-client-jobclient. |
| | | 154m 16s | |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-mapreduce-client-app |
|  |  Inconsistent synchronization of org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.reduceNodeLabelExpression; locked 66% of time  Unsynchronized access at RMContainerAllocator.java:66% of time  Unsynchronized access at RMContainerAllocator.java:[line 218] |
| Failed unit tests | hadoop.mapred.TestMiniMRClientCluster |
|   | hadoop.mapred.TestMRTimelineEventHandling |
| Timed out tests | org.apache.hadoop.mapreduce.TestLargeSort |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12729764/MAPREDUCE-6304.20150501-1.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 1b3b9e5 |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5483/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-core.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5483/artifact/patchprocess/whitespace.txt |
| Findbugs warnings | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5483/artifact/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-app.html |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5483/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5483/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| hadoop-mapreduce-client-jobclient test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5483/artifact/patchprocess/testrun_hadoop-mapreduce-client-jobclient.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5483/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf903.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5483/console |


This message was automatically generated.","01/May/15 18:02;leftnoteasy;[~Naganarasimha], 
Patch generally LGTM, one minor comment:
- If specify label="""" in mapred-default.xml, which means the job will always set label="""" while adding resource-request, but the previously behavior is send ""null"" instead of empty string. So I suggest to change the default to be ""NOT SPECIFIED"" (which is not a valid label since it contains space) as default value, and we will replace it by null. Does this make sense to you? Do you have any other ideas on this? 

And could you deploy a few nodes cluster, with 2-3 labels, and run MR job with am.label, mapper.label, reducer.label, etc. that will be very important to make sure everything works well.","02/May/15 15:41;Naganarasimha;Thanks for reviewing  [~wangda], 
bq. If specify label="""" in mapred-default.xml, which means the job will always set label=""""  ....   So I suggest to change the default to be ""NOT SPECIFIED"" (which is not a valid label since it contains space) as default value, and we will replace it by null. 
Good catch, but AFAIK we need not do so much handling and if we just remove {{<value></value>}} in mapred-default.xml then the value will be taken as null by xml parser in the Configuration class. while i am testing in the multi node cluster will verify this and then update the patch.

bq. could you deploy a few nodes cluster, with 2-3 labels, and run MR job with am.label, mapper.label, reducer.label, etc. that will be very important to make sure everything works well.
Will try to get this tested on monday ( currenty only left with laptop :) ).
Test case failures doesn't seem to be related to the patch based on logs. 
","04/May/15 22:36;leftnoteasy;[~Naganarasimha], remove from mapred-default.xml is not good. mapred-default.xml is a way to document all options we have, how about make its default to be ""USE_QUEUE_DEFINED_DEFAULT"" or better name, it's not ""not specified"" or ""null"" actually, it's using queue defined node label expression. Thoughts?","05/May/15 01:05;Naganarasimha;hi [~Wangda], 
I meant actually only deletion of  {{<value></value>}}  for example 
{quote}
<property>
	  <name>mapreduce.job.am.node-label-expression</name>
	 {color:red} <!-- value></value -->{color}
	  <description>Overrides mapreduce.job.node-label-expression for application
	  master containers
	  </description>
</property>
{quote}
which is similar to the existing configs like yarn.ipc.*.factory.class, yarn.resourcemanager.cluster-id,yarn.resourcemanager.ha.rm-ids,yarn.resourcemanager.ha.id ....
In general your approach is fine but as we can make use of the above approach of configuration did not want to bring in additional default configs. thoughts ? ","05/May/15 18:07;leftnoteasy;[~Naganarasimha], thanks for pointing me about yarn.ipc.*.factory, etc. I think it's important to
- Not bring in additional unncessary default config
- Follow what we have in *default.xml
- Make admin easy to understand

So I think it's fine to do as what you suggested, but could you please mention in description that, by default the node-label-expression for job is not set, it will use queue's default-node-label-expression.","05/May/15 23:07;Naganarasimha;Thanks [~Wangda] for your comments,
+1 for {{mention in description that, by default the node-label-expression for job is not set, it will use queue's default-node-label-expression.}}. I am getting it tested in cluster setup, will upload the updated patch today.","10/May/15 18:17;Naganarasimha;Updated the description and tested in 2 NM with 3 labels and also specifying different combination of labels","11/May/15 02:41;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 38s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 2 new or modified test files. |
| {color:green}+1{color} | javac |   7m 29s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 36s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   3m 51s | The applied patch generated  12 new checkstyle issues (total was 497, now 509). |
| {color:red}-1{color} | whitespace |   0m  2s | The patch has 2  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 41s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:red}-1{color} | findbugs |   2m 53s | The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | mapreduce tests |   9m 30s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   1m 38s | Tests passed in hadoop-mapreduce-client-core. |
| {color:green}+1{color} | mapreduce tests | 108m 46s | Tests passed in hadoop-mapreduce-client-jobclient. |
| | | 161m 10s | |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-mapreduce-client-app |
|  |  Inconsistent synchronization of org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.reduceNodeLabelExpression; locked 66% of time  Unsynchronized access at RMContainerAllocator.java:66% of time  Unsynchronized access at RMContainerAllocator.java:[line 218] |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12731811/MAPREDUCE-6304.20150510-1.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 4536399 |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5699/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-core.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5699/artifact/patchprocess/whitespace.txt |
| Findbugs warnings | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5699/artifact/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-app.html |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5699/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5699/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| hadoop-mapreduce-client-jobclient test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5699/artifact/patchprocess/testrun_hadoop-mapreduce-client-jobclient.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5699/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf905.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5699/console |


This message was automatically generated.","11/May/15 11:32;Naganarasimha;Attaching patch with applicable white-space and check style issues fixed","11/May/15 14:16;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 38s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 2 new or modified test files. |
| {color:green}+1{color} | javac |   7m 30s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 39s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   1m 30s | The applied patch generated  8 new checkstyle issues (total was 497, now 505). |
| {color:green}+1{color} | whitespace |   0m  2s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 38s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |
| {color:red}-1{color} | findbugs |   2m 53s | The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | mapreduce tests |   8m 58s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   1m 36s | Tests passed in hadoop-mapreduce-client-core. |
| {color:green}+1{color} | mapreduce tests | 106m 15s | Tests passed in hadoop-mapreduce-client-jobclient. |
| | | 155m 44s | |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-mapreduce-client-app |
|  |  Inconsistent synchronization of org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.reduceNodeLabelExpression; locked 66% of time  Unsynchronized access at RMContainerAllocator.java:66% of time  Unsynchronized access at RMContainerAllocator.java:[line 218] |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12731906/MAPREDUCE-6304.20150511-1.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 3fa2efc |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5700/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-core.txt |
| Findbugs warnings | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5700/artifact/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-app.html |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5700/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5700/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| hadoop-mapreduce-client-jobclient test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5700/artifact/patchprocess/testrun_hadoop-mapreduce-client-jobclient.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5700/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf902.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5700/console |


This message was automatically generated.","12/May/15 01:01;leftnoteasy;Thanks [~Naganarasimha] for working on this and testing, mostly LGTM, could you add the overwriting behavior in mapred-default.xml? (For example, by default is using queue's default-node-label-expression, AM.expression can overwrite job.expression, etc.","12/May/15 05:50;Naganarasimha;Hi [~Wangda], have modified the documentation in Mapred-default.xml. Please check.","12/May/15 08:36;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 43s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 2 new or modified test files. |
| {color:green}+1{color} | javac |   7m 32s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 39s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   1m 34s | The applied patch generated  8 new checkstyle issues (total was 503, now 511). |
| {color:green}+1{color} | whitespace |   0m  2s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 40s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |
| {color:red}-1{color} | findbugs |   2m 53s | The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | mapreduce tests |   9m  0s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   1m 37s | Tests passed in hadoop-mapreduce-client-core. |
| {color:green}+1{color} | mapreduce tests | 105m 52s | Tests passed in hadoop-mapreduce-client-jobclient. |
| | | 155m 38s | |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-mapreduce-client-app |
|  |  Inconsistent synchronization of org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.reduceNodeLabelExpression; locked 66% of time  Unsynchronized access at RMContainerAllocator.java:66% of time  Unsynchronized access at RMContainerAllocator.java:[line 218] |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12732135/MAPREDUCE-6304.20150512-1.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 3d28611 |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5708/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-core.txt |
| Findbugs warnings | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5708/artifact/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-app.html |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5708/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5708/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| hadoop-mapreduce-client-jobclient test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5708/artifact/patchprocess/testrun_hadoop-mapreduce-client-jobclient.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5708/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf901.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5708/console |


This message was automatically generated.","12/May/15 20:52;leftnoteasy;Thanks update, [~Naganarasimha], +1 for latest patch.","14/May/15 04:10;Naganarasimha;Hi [~wangda]
One suggestion for this patch : labels will be applicable for only for {{Any}} resource requests, so would it be better to have a config, based on which rack local and node specific will not be submitted and only {{Any}}  resource requests with label will be submitted ? This will ensure that MR tasks run only on the labelled nodes.
If the existing support to labels are sufficient then can we push this in ?
","14/May/15 17:46;leftnoteasy;[~Naganarasimha],
Actually now CS will check label-expression of ANY resource request matches labels on nodes before allocating rack/node local requests. IAW, node-label-expression of rack/node local request will be ignored and will be treated as same as ANY request. To reply your concern:
MR tasks run only on the labeled nodes.

Since this changes MR configuration, I will wait  for another couple of days before commit this.

Thanks,","14/May/15 18:31;Naganarasimha;Thanks [~wangda] for clarifying my query, and ok for waiting for couple days for committing this.","15/May/15 17:28;leftnoteasy;[~Naganarasimha], I think I made a mistake in previous comment, rack/node local resource request is not allowed to specify label-expression for now. See SchedulerUtils.validateResourceRequest, I think you need to add a check in MR side, if the resourceName is not ANY, don't set node-label-expression to it.","15/May/15 18:03;Naganarasimha;Hi [~wangda]
I have already taken care of this 
* {{JOB_NODE_LABEL_EXP}} is set to the Application submission context and yarn takes care of setting only when ResourceRequest is ANY
* {{AM_NODE_LABEL_EXP}} is set to the AM's Resource request which is of Any type always. 
* {{MAP_NODE_LABEL_EXP}} & {{REDUCE_NODE_LABEL_EXP}} are added to the resource request only when its of {{ANY}} type in RMContainerRequestor.addContainerReq(ContainerRequest). Test case is also present for it.

My question was in most cases container is assigned to either Node Local or Rack local, So in these cases container cannot be assigned to a Node satisfying the Node Label exp. So my suggestion was based on boolean config flag can we decide whether to add node local and rack local request or not. for example expose  ENABLE_ANY_RESOURCE_REQUEST_ONLY config based on which node local and rack local request will not be set.","15/May/15 21:53;leftnoteasy;[~Naganarasimha],
Thanks for replying, it's glad to hear such cases are already addressed.

I think that will be problematic if we enable_any_resource_request_only, now we calculate pending-resource-by-label only for ANY request. I think it's fine to me to leave behind the node-local/rack-local out of partition issue. MR job can still get start and running. ","16/May/15 01:28;Naganarasimha;Thanks for replying [~wangda]
bq. Thanks for replying, it's glad to hear such cases are already addressed.
:)

bq. I think that will be problematic if we enable_any_resource_request_only, now we calculate pending-resource-by-label only for ANY request. I think it's fine to me to leave behind the node-local/rack-local out of partition issue. MR job can still get start and running.
I am ok with not adding, but my intention was not to add  node-local/rack-local resource requests at all so that it there are no issues with the ones mentioned in YARN-2694. Also in small clusters(<50) all are in the same rack i.e. default rack and suppose i want to run the reducers in the nodes with good HW config (high  mem/fast cpu) then this kind of config might be helpful.
","22/May/15 15:15;Naganarasimha;Hi [~wangda],
    Any thoughts/updates on my previous comment ? if no more changes then can we get this in ?","22/May/15 17:13;leftnoteasy;It's the last call of comments, I plan to get this in today. :)","27/May/15 21:36;hudson;FAILURE: Integrated in Hadoop-trunk-Commit #7909 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7909/])
MAPREDUCE-6304. Specifying node labels when submitting MR jobs. (Naganarasimha G R via wangda) (wangda: rev 3164e7d83875aa6b7435d1dfe61ac280aa277f1c)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/rm/TestRMContainerAllocator.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
","27/May/15 21:59;leftnoteasy;Just committed to branch-2/trunk. Thanks [~Naganarasimha] and review from [~john.jian.fang] and [~yufeldman]. I cannot resolve it because JIRA system is so slow today.","28/May/15 05:19;Naganarasimha;Thanks for reviewing and commiting [~Wangda],[~john.jian.fang] & [~yufeldman].","28/May/15 10:43;hudson;FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #211 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/211/])
MAPREDUCE-6304. Specifying node labels when submitting MR jobs. (Naganarasimha G R via wangda) (wangda: rev 3164e7d83875aa6b7435d1dfe61ac280aa277f1c)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/rm/TestRMContainerAllocator.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
","28/May/15 11:59;hudson;SUCCESS: Integrated in Hadoop-Yarn-trunk #941 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/941/])
MAPREDUCE-6304. Specifying node labels when submitting MR jobs. (Naganarasimha G R via wangda) (wangda: rev 3164e7d83875aa6b7435d1dfe61ac280aa277f1c)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/rm/TestRMContainerAllocator.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java
","28/May/15 14:22;hudson;FAILURE: Integrated in Hadoop-Hdfs-trunk #2139 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2139/])
MAPREDUCE-6304. Specifying node labels when submitting MR jobs. (Naganarasimha G R via wangda) (wangda: rev 3164e7d83875aa6b7435d1dfe61ac280aa277f1c)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/rm/TestRMContainerAllocator.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java
","28/May/15 14:28;hudson;FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #199 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/199/])
MAPREDUCE-6304. Specifying node labels when submitting MR jobs. (Naganarasimha G R via wangda) (wangda: rev 3164e7d83875aa6b7435d1dfe61ac280aa277f1c)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/rm/TestRMContainerAllocator.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java
","28/May/15 15:29;hudson;SUCCESS: Integrated in Hadoop-Mapreduce-trunk-Java8 #209 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/209/])
MAPREDUCE-6304. Specifying node labels when submitting MR jobs. (Naganarasimha G R via wangda) (wangda: rev 3164e7d83875aa6b7435d1dfe61ac280aa277f1c)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/rm/TestRMContainerAllocator.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java
","28/May/15 15:54;hudson;SUCCESS: Integrated in Hadoop-Mapreduce-trunk #2157 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2157/])
MAPREDUCE-6304. Specifying node labels when submitting MR jobs. (Naganarasimha G R via wangda) (wangda: rev 3164e7d83875aa6b7435d1dfe61ac280aa277f1c)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/rm/TestRMContainerAllocator.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java
","14/Jul/15 00:20;leftnoteasy;[~Naganarasimha], I just found there's one findbugs warning, I missed that before committing, could you reopen this issue and add an addendum patch to fix that?

Thanks,","14/Jul/15 00:40;leftnoteasy;[~Naganarasimha], I just found the problem is already resolved by MAPREDUCE-6421, please ignore my previous comment.","14/Jul/15 01:41;Naganarasimha;Hi [~wangda]
Thanks for informing but while working on this jira had looked into it and [commented|https://issues.apache.org/jira/browse/MAPREDUCE-6304?focusedCommentId=14523205&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14523205] also, but missed to see the jira MAPREDUCE-6421 and could have been added to the exclude list ! Will keep this in mind for next time .","10/Feb/16 05:42;sunilg;cc/ [~Naganarasimha],  [~leftnoteasy],  [~junping_du] 

Currently in 2.6 branch,  this patch s not present. So for users who wants to specify label at app submission time,  this feature is not available .  I think we can backport this to 2.6 line. I had mentioned this point in release mail thread also. Thoughts?","16/Feb/16 21:07;leftnoteasy;[~sunilg],

I would suggest don't backport this issue, it's a new feature instead of criticial bug fix. And I think user who wants to try node label feature should move to more stable 2.7.x release","17/Feb/16 12:24;Naganarasimha;Hi [~sunilg] & [~wangda],
Though this patch can be applied with slight modification, its not a blocking bug/ feature so i am ok even if its not merged with 2.6.x as usually we try to back port only the critical/blocker issues.","17/Feb/16 16:55;sunilg;Thanks [~leftnoteasy] and [~Naganarasimha Garla] for the clarification. It makes sense to me to backport critical/blocker alone. But one more question, since we are not porting back features, do we have a document which specifies what all functionalities of NodeLabel is available in 2.6 line. I couldn't get much information from 2.6.4 docs.","19/Feb/16 03:23;Naganarasimha;Hi [~sunilg], 
YARN-2801 introduced the documentation for the nodelabels and it was committed to branch 2.7.2, so even if we back port we need to ensure only the applicable part of the document needs to be backported. but the question is anyone using nodelabels in 2.6.x release ? if so taking that effort is fine else may be not worth it. Thoughts ?","19/Feb/16 03:31;sunilg;Yes.  I also was having similar thoughts. If we know any user s using labels in 2.6 line,  we can document only that much. I have seen the doc in 307 line,  and it's pretty much fine. But I agree that effort of simple backport won't be easy as we do not have all set of functionality of labels in 2.6. 
I am fine with current way,  but was getting confused  sometimes as Eric used a deprecated way to test labels. Also I too tested the way I do in 2.7 or trunk and met with some pblm. So it may help users if they use labels in 2.6. If its not intended, then we may not  need it.  ","17/May/17 22:43;elgoiri;Backporting to 2.7.4","17/May/17 22:44;elgoiri;Backport to 2.7.4.","18/May/17 01:27;jhung;+1 (non-binding) for the 2.7.4 patch.","18/May/17 02:00;shv;+1 for backport. Thanks [~elgoiri] for taking on it. Will commit shortly.","18/May/17 07:31;shv;I just committed this to branch-2.7. Thanks [~elgoiri] for backport.","18/Aug/17 08:52;Feng Yuan;If there are some problem, if labelA map preempt reduce(LabelB).And maybe reduce container will assign to map.
Because if rm allocate redundance reduce container, map will get assigned:
{code}
 while(it.hasNext() && maps.size() > 0 && canAssignMaps()){
        Container allocated = it.next();
        Priority priority = allocated.getPriority();
        assert PRIORITY_MAP.equals(priority);
        TaskAttemptId tId = maps.keySet().iterator().next();
        ContainerRequest assigned = maps.remove(tId);
        containerAssigned(allocated, assigned);
        it.remove();
        JobCounterUpdateEvent jce =
          new JobCounterUpdateEvent(assigned.attemptID.getTaskId().getJobId());
        jce.addCounterUpdate(JobCounter.OTHER_LOCAL_MAPS, 1);
        eventHandler.handle(jce);
        if (LOG.isDebugEnabled()) {
          LOG.debug(""Assigned based on * match"");
        }
      }
    }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable to limit running map and reduce tasks when job is running,MAPREDUCE-7274,13298604,New Feature,Patch Available,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,smarthan,smarthan,15/Apr/20 15:50,11/Jun/20 09:07,12/Jan/21 09:52,,,,,,,,,,mr-am,mrv2,,,,,0,,,,,"MRv2 enabled users to control the number of map or reduce tasks running simultaneously by configuration  *_mapreduce.job.running.map.limit_* or _*mapreduce.job.running.reduce.limit*._ But users can only set limit number before submitting the job to rm. So, it's meaningful  to enable users to set the limit of running map or reduce tasks when job is running, which can help users to restrict  resource usage of job and give resources to high-priority job.",,smarthan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-5583,,,,,,,,,,,,,,,,,,,,,"11/Jun/20 03:49;smarthan;MAPREDUCE-7274.002.patch;https://issues.apache.org/jira/secure/attachment/13005441/MAPREDUCE-7274.002.patch","26/May/20 06:36;smarthan;MAPREDUCE-7274.patch;https://issues.apache.org/jira/secure/attachment/13004007/MAPREDUCE-7274.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2020-04-23 03:10:46.29,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 11 09:07:51 UTC 2020,,,,,,,"0|z0do74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"16/Apr/20 08:19;smarthan;upload patch for branch 3.2.0. 

It enable us to set limit for running tasks when job is running by mapred client command:
{code:bash}
mapred job -set-running-task-limit JOB_ID TASK_TYPE LIMIT

e.g.
   mapred job -set-running-task-limit job_1583809537551_21297 MAP 100
   mapred job -set-running-task-limit job_1583809537551_21297 REDUCE 100
{code}
it send a rpc resquest to AM, and AM would update the max running limit of the specified type task as specified limit count.

 
  ","16/Apr/20 15:11;smarthan;Hi [~jlowe]  [~acmurthy]  [~junping_du] Could you have time to review this patch? Thank you .","23/Apr/20 03:10;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m 10s{color} | {color:red} MAPREDUCE-7274 does not apply to branch-3.2.0. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | MAPREDUCE-7274 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/13000898/MAPREDUCE-7274-branch-3.2.0.patch |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7770/console |
| versions | git=2.17.1 |
| Powered by | Apache Yetus 0.12.0 https://yetus.apache.org |


This message was automatically generated.

","29/Apr/20 08:58;smarthan;Upload patch v2.

Add unit test, modify some improper naming and fix the patch does not apply to branch-3.2.0.","29/Apr/20 09:01;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m 13s{color} | {color:red} MAPREDUCE-7274 does not apply to branch-3.2.0. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | MAPREDUCE-7274 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/13001559/MAPREDUCE-7274-branch-3.2.0-v2.patch |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7774/console |
| versions | git=2.17.1 |
| Powered by | Apache Yetus 0.12.0 https://yetus.apache.org |


This message was automatically generated.

","29/Apr/20 11:19;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} docker {color} | {color:red}  8m 18s{color} | {color:red} Docker failed to build yetus/hadoop:e213563c3cb. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | MAPREDUCE-7274 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/13001579/MAPREDUCE-7274-branch-3.2.0-v2.patch |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7775/console |
| versions | git=2.17.1 |
| Powered by | Apache Yetus 0.12.0 https://yetus.apache.org |


This message was automatically generated.

","26/May/20 06:38;smarthan;delete unavailable patch for branch 3.2.0

upload patch for trunk.  [^MAPREDUCE-7274.patch]","26/May/20 10:27;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 41s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} dupname {color} | {color:green}  0m  0s{color} | {color:green} No case conflicting files found. {color} |
| {color:blue}0{color} | {color:blue} prototool {color} | {color:blue}  0m  0s{color} | {color:blue} prototool was not available. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m  8s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 19m 15s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 55s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 54s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 57s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 17m 52s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 11s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} spotbugs {color} | {color:blue}  0m 44s{color} | {color:blue} Used deprecated FindBugs config; considering switching to SpotBugs. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  4m 25s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 26s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  2m 29s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 48s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} cc {color} | {color:red}  1m 48s{color} | {color:red} hadoop-mapreduce-project_hadoop-mapreduce-client generated 34 new + 95 unchanged - 34 fixed = 129 total (was 129) {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red}  1m 48s{color} | {color:red} hadoop-mapreduce-project_hadoop-mapreduce-client generated 1 new + 336 unchanged - 0 fixed = 337 total (was 336) {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 49s{color} | {color:orange} hadoop-mapreduce-project/hadoop-mapreduce-client: The patch generated 37 new + 594 unchanged - 0 fixed = 631 total (was 594) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 23s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 13m 39s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 25s{color} | {color:red} hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-common generated 4 new + 96 unchanged - 4 fixed = 100 total (was 100) {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m  2s{color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  6m 46s{color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m  2s{color} | {color:green} hadoop-mapreduce-client-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m  1s{color} | {color:green} hadoop-mapreduce-client-app in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  4m 12s{color} | {color:green} hadoop-mapreduce-client-hs in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}127m 45s{color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 49s{color} | {color:red} The patch generated 6 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}226m 20s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app |
|  |  Inconsistent synchronization of org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.maxRunningMaps; locked 57% of time  Unsynchronized access at RMContainerAllocator.java:57% of time  Unsynchronized access at RMContainerAllocator.java:[line 964] |
|  |  Inconsistent synchronization of org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.maxRunningReduces; locked 57% of time  Unsynchronized access at RMContainerAllocator.java:57% of time  Unsynchronized access at RMContainerAllocator.java:[line 230] |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | ClientAPI=1.40 ServerAPI=1.40 base: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7786/artifact/out/Dockerfile |
| JIRA Issue | MAPREDUCE-7274 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/13004007/MAPREDUCE-7274.patch |
| Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient findbugs checkstyle cc prototool |
| uname | Linux 3c325c13f281 4.15.0-58-generic #64-Ubuntu SMP Tue Aug 6 11:12:41 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | personality/hadoop.sh |
| git revision | trunk / f43a152b972 |
| Default Java | Private Build-1.8.0_252-8u252-b09-1~18.04-b09 |
| cc | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7786/artifact/out/diff-compile-cc-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
| javac | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7786/artifact/out/diff-compile-javac-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7786/artifact/out/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
| javadoc | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7786/artifact/out/diff-javadoc-javadoc-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-common.txt |
| findbugs | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7786/artifact/out/new-findbugs-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.html |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7786/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7786/artifact/out/patch-asflicense-problems.txt |
| Max. process+thread count | 1525 (vs. ulimit of 5500) |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7786/console |
| versions | git=2.17.1 maven=3.6.0 findbugs=3.1.0-RC1 |
| Powered by | Apache Yetus 0.12.0 https://yetus.apache.org |


This message was automatically generated.

","11/Jun/20 06:33;smarthan;upload patch v002 to fix problems in building.[^MAPREDUCE-7274.002.patch]","11/Jun/20 09:07;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 49s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} dupname {color} | {color:green}  0m  1s{color} | {color:green} No case conflicting files found. {color} |
| {color:blue}0{color} | {color:blue} prototool {color} | {color:blue}  0m  0s{color} | {color:blue} prototool was not available. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  3m 20s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 27m  0s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 59s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 51s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 51s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 18m  7s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m  0s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} spotbugs {color} | {color:blue}  0m 43s{color} | {color:blue} Used deprecated FindBugs config; considering switching to SpotBugs. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  4m 50s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 28s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  2m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  2m  0s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} cc {color} | {color:red}  2m  0s{color} | {color:red} hadoop-mapreduce-project_hadoop-mapreduce-client generated 25 new + 104 unchanged - 25 fixed = 129 total (was 129) {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red}  2m  0s{color} | {color:red} hadoop-mapreduce-project_hadoop-mapreduce-client generated 1 new + 336 unchanged - 0 fixed = 337 total (was 336) {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 47s{color} | {color:orange} hadoop-mapreduce-project/hadoop-mapreduce-client: The patch generated 10 new + 595 unchanged - 0 fixed = 605 total (was 595) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 18s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  1s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 13m 51s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 42s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m  1s{color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  6m 55s{color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m  0s{color} | {color:green} hadoop-mapreduce-client-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 25s{color} | {color:green} hadoop-mapreduce-client-app in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  4m 12s{color} | {color:green} hadoop-mapreduce-client-hs in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}130m 38s{color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 49s{color} | {color:red} The patch generated 1 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}240m 42s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app |
|  |  Inconsistent synchronization of org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.maxRunningMaps; locked 57% of time  Unsynchronized access at RMContainerAllocator.java:57% of time  Unsynchronized access at RMContainerAllocator.java:[line 964] |
|  |  Inconsistent synchronization of org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.maxRunningReduces; locked 57% of time  Unsynchronized access at RMContainerAllocator.java:57% of time  Unsynchronized access at RMContainerAllocator.java:[line 230] |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | ClientAPI=1.40 ServerAPI=1.40 base: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7788/artifact/out/Dockerfile |
| JIRA Issue | MAPREDUCE-7274 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/13005441/MAPREDUCE-7274.002.patch |
| Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient findbugs checkstyle cc prototool |
| uname | Linux 35852ee81693 4.15.0-60-generic #67-Ubuntu SMP Thu Aug 22 16:55:30 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | personality/hadoop.sh |
| git revision | trunk / 93b121a9717 |
| Default Java | Private Build-1.8.0_252-8u252-b09-1~18.04-b09 |
| cc | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7788/artifact/out/diff-compile-cc-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
| javac | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7788/artifact/out/diff-compile-javac-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7788/artifact/out/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
| findbugs | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7788/artifact/out/new-findbugs-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.html |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7788/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7788/artifact/out/patch-asflicense-problems.txt |
| Max. process+thread count | 1596 (vs. ulimit of 5500) |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7788/console |
| versions | git=2.17.1 maven=3.6.0 findbugs=3.1.0-RC1 |
| Powered by | Apache Yetus 0.12.0 https://yetus.apache.org |


This message was automatically generated.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MapReduce input format/record readers to support S3 select queries,MAPREDUCE-7182,13213727,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,stevel@apache.org,stevel@apache.org,04/Feb/19 17:32,09/Apr/20 18:22,12/Jan/21 09:52,,3.3.0,,,,,,,,mrv2,,,,,,0,,,,,"HADOOP-15229 adds S3 select through the (new) async openFile API, but the classic RecordReader &c can't handle it because

# the files are shorter than they are in a getFileStatus, and the readers assume that an EOFException is an error in that situation
# everything assumes plain text is splitable
# if a file has a gz extension, the gunzip codec should be used. So breaks transcoded/uncompressed data

to handle s3 select data sources  we need to be able to address them, either through changes to the existing code (danger?) or some new readers",,sseth,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-15364,HADOOP-15229,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2019-02-04 17:32:49.0,,,,,,,"0|yi0n08:",9223372036854775807,,,,,,,,,,,,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make hadoop consider wildcard host as datalocal,MAPREDUCE-7120,13170364,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,bdscheller,bdscheller,05/Jul/18 21:01,09/Apr/20 18:22,12/Jan/21 09:52,,2.8.4,,,,,,,,client,,,,,,0,feature,option,performance,,"Make hadoop consider wildcard host as datalocal.

This allows hadoop to treat ""\*"" as data local. This allows remote filesystems to increase performance by skipping retrys for data locality by returning ""\*"" as the block host when the filesystem is asked for the block location.",,bdscheller,githubbot,junyue.ling,sunilg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-07-05 22:05:36.111,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 23 12:00:24 UTC 2018,,,,,,,"0|i3vlcf:",9223372036854775807,,,,,,,,,,,,,3.4.0,,,,,,,,"05/Jul/18 22:05;githubbot;GitHub user bschell opened a pull request:

    https://github.com/apache/hadoop/pull/399

    MAPREDUCE-7120. Make hadoop consider wildcard host as datalocal

    This allows hadoop to treat ""*"" as data local. This allows remote filesystems to increase performance by skipping retrys for data locality by returning ""*"" as the block host when the filesystem is asked for the block location.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/bschell/hadoop bschelle/stringbuildfix

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/hadoop/pull/399.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #399
    
----
commit 002719c586934c5e40a5201fb8ce57268d7dde99
Author: Scheller <bschelle@...>
Date:   2018-06-27T20:17:24Z

    Make hadoop consider wildcard host as datalocal
    
    This allows hadoop to treat ""*"" as data local. This allows remote filesystems to increase performance by skipping retrys for data locality by returning ""*"" as the block host when the filesystem is asked for the block location.

----
","23/Nov/18 12:00;sunilg;Bulk update: moved all 3.2.0 non-blocker issues, please move back if it is a blocker.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports config the shuffle's path cache related parameters,MAPREDUCE-7237,13253657,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,jiwq,jiwq,jiwq,29/Aug/19 09:24,03/Apr/20 02:00,12/Jan/21 09:52,16/Mar/20 02:29,,,,,,3.3.0,,,mrv2,,,,,,0,,,,,Nowadays the ShuffleHandler#Shuffle#pathCache's related parameters is hard coding. We should support config these.,,aajisaka,ayushtkn,hudson,jiwq,wanwgzhe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-7268,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-03-16 02:29:27.103,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Apr 02 13:17:42 UTC 2020,,,,,,,"0|z065d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"13/Jan/20 09:34;jiwq;Hi [~aajisaka], can u help to review?","16/Mar/20 02:29;aajisaka;Merged the PR into trunk. Thanks [~jiwq] for the contribution.","16/Mar/20 03:03;hudson;ABORTED: Integrated in Jenkins build Hadoop-trunk-Commit #18057 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/18057/])
MAPREDUCE-7237. Supports config the shuffle's path cache related (github: rev ea688631b02bee4d514b4baa4d754fac8c41ff3a)
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
","02/Apr/20 10:48;ayushtkn;Seems this breaks TestMapreduceConfigFields
https://builds.apache.org/job/hadoop-qbt-trunk-java8-linux-x86/1452/testReport/junit/org.apache.hadoop.mapreduce/TestMapreduceConfigFields/testCompareXmlAgainstConfigurationClass/

Please check once","02/Apr/20 13:17;jiwq;Thanks [~ayushtkn] for the reporting. I'll check it, sorry for the trouble. I created [MAPREDUCE-7268 | https://issues.apache.org/jira/browse/MAPREDUCE-7268] tracking it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Relaunching Failed Containers,MAPREDUCE-7180,13213104,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,belugabehr,belugabehr,31/Jan/19 15:08,26/Feb/20 05:29,12/Jan/21 09:52,,,,,,,,,,mrv1,mrv2,,,,,0,,,,,"In my experience, it is very common that a MR job completely fails because a single Mapper/Reducer container is using more memory than has been reserved in YARN.  The following message is logging the the MapReduce ApplicationMaster:

{code}
Container [pid=46028,containerID=container_e54_1435155934213_16721_01_003666] is running beyond physical memory limits. 
Current usage: 1.0 GB of 1 GB physical memory used; 2.7 GB of 2.1 GB virtual memory used. Killing container.
{code}

In this case, the container is re-launched on another node, and of course, it is killed again for the same reason.  This process happens three (maybe four?) times before the entire MapReduce job fails.  It's often said that the definition of insanity is doing the same thing over and over and expecting different results.

For all intents and purposes, the amount of resources requested by Mappers and Reducers is a fixed amount; based on the default configuration values.  Users can set the memory on a per-job basis, but it's a pain, not exact, and requires intimate knowledge of the MapReduce framework and its memory usage patterns.

I propose that if the MR ApplicationMaster detects that a container is killed because of this specific memory resource constraint, that it requests a larger container for the subsequent task attempt.

For example, increase the requested memory size by 50% each time the container fails and the task is retried.  This will prevent many Job failures and allow for additional memory tuning, per-Job, after the fact, to get better performance (v.s. fail/succeed).",,anooppavi,belugabehr,Jim_Brennan,pbacsko,templedf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,YARN-9347,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-02-01 15:29:03.096,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 05 14:53:41 UTC 2019,,,,,,,"0|yi0j5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"01/Feb/19 15:29;Jim_Brennan;[~belugabehr] While this sounds like a good idea, my concern is that this will allow undersized jobs to continue to silently work, using additional cluster resources every time they run.  In our experience, users often don't fix their jobs in the presence of failures that succeed on retries, so problems like this can go on for a long time.   I think if we do a feature like this, it should be an opt-in configuration option.","01/Feb/19 18:45;belugabehr;[~Jim_Brennan] Thank you for the feedback!

I could see a new configuration called growth-factor which specifies how large to grow the container each time it is re-tried. This would be a percentage, therefore, a growth-factor of 1.0f (100%) would preserve the current behavior.

I would just say, that, from my perspective, it is far more important that MapReduce jobs complete than fail. Please consider the current workflow:
 # User launches MapReduce job with 100 Mappers @ 1GB/container (for a total of 100GB)
 # MapReduce job fails
 # User alerts cluster administrator
 # Cluster administrator diagnosis the issue (not exactly trivial)
 # Administrator informs user that their job needs more memory
 # User re-runs MapReduce job with 100 Mapper @ 2GB/container (for a total of 200GB)
 # Job completes

This is a very tedious process and can add large lag times to resolution. Even with all this, the fix is almost always the same: ""throw more memory at the problem."" Also note, that if this failure was caused by a single Mapper that required the 2GB container, then a single retry with a failed container would require 102GB of memory on the cluster, whereas running the MapReduce job again with all of the containers set to a larger size consumes many more resources ((100GB initial) + (3 container retries x 1GB) + (1 Job retry x 200GB) = 303GB).

For additional context, I work often with Hive-on-MapReduce. Users using a SQL front-end do not want (and should not have to) know how to troubleshoot MapReduce for failures. As long as there is some sort of logging or counter to track the number of times a retry occurs due to OOM, configurations can be tuned after the fact. Even if containers are failing sporadically, it may be the case that users would rather fail a couple of containers (and retry with larger container sizes) than to launch every container with a larger size.","04/Feb/19 16:35;Jim_Brennan;[~belugabehr] I agree that there are use cases where this would be useful.  But there are also cases where the automatic growth would not be desirable.   For example:


 # On a large multi-tenant cluster that serves a large number of users
 # Hourly MapReduce job with 100 Mappers @ 1GB/container (for a total of 100GB)
 # All mappers fail first attempt
 # All 100 mappers are retried with 2 GB containers
 # Job completes successfully, still well within required time limits for the job.
 # Because the job did not fail and was not late, nobody complains.

In this scenario, we waste 100 GB of cluster capacity for some period of time every hour.  Possibly more, if the maps really only needed an additional 512MB, for example.    Ideally, the user responsible for this job would notice the large number of map failures, and follow-up, but this does not always happen in a timely fashion.  If the job fails the first time it goes over its memory limit, the problem will more likely be addressed sooner and avoid wasting cluster resources.

 
{quote}I could see a new configuration called growth-factor which specifies how large to grow the container each time it is re-tried. This would be a percentage, therefore, a growth-factor of 1.0f (100%) would preserve the current behavior.
{quote}
I think something like this would work.

 ","05/Feb/19 02:17;belugabehr;{quote}
Ideally, the user responsible for this job would notice the large number of map failures, and follow-up, but this does not always happen in a timely fashion.  If the job fails the first time it goes over its memory limit, the problem will more likely be addressed sooner and avoid wasting cluster resources.
{quote}

Yes.  Your point is well taken.  There should be a clear warning presented in a log (in a UI?) when this condition is happening to alleviate this exact scenario.

Note to self: I mentioned a growth-factor of 1.0f.  I can't think of a scenario where we would want to shrink the container size in response to a failure, so the configuration should be indexed on 0.  A growth factor of 0.0 would be disabling and a 0.1f would be mean increase size 10% each attempt.","05/Feb/19 02:46;wilfreds;I have some reservations also on just growing on a failure. Letting the application fail is the best way to get the job reviewed and configured correctly. For a properly configured job we should see the GC kick in way before we run over the size of the container. If your default settings do not take care of that you are not managing the cluster correctly.

In MAPREDUCE-5785 we introduced the automatic calculation of the heap size based on the container size and vice versa. If you use that control it should mean that you never get into this situation. What happens when the application relies on that calculation for the heap and or container size and still fails?
How are you going to handle that case if the container fails with the same message? Are you going to also change the ratio heap to container that is configured. That case could be caused by the mapper or reducer using more off heap memory (3rd party library). How is that going to work with this auto re-run?

Another point to consider is that I can always run over the container by setting an overly large heap. As an example: I know my job can run in a 1GB heap as I have tried it. I now set 10GB as the heap as a test. GCs will not kick in as the heap is not really full and will just keep growing way above the 1GB. If I would configure the job to run in a 2GB container then the overly large heap will cause it to fail. It might even fail when I make the container 4GB or 8GB. Just doubling and re-running is going to be problematic.

Using the available configuration and the smarts that is build in is a far better solution.","05/Feb/19 17:46;belugabehr;{quote}
Letting the application fail is the best way to get the job reviewed and configured correctly.
{quote}

Sure, perhaps, but no one wants to deal with a failure at 2AM on a production system.  That also assumes the end user (example: business analyst using SQL through Hive) understands how to configure YARN containers.  Part of the goal is to make this more simple for users: If the configurations aren't perfect, then resources/run-time may not be optimal, but it will work.

Because of [MAPREDUCE-5785] this issue need only worry about the container size for now.  Adjusting the container size larger, will mean that both JVM heap size and overhead size increase together (80/20).  If a user does something silly like set the Xmx manually to 10GB with a 1GB container size, well the job will still fail.  The job should still adhere to the configured number of retries and not just re-run until it succeeds (it obviously may never succeed in certain scenarios).

Thanks!","05/Feb/19 17:47;belugabehr;And yes, this is not intended to be a solution for every failure scenario, but it will decrease failure rates overall (re: third-party libraries, off-heap memory, etc.)","06/Feb/19 03:16;wilfreds;When you use MAPREDUCE-5785 you should not see the type of failures that you are trying to prevent. The heap and its overhead should always fit in the container unless you have some special off heap case. You should thus only expect to see them for the 3rd party library and or off heap issues. What you are trying to implement is really only relevant for the edge cases like the misconfiguration which you state are not really the goal as the job will still fail.

That is why I think adding all this to hide a misconfiguration is the wrong thing to do. ","06/Feb/19 03:26;belugabehr;Thanks for the clarification [~wilfreds] I see where you are coming from.

How confident are we that the 80/20 split will cover all cases? Are there any stats to back this up?  Really the biggest thing is when an application fails with OOM - Mapper or Reducer needs more heap, enlarging the container size will automatically increase the heap size at 80/20 split.  ","06/Feb/19 03:38;belugabehr;[~wilfreds] I wanted to further edit my previous comment, but I'll just write a new one.  Your last message was very clarifying.  Thanks.  I need more explanation on my side.


I have witnessed this particular errors many times with default settings (no Xmx defined manually):

bq. Container ... is running beyond physical memory limits

So it may be the case that an 80/20 split is too aggressive for general use.  However, how to determine exactly the right number to use here?  I don't know.  Can anyone know?  I do know however that if a few workloads require more headroom (say, 75-25) they shouldn't fail.  By increasing the container size, the headroom expands nominally as well and this case is covered.

*In addition* this change would cover cases where a Mapper or Reducer fail because of an OOM situation.  Simply increasing the size of the container increases the JVM heap size and provides tasks additional chances to succeed.","01/Mar/19 22:22;templedf;This sounds like a fun discussion.  Let me throw in my 2 cents!

The 80/20 should cover most cases, except for blatant misconfiguration.  That makes this feature very niche.  In the cases where there is a misconfiguration, we want the user to fix it, not limp along on the good will of a self-healing system.

The 80/20 probably isn't going to cover 100% of the cases, and there are definitely cases where the configuration is out of the user's hands, e.g. Hive.  In those cases, which can be enumerated by the cluster owner, it would make sense to let the admin turn on the ""idiot switch"" to hide failures because of memory overruns behind a smart retry policy.

On the flip side, if we see an MR container fail due to memory overruns, we could save some resources by not retrying the task.  If it fails once on a memory overrun, it will fail on all the retries as well.  Just kill the job and be done.  Since YARN can't know anything about MR, it would have to be the MR AM that notices and kills itself.

I like the idea of using the power of YARN for good (for once), to hide details and reduce end-user pain.  Yeah, we should be careful to make sure it's not going to unexpectedly hide problems in a bad way, but I think that should be pretty easy to do.","04/Mar/19 03:39;wilfreds;The 80/20 case as DanieI said will not work for all cases but it handles almost all use cases. The headroom ratio is configurable which means  that if you know you have a high overhead due to the type of code you run you can set it cluster wide I would be in favour of not wasting resources and fail the application when the JVM goes OOM for one or more tasks. The re-run with adjusted settings has more drawbacks than advantages I think.

The main reason I am not in favour of the auto retries is the hiding of possible issues and not providing a guarantee that it will work. There is a good chance that when one mapper or reducer fails due to memory issues that there are more mappers or reducers that will fail in the same way. Multiple tasks failing increases the overhead on the cluster like Jim mentioned in his example. With data growing or small code changes either in the app, MR framework or JVM over time you could be putting a lot of extra strain on a cluster. 

What if the application still failed due to task failures: how do we handle an application re-run? Won't that start from scratch again and thus waste more resources.  
","04/Mar/19 14:30;belugabehr;Hey [~wilfreds],

Thanks for the input.

I'm not exactly sure what the 'application re-run' is referring to.  My intention in this JIRA was to address only the Mappers/Reducers.   I have opened [YARN-9260] to discuss the application masters as a separate issue.  As I understand it tough, the AMs are retried one time if they fail, so if the retry is going to happen anyway, might as well throw some more memory at it.

I think this feature is helpful to have as yet another tool in the toolbox.  One thing I still don't think we're on the same page about is that these retries, these various scenarios you point out, all valid, and all currently exist in the YARN framework.  There is already a retry capability implemented that allows for retries at both the application and the worker level.  For me, this is what started this conversation.  I noticed that the retry logic was too naive and would re-launch failed containers that had no hope of succeeding  (because they would OOM every time, or be killed by NM for having too much overhead, regardless of where they ran).  This request is simply an extension of the retry capability to at least give the containers at least some opportunity at completing.  This ask is in line with standard retry and backoff strategies.","05/Mar/19 05:59;wilfreds;What I meant is that if an application fails we re-run the application. Any finished tasks are OK they are recovered, running tasks are killed and restarted. If they had failed once or more times for the first attempt and we relaunched them with larger heaps we start the process of increasing the containers again from scratch, wasting more resources.

I think what Daniel proposed is the simplest most elegant solution. If we have a task that fails due to exceeding the container we should fail the application and let the end user and or admin sort it out. Even for an Oozie workflow or in the Hive case running jobs through beeline you can set the size of the container etc via the command line.
I think finding the cause is not that difficult but as part of the change to fail the application we could make it really clear in the diagnostics of the application what failed and which action to take. The message for the container exceeding the settings has already been extended via YARN-7580 and should be clearer in 3.1 and later.","05/Mar/19 13:43;belugabehr;Thanks [~wilfreds].

I have opened [YARN-9347].  I think a V1 of this feature can build on [YARN-9347].  If the growth-factor is 0, the entire application exits immediately, if the growth factor is defined, the target container is retried with a larger container (N times).

I'm not sure how I can put further emphasis on this idea of _let the end user and or admin sort it out_.  That should be an absolute *last* resort.  Software should automate things and make our lives easier.  No end user or admin wants to be called into work in the middle of night to resolve an issue with a failed workflow.    This change would at least give a chance for users to sort it out when they get into the office the next day, on their schedule.  And, speaking from experience, the the first attempt at a corrective action a user/admin implements is to simply increase the memory, by some arbitrary amount, and retry the application again.  The software should just do this automatically.

Thanks.","05/Mar/19 14:53;belugabehr;Also, just to clarify, this is not intended to be applied in every failure scenario.  I could imagine a ""V2"" of this feature that would detect if an application fails due to OOM and re-launches the container with a larger size only in the scenario.  In that way, both scenarios would be covered: not enough headroom and not enough application memory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add MR Counters for bytes-read-by-network-distance FileSystem metrics,MAPREDUCE-6660,12953260,New Feature,Patch Available,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,mingma,mingma,mingma,24/Mar/16 15:57,25/Oct/19 20:25,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,This is the MR part of the change which is to consume bytes-read-by-network-distance metrics generated by https://issues.apache.org/jira/browse/HDFS-9579.,,brahmareddy,junping_du,liuml07,mingma,shahrs87,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HDFS-9579,HADOOP-13031,,,,,,,,,,,,,,,,,,,,"25/Mar/16 22:51;mingma;MAPREDUCE-6660.patch;https://issues.apache.org/jira/secure/attachment/12795485/MAPREDUCE-6660.patch","25/Mar/16 22:51;mingma;MAPREDUCE-6660.png;https://issues.apache.org/jira/secure/attachment/12795486/MAPREDUCE-6660.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2016-04-09 00:22:02.145,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Apr 16 03:34:14 UTC 2016,,,,,,,"0|i2v60v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"25/Mar/16 22:51;mingma;Here is the draft patch and the MR webUI. The webUI becomes somewhat busy given these new counters will be created for each FileSystem. We can consider skip the rows if the values are zero if there is no compatibility issue here.","09/Apr/16 00:22;junping_du;Thanks [~mingma] for delivering the patch! Quickly go through patch, and a few comments:
1. About naming of new FileSystemCounter:
{noformat}
+  BYTES_READ_LOCAL_HOST,
+  BYTES_READ_LOCAL_RACK,
+  BYTES_READ_FIRST_DEGREE_REMOTE_RACK,
+  BYTES_READ_SECOND_OR_MORE_DEGREE_REMOTE_RACK,
{noformat}
Shall we just simply name it as: BYTES_READ_LOCAL_HOST,  BYTES_READ_LOCAL_RACK,  BYTES_READ_LOCAL_DATACENTER, BYTES_READ_REMOTE_DATACENTER and put some comments on it? I think it sounds more understandable. 
BTW, the last comma is not necessary.

Also, shall we add some simple test to verify number of BYTES_READ = sum of 4 new read counters?


 ","16/Apr/16 02:36;liuml07;Thanks for working on this [~mingma]. Adding some tests will definitely make it better. Otherwise +1 (non-binding).","16/Apr/16 03:34;mingma;Thank you [~junping_du] and [~liuml07]! Sure let me update the unit test. Regarding the rename to BYTES_READ_LOCAL_DATACENTER, etc. it seems reasonable as it covers the common scenario, however it might not be general enough to cover all sort of topologies. For a large cluster with 4 tiers, /edge router/core switch/TOR/local machine, BYTES_READ_SECOND_OR_MORE_DEGREE_REMOTE_RACK might mean BYTES_READ_LOCAL_DATACENTER. What do you think?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Umbrella] Make MapReduce work with Timeline Service Nextgen (YARN-2928),MAPREDUCE-6331,12823105,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,sjlee0,vinodkv,vinodkv,22/Apr/15 17:08,25/Oct/19 20:24,12/Jan/21 09:52,11/Jul/16 03:46,,,,,,2.9.0,3.0.0-alpha1,,,,,,,,0,,,,,Tracking umbrella for all MR changes to make it work with Timeline Service Nextgen - YARN-2928.,,cdouglas,haibochen,Jobo,junping_du,Naganarasimha,rohithsharma,sjlee0,varun_saxena,vinodkv,xinxianyin,zjshen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-6732,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-04-22 17:54:37.447,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Jul 11 03:46:25 UTC 2016,,,,,,,"0|i2dm33:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"22/Apr/15 17:54;zjshen;Link to previous MR integration umbrella","11/Jul/16 03:46;sjlee0;It's been merged to trunk. Huge thanks to the contributors who worked on this feature ([~jrottinghuis], [~junping_du], [~gtCarrera9], [~Naganarasimha], [~varun_saxena], [~vinodkv], [~vrushalic], and [~zjshen]) and everyone who participated in reviews and feedback!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mapreduce tasks for YARN Timeline Service v.2: (post GA features),MAPREDUCE-6943,13096224,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,varun_saxena,varun_saxena,varun_saxena,19/Aug/17 16:25,05/Sep/19 06:17,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,,,prabhujoseph,varun_saxena,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,YARN-9802,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-08-19 16:25:59.0,,,,,,,"0|i3j20n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Abortable support to mapreduce tasks,MAPREDUCE-7121,13171264,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,bdscheller,bdscheller,10/Jul/18 21:08,03/Sep/19 04:09,12/Jan/21 09:52,,3.0.2,,,,,,,,client,,,,,,0,,,,,"This patch adds abortable to mapreduce outputs

In the case of using a remote filesystem with mapreduce, upon task attempt failure we ensure the file output stream gets aborted before being closed which prevents partial results from being uploaded.
",,bdscheller,githubbot,sunilg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-07-10 21:24:29.593,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 05 18:24:23 UTC 2018,,,,,,,"0|i3vqtb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"10/Jul/18 21:24;githubbot;GitHub user bschell opened a pull request:

    https://github.com/apache/hadoop/pull/402

    MAPREDUCE-7121. add Abortable so that we can abort output

    This patch adds abortable to mapreduce outputs
    
    In the case of using a remote filesystem with mapreduce, upon task attempt failure we ensure the file output stream gets aborted before being closed which prevents partial results from being uploaded.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/bschell/hadoop bschelle/abortable

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/hadoop/pull/402.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #402
    
----
commit 1d0c3932299f0bc9a8a92b41048626ab953df533
Author: Scheller <bschelle@...>
Date:   2018-07-04T00:13:32Z

    add Abortable so that we can abort output
    
    This adds abortable so we can abort output in the case of a task attempt that is successful even though it failed to generate output files

----
","05/Nov/18 18:24;sunilg;Removed Fixed version as Jira is still open.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a tool to combine aggregated logs into HAR files,MAPREDUCE-6415,12840372,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,rkanter,rkanter,rkanter,24/Jun/15 23:19,28/Aug/19 12:02,12/Jan/21 09:52,10/Sep/15 00:56,2.8.0,,,,,2.8.0,3.0.0-alpha1,,,,,,,,1,,,,,"While we wait for YARN-2942 to become viable, it would still be great to improve the aggregated logs problem.  We can write a tool that combines aggregated log files into a single HAR file per application, which should solve the too many files and too many blocks problems.  See the design document for details.

See YARN-2942 for more context.",,asuresh,aw,chackra,h_o,hudson,jlowe,junping_du,kasha,leftnoteasy,mdeguzis,Naganarasimha,rchiang,rkanter,rohithsharma,shihaoliang,sijing0410,tuyu,varun_saxena,zhz,zxu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-7236,MAPREDUCE-7235,YARN-4946,YARN-2942,YARN-4086,MAPREDUCE-6480,MAPREDUCE-6494,MAPREDUCE-6495,MAPREDUCE-6503,MAPREDUCE-6550,MAPREDUCE-6970,MAPREDUCE-7027,MAPREDUCE-7023,,,,,YARN-3950,,,,"24/Jun/15 23:21;rkanter;HAR-ableAggregatedLogs_v1.pdf;https://issues.apache.org/jira/secure/attachment/12741730/HAR-ableAggregatedLogs_v1.pdf","26/Aug/15 20:25;rkanter;MAPREDUCE-6415.001.patch;https://issues.apache.org/jira/secure/attachment/12752558/MAPREDUCE-6415.001.patch","02/Sep/15 16:49;rkanter;MAPREDUCE-6415.002.patch;https://issues.apache.org/jira/secure/attachment/12753801/MAPREDUCE-6415.002.patch","02/Sep/15 08:42;rkanter;MAPREDUCE-6415.002.patch;https://issues.apache.org/jira/secure/attachment/12753722/MAPREDUCE-6415.002.patch","08/Sep/15 23:37;rkanter;MAPREDUCE-6415.003.patch;https://issues.apache.org/jira/secure/attachment/12754769/MAPREDUCE-6415.003.patch","26/Aug/15 20:25;rkanter;MAPREDUCE-6415_branch-2.001.patch;https://issues.apache.org/jira/secure/attachment/12752559/MAPREDUCE-6415_branch-2.001.patch","02/Sep/15 08:42;rkanter;MAPREDUCE-6415_branch-2.002.patch;https://issues.apache.org/jira/secure/attachment/12753723/MAPREDUCE-6415_branch-2.002.patch","08/Sep/15 23:36;rkanter;MAPREDUCE-6415_branch-2.003.patch;https://issues.apache.org/jira/secure/attachment/12754768/MAPREDUCE-6415_branch-2.003.patch","21/Jul/15 21:49;rkanter;MAPREDUCE-6415_branch-2_prelim_001.patch;https://issues.apache.org/jira/secure/attachment/12746425/MAPREDUCE-6415_branch-2_prelim_001.patch","04/Aug/15 19:33;rkanter;MAPREDUCE-6415_branch-2_prelim_002.patch;https://issues.apache.org/jira/secure/attachment/12748722/MAPREDUCE-6415_branch-2_prelim_002.patch","21/Jul/15 21:49;rkanter;MAPREDUCE-6415_prelim_001.patch;https://issues.apache.org/jira/secure/attachment/12746424/MAPREDUCE-6415_prelim_001.patch","04/Aug/15 19:33;rkanter;MAPREDUCE-6415_prelim_002.patch;https://issues.apache.org/jira/secure/attachment/12748721/MAPREDUCE-6415_prelim_002.patch",,,,,,,,,,,,,,,,,,,,,,,12.0,,,,,,,,,,,,,,,,,,,,2015-06-25 02:45:54.719,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Sep 10 04:54:23 UTC 2015,,,,,,,"0|i2ggpj:",9223372036854775807,,,,,,,,,,,,,2.8.0,,,,,,,,"25/Jun/15 02:45;brahmareddy;[~rkanter] I gone through attached proposal,same we are going to implement as part of MAPREDUCE-6283.. Can you please have look at this jira and let me know if you have some suggestions..","25/Jun/15 03:58;varun_saxena;[~rkanter], thats correct. We actually have a private implementation which combines aggregated files into HAR files. It runs as a service in JHS and combines aggregated logs periodically. From [~vinodkv]'s comment it seemed that the community does not need it(because of YARN-2942) hence did not push it.","25/Jun/15 15:53;rkanter;Yahoo! also has their own private implementation.  So, it seems like there's a need for something like this, and it would be great if everyone could use and contribute to the same version of it.

YARN-2942 is being put on hold for the moment because of concerns about HDFS-3689.  We can still do it eventually.

As for MAPREDUCE-6283, I agree with Vinod's comment there that it seems to be a duplicate of YARN-2942 for the logs part, and a duplicate of the ATSv2 work for the jhist part.","21/Jul/15 21:49;rkanter;I've uploaded a preliminary patch.  It adds a command that will look for eligible apps to process, generate a script that will run the 'hadoop archive' command, and runs the script in the distributed shell.  It also modifies the 'yarn logs' command and JHS to be able to read the har files.  All as described in the design document.

I still have to write some unit tests and split up the patch into MAPREDUCE and YARN (and HADOOP?) JIRAs.

We can also discuss if we have the right criteria for eligibility.  I implemented the ones mentioned in the design document, but it shouldn't be too hard to change them.

Here's the CLI usage:
{noformat}
>> bin/mapred archive-logs -help
usage: yarn archive-logs
 -help                       Prints this message
 -maxEligibleApps <n>        The maximum number of eligible apps to
                             process (default: -1 (all))
 -maxTotalLogsSize <bytes>   The maximum total logs size required to be
                             eligible (default: 1GB)
 -memory <megabytes>         The amount of memory for each container
                             (default: 1024)
 -minNumberLogFiles <n>      The minimum number of log files required to
                             be eligible (default: 20)
{noformat}

I know it's a bit hard to tell from the Java code what the shell script looks like, so here's an example of one:
{code}
#!/bin/bash
set -e
set -x
CONTAINER_ID_NUM=`echo $CONTAINER_ID | cut -d ""_"" -f 5`
if [ ""$CONTAINER_ID_NUM"" == ""000002"" ]; then
        appId=""application_1437514991365_0004""
        user=""rkanter""
elif [ ""$CONTAINER_ID_NUM"" == ""000003"" ]; then
        appId=""application_1437514991365_0005""
        user=""rkanter""
elif [ ""$CONTAINER_ID_NUM"" == ""000004"" ]; then
        appId=""application_1437514991365_0003""
        user=""rkanter""
elif [ ""$CONTAINER_ID_NUM"" == ""000005"" ]; then
        appId=""application_1437514991365_0007""
        user=""rkanter""
elif [ ""$CONTAINER_ID_NUM"" == ""000006"" ]; then
        appId=""application_1437514991365_0006""
        user=""rkanter""
else
        echo ""Unknown Mapping!""
        exit -1
fi
export HADOOP_CLIENT_OPTS=""-Xmx1024m""
$HADOOP_HOME/bin/hadoop archive -Dmapreduce.framework.name=local -archiveName $appId.har -p /tmp/logs/$user/logs/$appId \* /tmp/logs/archive-logs-work
$HADOOP_HOME/bin/hadoop fs -mv /tmp/logs/archive-logs-work/$appId.har /tmp/logs/$user/logs/$appId/$appId.har
originalLogs=`$HADOOP_HOME/bin/hadoop fs -ls /tmp/logs/$user/logs/$appId | grep ""^-"" | awk '{print $8}'`
if [ ! -z ""$originalLogs"" ]; then
        $HADOOP_HOME/bin/hadoop fs -rm $originalLogs
fi
{code}","21/Jul/15 23:07;jlowe;Note that container IDs are not guaranteed to be consecutive nor are they guaranteed to start at 1 for the AM.  Due to how reservations are processed and other race conditions, a container ID may not actually correspond to a physically launched container.  For example, on our busy clusters it is not rare for the AM container to have an ID greater than 000001.  So the danger here is that if the RM ends up skipping one or more container IDs when handing out containers to the application then we will skip one or more applications to aggregate.  We'll get another crack at it on the next pass, but again on a busy cluster we could fairly consistently fail to hit a number of them and we could have indefinite postponement on the aggregation of some applications (especially the first few in the list).

A more robust approach would be to have the distributed shell explicitly set something in the container's environment that is a sequence number from the distributed shell's point of view.  In other words, regardless of what container ID is allocated, the distributed shell can set a monotonically increasing number in each new container's env that the script can leverage to do instance-specific behavior.  This is akin to the task ID in MapReduce which again is disconnected from YARN's container ID.
","21/Jul/15 23:49;rkanter;I didn't realize that that can happen.  In that case, having a monotonically increasing number in each container's env independent of the CONTAINER_ID sounds like a good solution.  Plus, I won't have to do any parsing to get the unique number.

I'll double check, but I think each shell has the same env (other than the CONTAINER_ID), and there's no way to set different ones per shell.  If that's the case, it should be fairly easy to add a ""SHELL_ID"" env var to the DistributedShell AM that behaves how we want, as a separate JIRA.  ","22/Jul/15 01:03;rkanter;I've created YARN-3950 to add the SHELL_ID and put up a patch there.","22/Jul/15 02:49;aw;Maybe I'm missing it, but why is this being written in bash instead of as an actual yarn application?  The JVM startup costs are going to be massive. Also, is there something that is guaranteeing that HADOOP_HOME is set?  ","22/Jul/15 02:51;aw;Here's what shellcheck had to say about the generated bash:

{code}
In /tmp/1 line 4:
CONTAINER_ID_NUM=`echo $CONTAINER_ID | cut -d ""_"" -f 5`
                 ^-- SC2006: Use $(..) instead of legacy `..`.
                       ^-- SC2086: Double quote to prevent globbing and word splitting.


In /tmp/1 line 25:
$HADOOP_HOME/bin/hadoop archive -Dmapreduce.framework.name=local -archiveName $appId.har -p /tmp/logs/$user/logs/$appId \* /tmp/logs/archive-logs-work
^-- SC2086: Double quote to prevent globbing and word splitting.


In /tmp/1 line 26:
$HADOOP_HOME/bin/hadoop fs -mv /tmp/logs/archive-logs-work/$appId.har /tmp/logs/$user/logs/$appId/$appId.har
^-- SC2086: Double quote to prevent globbing and word splitting.


In /tmp/1 line 27:
originalLogs=`$HADOOP_HOME/bin/hadoop fs -ls /tmp/logs/$user/logs/$appId | grep ""^-"" | awk '{print $8}'`
             ^-- SC2006: Use $(..) instead of legacy `..`.
              ^-- SC2086: Double quote to prevent globbing and word splitting.


In /tmp/1 line 29:
        $HADOOP_HOME/bin/hadoop fs -rm $originalLogs
        ^-- SC2086: Double quote to prevent globbing and word splitting.
                                       ^-- SC2086: Double quote to prevent globbing and word splitting.
{code}","22/Jul/15 22:24;rkanter;{quote}Maybe I'm missing it, but why is this being written in bash instead of as an actual yarn application? The JVM startup costs are going to be massive.{quote}
The 'hadoop archive' command starts up a JVM.  I don't see how we can get around that unless we call it programmatically from an existing JVM and also do it serially, which is going to take a lot longer overall.
I figured it would be simpler to use the DistributedShell because it already exists and does most of what we need, than to write a whole new AM that creates containers to run 'hadoop archive'.

{quote}Also, is there something that is guaranteeing that HADOOP_HOME is set?{quote}
The shell inherits the env of the NodeManager as a base.  HADOOP_HOME should be defined for the NM, so it ends up in env of the shell.

I wasn't aware of shellcheck before, but that looks like a really useful tool.  I'll fix those.","23/Jul/15 00:45;aw;bq.  The shell inherits the env of the NodeManager as a base. HADOOP_HOME should be defined for the NM, so it ends up in env of the shell.

a) This is only true for Windows.  Unix has been using HADOOP_PREFIX since 0.21.  If it's being defined, it's not by the bash code that starts the NM that ships with Apache Hadoop.

b) I'm unsure if LCE actually inherits all of the shell environment or only specific variables.

bq. The 'hadoop archive' command starts up a JVM. I don't see how we can get around that unless we call it programmatically from an existing JVM and also do it serially, which is going to take a lot longer overall.

There are several hadoop command in the generated shell code.  That's many many JVM startup costs.  Granted there has been a lot of work in trunk to minimize those costs (classpath dedupe, etc), but it's still very expensive.","23/Jul/15 21:14;rkanter;I that case, I suppose I could write a Java program that calls the 'hadoop archive' command programmatically, and then the equivalent 'hadoop fs' operations with the Java API.  This would only require the one JVM startup.","27/Jul/15 21:30;aw;I forgot that HADOOP_HOME got exported in trunk for all platforms as part of HADOOP-11464.","04/Aug/15 19:33;rkanter;The prelim_002 patch:
- Uses {{YARN_SHELL_ID}} from YARN-3950 instead of parsing {{CONTAINER_ID}}
- Runs 'hadoop archive' and the FileSystem commands from a Java program, so we can limit the JVM startup cost","21/Aug/15 06:11;asuresh;[~rkanter], The patch looks good to me. You might want to clean up the TODOs and add some javaDocs though.
+1 pending that.","21/Aug/15 16:36;rkanter;Thanks for the review [~asuresh].  This is just the preliminary patch.  I still have to write unit tests, javadocs, and split out the yarn changes into a YARN JIRA.  But it sounds like you're good with the approach.

[~aw], any other comments?
How about you [~jlowe]?","22/Aug/15 23:32;kasha;Skimmed through the patch. Looks generally good. Can do a more thorough review on the non-prelim patch(es). 

May be, we should avoid logging to System.out and System.err and use the LOG instead? It is possible users invoke this through other programs in a non-interactive mode.","24/Aug/15 18:32;rkanter;Ok, I'll change the logging, start adding unit tests, and clean up some things.","26/Aug/15 20:25;rkanter;MAPREDUCE-6415.001.patch and MAPREDUCE-6415_branch-2.001.patch contain the MapReduce changes, though most of it's actually under hadoop-tools.  This includes all of the code to find and process the aggregated log files into HAR files.  It's mostly the same as the prelim patch, with some minor changes and unit tests.  I've uploaded the YARN changes to YARN-4086.  The patches for this and YARN-4086 can be applied independently.","26/Aug/15 20:29;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  1s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12752559/MAPREDUCE-6415_branch-2.001.patch |
| Optional Tests | javadoc javac unit shellcheck findbugs checkstyle |
| git revision | trunk / a4d9acc |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5955/console |


This message was automatically generated.","27/Aug/15 19:47;rkanter;[~jlowe], can you take a look at this?","28/Aug/15 16:34;jlowe;Have been very busy, but I will try to get a look at this next week.","31/Aug/15 20:14;jlowe;mvn dependency:analyze says there's a number of things that should be cleaned up in the new pom:
{noformat}
[INFO] --- maven-dependency-plugin:2.2:analyze (default-cli) @ hadoop-archive-logs ---
[WARNING] Used undeclared dependencies found:
[WARNING]    org.apache.hadoop:hadoop-yarn-common:jar:2.8.0-SNAPSHOT:provided
[WARNING]    com.google.guava:guava:jar:11.0.2:provided
[WARNING]    commons-io:commons-io:jar:2.4:compile
[WARNING]    commons-logging:commons-logging:jar:1.1.3:provided
[WARNING]    org.apache.hadoop:hadoop-yarn-client:jar:2.8.0-SNAPSHOT:provided
[WARNING]    org.apache.hadoop:hadoop-yarn-server-resourcemanager:jar:2.8.0-SNAPSHOT:test
[WARNING]    org.apache.hadoop:hadoop-yarn-api:jar:2.8.0-SNAPSHOT:provided
[WARNING]    commons-cli:commons-cli:jar:1.2:provided
[WARNING] Unused declared dependencies found:
[WARNING]    org.apache.hadoop:hadoop-annotations:jar:2.8.0-SNAPSHOT:provided
[WARNING]    org.apache.hadoop:hadoop-mapreduce-client-hs:jar:2.8.0-SNAPSHOT:test
[WARNING]    org.apache.hadoop:hadoop-mapreduce-client-jobclient:jar:2.8.0-SNAPSHOT:provided
[WARNING]    org.apache.hadoop:hadoop-mapreduce-client-jobclient:test-jar:tests:2.8.0-SNAPSHOT:test
[WARNING]    org.apache.hadoop:hadoop-hdfs:jar:2.8.0-SNAPSHOT:provided
[WARNING]    org.apache.hadoop:hadoop-common:test-jar:tests:2.8.0-SNAPSHOT:test
{noformat}

It would be nice if the usage output used the actual values in the code rather than hardcoded strings.  For example, we now have to keep minNumLogFiles and the usage string manually in sync.  If the usage output leveraged the minNumLogFiles value directly then updating it would automatically correct the usage message.  On a related note the usage currently mentions values like ""1GB"", but I don't believe the code supports memory units.

Do we only want to consider aggregating logs that have totally succeeded?  What about the FAILED case or other terminal states?  Seems like any terminal state where we know there aren't going to be any more logs arriving should be eligible.

Nit: it's wasteful for checkFiles to continue iterating the files once it finds an excluding condition.  We can also eliminate the need to track file counts explicitly and simply check files.length directly before we even start looping.

Is there a reason to support maxEligible being zero?  Wondering if that should be equivalent to a negative value and just cover everything.

Should the working directory contain something unique like the application ID in it somewhere?  This has the benefit of making it easier to cleanup after a run and not worry about affecting other, possibly simultaneous runs.","02/Sep/15 08:42;rkanter;Thanks for the review [~jlowe]!

The 002 patch address most of the issues Jason brought up:
- fixes dependencies, though I had to keep some of the ones that maven didn't think it needed
- fixes usage output to use variables for the defaults.  I also changed the units for the max total logs size to megabytes instead of bytes to be easier to use.
- now SUCCEEDED and FAILED log aggregation statuses are considered.
- improves checkFiles to be more efficient
- if maxEligible is 0, it will now print out a message and exit right away.  I think having 0 be equivalent to all might be confusing?  I'm fine either way; let me know if you think it's better to treat it as equivalent to a negative value.

I don't think we should add a unique ID to the working directory.  The tool won't work correctly with simultaneous runs anyway because it doesn't acquire any sort of ""lock"" that would stop another instance from trying to process the same application's logs.  As it is now, by using a non-unique directory, anything left over will get cleaned up when you run the tool again (presumably, you're running it at some interval).

On that last point, it would be good if we could prevent two instances of the tool from running at the same time.  I think the best way to do (without using a lock) is for the tool to check for a RUNNING job named ""ArchiveLogs"" in the RM, though this won't protect against all situations and will have a false positive if the user has another job named ""ArchiveLogs"".","02/Sep/15 09:02;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12753723/MAPREDUCE-6415_branch-2.002.patch |
| Optional Tests | javadoc javac unit shellcheck findbugs checkstyle |
| git revision | trunk / 095ab9a |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5965/console |


This message was automatically generated.","02/Sep/15 16:49;rkanter;Reuploading trunk patch; Jenkins tried to ran the branch-2 patch.","02/Sep/15 17:35;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | pre-patch |  15m 49s | Findbugs (version 3.0.0) appears to be broken on trunk. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 2 new or modified test files. |
| {color:green}+1{color} | javac |   8m  2s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |  10m 23s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 22s | There were no new checkstyle issues. |
| {color:green}+1{color} | shellcheck |   0m  6s | There were no new shellcheck (v0.3.3) issues. |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 29s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:red}-1{color} | findbugs |   0m 13s | Post-patch findbugs hadoop-assemblies compilation is broken. |
| {color:red}-1{color} | findbugs |   0m 27s | Post-patch findbugs hadoop-tools/hadoop-tools-dist compilation is broken. |
| {color:green}+1{color} | findbugs |   0m 27s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | assemblies tests |   0m 11s | Tests passed in hadoop-assemblies. |
| {color:green}+1{color} | tools/hadoop tests |   0m 13s | Tests passed in hadoop-tools-dist. |
| | |  38m  0s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12753801/MAPREDUCE-6415.002.patch |
| Optional Tests | javadoc javac unit shellcheck findbugs checkstyle |
| git revision | trunk / 7d6687f |
| hadoop-assemblies test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5966/artifact/patchprocess/testrun_hadoop-assemblies.txt |
| hadoop-tools-dist test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5966/artifact/patchprocess/testrun_hadoop-tools-dist.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5966/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf909.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5966/console |


This message was automatically generated.","08/Sep/15 20:34;asuresh;The latest patch looks good, 
+1, as long as [~jlowe] / [~kasha] has no other issues..

Thanks [~rkanter]","08/Sep/15 22:26;kasha;The patch looks mostly good to me, but for the following nits:
# HadoopArchiveLogs constructor doesn't need type on HashSet in Java 7
# HadoopArchiveLogs#run returns -1. Could we return a positive value, say 1, instead?
# HadoopArchiveLogs#checkFiles has an unused variable

Once the nits are fixed, I think we should get this in. Let us work on avoiding concurrent runs and any other bugs we find in a follow-up JIRA? ","08/Sep/15 23:37;rkanter;The 003 patch addresses the issues Karthik pointed out.  I agree that we can follow up with those other things in new JIRAs.","09/Sep/15 00:16;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | pre-patch |  15m 34s | Findbugs (version 3.0.0) appears to be broken on trunk. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 2 new or modified test files. |
| {color:green}+1{color} | javac |   7m 44s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |  10m  6s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 38s | There were no new checkstyle issues. |
| {color:green}+1{color} | shellcheck |   0m  5s | There were no new shellcheck (v0.3.3) issues. |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 30s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 35s | The patch built with eclipse:eclipse. |
| {color:red}-1{color} | findbugs |   0m 12s | Post-patch findbugs hadoop-assemblies compilation is broken. |
| {color:red}-1{color} | findbugs |   0m 24s | Post-patch findbugs hadoop-tools/hadoop-tools-dist compilation is broken. |
| {color:green}+1{color} | findbugs |   0m 24s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | assemblies tests |   0m 10s | Tests passed in hadoop-assemblies. |
| {color:green}+1{color} | tools/hadoop tests |   0m 13s | Tests passed in hadoop-tools-dist. |
| | |  37m 25s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12754769/MAPREDUCE-6415.003.patch |
| Optional Tests | javadoc javac unit shellcheck findbugs checkstyle |
| git revision | trunk / d9c1fab |
| hadoop-assemblies test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5974/artifact/patchprocess/testrun_hadoop-assemblies.txt |
| hadoop-tools-dist test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5974/artifact/patchprocess/testrun_hadoop-tools-dist.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5974/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf905.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5974/console |


This message was automatically generated.","09/Sep/15 14:55;jlowe;+1 latest patch looks good to me.  Will commit this later today if there are no objections.","09/Sep/15 17:38;kasha;LGTM too. Checked with Robert on the findbugs, looks like it was broken before the patch as well. 

+1","10/Sep/15 00:38;kasha;Checking this in. (Jason, sorry for the jumping the gun here).","10/Sep/15 00:52;kasha;Committed to trunk and branch-2. Thanks [~rkanter] for this handy tool, and [~jlowe] for your reviews. ","10/Sep/15 00:53;hudson;FAILURE: Integrated in Hadoop-trunk-Commit #8424 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/8424/])
MAPREDUCE-6415. Create a tool to combine aggregated logs into HAR files. (Robert Kanter via kasha) (kasha: rev 119cc75e7ebd723790f6326498383304aba384a2)
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-tools/hadoop-archive-logs/src/main/java/org/apache/hadoop/tools/HadoopArchiveLogsRunner.java
* hadoop-tools/hadoop-archive-logs/src/test/java/org/apache/hadoop/tools/TestHadoopArchiveLogs.java
* hadoop-mapreduce-project/bin/mapred
* MAPREDUCE-6415.003.patch
* hadoop-tools/hadoop-archive-logs/src/main/java/org/apache/hadoop/tools/HadoopArchiveLogs.java
* hadoop-tools/pom.xml
* hadoop-tools/hadoop-archive-logs/src/test/java/org/apache/hadoop/tools/TestHadoopArchiveLogsRunner.java
* hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml
* hadoop-tools/hadoop-tools-dist/pom.xml
* hadoop-project/pom.xml
* hadoop-tools/hadoop-archive-logs/pom.xml
","10/Sep/15 00:54;rkanter;Thanks everyone!  I'm glad we finally have a workable solution to this issue in now","10/Sep/15 01:36;hudson;FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #364 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/364/])
MAPREDUCE-6415. Create a tool to combine aggregated logs into HAR files. (Robert Kanter via kasha) (kasha: rev 119cc75e7ebd723790f6326498383304aba384a2)
* hadoop-tools/hadoop-tools-dist/pom.xml
* hadoop-tools/hadoop-archive-logs/pom.xml
* hadoop-project/pom.xml
* hadoop-tools/hadoop-archive-logs/src/main/java/org/apache/hadoop/tools/HadoopArchiveLogs.java
* hadoop-mapreduce-project/CHANGES.txt
* MAPREDUCE-6415.003.patch
* hadoop-tools/hadoop-archive-logs/src/test/java/org/apache/hadoop/tools/TestHadoopArchiveLogs.java
* hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml
* hadoop-tools/pom.xml
* hadoop-tools/hadoop-archive-logs/src/main/java/org/apache/hadoop/tools/HadoopArchiveLogsRunner.java
* hadoop-mapreduce-project/bin/mapred
* hadoop-tools/hadoop-archive-logs/src/test/java/org/apache/hadoop/tools/TestHadoopArchiveLogsRunner.java
","10/Sep/15 01:41;hudson;FAILURE: Integrated in Hadoop-trunk-Commit #8426 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/8426/])
removing accidental file in MAPREDUCE-6415 (rkanter: rev f15371062f1bbcbb79bf44fd67ec647020d56c69)
* MAPREDUCE-6415.003.patch
","10/Sep/15 02:11;hudson;FAILURE: Integrated in Hadoop-Yarn-trunk #1102 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/1102/])
MAPREDUCE-6415. Create a tool to combine aggregated logs into HAR files. (Robert Kanter via kasha) (kasha: rev 119cc75e7ebd723790f6326498383304aba384a2)
* hadoop-tools/hadoop-archive-logs/src/test/java/org/apache/hadoop/tools/TestHadoopArchiveLogs.java
* MAPREDUCE-6415.003.patch
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml
* hadoop-mapreduce-project/bin/mapred
* hadoop-tools/pom.xml
* hadoop-tools/hadoop-archive-logs/src/main/java/org/apache/hadoop/tools/HadoopArchiveLogsRunner.java
* hadoop-tools/hadoop-archive-logs/src/test/java/org/apache/hadoop/tools/TestHadoopArchiveLogsRunner.java
* hadoop-tools/hadoop-archive-logs/pom.xml
* hadoop-project/pom.xml
* hadoop-tools/hadoop-archive-logs/src/main/java/org/apache/hadoop/tools/HadoopArchiveLogs.java
* hadoop-tools/hadoop-tools-dist/pom.xml
","10/Sep/15 03:29;hudson;FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #371 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/371/])
MAPREDUCE-6415. Create a tool to combine aggregated logs into HAR files. (Robert Kanter via kasha) (kasha: rev 119cc75e7ebd723790f6326498383304aba384a2)
* hadoop-tools/pom.xml
* hadoop-tools/hadoop-archive-logs/src/main/java/org/apache/hadoop/tools/HadoopArchiveLogs.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-project/pom.xml
* hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml
* MAPREDUCE-6415.003.patch
* hadoop-tools/hadoop-archive-logs/src/test/java/org/apache/hadoop/tools/TestHadoopArchiveLogs.java
* hadoop-tools/hadoop-tools-dist/pom.xml
* hadoop-tools/hadoop-archive-logs/src/test/java/org/apache/hadoop/tools/TestHadoopArchiveLogsRunner.java
* hadoop-mapreduce-project/bin/mapred
* hadoop-tools/hadoop-archive-logs/src/main/java/org/apache/hadoop/tools/HadoopArchiveLogsRunner.java
* hadoop-tools/hadoop-archive-logs/pom.xml
removing accidental file in MAPREDUCE-6415 (rkanter: rev f15371062f1bbcbb79bf44fd67ec647020d56c69)
* MAPREDUCE-6415.003.patch
","10/Sep/15 03:54;hudson;SUCCESS: Integrated in Hadoop-Yarn-trunk #1103 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/1103/])
removing accidental file in MAPREDUCE-6415 (rkanter: rev f15371062f1bbcbb79bf44fd67ec647020d56c69)
* MAPREDUCE-6415.003.patch
","10/Sep/15 04:25;hudson;SUCCESS: Integrated in Hadoop-Mapreduce-trunk #2313 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2313/])
MAPREDUCE-6415. Create a tool to combine aggregated logs into HAR files. (Robert Kanter via kasha) (kasha: rev 119cc75e7ebd723790f6326498383304aba384a2)
* hadoop-tools/hadoop-tools-dist/pom.xml
* hadoop-tools/hadoop-archive-logs/pom.xml
* hadoop-project/pom.xml
* MAPREDUCE-6415.003.patch
* hadoop-tools/pom.xml
* hadoop-tools/hadoop-archive-logs/src/main/java/org/apache/hadoop/tools/HadoopArchiveLogs.java
* hadoop-mapreduce-project/bin/mapred
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml
* hadoop-tools/hadoop-archive-logs/src/test/java/org/apache/hadoop/tools/TestHadoopArchiveLogsRunner.java
* hadoop-tools/hadoop-archive-logs/src/test/java/org/apache/hadoop/tools/TestHadoopArchiveLogs.java
* hadoop-tools/hadoop-archive-logs/src/main/java/org/apache/hadoop/tools/HadoopArchiveLogsRunner.java
removing accidental file in MAPREDUCE-6415 (rkanter: rev f15371062f1bbcbb79bf44fd67ec647020d56c69)
* MAPREDUCE-6415.003.patch
","10/Sep/15 04:38;hudson;SUCCESS: Integrated in Hadoop-Mapreduce-trunk-Java8 #365 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/365/])
removing accidental file in MAPREDUCE-6415 (rkanter: rev f15371062f1bbcbb79bf44fd67ec647020d56c69)
* MAPREDUCE-6415.003.patch
","10/Sep/15 04:45;hudson;FAILURE: Integrated in Hadoop-Hdfs-trunk #2290 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2290/])
MAPREDUCE-6415. Create a tool to combine aggregated logs into HAR files. (Robert Kanter via kasha) (kasha: rev 119cc75e7ebd723790f6326498383304aba384a2)
* hadoop-tools/hadoop-archive-logs/src/test/java/org/apache/hadoop/tools/TestHadoopArchiveLogsRunner.java
* hadoop-tools/hadoop-tools-dist/pom.xml
* hadoop-tools/hadoop-archive-logs/src/main/java/org/apache/hadoop/tools/HadoopArchiveLogsRunner.java
* hadoop-tools/hadoop-archive-logs/pom.xml
* hadoop-mapreduce-project/bin/mapred
* MAPREDUCE-6415.003.patch
* hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml
* hadoop-tools/hadoop-archive-logs/src/main/java/org/apache/hadoop/tools/HadoopArchiveLogs.java
* hadoop-tools/pom.xml
* hadoop-tools/hadoop-archive-logs/src/test/java/org/apache/hadoop/tools/TestHadoopArchiveLogs.java
* hadoop-project/pom.xml
* hadoop-mapreduce-project/CHANGES.txt
removing accidental file in MAPREDUCE-6415 (rkanter: rev f15371062f1bbcbb79bf44fd67ec647020d56c69)
* MAPREDUCE-6415.003.patch
","10/Sep/15 04:54;hudson;FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #351 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/351/])
MAPREDUCE-6415. Create a tool to combine aggregated logs into HAR files. (Robert Kanter via kasha) (kasha: rev 119cc75e7ebd723790f6326498383304aba384a2)
* hadoop-tools/pom.xml
* hadoop-tools/hadoop-archive-logs/src/test/java/org/apache/hadoop/tools/TestHadoopArchiveLogsRunner.java
* hadoop-tools/hadoop-archive-logs/src/main/java/org/apache/hadoop/tools/HadoopArchiveLogsRunner.java
* hadoop-mapreduce-project/bin/mapred
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-project/pom.xml
* hadoop-tools/hadoop-archive-logs/src/test/java/org/apache/hadoop/tools/TestHadoopArchiveLogs.java
* hadoop-tools/hadoop-archive-logs/src/main/java/org/apache/hadoop/tools/HadoopArchiveLogs.java
* hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml
* hadoop-tools/hadoop-tools-dist/pom.xml
* MAPREDUCE-6415.003.patch
* hadoop-tools/hadoop-archive-logs/pom.xml
removing accidental file in MAPREDUCE-6415 (rkanter: rev f15371062f1bbcbb79bf44fd67ec647020d56c69)
* MAPREDUCE-6415.003.patch
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add ability to shuffle intermediate map task output to a distributed filesystem,MAPREDUCE-7173,13205609,New Feature,Patch Available,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,mkonst,mkonst,mkonst,20/Dec/18 02:04,14/Aug/19 01:48,12/Jan/21 09:52,,2.9.2,,,,,,,,mrv2,,,,,,2,,,,,"If nodes are lost during the course of a mapreduce job, the map tasks that ran on those nodes need to be re-run. Writing intermediate map task output to a distributed file system eliminates this problem in environments in which nodes are frequently lost, for example, in clusters that make heavy use of Google's Preemptible VMs or AWS's Spot Instances.

*Example Usage:*

*Job-scoped properties:*

1. Don't re-run an already-finished map task when we realize the node it ran on is now unusable:

mapreduce.map.rerun-if-node-unusable=false (see MAPREDUCE-7168)

2. On the map side, use a new implementation of MapOutputFile that provides paths relative to the staging dir for the job (which is cleaned up when the job is done):

mapreduce.task.general.output.class=org.apache.hadoop.mapred.HCFSOutputFiles

3. On the reduce side, use a new implementation of ShuffleConsumerPlugin that fetches map task output directly from a distributed filesystem:

mapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.HCFSShuffle

4. (Optional) Edit the buffer size for the output stream used when writing map task output

mapreduce.map.shuffle.output.buffer.size=8192

*Cluster-scoped properties* (see YARN-9106):

1. When gracefully decommissioning a node, only wait for the containers on that node to finish, not the applications associated with those containers (we don't need to wait on the applications to finish since this node is not serving shuffle data)

yarn.resourcemanager.decommissioning-nodes-watcher.wait-for-applications=false

2. When gracefully decommissioning a node, do not wait for app masters running on the node to finish so that this node can be decommissioned as soon as possible (failover to an app master on another node that isn't being decommissioned is pretty quick)

yarn.resourcemanager.decommissioning-nodes-watcher.wait-for-app-masters=false",,jira.shegalov,joshcurtis,medb,mkonst,pbacsko,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/18 02:05;mkonst;MAPREDUCE-7173.patch;https://issues.apache.org/jira/secure/attachment/12952439/MAPREDUCE-7173.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2018-12-20 03:04:40.755,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 14 01:48:31 UTC 2019,,,,,,,"0|u0066o:",9223372036854775807,,,,,,,,,,,,,2.9.2,,,,,,,,"20/Dec/18 03:04;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 15s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 19m 25s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 41s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 32s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 55s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 14s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 33s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 26s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red}  0m 26s{color} | {color:red} hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core generated 1 new + 153 unchanged - 0 fixed = 154 total (was 153) {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 30s{color} | {color:orange} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core: The patch generated 42 new + 1044 unchanged - 4 fixed = 1086 total (was 1048) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 31s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 56s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 13s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  4m  9s{color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 24s{color} | {color:red} The patch generated 2 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 53m 10s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8f97d6f |
| JIRA Issue | MAPREDUCE-7173 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12952439/MAPREDUCE-7173.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  xml  |
| uname | Linux a815afd4f152 4.4.0-138-generic #164-Ubuntu SMP Tue Oct 2 17:16:02 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 499c70e |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_181 |
| findbugs | v3.1.0-RC1 |
| javac | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7566/artifact/out/diff-compile-javac-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7566/artifact/out/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7566/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7566/artifact/out/patch-asflicense-problems.txt |
| Max. process+thread count | 1232 (vs. ulimit of 10000) |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7566/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.

","14/Aug/19 01:48;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 44s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 30m 14s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 44s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 51s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 43s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 15m 10s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 12s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 25s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 38s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 32s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red}  0m 32s{color} | {color:red} hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core generated 1 new + 146 unchanged - 0 fixed = 147 total (was 146) {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 41s{color} | {color:orange} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core: The patch generated 42 new + 1042 unchanged - 4 fixed = 1084 total (was 1046) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 41s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  2s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 15m 19s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 21s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  5m 44s{color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 37s{color} | {color:red} The patch generated 2 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 75m 48s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=19.03.1 Server=19.03.1 Image:yetus/hadoop:bdbca0e53b4 |
| JIRA Issue | MAPREDUCE-7173 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12952439/MAPREDUCE-7173.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  xml  |
| uname | Linux 9e0e5f0a94a4 4.15.0-48-generic #51-Ubuntu SMP Wed Apr 3 08:28:49 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / e6d240d |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_212 |
| findbugs | v3.1.0-RC1 |
| javac | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7654/artifact/out/diff-compile-javac-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7654/artifact/out/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7654/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7654/artifact/out/patch-asflicense-problems.txt |
| Max. process+thread count | 1067 (vs. ulimit of 5500) |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7654/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Derive heap size or mapreduce.*.memory.mb automatically,MAPREDUCE-5785,12699603,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,jira.shegalov,jira.shegalov,jira.shegalov,08/Mar/14 12:44,27/Jun/19 08:54,12/Jan/21 09:52,22/Jan/15 02:57,,,,,,3.0.0-alpha1,,,mr-am,task,,,,,0,,,,,"Currently users have to set 2 memory-related configs per Job / per task type.  One first chooses some container size map reduce.\*.memory.mb and then a corresponding maximum Java heap size Xmx < map reduce.\*.memory.mb. This makes sure that the JVM's C-heap (native memory + Java heap) does not exceed this mapreduce.*.memory.mb. If one forgets to tune Xmx, MR-AM might be 
- allocating big containers whereas the JVM will only use the default -Xmx200m.
- allocating small containers that will OOM because Xmx is too high.

With this JIRA, we propose to set Xmx automatically based on an empirical ratio that can be adjusted. Xmx is not changed automatically if provided by the user.
",,anthonyr,aw,brahmareddy,cdouglas,hitesh,hudson,jianhe,jira.shegalov,jlowe,Jobo,kasha,kkambatl,krisden,neelesh77,nroberts,philip,qwertymaniac,rdsr,rkanter,rohini,tomwhite,vrushalic,zhz,zjshen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-21929,MAPREDUCE-6343,TEZ-699,,,,,,,,,,,,MAPREDUCE-6223,MAPREDUCE-6234,,,,,,,"08/Mar/14 12:52;jira.shegalov;MAPREDUCE-5785.v01.patch;https://issues.apache.org/jira/secure/attachment/12633538/MAPREDUCE-5785.v01.patch","22/Mar/14 20:22;jira.shegalov;MAPREDUCE-5785.v02.patch;https://issues.apache.org/jira/secure/attachment/12636218/MAPREDUCE-5785.v02.patch","11/May/14 05:44;jira.shegalov;MAPREDUCE-5785.v03.patch;https://issues.apache.org/jira/secure/attachment/12644318/MAPREDUCE-5785.v03.patch","20/Nov/14 23:00;kasha;mr-5785-4.patch;https://issues.apache.org/jira/secure/attachment/12682773/mr-5785-4.patch","21/Nov/14 17:14;kasha;mr-5785-5.patch;https://issues.apache.org/jira/secure/attachment/12682912/mr-5785-5.patch","21/Nov/14 23:04;kasha;mr-5785-6.patch;https://issues.apache.org/jira/secure/attachment/12682982/mr-5785-6.patch","08/Jan/15 18:27;kasha;mr-5785-7.patch;https://issues.apache.org/jira/secure/attachment/12690903/mr-5785-7.patch","19/Jan/15 15:42;kasha;mr-5785-8.patch;https://issues.apache.org/jira/secure/attachment/12693106/mr-5785-8.patch","21/Jan/15 02:05;kasha;mr-5785-9.patch;https://issues.apache.org/jira/secure/attachment/12693467/mr-5785-9.patch",,,,,,,,,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,2014-03-08 16:59:11.347,,,false,,,,,,,,,,,,,,,,,,377950,Incompatible change,,,,Sat Mar 07 19:44:54 UTC 2015,,,,,,,"0|i1t4iv:",378242,"The memory values for mapreduce.map/reduce.memory.mb keys, if left to their default values of -1, will now be automatically inferred from the heap size value system property (-Xmx) specified for mapreduce.map/reduce.java.opts keys.

The converse is also done, i.e. if mapreduce.map/reduce.memory.mb values are specified, but no -Xmx is supplied for mapreduce.map/reduce.java.opts keys, then the -Xmx value will be derived from the former's value.

If neither is specified, then a default value of 1024 MB gets used.

For both these conversions, a scaling factor specified by property mapreduce.job.heap.memory-mb.ratio is used, to account for overheads between heap usage vs. actual physical memory usage.

Existing configs or job code that already specify both the set of properties explicitly would not be affected by this inferring change.",,,,,,,,,,,,,,,,,,,,"08/Mar/14 12:52;jira.shegalov;v01 for review","08/Mar/14 16:59;kkambatl;Good idea. Thanks for filing and working on this, [~jira.shegalov]. What are your thoughts on having a constant headroom (say, 200 MB) instead of a ratio? Also, it would be nice to pick mapreduce.task.io.sort.mb automatically using a ratio, by default. Would be nice to do that too.","09/Mar/14 09:32;sandyr;Something like this has been long-needed.

Though I'm worried that it's not backwards-compatible - users would see their JVM max heaps change in certain situations.  In situations where they didn't set the max heap, were cutting it close, but were still OK, they could see OutOfMemoryErrors after the change.

Another thing is that, as a user, I care more about my max heap size than how much I request from YARN.  The latter is usually a consequence of the former.

One possible way around both of these would be to add a new parameter that controls max heap size and sets mapreduce.*.memory.mb accordingly.","10/Mar/14 13:12;jira.shegalov;[~kkambatl], the constant headroom has been discussed. My thinking in favor of percentage-kind of overhead is that
- it's easy to reason about direct proportional overhead
- reasons a larger container size is specified is to process more data. Side effects of it is that some code is executed more frequently and more byte code is compiled into native.  The more native memory is used through the NIO stack, and native compressor libraries. The more tracking structures a GC might have.

I like your idea to tune io.sort.mb accordingly. I'd pick the default 50% of Xmx to match the current defaults: io.sort.mb=100 and -Xmx200m. I'll add this to the patch.






","10/Mar/14 13:59;jira.shegalov;[~sandyr], please elaborate regarding increased chances of OOM. Currently, if the users have not tuned Xmx, they'll get 200m.  With the patch, the'll get 770m. If the user specified map reduce.\*.memory.mb smaller than the default 1024 (also minimum-allocation-mb), I don't allow the Xmx be lower than the previous default Xmx200m in the patch.

Regarding the reversal of which parameter controls the other, I can see it either way. Your point works for me. But it is also convenient to explicitly state the cap for the container. The latter seems to be more backwards-compatible.","10/Mar/14 14:44;jlowe;Haven't had a chance to look into the patch into great detail, but here are some initial comments:

- should 'memory.mb.xmx.ratio' be 'memory.mb.heap.ratio'?  Even the code names it that internally. ;-)
- rather than commenting out the mapred-default property it should leave it in without a value set.  See the mapred.child.env entry as an example.
- should be easy to add a unit test that verifies the ratio is working as intended, e.g.: changing it sees a corresponding jvm argument change out of MapReduceChildJVM.getVMCommand and setting an explicit heap setting in the config prevents the ratio from taking effect.","22/Mar/14 20:22;jira.shegalov;[~lohit] suggested offline to consider modifying the conf on the client-side during job submission. It definitely works well for Xmx. io.sort.mb is safer to set  on in the MapTask JVM due to [Xmx-Runtime.maxMemory discrepancy|http://bugs.java.com/bugdatabase/view_bug.do?bug_id=4391499]. Server-side patch does not need to worry about potential multiple client implementations. Thoughts?

For now, uploading v02 based on the feedback by [~jlowe] and [~kkambatl]. 

","22/Mar/14 21:06;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12636218/MAPREDUCE-5785.v02.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core:

                  org.apache.hadoop.mapreduce.v2.app.job.impl.TestMapReduceChildJVM

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4457//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4457//console

This message is automatically generated.","25/Mar/14 01:09;jira.shegalov;The test passes on my notebook. The failing part of the test is Xmx3000m with io.sort.mb=2000, i.e, where actually that much memory is requested. The build machine was not able to handle it. Independently of this, I would like to hear comments on the approach.","03/Apr/14 23:36;jira.shegalov;[~kasha], can you take a look regarding your suggestion of including sort buffer size?","04/Apr/14 16:59;jlowe;Briefly looked at the new patch, a few comments:

* I thought MAPREDUCE-5028 solved the integer sign overflow issues, so do we still need to limit it to 1024 instead of 2047?
* Nit: IO_SORT_MB_PERCENTAGE is actually treated as a ratio rather than a percentage, otherwise users may try to set it to something like 50 rather than 0.5 based on the description.  It also doesn't say what it's relative to, so maybe IO_SORT_HEAP_RATIO/mapreduce.task.io.sort.heap.ratio or IO_SORT_MB_HEAP_RATIO/mapreduce.task.io.sort.mb.heap.ratio would be more clear and consistent with the other property?
* mapred-default description of mapreduce.task.io.sort.mb has ""perecentage""
* Technically this may need to be marked as an incompatible change, as jobs that were setting an explicit large heap due to their particular code needs but not setting any value for io.sort.mb may now fail with OOM errors since they will implicitly get a smaller heap due to an automatically enlarged io.sort.mb.","11/May/14 05:44;jira.shegalov;Thanks for review, Jason! Here is v03 patch.","15/May/14 22:34;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12644318/MAPREDUCE-5785.v03.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4604//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4604//console

This message is automatically generated.","14/Jul/14 17:43;rohini;I was taking a look at https://issues.apache.org/jira/browse/MAPREDUCE-5785 and https://issues.apache.org/jira/browse/TEZ-699 . 
 
MR:
{code}
public static final float DEFAULT_MEMORY_MB_HEAP_RATIO = 1.33f;
float heapRatio = conf.getFloat(MRJobConfig.MEMORY_MB_HEAP_RATIO,
          MRJobConfig.DEFAULT_MEMORY_MB_HEAP_RATIO);
int taskHeapSize =(int)Math.ceil(taskContainerMb / heapRatio);
public static final float DEFAULT_IO_SORT_MB_HEAP_RATIO = 0.5f;
ioSortMbPer = JobContext.DEFAULT_IO_SORT_MB_HEAP_RATIO;
+        }
+        sortmb = (int)(maxHeapMb * ioSortMbPer);
{code}
Tez:
{code}
public static final String TEZ_CONTAINER_MAX_JAVA_HEAP_FRACTION =
+      TEZ_PREFIX + ""container.max.java.heap.fraction"";
+  public static final double TEZ_CONTAINER_MAX_JAVA_HEAP_FRACTION_DEFAULT = 0.8;
int maxMemory = (int)(resource.getMemory() * maxHeapFactor);
 {code}

Few issues, inconsistencies that I see:
  - The MR one is really confusing. For heap it is divide by and io.sort.mb it is multiplication. I think it would be easier to keep both same to avoid confusion.  I had to apply more of my grey cells to do the division. Would prefer multiplication to determine the percentage as it is more easy to compute in mind than division.
   -  io.sort.mb as 50% of heap seems too high for default value. Most of the pig jobs which have huge bags would start failing.
- Another issue is taking the defaults now, for a 
4G container – Tez Xmx = 3.2G, MR Xmx=3.0G
8G container – Tez Xmx = 6.2G, MR xmx = 6G. 
Though the defaults work well for 1 or 2G of memory, for higher memories they seem to be actually wasting a lot of memory considering not more than 500M is usually required for native memory even if the Xmx keeps increasing. We should account that factor into calculation instead of doing Xmx as just a direct percentage of resource.mb.

Tez settings are usually equivalent of MR settings with an internal mapping to the MR setting taking them if they are specified, so that it is easier for users to switch between frameworks. This is one thing I am seeing inconsistent in terms of how the value is specified and would be good to reconcile both to have same behavior.","17/Nov/14 22:03;kasha;Sorry for the dropping the ball on this. Few comments:
# For those moving from MR1 to MR2, specifying Xmx might be easier. In those cases, it would be nice to estimate the memory.mb values. I am open to doing this in another JIRA, and would be happy to work on it and post a patch.
# Multiplying container-size by 0.8 to arrive at the heapsize does read better. We will have to divide by 0.8 to do it in the other direction. 
# In the patch itself, deprecating mapred.child.java.opts is fine but I would refer to .map.java.opts and .reduce.java.opts instead of the container size values. Those properties instead could point to the memory.mb properties. 

I understand it has been a while since we last looked at this. [~jira.shegalov] - will you be able to address these comments? I can help with addressing the comments if you are otherwise caught up. ","17/Nov/14 22:10;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12644318/MAPREDUCE-5785.v03.patch
  against trunk revision 48d62fa.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5025//console

This message is automatically generated.","17/Nov/14 22:35;aw;bq. Technically this may need to be marked as an incompatible change

I agree: this will likely break all kinds of user jobs.  I don't even want to consider what happens in the streaming and pipes use cases.... ","18/Nov/14 05:05;jira.shegalov;[~kasha], I agree that for MRv1->MRv2 transition deriving the container size from Xmx is a better fit. Feel free to take over this JIRA.

A challenge here might be is that there is a (default) value for *.memory.mb. How do you know it's ok to modify it? Are you going to check {{conf.getPropertySources}} for whether it was modifed. Or do you want to introduce another boolean switch that disables overrides?","19/Nov/14 01:13;kasha;bq. Are you going to check conf.getPropertySources for whether it was modifed. 
I was considering just removing the default value from mapred-default, and use the one from code. 

If one of heap-size or memory.mb is specified, I believe we should automatically pick the other one. If neither are specified, we should go off memory-mb default. ","20/Nov/14 23:00;kasha;Uploading a patch that does the following:
# If -Xmx is not set in the conf, it calculates this from the memory.mb value.
# If memory.mb is not set in the conf, it calculates this from the -Xmx value.
# If neither are set, it uses the default memory.mb value and calculates -Xmx from this.
# Updates a bunch of descriptions to mapred-default.xml to capture this clearly. Let me know if they are not clear enough.
# A little bit of test cleanup. 

The patch reverts changes to io.sort.mb from earlier versions to be addressed in a follow-up JIRA.

To parse the -Xmx value from java-opts, the patch ""borrows"" [~jeagles] patch from TEZ-1508. [~jeagles] - hope you are fine with that. 
 ","20/Nov/14 23:02;kasha;[~jira.shegalov], [~jlowe] - can you take a look at the patch when you get a chance? ","20/Nov/14 23:53;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12682773/mr-5785-4.patch
  against trunk revision eb4045e.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5040//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5040//console

This message is automatically generated.","21/Nov/14 08:02;sandyr;Hi Karthik.  Took a look at the patch.  Had some comments - mostly stylistic.

{code}
-    return adminClasspath + "" "" + userClasspath;
+      return jobConf.getTaskJavaOpts(isMapTask ? TaskType.MAP : TaskType.REDUCE);
{code}
Wrong indentation?

{code}
+  public String getTaskJavaOpts(TaskType taskType) {
+
+    String javaOpts = getConfiguredTaskJavaOpts(taskType);
{code}
Unnecessary blank line.

{code}
+      LOG.info(""Task java.opts does not specify max heap size, setting using""
+          + "" mapreduce.*.memory.mb * ""
+          + MRJobConfig.HEAP_MEMORY_MB_RATIO);
{code}
Can we condense this and the log further down into a single message?

{code}
+        if (LOG.isWarnEnabled()) {
{code}
Why use isWarnEnabled when we don't use isInfoEnabled?

{code}
+      final int taskContainerMb = getMemoryRequired(taskType);
{code}
Any reason this should be final? Convention is usually not to declare local variables final unless they need to be (like referenced by an anonymous class).

{code}
+      int taskHeapSize =(int)Math.ceil(taskContainerMb * heapRatio);
{code}
Should have a space after the ""="".

{code}
+  public static int parseMaximumHeapSizeMB(String javaOpts) {
{code}
Can this (and others) be marked Private?

{code}
+    int memory = 1024;
{code}
It looks like this value will be overwritten in all cases.

{code}
+          (heapSize = parseMaximumHeapSizeMB(
+              getConfiguredTaskJavaOpts(taskType))) > 0) {
{code}
This is a little weird.  Can we assign heapSize outside of the condition?

{code}
+        memory =
+            getInt(MRJobConfig.REDUCE_MEMORY_MB,
+                MRJobConfig.DEFAULT_REDUCE_MEMORY_MB);
{code}
This can be on 2 lines.

{code}
+  If -Xmx is not set, it is inferred from mapreduc.{map|reduce}.memory.mb and
{code}
Missing an ""e"" at the end of mapreduc.

{code}
+  <description>The ratio container between heap-size and container-size
+    If no -Xmx specified, it's calculated from the container memory
+    requirement: mapreduce.*.memory.mb * mapreduce.heap.memory-mb.ratio.
+    If -Xmx is specified but not mapreduce.*.memory.mb, it's calculated as
+    heapSize / mapreduce.heap.memory-mb.ratio.
{code}
Need a period after container size.  ""*"" meaning both multiplication and either map or reduce is a little confusing here. It might be better to spell out {map|reduce} inside the config properties, which would also be consistent with how they're references above.  Also, other descriptions tend to use ""it is"" instead of ""it's"".

{code}
   <description>The amount of memory to request from the scheduler for each
-  reduce task.
+    reduce task. If this is not specified, it is inferred from
{code}
Indentation here is inconsistent with other places.

Any reason to have getTaskJavaOpts in JobConf instead of MapReduceChildJVM?
","21/Nov/14 17:20;kasha;Thanks for the review, Sandy. Addressed most of your comments. 

bq. It looks like this value will be overwritten in all cases.
Actually no. We use an if - else if without an else. 

bq. This is a little weird. Can we assign heapSize outside of the condition?
I changed this in the updated patch, but this was to avoid parsing the heapSize unnecessarily or complicating the code with more if-else's. 

bq. Indentation here is inconsistent with other places.
The file has different kinds of indentation. I would like to leave this as is, if you are not particular about it. 

bq. Any reason to have getTaskJavaOpts in JobConf instead of MapReduceChildJVM?
MapReduceChildJVM needs the javaopts and TaskAttemptImpl needs the memory. We could keep those two methods closer to the access points, but I felt the two methods should be closer together than to the access points. ","21/Nov/14 18:02;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12682912/mr-5785-5.patch
  against trunk revision c298a9a.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5041//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5041//console

This message is automatically generated.","21/Nov/14 22:39;sandyr;This looks almost there.  Leaving the indentation seems fine to me.

JobConf is already kind of a god class.  I think that the more we can avoid further cluttering it by moving code closer to its access points, the better.

{code}
+    final JobConf conf = new JobConf(new Configuration());
{code}
The patch uses final in a lot of places that MR code conventionally does not.  Even if this is better practice, I don't think now is the time to start.","21/Nov/14 23:04;kasha;Thanks Sandy. Missed that - removed that and other cases where we use final for method-local variables. ","21/Nov/14 23:53;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12682982/mr-5785-6.patch
  against trunk revision 1e9a3f4.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5042//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5042//console

This message is automatically generated.","22/Nov/14 00:14;sandyr;+1","22/Nov/14 02:43;hudson;SUCCESS: Integrated in Hadoop-trunk-Commit #6591 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6591/])
MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via kasha) (kasha: rev a4df9eed059977374c8e889cb85d79e8e514ad30)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
","22/Nov/14 02:45;kasha;Thanks for your review, Sandy. I just committed this to trunk. 

I think we should commit this to branch-2 as well: 
# The patch doesn't affect users who set java.opts
# For users that set java.opts but without -Xmx, this would limit their heap size and could lead to task failures. I am not too concerned about this, as we expect them to set java.opts and anyway the container would have been killed by YARN if physical memory constraints are enforced. 
# For users that don't set java.opts, this increases the JVM size from 200 MB to 820 MB. This should be okay for most jobs; streaming tasks might end up needing more memory because of their java process taking their total usage over the container size. Even here, this would likely happen only for those tasks relying on aggressive GC to keep the java.opts under 200 MB. 

All in all, I believe the likely cases affect by this is small enough that it should be okay to include this in a release, provided we release-note carefully. 

[~aw], [~jlowe], [~sandyr] - thoughts? ","22/Nov/14 11:52;hudson;SUCCESS: Integrated in Hadoop-Yarn-trunk-Java8 #13 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/13/])
MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via kasha) (kasha: rev a4df9eed059977374c8e889cb85d79e8e514ad30)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
","22/Nov/14 11:52;hudson;SUCCESS: Integrated in Hadoop-Yarn-trunk #751 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/751/])
MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via kasha) (kasha: rev a4df9eed059977374c8e889cb85d79e8e514ad30)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
","22/Nov/14 14:21;hudson;SUCCESS: Integrated in Hadoop-Hdfs-trunk #1941 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1941/])
MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via kasha) (kasha: rev a4df9eed059977374c8e889cb85d79e8e514ad30)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/CHANGES.txt
","22/Nov/14 14:22;hudson;SUCCESS: Integrated in Hadoop-Hdfs-trunk-Java8 #13 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/13/])
MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via kasha) (kasha: rev a4df9eed059977374c8e889cb85d79e8e514ad30)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
","22/Nov/14 15:37;hudson;FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #13 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/13/])
MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via kasha) (kasha: rev a4df9eed059977374c8e889cb85d79e8e514ad30)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
","22/Nov/14 15:38;hudson;FAILURE: Integrated in Hadoop-Mapreduce-trunk #1965 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1965/])
MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via kasha) (kasha: rev a4df9eed059977374c8e889cb85d79e8e514ad30)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
","24/Nov/14 01:40;jira.shegalov;Hi [~kasha], 

Sorry, I have not had a chance to look at your changes earlier.
In my patch I was setting DEFAULT_MAPRED_TASK_JAVA_OPTS to """" to make sure that we don't get null. In the latest patch it actually happens and is not caught because the tests are weakened by checking only for containment of command fragments instead of checking the whole command.
","24/Nov/14 02:27;hitesh;bq. I think we should commit this to branch-2 as well

This change is incompatible especially as it modifies mapred-default.xml. Not sure why it would be committed to branch-2. 
","24/Nov/14 06:32;aw;bq. I think we should commit this to branch-2 as well: 

Nope, no way.  As [~hitesh] points out, this is an incompatible change that will impact a portion of the of the user base.  The list of potential impacts are all pretty nasty for certain use cases.  (hint: some people run Hadoop on embedded and small memory devices...)","24/Nov/14 22:37;kasha;bq. In my patch I was setting DEFAULT_MAPRED_TASK_JAVA_OPTS to """" to make sure that we don't get null. In the latest patch it actually happens and is not caught because the tests are weakened by checking only for containment of command fragments instead of checking the whole command.

Thanks for the input, Gera. You are right. I didn't realize I was weakening the test when I refactored it.  Let me reopen the JIRA and revert the refactoring in an addendum patch. ","26/Nov/14 00:20;jianhe;After this patch, job somehow fails due to not able to launch task container {{Error: Could not find or load main class null}}.  (might be my own setup problem)","26/Nov/14 00:23;kasha;Actually, this is a bug with the patch itself. It might be best to revert it for now until we fix the issue. Reverting it. ","26/Nov/14 00:25;kasha;Reverted. Let me fix the bug and post another patch. ","26/Nov/14 00:31;hudson;FAILURE: Integrated in Hadoop-trunk-Commit #6607 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6607/])
Revert ""MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via kasha)"" (kasha: rev a655973e781caf662b360c96e0fa3f5a873cf676)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
","26/Nov/14 11:46;hudson;FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #17 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/17/])
Revert ""MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via kasha)"" (kasha: rev a655973e781caf662b360c96e0fa3f5a873cf676)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
","26/Nov/14 11:54;hudson;SUCCESS: Integrated in Hadoop-Yarn-trunk #755 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/755/])
Revert ""MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via kasha)"" (kasha: rev a655973e781caf662b360c96e0fa3f5a873cf676)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* hadoop-mapreduce-project/CHANGES.txt
","26/Nov/14 14:30;hudson;FAILURE: Integrated in Hadoop-Hdfs-trunk #1945 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1945/])
Revert ""MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via kasha)"" (kasha: rev a655973e781caf662b360c96e0fa3f5a873cf676)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
","26/Nov/14 14:31;hudson;FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #17 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/17/])
Revert ""MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via kasha)"" (kasha: rev a655973e781caf662b360c96e0fa3f5a873cf676)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
","26/Nov/14 15:21;hudson;FAILURE: Integrated in Hadoop-Mapreduce-trunk #1969 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1969/])
Revert ""MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via kasha)"" (kasha: rev a655973e781caf662b360c96e0fa3f5a873cf676)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
","26/Nov/14 15:29;hudson;SUCCESS: Integrated in Hadoop-Mapreduce-trunk-Java8 #17 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/17/])
Revert ""MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via kasha)"" (kasha: rev a655973e781caf662b360c96e0fa3f5a873cf676)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
","08/Jan/15 18:27;kasha;Here is an updated patch that fixes the earlier issues. As Gera suggested, I set the default value to be an empty string and left the tests as is. 

[~jira.shegalov] - can you review the latest patch when you get a chance? ","08/Jan/15 19:20;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12690903/mr-5785-7.patch
  against trunk revision 708b1aa.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 13 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5101//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5101//artifact/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-core.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5101//console

This message is automatically generated.","09/Jan/15 18:00;kasha;Assigned to myself for better tracking, will assign back to Gera at commit time. 

The findbugs warnings seem unrelated. ","09/Jan/15 18:02;jira.shegalov;Hi Karthik, thanks for updating the patch. I'll take a look early next week.","14/Jan/15 21:43;rkanter;Looks good to me.  +1 pending [~jira.shegalov]'s review.  
[~kasha], because this is an incompatible change, don't forget to add something to the ""Release Note"" field in the JIRA.","15/Jan/15 08:30;jira.shegalov;[~kasha], thanks for working on this patch. It's look good. I've got a few suggestions:
 
1. {{TaskAttemptImpl#getMemoryRequired}}:
It can be written as a one-liner as follows:
{code}
  private int getMemoryRequired(JobConf conf, TaskType taskType) {
    return conf.getMemoryRequired(TypeConverter.fromYarn(taskType));
  }
{code}

2. {{TestMapReduceChildJVM#testAutoHeapSize}}
We don't need to create 2 objects via {{new JobConf(new Configuration())}}, why not simply have
{code}
   JobConf conf = new JobConf();
{code}

3. {{JobConf#getConfiguredTaskJavaOpts}}
Instead of doing {{String#equals("""")}} let us use {{String#isEmpty}}
Checking {{user/adminClasspath}} for null is redundant because they are actually known to be not null. I also wonder whether preventing a single extra space in the command line is worth 7 lines of code :)

4. {{mapred-default.xml}}
Unsetting default values is inconsistent within the patch and outside the patch. For {{mapred.child.java.opts}} we use an empty <value> like for many other parameters, but for {{mapreduce.*.memory.mb}} the value element is removed by commenting it out. I think it should be done the same way as {{mapred.child.java.opts}} , and the comment explaining the defaults is enough.","19/Jan/15 15:42;kasha;Thanks for your thorough review, [~jira.shegalov]. The updated patch (v8) addresses your first 3 comments. 

Regarding the mapred-default suggestions for memory.mb, I don't think it is an issue of consistency. Unlike other configs, an empty string is not really a valid value. The latest patch has changes to gracefully handle invalid values - those that lead to NumberFormatExceptions. If people insist, I am okay with changing the default value to an empty string. ","19/Jan/15 16:37;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12693106/mr-5785-8.patch
  against trunk revision 4a5c3a4.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 13 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5107//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5107//artifact/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-core.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5107//console

This message is automatically generated.","20/Jan/15 21:14;jira.shegalov;Hi [~kasha], thanks for updating the patch. 

nit: v8 patch introduced a line break in the class declaration 
{code}
public class
    TestMapReduceChildJVM {
{code}

The config convention/implementation equates an empty value in xml to the unset value. So I think your v7 was fine in this respect. 

If it looks too hacky to you, we can set memory.mb keys to {{-1}}, the default you used in v8 or maybe more natural {{0}}. Then we can say {{memory.mb <= 0}} implies {{not set}}.

I prefer fail-fast validation, so I am not in favor of catching NumberFormatException. At least not in this patch. Otherwise, it's not clear why are we not doing this for all numeric parameters.","21/Jan/15 02:05;kasha;Updated patch to address Gera's latest comments. ","21/Jan/15 02:53;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12693467/mr-5785-9.patch
  against trunk revision fc20129.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 13 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5113//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5113//artifact/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-core.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5113//console

This message is automatically generated.","21/Jan/15 05:34;jira.shegalov;findbugs warnings are unrelated. +1 for the v9 patch. Thanks for accommodating my comments, Karthik. I'll commit it tomorrow evening if there are no objections.","22/Jan/15 02:57;jira.shegalov;Committed to trunk. Thanks [~kasha] for collaborating on this patch! ","22/Jan/15 03:05;hudson;SUCCESS: Integrated in Hadoop-trunk-Commit #6910 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6910/])
MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via gera) (gera: rev a003f71cacd35834a1abbc2ffb5446a1166caf73)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
","22/Jan/15 10:42;hudson;FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #81 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/81/])
MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via gera) (gera: rev a003f71cacd35834a1abbc2ffb5446a1166caf73)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
","22/Jan/15 10:45;hudson;FAILURE: Integrated in Hadoop-Yarn-trunk #815 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/815/])
MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via gera) (gera: rev a003f71cacd35834a1abbc2ffb5446a1166caf73)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
","22/Jan/15 14:10;hudson;FAILURE: Integrated in Hadoop-Hdfs-trunk #2013 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2013/])
MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via gera) (gera: rev a003f71cacd35834a1abbc2ffb5446a1166caf73)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
","22/Jan/15 14:21;hudson;FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #78 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/78/])
MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via gera) (gera: rev a003f71cacd35834a1abbc2ffb5446a1166caf73)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
","22/Jan/15 15:11;hudson;FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #82 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/82/])
MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via gera) (gera: rev a003f71cacd35834a1abbc2ffb5446a1166caf73)
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
","22/Jan/15 15:23;hudson;FAILURE: Integrated in Hadoop-Mapreduce-trunk #2032 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2032/])
MAPREDUCE-5785. Derive heap size or mapreduce.*.memory.mb automatically. (Gera Shegalov and Karthik Kambatla via gera) (gera: rev a003f71cacd35834a1abbc2ffb5446a1166caf73)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MapReduceChildJVM.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
","17/Feb/15 22:44;hudson;FAILURE: Integrated in Hadoop-trunk-Commit #7136 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7136/])
MAPREDUCE-6234. TestHighRamJob fails due to the change in MAPREDUCE-5785. (Masatake Iwasaki via kasha) (kasha: rev 409113d8f97fcfdb96cb028dbb6a20c9a1df81b0)
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java
","18/Feb/15 11:35;hudson;FAILURE: Integrated in Hadoop-Yarn-trunk #842 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/842/])
MAPREDUCE-6234. TestHighRamJob fails due to the change in MAPREDUCE-5785. (Masatake Iwasaki via kasha) (kasha: rev 409113d8f97fcfdb96cb028dbb6a20c9a1df81b0)
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java
","18/Feb/15 11:51;hudson;SUCCESS: Integrated in Hadoop-Yarn-trunk-Java8 #108 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/108/])
MAPREDUCE-6234. TestHighRamJob fails due to the change in MAPREDUCE-5785. (Masatake Iwasaki via kasha) (kasha: rev 409113d8f97fcfdb96cb028dbb6a20c9a1df81b0)
* hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java
* hadoop-mapreduce-project/CHANGES.txt
","18/Feb/15 14:22;hudson;FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #99 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/99/])
MAPREDUCE-6234. TestHighRamJob fails due to the change in MAPREDUCE-5785. (Masatake Iwasaki via kasha) (kasha: rev 409113d8f97fcfdb96cb028dbb6a20c9a1df81b0)
* hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java
* hadoop-mapreduce-project/CHANGES.txt
","18/Feb/15 14:27;hudson;SUCCESS: Integrated in Hadoop-Hdfs-trunk #2040 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2040/])
MAPREDUCE-6234. TestHighRamJob fails due to the change in MAPREDUCE-5785. (Masatake Iwasaki via kasha) (kasha: rev 409113d8f97fcfdb96cb028dbb6a20c9a1df81b0)
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java
","18/Feb/15 15:06;hudson;FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #109 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/109/])
MAPREDUCE-6234. TestHighRamJob fails due to the change in MAPREDUCE-5785. (Masatake Iwasaki via kasha) (kasha: rev 409113d8f97fcfdb96cb028dbb6a20c9a1df81b0)
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java
","18/Feb/15 15:30;hudson;FAILURE: Integrated in Hadoop-Mapreduce-trunk #2059 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2059/])
MAPREDUCE-6234. TestHighRamJob fails due to the change in MAPREDUCE-5785. (Masatake Iwasaki via kasha) (kasha: rev 409113d8f97fcfdb96cb028dbb6a20c9a1df81b0)
* hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java
* hadoop-mapreduce-project/CHANGES.txt
","07/Mar/15 19:44;aw;Someone should write a release note for this change, since it is a fairly significant change in behavior....",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Computing Input Splits on the MR Cluster,MAPREDUCE-207,12427845,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,jira.shegalov,philip,philip,14/Jun/09 22:46,05/Mar/19 13:59,12/Jan/21 09:52,,,,,,,,,,applicationmaster,mrv2,,,,,3,,,,,"Instead of computing the input splits as part of job submission, Hadoop could have a separate ""job task type"" that computes the input splits, therefore allowing that computation to happen on the cluster.",,aah,acmurthy,amareshwari,belugabehr,cutting,ddas,decster,dhruba,haibochen,hammer,hong.tang,jenvor,jira.shegalov,jlowe,jothipn,jrideout,kkambatl,liangly,lianhuiwang,longmi,matei,mingma,mzuehlke,oae,omalley,qwertymaniac,sandyr,schen,sharadag,srivas,sseth,tgraves,tomwhite,tucu00,vanyatka,vicaya,vinodkv,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-1227,MAPREDUCE-1484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/11 08:39;acmurthy;MAPREDUCE-207.patch;https://issues.apache.org/jira/secure/attachment/12496449/MAPREDUCE-207.patch","12/May/14 16:23;jira.shegalov;MAPREDUCE-207.v02.patch;https://issues.apache.org/jira/secure/attachment/12644428/MAPREDUCE-207.v02.patch","21/May/14 02:15;jira.shegalov;MAPREDUCE-207.v03.patch;https://issues.apache.org/jira/secure/attachment/12645924/MAPREDUCE-207.v03.patch","27/May/14 07:17;jira.shegalov;MAPREDUCE-207.v05.patch;https://issues.apache.org/jira/secure/attachment/12646848/MAPREDUCE-207.v05.patch","01/Jul/14 08:03;jira.shegalov;MAPREDUCE-207.v06.patch;https://issues.apache.org/jira/secure/attachment/12653342/MAPREDUCE-207.v06.patch","11/Jul/14 23:16;jira.shegalov;MAPREDUCE-207.v07.patch;https://issues.apache.org/jira/secure/attachment/12655331/MAPREDUCE-207.v07.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,2009-06-15 03:59:14.854,,,false,,,,,,,,,,,,,,,,,,3353,,,,,Tue Mar 05 13:59:33 UTC 2019,,,,,,,"0|i0ixiv:",108525,,,,,,,,,,,,,,,,,,,,,"14/Jun/09 22:58;philip;The motivation behind computing the input splits on the cluster is at least two-fold:
 * It would be great to be able to submit jobs to a cluster using a simple (REST?) API, from many languages.  (Similar to HADOOP-5633.)  The fact that job submission does a bunch of mapreduce-internal work makes such submission very tricky.  We're already seeing how workflow systems (here I'm thinking of Oozie and Pig) run MR jobs simply to launch more MR jobs, while inheriting the scheduling and isolation work that the JobTracker already does.
 * Sometimes computing the input splits is, in of itself, an operation that would do well to be run in parallel across several machines.  For example, splitting inputs may require going through many files on the DFS.  Moving input split calculations onto the cluster would pave the way for this to be possible.

Implementation-wise, we already have JOB_SETUP and JOB_CLEANUP tasks, so adding a JOB_SPLIT_CALCULATION, which could be colocated with JOB_SETUP makes some sense.","15/Jun/09 03:59;yhemanth;Before we do this, I think we should resolve HADOOP-4421. Atleast to the extent of agreeing to a design. Adding one more task, while we are trying to fix problems with the existing ones might make things a tad more difficult to manage.","15/Jun/09 04:05;ddas;Isn't it possible to do this as part of the JOB_SETUP task itself?","15/Jun/09 04:10;amareshwari;bq. Isn't it possible to do this as part of the JOB_SETUP task itself?
This can be done. We should move out the creation of setup/cleanup tasks from JobInProgress.initTasks(). 

","15/Jun/09 04:12;amareshwari;bq. This can be done. We should move out the creation of setup/cleanup tasks from JobInProgress.initTasks().
Related jira HADOOP-4472.","15/Jun/09 16:49;omalley;This patch should reintroduce checkInputSplits into org.apache.hadoop.mapreduce.InputFormat. This method should be documented as *optional*. It will only be invoked if Java code is doing the submission to detect errors in the user's job configuration, such as missing or read-protected input directory, before the job is submitted to the cluster.
","04/Jul/09 00:01;philip;I've been poking around here and am running into a fair amount of friction with how different task types are managed.

As far as I can tell, there are several ways that different task types are distinguished:

* There's a {{TaskType}} enum, which contains  MAP, REDUCE, JOB_SETUP, JOB_CLEANUP, and TASK_CLEANUP.  This is used quite a bit.
* TaskInProgress has isMapTask(), isJobCleanupTask(), isJobSetupTask().  I believe that TIP can report both isMapTask() and isJobCleanupTask() on the same object and that reduces are implied by !isMapTask().
* Task uses a hybrid approach.  There's MapTask and ReduceTask (a class hierarchy), but there's also isMapTask(), isJobSetupTask(), isTaskCleanupTask(), and isJobCleanuptask().
* Schedulers and TaskTrackers for the most part only deal with MAP and REDUCE tasks.  Really, these are ""slot types"", since other types of tasks can be run in them.  Schedulers are not aware of the ""special tasks""---the JobTracker schedules them ""manually"" on its own.

Does this sound about right?

-- Philip","04/Jul/09 01:20;matei;I think that's almost right, Philip. It looks to me like TASK_CLEANUP tasks can be both maps and reduces. The JobTracker will launch them in a reduce slot if they are cleaning up after a reducer. Therefore, isMapTask() might return false when the task is a cleanup task. To check whether a given Task is a plain old map task or plain old reduce task, you can use Task.isMapOrReduce().

This part of the code definitely leaves something to be desired. I believe Arun mentioned he'd look at it as part of JobTracker refactoring in the future.","26/Sep/11 05:29;acmurthy;This is fairly trivial in MRv2, I'll take a crack at this.","26/Sep/11 08:39;acmurthy;As foretold, here is a trivial, preliminary patch to move computation of input-splits inside the cluster - something we've craved for a very long time, as evinced by the interest in this jira and the number of times it comes up on user lists.

This is huge, because it's a significant step towards various improvements such as HTTP-based job submission etc.

Shameless plug for MRv2 - it took me 15 mins on a Sunday night to get this done... glory to MRv2! *smile*

----

It needs a tad more work to get delegation tokens on the client side, but it's nearly there.","04/Sep/12 09:49;oae;Currently in our hadoop applications we calculate the splits before we submit it to the client (then the client simply looks up the existing splits). We do that mainly to influence the reducer count base on the number of splits/map-tasks.
In case hadoop does the splitting on the cluster (which makes sense), it would be nice to have a hook to influence configuration!
Sometimes it also makes sense for us to decide on the map-reduce assembly after we know the splits (different join strategies for different data constellations).

Just dumping some ideas here...
","08/Mar/13 01:56;sandyr;Arun, are you still planning on working on this?  If not, do you mind if I pick it up?","12/May/14 20:21;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12644428/MAPREDUCE-207.v02.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.mapred.lib.aggregate.TestAggregates
                  org.apache.hadoop.mapreduce.lib.db.TestDataDrivenDBInputFormat
                  org.apache.hadoop.mapred.TestFieldSelection
                  org.apache.hadoop.mapred.TestOldCombinerGrouping
                  org.apache.hadoop.mapreduce.TestLocalRunner
                  org.apache.hadoop.mapred.TestUserDefinedCounters
                  org.apache.hadoop.mapreduce.TestMROutputFormat
                  org.apache.hadoop.mapreduce.lib.fieldsel.TestMRFieldSelection
                  org.apache.hadoop.mapred.TestLocalMRNotification
                  org.apache.hadoop.mapred.TestLineRecordReaderJobs
                  org.apache.hadoop.mapreduce.lib.map.TestMultithreadedMapper
                  org.apache.hadoop.mapreduce.TestNewCombinerGrouping
                  org.apache.hadoop.mapred.lib.TestChainMapReduce
                  org.apache.hadoop.mapreduce.TestMapReduce
                  org.apache.hadoop.mapreduce.lib.join.TestJoinDatamerge
                  org.apache.hadoop.mapred.lib.TestKeyFieldBasedComparator
                  org.apache.hadoop.mapred.lib.TestMultithreadedMapRunner
                  org.apache.hadoop.mapreduce.TestMapperReducerCleanup
                  org.apache.hadoop.mapred.lib.TestMultipleOutputs
                  org.apache.hadoop.mapred.TestJavaSerialization
                  org.apache.hadoop.mapreduce.lib.output.TestMRMultipleOutputs
                  org.apache.hadoop.mapred.TestCollect
                  org.apache.hadoop.mapred.join.TestDatamerge
                  org.apache.hadoop.mapreduce.TestMapCollection
                  org.apache.hadoop.mapreduce.lib.aggregate.TestMapReduceAggregates
                  org.apache.hadoop.mapred.TestMapRed
                  org.apache.hadoop.mapred.TestFileOutputFormat
                  org.apache.hadoop.mapreduce.TestValueIterReset
                  org.apache.hadoop.mapred.TestMapOutputType
                  org.apache.hadoop.mapred.TestJobCounters
                  org.apache.hadoop.conf.TestNoDefaultsJobConf
                  org.apache.hadoop.mapred.TestReporter
                  org.apache.hadoop.mapreduce.lib.partition.TestMRKeyFieldBasedComparator
                  org.apache.hadoop.mapreduce.lib.chain.TestChainErrors
                  org.apache.hadoop.mapreduce.lib.chain.TestSingleElementChain
                  org.apache.hadoop.mapreduce.lib.input.TestMultipleInputs
                  org.apache.hadoop.mapred.TestComparators
                  org.apache.hadoop.mapreduce.lib.input.TestLineRecordReaderJobs
                  org.apache.hadoop.mapreduce.lib.chain.TestMapReduceChain
                  org.apache.hadoop.mapred.jobcontrol.TestLocalJobControl
                  org.apache.hadoop.mapreduce.lib.jobcontrol.TestMapReduceJobControl

                                      The following test timeouts occurred in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskAttempt
org.apache.hadoop.mapreduce.v2.app.job.impl.TestMapReduceChildJVM
org.apache.hadoop.mapreduce.v2.app.webapp.TestAMWebApp
org.apache.hadoop.mapreduce.v2.app.TestMRClientService
org.apache.hadoop.mapreduce.v2.app.TestKill
org.apache.hadoop.mapreduce.v2.app.TestMRApp
org.apache.hadoop.mapreduce.v2.app.TestJobEndNotifier
org.apache.hadoop.mapreduce.v2.app.TestFail
org.apache.hadoop.mapreduce.v2.app.TestStagingCleanup
org.apache.hadoop.mapreduce.v2.app.TestRMContainerAllocator
org.apache.hadoop.mapreduce.v2.app.launcher.TestContainerLauncher
org.apache.hadoop.mapreduce.v2.app.TestRecovery
org.apache.hadoop.mapreduce.v2.app.TestAMInfos
org.apache.hadoop.mapreduce.v2.app.TestMRAppComponentDependencies
org.apache.hadoop.mapreduce.v2.app.TestFetchFailure
org.apache.hadoop.mapreduce.v2.TestSpeculativeExecutionWithMRApp
org.apache.hadoop.mapred.pipes.TestPipeApplication

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4595//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4595//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-app.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4595//console

This message is automatically generated.","13/May/14 09:08;jira.shegalov;v03 to handle local jobs correctly","14/May/14 20:05;jira.shegalov;[~stevel@apache.org], thanks for your [comment|https://issues.apache.org/jira/browse/MAPREDUCE-5887?focusedCommentId=13997431&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13997431]  in MAPREDUCE-5887. Moving it to here.

bq. One test to try there is what happens when the blocksize is reported as very, very small (you can configure this in swiftfs). in the client this will cause the submitting process to OOM and fail. Presumably the same outcome in the AM is the simplest to implement -we just need to make sure that YARN recognises this as a failure and only tries a couple of times

OOM's as any other AM failure are treated as an Application attempt failure ({{yarn.resourcemanager.am.max-attempts}}). We've experienced such issues in production, and it is actually usually indirectly related to splits, i.e. the job state comprising all map and reduce attempts is too big for the default MR-AM container size. 

Before doing the work on moving split calculation to MR-AM, I was actually thinking about auto-tuning {{yarn.app.mapreduce.am.resource.mb}} and Xmx opts in JobSubmitter. However, even if the split calculation happens in AM, we can come up with an AM-RM RPC like ""start a new attempt with the new settings"".","21/May/14 02:15;jira.shegalov;Hadoop QA did not kick in. Reuploading the same v03 again","21/May/14 04:29;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12645924/MAPREDUCE-207.v03.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The following test timeouts occurred in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

org.apache.hadoop.mapreduce.v2.app.TestJobEndNotifier
org.apache.hadoop.mapreduce.v2.app.TestRecovery
org.apache.hadoop.mapreduce.v2.app.TestMRAppComponentDependencies
org.apache.hadoop.mapreduce.v2.app.TestMRApp
org.apache.hadoop.mapreduce.v2.app.TestRMContainerAllocator
org.apache.hadoop.mapreduce.v2.app.TestFail
org.apache.hadoop.mapreduce.v2.app.TestFetchFailure
org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskAttempt
org.apache.hadoop.mapreduce.v2.app.job.impl.TestMapReduceChildJVM
org.apache.hadoop.mapreduce.v2.app.TestMRClientService
org.apache.hadoop.mapreduce.v2.app.TestAMInfos
org.apache.hadoop.mapreduce.v2.app.webapp.TestAMWebApp
org.apache.hadoop.mapreduce.v2.app.TestKill
org.apache.hadoop.mapreduce.v2.app.TestStagingCleanup
org.apache.hadoop.mapreduce.v2.app.launcher.TestContainerLauncher
org.apache.hadoop.mapred.pipes.TestPipeApplication
org.apache.hadoop.mapreduce.v2.TestSpeculativeExecutionWithMRApp

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4614//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4614//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-app.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4614//console

This message is automatically generated.","27/May/14 07:17;jira.shegalov;v05 patch, to restore the existing behavior of not adding job.split as local resource for non-AM containers.","27/May/14 09:36;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12646848/MAPREDUCE-207.v05.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The following test timeouts occurred in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

org.apache.hadoop.mapred.pipes.TestPipeApplication

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4624//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4624//console

This message is automatically generated.","27/May/14 21:00;jira.shegalov;Assuming that TestPipeApplication is MAPREDUCE-5868, v05 is ready for review. The code can further be optimized to avoid reading splits back when they are written for the first time. We can incorporate it if the approach is accepted in general. There is plenty of coverage for job submission that helped shape the patch. Since it's mere refactoring, no new functional tests are urgently needed. ","27/Jun/14 22:51;mingma;Thanks, Gera. Nice work and this will be quite useful. Overall it looks good. Per offline discussion with Gera,

1. It is unclear if there is any security related implication such as https://issues.apache.org/jira/browse/MAPREDUCE-5663.
2. The compatibility between new MR client with this feature and cluster with old MR. Given new MR client won't compute the split by default; the job will fail if the cluster still uses old MR. So in this case, new MR client needs to be configured to compute split. For a more general case where new MR client can talk to some cluster with old MR and some cluster with new MR, it will be nice if client can discover if the cluster supports this feature.","01/Jul/14 08:03;jira.shegalov;v06 that adds a unit test and fixes incorrect handling of the number of mappers other than 2 in a uberized job.","01/Jul/14 08:07;jira.shegalov;v06 does not address [~mingma]'s review yet (thank you) . Assigned this jira to myself as nobody else seems to be working on it.","01/Jul/14 10:26;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12653342/MAPREDUCE-207.v06.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers

                                      The following test timeouts occurred in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

org.apache.hadoop.mapred.pipes.TestPipeApplication

                                      The test build failed in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app 

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4699//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4699//console

This message is automatically generated.","11/Jul/14 23:16;jira.shegalov;v07:  disabling in-AM splits for GridMix because some InputFormats in this job use GridMixJob#descCache, which is on the client side","12/Jul/14 01:36;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12655331/MAPREDUCE-207.v07.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The test build failed in hadoop-tools/hadoop-gridmix 

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4730//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4730//console

This message is automatically generated.","05/Feb/15 21:10;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12655331/MAPREDUCE-207.v07.patch
  against trunk revision e1990ab.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5167//console

This message is automatically generated.","09/Feb/15 21:41;aw;Cancelling patch, as it no longer applies.","30/Jun/15 04:28;vinodkv;Moving features/enhancements out of previously closed releases into the next minor release 2.8.0.","09/May/18 20:28;belugabehr;This feature would be interesting to the Hive server since the server could have many MapReduce clients running in a single instance, at the same time, on large data sets.","05/Mar/19 13:59;belugabehr;Came across a situation lately where a user had the LZO compression codec enabled in the cluster.  The codec was installed across the cluster.  However, MR jobs, that did not even require the codec, were failing because the compression codec was not installed on the client node where the jobs were being submitted from.  As part of the client's role in calculating splits, the client loads the codec configuration and all the associated codec implementations.  This fails on external clients because they did not have the codec installed.  The user understandably did not want to have to install the LZO codec on every client node, but it was at the cost of having to maintain separate hdfs-site files for different client hosts.

Moving all of this work into the cluster removes this dependency from the clients.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for encrypting Intermediate data and spills in local filesystem,MAPREDUCE-5890,12714395,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,asuresh,tucu00,tucu00,15/May/14 04:05,12/Feb/19 19:29,12/Jan/21 09:52,11/Jul/14 00:48,2.4.0,,,,,2.6.0,fs-encryption,,security,,,,,,0,encryption,,,,"For some sensitive data, encryption while in flight (network) is not sufficient, it is required that while at rest it should be encrypted. HADOOP-10150 & HDFS-6134 bring encryption at rest for data in filesystem using Hadoop FileSystem API. MapReduce intermediate data and spills should also be encrypted while at rest.",,asuresh,cdouglas,devaraj,gkrishnan,hammer,hitliuyi,jianhe,kshukla,qwertymaniac,tucu00,vinodkv,vrushalic,vvasudev,xgong,yoderme,zjshen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-6257,,,,,,,,,,,,,,,,,,,,,"25/Jun/14 23:55;asuresh;MAPREDUCE-5890.10.patch;https://issues.apache.org/jira/secure/attachment/12652523/MAPREDUCE-5890.10.patch","26/Jun/14 10:04;asuresh;MAPREDUCE-5890.11.patch;https://issues.apache.org/jira/secure/attachment/12652595/MAPREDUCE-5890.11.patch","26/Jun/14 20:11;asuresh;MAPREDUCE-5890.12.patch;https://issues.apache.org/jira/secure/attachment/12652673/MAPREDUCE-5890.12.patch","08/Jul/14 23:55;asuresh;MAPREDUCE-5890.13.patch;https://issues.apache.org/jira/secure/attachment/12654715/MAPREDUCE-5890.13.patch","09/Jul/14 21:06;asuresh;MAPREDUCE-5890.14.patch;https://issues.apache.org/jira/secure/attachment/12654870/MAPREDUCE-5890.14.patch","10/Jul/14 23:52;asuresh;MAPREDUCE-5890.15.patch;https://issues.apache.org/jira/secure/attachment/12655116/MAPREDUCE-5890.15.patch","19/Jun/14 06:55;asuresh;MAPREDUCE-5890.3.patch;https://issues.apache.org/jira/secure/attachment/12651375/MAPREDUCE-5890.3.patch","24/Jun/14 09:32;asuresh;MAPREDUCE-5890.4.patch;https://issues.apache.org/jira/secure/attachment/12652166/MAPREDUCE-5890.4.patch","25/Jun/14 11:04;asuresh;MAPREDUCE-5890.5.patch;https://issues.apache.org/jira/secure/attachment/12652388/MAPREDUCE-5890.5.patch","25/Jun/14 17:14;asuresh;MAPREDUCE-5890.6.patch;https://issues.apache.org/jira/secure/attachment/12652458/MAPREDUCE-5890.6.patch","25/Jun/14 18:07;asuresh;MAPREDUCE-5890.7.patch;https://issues.apache.org/jira/secure/attachment/12652475/MAPREDUCE-5890.7.patch","25/Jun/14 18:33;asuresh;MAPREDUCE-5890.8.patch;https://issues.apache.org/jira/secure/attachment/12652482/MAPREDUCE-5890.8.patch","25/Jun/14 21:33;asuresh;MAPREDUCE-5890.9.patch;https://issues.apache.org/jira/secure/attachment/12652503/MAPREDUCE-5890.9.patch","21/Jun/14 04:50;asuresh;org.apache.hadoop.mapred.TestMRIntermediateDataEncryption-output.txt;https://issues.apache.org/jira/secure/attachment/12651806/org.apache.hadoop.mapred.TestMRIntermediateDataEncryption-output.txt","21/Jun/14 04:50;asuresh;syslog.tar.gz;https://issues.apache.org/jira/secure/attachment/12651805/syslog.tar.gz",,,,,,,,,,,,,,,,,,,,15.0,,,,,,,,,,,,,,,,,,,,2014-06-18 05:32:59.015,,,false,,,,,,,,,,,,,,,,,,392708,Reviewed,,,,Tue Feb 12 00:39:24 UTC 2019,,,,,,,"0|i1vml3:",392884,,,,,,,,,,,,,,,,,,,,,"15/May/14 04:09;tucu00; HADOOP-10603 introduces crypto streams to be used by for filesystem encryption. We could leverage it for encrypting map output data, the Reducer shuffle would decrypt it (no need for network encryption as data would be encrypted in transit). The reducer, when writing spills to disk woudl encrypt and it would decrypt while reading the spills.

It may make sense to do this JIRA as part of fs-encryption branch.","18/Jun/14 05:32;asuresh;Attaching initial patch that encrypts intermediate MapReduce spill files.
NOTE : This is to be applied to the 'fs-encyption' branch

The following are locations in the code path modified by the patch :

1) In the Map phase, when any on-disk file is created : When the Merger writes segments to disk as well as when spill files are written to disk in the MapTask, an IV (Initialization Vector) is initialized for the file and written to the same directory (like the index file) with an appropriate suffix. No encryption happens for in-memory data (when segments are sorted in-memory). At the end of the Map phase, each Map task will have written a single spill file to disk, which is encrypted and an associated IV file will also be present in the directory.

2) The Shuffle Handler : When request for partition comes in from the Fetcher, The ShuffleHandler checks to see if the spill file for the map attempt is encrypted (which would be true if it finds an associated 'crypto-iv' suffixed file). It then adds the IV into the ShuffleHeader for that spillfile and sends the encrypted stream as is to the Fetcher.

3) In the Reduce Phase : The fetcher receives the ShuffleHeader for the HTTP stream and if it finds the IV, will use the IV to wrap the InputStream with a CryptoInputStream and pass it on to the Reduce stage Mergers. Before the Merger writes to disk (Either the OnDiskMerger or the InMemoryMerger)

Other Notes :
* There is no need for over the network encryption of shuffle data as it is already encrypted.
* The Encryption keys are set into the TokenCache in the JobSubmitter.



","18/Jun/14 06:01;asuresh;Adding an end-to-end TestCase","18/Jun/14 23:59;tucu00;Suresh, any special reason why the test is not included in the main patch?

I’m not quite happy with the IF blocks scattered around:

{code}
      if (CryptoUtils.isShuffleEncrypted(conf)) {
        byte[] iv = CryptoUtils.createIVFile(conf, fs, file);
        out = CryptoUtils.wrap(conf, iv, out);
      }
{code}

Given that current abstraction does not provide a clean cut to hide this within the {{IFile}} without a significant refactoring throughout the code, I think is the least evil.

Nice job.

Could you try running test-patch locally on the fs-encryption branch with this patch?
","19/Jun/14 06:55;asuresh;Updating Patch combined with Test Case","19/Jun/14 07:00;asuresh;[~tucu00],

Yup, I did not find any better way of doing this.. 
I have combined the code and test case into a single patch. I have also run the test case locally varying the number of Maps and Reduces.. It works fine..

","21/Jun/14 03:37;tucu00;LGTM. [~asuresh], can you run test-patch locally on the patch and paste the result in the JIRA? After that, I think we are good to go.","21/Jun/14 04:50;asuresh;Attaching output of TestCase and tar of the syslog files generated by the Containers..

Screen dump :
{noformat}
-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.mapred.TestMRIntermediateDataEncryption
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 79.342 sec - in org.apache.hadoop.mapred.TestMRIntermediateDataEncryption

Results :

Tests run: 1, Failures: 0, Errors: 0, Skipped: 0

[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 01:28 min
[INFO] Finished at: 2014-06-20T20:49:58-08:00
[INFO] Final Memory: 39M/710M
[INFO] ------------------------------------------------------------------------
{noformat}","22/Jun/14 20:31;cdouglas;bq. Given that current abstraction does not provide a clean cut to hide this within the IFile without a significant refactoring throughout the code, I think is the least evil.

It's expedient, but this code is already difficult to follow. Arun, would you mind making an attempt at refactoring? The current code doesn't have an existing abstraction for this, but writing a separate file for every spill just to store a few bytes of IV doesn't seem like a reasonable tradeoff in either performance or complexity. Adding a metadata block to the {{IFile}} segment or adding the IV to the spill index (to be added in the header, as in the current patch) would both work.

A couple nits:
* In {{OnDiskMapOutput}}, the {{disk}} field can stay final, since the only assignment is in the cstr
* Minor indentation/braces issue in {{MapTask}}:
{{noformat}}
+          if (CryptoUtils.isShuffleEncrypted(job))
+          CryptoUtils.deleteIVFile(rfs, filename[i]);
{{noformat}}

Minor nit: please leave old patches attached to avoid orphaning the discussion around them.","24/Jun/14 09:32;asuresh;[~chris.douglas],
Updating patch.. 
As per your suggestion, I've refactored out the need for another file to store the IV.

But I still could not find a consistent way for seamlessly handling the IV : 
* The IV for the Spill file created at the end of the Map phase is now added to the spill {{IndexRecord}} and is transmitted to the {{Fetcher}} via the {{ShuffleHeader}} in the {{ShuffleHandler}}
* Unfortunately, the intermediate files created by the OnDisk mergers do not have an index file associated with them. I was thus forced to write the IV as a prefix into the stream. This happens when I ""wrap"" the outputStream, before passing it to the {{IFile}} writer.","24/Jun/14 10:32;cdouglas;Thanks for updating the patch, Arun. Adding seeks for serving map output would be regrettable.

Few nits:
* unused, private static field {{counter}} added to {{Fetcher}}
* unit test should use JUnit4 annotations rather than extending {{TestCase}}
* {noformat}
+      InputStream is = input;
+      is = CryptoUtils.wrap(jobConf, iv, is, offset, compressedLength);
{noformat} is equivalently {{InputStream is = CryptoUtils.wrap(jobConf, iv, input, offset, compressedLength);}}
* While not terribly expensive, there are a lot of redundant lookups for the encrypted shuffle config parameter.
* There are many counterexamples, but running a MR job is a heavy way to test this.
* To be sure I understand the IV logic, it's injected in the stream as a prefix to the segment during a merge, but is part of the index record during a spill. Is that accurate? Adding a few comments calling this out would be appreciated, particularly since it's hard to spot in the merge.
* Has this been tested on spills with intermediate merges? With more than a single reduce? Looking at the patch, it looks like it creates the stream with the IV, it doesn't reset the IV for each segment (apologies, I haven't tried applying it, so I might just be misreading the context).
* Since the IV size is hard-coded in {{CryptoUtils}} to 16 bytes (and part of the {{IndexRecord}} format), it should probably fail if the {{CryptoCodec::getAlgorithmBlockSize}} returns anything else.

Much of the logic in here is internal to MapReduce, so it would be unfair to ask that this create better abstractions than what exists, but the IV handling is pretty ad hoc. Other improvements under consideration- particularly native implementations and other frameworks building on the {{ShuffleHandler}}- may rely on this code, as well as older versions of MapReduce that will fail without deploying two versions of the ShuffleHandler.

To make it backwards compatible, the IV can be part of each {{IFile}} segment (requiring no changes to {{ShuffleHandler}} or the {{SpillRecord}}/{{IndexRecord}} format), or the IVs can be added to the end of the {{SpillRecord}}. In the latter case, the {{Fetcher}} will need to request that the alternate interpretation by including a header; old versions will get the existing interpretation of the {{SpillRecord}}.","24/Jun/14 17:04;tucu00;Hi [~chris.douglas],

I would prefer to keep the current MR job test because it test spills/merges on both sides of the MR job making sure no edge cases are not covered.

The {{ShuffleHandler}} is a private class of MapReduce, if other frameworks use it, it is at their own risk.

Regarding adding new abstractions, I’m OK if they are small and non-intrusive. I just don’t want to send Arun chasing a goose a wild goose and when he finally does we backtrack because the changes are too pervasive in the core of MapReduce (this happened in MAPREDUCE-2454).
","24/Jun/14 18:00;cdouglas;The repeated config lookup and unit test are not blockers, but they're places where the patch could be improved.

bq. The ShuffleHandler is a private class of MapReduce, if other frameworks use it, it is at their own risk.

Every version of the patch has broken compatibility with existing versions of _MapReduce_. Other frameworks may rely on functionality we don't guarantee, but breaking them is avoidable.

bq. Regarding adding new abstractions, I’m OK if they are small and non-intrusive. I just don’t want to send Arun chasing a goose a wild goose and when he finally does we backtrack because the changes are too pervasive in the core of MapReduce

Adding a new file just to pass 16 bytes to the {{ShuffleHandler}} will harm performance; breaking backwards compatibility is not OK, and not necessary for this feature. Aside from those, I've asked for some formatting fixes and that the code not return an IV that doesn't match the hard-coded 16-byte size. These are reasonable, limited requests and bug fixes, and I've suggested two possible implementations that would address them. These would be blockers during the merge, too.","24/Jun/14 22:13;tucu00;[~chris.douglas], on the last section of the previous comment. I didn't mean to say your refactoring asks are a wild goose, I just wanted to say I don't want to end up on that situation. My apologies if I've given the wrong impression with my comment. I've talked with Arun and he is already exploring along the lines of your suggestions to see their feasibility. 
","25/Jun/14 11:04;asuresh;
Hi [~chris.douglas]
Thank you for the feedback. Updating the patch to address most of your nits :

bq. There are many counterexamples, but running a MR job is a heavy way to test this

Agreed. But a MR Job will ensure all code paths are handled. I will be adding testcases to existing classs (for eg. TestMerger) to validate that the merging works fine with Shuffle turned on. But I don't see too many tests cases to validate that mapOutput spillfiles are correctly being partitioned and sent to the correct reduces.

bq. Has this been tested on spills with intermediate merges? With more than a single reduce? Looking at the patch, it looks like it creates the stream with the IV, it doesn't reset the IV for each segment (apologies, I haven't tried applying it, so I might just be misreading the context).

Modifying the TestMerger class to use the CryptoShuffle will ensure the former. The current Test case Included with the patch tests with multiple reducers.. I will refactor it a bit to explicitly test these scenarios

bq. To make it backwards compatible, the IV can be part of each IFile segment (requiring no changes to ShuffleHandler or the SpillRecord/IndexRecord format), or the IVs can be added to the end of the SpillRecord. In the latter case, the Fetcher will need to request that the alternate interpretation by including a header; old versions will get the existing interpretation of the SpillRecord.

As per your suggestion, I was actually able to get the end to end flow working without having to touch {{ShuffleHandler}}, {{ShuffleHeader}} or {{IndexRecord}}. Although, what I did was add the IV to the prefix of an {{IFile}} before it is written.. and during {{Segment::init()}} when it is read from disk. Only nit is I have to do some amount of book-keeping on the {{MapTask}} and {{Fetcher}} to add/remove the 16 bytes.

bq. Since the IV size is hard-coded in CryptoUtils to 16 bytes (and part of the IndexRecord format), it should probably fail if the CryptoCodec::getAlgorithmBlockSize returns anything else.

Yup.. this would have been an issue had I had to modify the {{IndexRecord}}/{{ShuffleHeader}}. But now we don't, so this is not an issue anymore


","25/Jun/14 17:14;asuresh;Updating Patch..
Added an additional testcase {{TestMRIntermediateDataEncryption::testMapSideMerge}}","25/Jun/14 18:07;asuresh;Looks like the latest patch was not applying... updating with new patch..","25/Jun/14 18:27;vinodkv;Sorry for jumping in late on this one.

bq. For some sensitive data, encryption while in flight (network) is not sufficient, it is required that while at rest it should be encrypted.
Not sure why this is a requirement. I can understand the requirement for encryption it on the wire, but on disk the intermediate files are already secure, readable only by users who need to read them and more importantly all this intermediate data is _very transitory_.

bq. .. writing a separate file for every spill just to store a few bytes of IV doesn't seem like a reasonable tradeoff in either performance or complexity.
I too echo the same sentiment.

If this really is a requirement, aren't we better off asking cluster admins to either install disks with local file-systems that support encryption specifically for intermediate data or just create some partitions that support encryption? That seems like the right layer to handle something like this instead of adding a whole lot of complexity into the software that only has a downside of performance.

Wearing my YARN hat, it is not enough to do this just for MapReduce. Every other framework running on YARN will need to add this complexity - this is asking for too much complexity. We are better off handling it at the file-system/partition/disk level.","25/Jun/14 18:33;tucu00;Fetcher.java
MapTask.java
MergerManagerImpl.java
Merger.java
ShuffleHandler.java
ShuffleHeader.java

* several space changes (configure your editor not to trim unmodified lines

CryptoUtils.java

* createIV(): javadocs, invalid params
* wrap() OUT/IN methods: any change to consolidate all/most signatures to delegate to a single one doing the repetitive logic?
* a couple wrap() methods have a funny LOG message ####
* wrap() OUT methods use cc.AlgorithmBlockSize(), but wrap() IN methods use 16, for IN methods you can use the cc already avail in the method.
* wrap() methods wrap if necessary (the IF ENCRYTPED has been moved inside), the name should reflect that, maybe something like 'wrapIfNecessary()'

Fetcher.java

* copyMapOutput() is unconditionally correct the offset, this seems wrong.

* No need to define out2, just reuse out
","25/Jun/14 18:33;asuresh;Rebasing due to API changes caused by HADOOP-10713","25/Jun/14 18:42;tucu00;bq. If this really is a requirement, aren't we better off asking cluster admins to either install disks with local file-systems that support encryption specifically for intermediate data or just create some partitions that support encryption? That seems like the right layer to handle something like this instead of adding a whole lot of complexity into the software that only has a downside of performance.

Asking to install additional soft to encrypt local FS means installing Kernel modules.

Also, this would mean that  ALL MR jobs are going to pay the penalty of encrypted intermediate data. That is not reasonable.

I don't agree on the statement that this is ""adding a lot of complexity"", it is simply wrapping the streams where necessary.

bq. Wearing my YARN hat, it is not enough to do this just for MapReduce. Every other framework running on YARN will need to add this complexity - this is asking for too much complexity. We are better off handling it at the file-system/partition/disk level.

This patch is not touching anything in Yarn, but in MapReduce, private/evolving classes of it.

","25/Jun/14 19:05;vinodkv;bq. Asking to install additional soft to encrypt local FS means installing Kernel modules.
Agreed, I wasn't trying to make it look a simple step.

bq. Also, this would mean that ALL MR jobs are going to pay the penalty of encrypted intermediate data. That is not reasonable.
That's a fair argument, but see below..

bq. I don't agree on the statement that this is ""adding a lot of complexity"", it is simply wrapping the streams where necessary.
bq. This patch is not touching anything in Yarn, but in MapReduce, private/evolving classes of it.
I didn't mean that we are touching YARN here. I meant that we will have to keep implementing this for _every_ framework that runs on YARN - MR, YARN, Storm, Spark, HBase and the list goes on an on. I am trying to trade off that complexity in software with an admin prerequisite to install one or few disks/partitions that selective users can chose to use via their job-configuration.","25/Jun/14 21:33;asuresh;[~tucu00]
Attaching a 'scrub' patch (removing the new-line issues) for review..
Will be updating with a final patch once I address all your feedback comments.
","25/Jun/14 22:38;cdouglas;bq. I am trying to trade off that complexity in software with an admin prerequisite to install one or few disks/partitions that selective users can chose to use via their job-configuration.

This would work also, but (Alejandro/Arun, correct me if this is mistaken) encrypted intermediate data is probably motivated by compliance regimes that require it. An audit would need to verify that every job used the encrypted local dirs, that those mounts were configured to encrypt when the job ran, etc. One would also need to do capacity planning for encrypted vs unencrypted space across nodes, possibly even federating jobs. It's workable, but kind of ad hoc. In contrast, verifying that the MR job set this switch is straightforward and has no ops overhead. I have no idea whether it's common to combine these workloads, but this would make it easier.

It's not so inconsistent to add this to MapReduce... frameworks are currently responsible for intra-application security, particularly RPC. If there's a general mechanism then this should use it. If that layer were developed, we'd want MapReduce to use it instead of its own, custom encryption. Today, the alternative is to develop that general-purpose layer.

To reduce the overhead, this could use the plugin mechanism in MAPREDUCE-2454 because this no longer requires any changes to the {{ShuffleHandler}} or index formats. I haven't looked at the latest patch, but if the {{IFile}} format omits the 16 byte IV for each spill, then the only overhead it's adding is for the checks in the config (most of which can be pulled into the buffer init and cached).

Has this been tested in a cluster? Would the perf hit be simple to measure?","25/Jun/14 23:55;asuresh;Updating patch to address all the feedback. Thanks !!

[~tucu00],
bq. copyMapOutput() is unconditionally correct the offset, this seems wrong
wrt, to the {{Fetcher::copyMapOutput()}} unconditionally setting the offset. It shouldn't be a problem not, since in the latest patch, there is no offset sent anymore

bq. No need to define out2, just reuse out
We still do require out2 (I have since renamed the variable). Since I still need a reference of the original 'out'. I can then wrap the original 'out' for each partition of the SpillFile (and thereby prefix the IV and offset at the beginning of EACH partition of the spill file). This will ensure that I won't have to send either the stream offset or the IV via the {{ShuffleHandler}}/{{ShuffleHeader}}


[~chris.douglas]
To address your concerns about backward compatibility, Since we don't touch the {{ShuffleHandler}} in the latest patch (Think in my previous patch, I still had the offset sent in the shuffle header.. that's been moved out now).. it should be fine.

bq. but if the IFile format omits the 16 byte IV for each spill, then the only overhead it's adding is for the checks in the config (most of which can be pulled into the buffer init and cached)
Just to clarify, we are now sending 24 bytes (16 for the IV an 8 for the long offset)


","26/Jun/14 10:04;asuresh;Updating patch..
Made some formatting / variable name changes.

Performed some backward compatibility testing :
Tested on a cluster with 1/2 the Nodes running with the patch and 1/2 that aren't. Set the Encryption flag to OFF and ran some jobs that utilized the whole cluster. Ensured that there weren't any failures.  

[~chris.douglas],
bq. Has this been tested in a cluster? Would the perf hit be simple to measure?
So given that the extra bits (IV and offset) are sent only if Encryption is turned ON,  I ran some basic terasort tests and I could not find any perceptible difference in performance. But I guess there are various variables that can the tuned during testing. For e.g., I can play around with {{mapreduce.task.io.sort.mb}} and {{mapreduce.map.sort.spill.percent}} to vary the number of spills/on-disk merges. So, to answer your question, don't think it is easy to measure the performance hit.","26/Jun/14 13:55;tucu00;On the performance hit, if encryption is OFF I would say it is NILL (the only extra thing being don is resolving a boolean config to check if encryption is ON or OFF). if encryption is ON, you are hitting the encryption/decryption overhead. Doing prelimiaries encrytion benchmarks with the crypto streams using Diceros (CryptoCodec->JCE->JNI->OpenSSL) I've got >1000MB/sec both on encrypt/decrypt on my laptop. Once we have HADOOP-10693 and this JIRA, will be able to do some end to end benchmarks.","26/Jun/14 17:51;asuresh;Ran test-patch locally.
Results :

{color:red}-1 overall{color}.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 5 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 6 warning messages.
        See /artifact/trunk/patchprocess/diffJavadocWarnings.txt for details.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version ) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 3 release audit warnings.


","26/Jun/14 20:11;asuresh;Updating patch to fix javadoc warnings..","26/Jun/14 20:12;asuresh;Re-ran test-patch locally.
Results:



{color:red}-1 overall{color}.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 5 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version ) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.
","26/Jun/14 20:16;asuresh;The -1 on the release audit is expected and caused due to the {{CHANGES-fs-encryption.txt}} file having no license header.. This file is not related to this patch.. And should go away once merged..

Output of patchReleaseAuditProblems.txt :
{noformat}
 !????? /home/asuresh/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/CHANGES-fs-encryption.txt
Lines that start with ????? in the release audit report indicate files that do not have an Apache license header.
{noformat}

","28/Jun/14 13:53;tucu00;LGTM. 

One minor nit (i can take care of it when committing), in {{JObSubmitter.java#copyAndConfigureFiles()}} javadoc, line 295 no needed change.

[~chris.douglas], I believe all your suggestions/concerns have been addressed. Do you want to do a new pass on the patch?

I'll wait a few days to commit.","29/Jun/14 18:44;cdouglas;The current patch still injects the IV and length into the stream, then fixes up the offsets. If the IV were part of the {{IFile}} format, then this would not be necessary. If this format were ever changed, then someone would need to go back and fix all this arithmetic or take its framing as a requirement for any intermediate data format. 

Am I missing why it's easier to wrap/unwrap streams?","01/Jul/14 00:23;asuresh;[~chris.douglas],
I had initially tried to directly modify the {{IFile}} format to handle the iv. The reason I felt this would not be such a clean solution is :
* The {{IFile}} currently does not have a notion of an explicit header/metadata.
* While it is possible to use the {{IFile.Writer}} constructor to write the IV and (thus make it transparent to the rest of the code-base). The reading code-path is not so straight-forward. There are two classes that extend the {{IFile.Reader}} ({{InMemoryReader}} and {{RawKVIteratorReader}}). The {{InMemoryReader}} totally ignores the inputStream that is initialized in the base class constructor and there are places in the codeBase that the input stream is not initialized in the Reader but in the {{Segment::init()}} method (which in my opinion makes the {{IFile}} abstraction a bit leaky since the underlying stream should be handled in its entirity in the IFile Writer/Reader.. the {{Segment}} class (which is part of the {{Merger}} framework) should avoid dealing with the internals of the ).
* Also, I was not able to do away with a lot of if-then checks in the Shuffle phase... (another instance of leaky abstraction mentioned in the previous point), the implementations of {{MapOutput::shuffle}} method creates {{IFileInputStreams}} directly without an associated {{IFile.Reader}}","06/Jul/14 23:40;cdouglas;OK... untangling the abstractions can be deferred. The current patch spreads the feature across the code in a way that's not ideal to maintain, but it addresses all the functional feedback by moving the IV inline.

Thanks [~asuresh] for all the iterations on this.","07/Jul/14 16:34;tucu00;[~chris.douglas], thanks for the detailed feedback/review iterations on this. Does this means you are OK with committing the current patch?","08/Jul/14 22:29;cdouglas;Yes, I'm OK with the current patch. This approach won't scale to another feature, but it can be preserved in a refactoring.

My only remaining ask (fine to add during commit) is that {{CryptoUtils}} be annotated with {{@Private}} and {{@Unstable}}, so it's clearly marked as an implementation detail. If it could be package-private that would be even better, though I haven't checked to see if there's anything else in the {{o.a.h.mapreduce.task.crypto}} package.","08/Jul/14 23:55;asuresh;Uploaded updated patch.. Thanks [~chris.douglas] for all the feedback !!
I've maked the class {{Private}} and {{Unstable}} but can't make the class itself package protected since it exposes public static methods used in a number of places..","09/Jul/14 00:56;cdouglas;Sorry, I meant that if {{o.a.h.mapreduce.task.crypto}} only has {{CryptoUtils}} in it, then maybe the new package isn't necessary.","09/Jul/14 17:35;asuresh;[~chris.douglas],
So, if our objective is to have CryptoUtils contained, and since it is currently being used only by the mapped framework, I'd prefer it remains where it is : in {{hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/o/a/h/mapreduce/task/crypto/}}
else It looks like the only other place it fits in is {{./hadoop-common-project/hadoop-common/src/main/java/o/a/h/crypto/}} in which case, it will look like a more generic utility and we will invite more people using it before it becomes stable.

I'd rather move it over once it stabilizes","09/Jul/14 19:17;cdouglas;I was thinking {{o.a.h.mapred}}, with other internal classes.","09/Jul/14 21:06;asuresh;Ok.. Uploaded patch with {{CryptoUtils}} moved. I still can't make it package protected though, since some code that uses it is in {{o.a.h.mapreduce}} and some in {{o.a.h.mapred}}","10/Jul/14 23:52;asuresh;Updated with testcase fix","11/Jul/14 00:48;tucu00;I've just committed this JIRA to fs-encryption branch.

[~chris.douglas], thanks for all the review cycles you spent on this.

[~asuresh], thanks for persevering until done, nice job.
","11/Jul/14 04:40;cdouglas;Yes; thanks [~asuresh] for your patience in seeing this through.","30/Jun/15 07:18;vinodkv;Closing old tickets that are already part of a release.","12/Feb/19 00:39;gkrishnan;Hello [~vinodkv], [~chris.douglas], [~tucu00], [~asuresh],

 

Had a question around why this snippet of code was removed (which was added as part of this JIRA - MAPREDUCE-5890) in the File: JobSubmitter.java :
 {{int keyLen = CryptoUtils.isShuffleEncrypted(conf)}}? conf.getInt(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS, MRJobConfig.DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS): SHUFFLE_KEY_LENGTH;
  
 and later reverted and replaced with a constant value:
 {{keyGen.init(SHUFFLE_KEY_LENGTH);}}
 as part of this change:[https://github.com/apache/hadoop/commit/d9d7bbd99b533da5ca570deb3b8dc8a959c6b4db]
  
 Some context around this question: We are trying to go for FedRamp High Certification and that mandates a key length for HMAC-SHA1 to be at least 112 bits and the current key length is 64 bits. Would be great to know your thoughts on this one.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add aggregated webservice endpoint to fetch all tasks including their attempts,MAPREDUCE-7127,13176779,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,bdscheller,bdscheller,03/Aug/18 18:07,06/Feb/19 11:27,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,webservices,,,,"We have a usecase where we would like to poll Hadoop for various running Tasks and display info on each individual task attempt to the user.

On large clusters with many tasks, this is an issue because using the current webservice APIs we would have to make many repeat calls to the same task attempts endpoints.

To improve performance, we have added additional endpoints to HS and AM webservices that will fetch aggregated data on hadoop tasks including their task attempts for a job.

This would look like

- taskDescriptions webservice endpoint to AMWebServices
 @Path(""/jobs/ \{jobid}/taskDescriptions"") 
 
 - describeTasks webservice endpoint to HSWebServices 
 @Path(""/mapreduce/jobs/\{jobid}/describeTasks"")

It would be similar to the current getJobTasks() in HS and AM webservices except it would include each of the task attempts for each task.",,bdscheller,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-08-03 22:32:43.137,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 03 22:32:43 UTC 2018,,,,,,,"0|i3wobj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"03/Aug/18 22:32;githubbot;GitHub user bschell opened a pull request:

    https://github.com/apache/hadoop/pull/407

    MAPREDUCE-7127. Add aggregated webservice endpoints to fetch all tasks & their taskAttempts

    There is a usecase to poll Hadoop for various running Tasks and display info on each individual task attempt to the user. To improve performance add additional endpoints to HS and AM webservices that will fetch aggregated data on hadoop tasks including their task attempts for a current job.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/bschell/hadoop bschelle/taskservice2

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/hadoop/pull/407.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #407
    
----
commit 5c314b7fd30c2af803d6099abda22ad2546ba5bb
Author: Scheller <bschelle@...>
Date:   2018-07-11T20:56:54Z

    Add aggregated webservice endpoints to fetch all tasks including their attempts
    
    There is a usecase to poll Hadoop for various running Tasks and display info on each individual task attempt to the user. To improve performance add additional endpoints to HS and AM webservices that will fetch aggregated data on hadoop tasks including their task attempts for a current job.

----
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hamster: Hadoop And Mpi on the same cluSTER,MAPREDUCE-2911,12520696,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Later,,milindb,milindb,30/Aug/11 07:04,04/Jun/18 19:54,12/Jan/21 09:52,22/May/12 17:36,0.23.0,,,,,,,,mrv2,,,,,,6,,,,,"MPI is commonly used for many machine-learning applications. OpenMPI (http://www.open-mpi.org/) is a popular BSD-licensed version of MPI. In the past, running MPI application on a Hadoop cluster was achieved using Hadoop Streaming (http://videolectures.net/nipsworkshops2010_ye_gbd/), but it was kludgy. After the resource-manager separation from JobTracker in Hadoop, we have all the tools needed to make MPI a first-class citizen on a Hadoop cluster. I am currently working on the patch to make MPI an application-master. Initial version of this patch will be available soon (hopefully before September 10.) This jira will track the development of Hamster: The application master for MPI.",All Unix-Environments,acmurthy,adiaz@hortonworks.com,ahmed.radwan,anty,ashutoshc,atm,avik_dey@yahoo.com,aw,azuryy,benoyantony,bowang,brainlounge,cdouglas,chl501,clarkyzl,cloudeagle_bupt,colorant,cutting,ddas,decster,devaraj,drankye,dreamquster,ekoontz,enis,erankimadhu,erik.fang,esohpromatem,fangy,fengshen,forest520,gsingers,hammer,Hamza100,hsaputra,ivanmi,jayf,jeagles,jlowe,john.huang,jrideout,kavn,kramachandran,leftnoteasy,lei_chang,lianhuiwang,lihui,lt_schmidt_jr,mahadev,milindb,naisbitt,nroberts,pfxuan,qwertymaniac,rahulv89,rajesh.balamohan,raviprak,revans2,rhc,russell.jurney,rvs,sandflee,sandyr,sharadag,shawnh,srivas,sseth,stevel@apache.org,sudhan65,szetszwo,Tagar,tgraves,thomas.jungblut,tlipcon,tomwhite,udanax,varun,vicaya,vinodkv,wangmeng,xicheng,,,,1209600,1209600,,0%,1209600,1209600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-08-30 07:15:10.006,,,false,,,,,,,,,,,,,,,,,,61803,,,,,Wed Apr 29 18:56:26 UTC 2015,,,,,,,"0|i0jjgv:",112084,,,,,,,,,,,,,,,,,,,,,"30/Aug/11 07:08;milindb;where should I place it in the source hierarchy ? Also, I am currently working off the trunk. IIn case, I get busy in other stuff, I do not want it to be blocker for 0.23.0. What's the timeline for 0.23.0 release ? I know that I wont be able to make it work on windows in the first version. I hope that does not become a blocker, too.","30/Aug/11 07:15;acmurthy;Milind, I'm more than happy to ship this in 0.23.0 if possible - else it can go in 0.23.1 if necessary. Thanks for starting work on this!","30/Aug/11 07:22;acmurthy;bq. where should I place it in the source hierarchy ?

One option is to place it alongside hadoop-mapreduce-client i.e. call it hadoop-openmpi-client. Other ideas?","30/Aug/11 07:23;milindb;The design is deliberately kept simple.

One script, ""start-mpi -np <numnodes> -out hdfs://user/milind/nodes.lst"" starts the application master, which requests <numnodes> containers from resource manager, and waits till all those containers become available. The ""job client"" polls for application master to write a file called nodes.lst in specified location on HDFS.

As containers become available, the application master spawns openmpi runtime environment daemon (orted) in each of those containers.

When job client notices that nodes.lst is available on HDFS, it downloads it to local directory, and exits.

MPI jobs are launched with regular:

mpirun -np <numnodes> -nodes <nodes.lst> executable

Multiple MPI jobs can be launched in the same virtual MPI cluster created by start-mpi script.

After all MPI jobs are done, the cluster is dismantled with

stop-mpi <nodes.lst>

(first line of nodes.lst contains application master location and port.)

Currently, there is no authentication for MPI job submission on the cluster started by the user. Thus, anyone can submit MPI jobs to any virtual MPI cluster. (I promise to do it in the next version.)

Also, if any of the container (running orte), exits abnormally, entire virtual MPI cluster is terminated. (This limitation will be removed in the next version.)

There is one issue I am currently facing. I need at most one MPI container per physical node (until I figure out how to avoid port conflicts etc). Any input regarding how to achieve that, is welcome.  My code walkthrough of resource manager did not suggest anything obvious.
","30/Aug/11 07:25;milindb;@Arun I will try my best to get the first version into 0.23.0 (but as noted above there will be a huge security hole.)","30/Aug/11 07:28;milindb;@Arun, hadoop-openmpi-client makes most sense (however, it also contains an app master.)","30/Aug/11 07:32;milindb;Just realized that if I make nodes.lst permissions 600, no other user will be able to accidentally submit jobs to the virtual MPI cluster (but malicious users can check the RM UI to see MPI AMs, and recreate nodes.lst.)","30/Aug/11 07:36;acmurthy;bq. I need at most one MPI container per physical node (until I figure out how to avoid port conflicts etc).

Do you need one container per node or the whole node?

I suspect you want the whole node to the 'virtual' cluster, if so just ask for all resources (RAM) on that node.

If you need one container per node then ask for 1 container on a node and only 1 on the rack - of course you can reject containers on 'known' nodes too.","30/Aug/11 07:45;milindb;@Arun No, I don't need the whole node. In fact, looking more into the orted code, multiple mpi daemons can be made to exist on the same node. So, no issue there.

Re: security, mentioned above, is the following acceptable?

1. MPI App Master generates a random passphrase, and writes into nodes.lst (which is 600).
2. Job client downloads nodes.lst to local directory (and preserves permissions).
3. mpirun reads nodes.lst and supplies this passphrase to orted when asking it to fork executable.
4. If passphrase matches, user-specified executable runs, otherwise it as an error. ","30/Aug/11 07:53;acmurthy;Milind, can you use ApplicationSubmissionContext.tokens or ContainerLaunchContext.serviceData?","30/Aug/11 10:45;sharadag;bq. hadoop-openmpi-client makes most sense (however, it also contains an app master.)

hadoop-mapreduce-client is a misnomer. It is not a client. I think we should fix it to have hadoop-mapreduce instead. Actual client is hadoop-mapreduce-client-jobclient which should be named as hadoop-mapreduce-jobclient. If it makes sense then we should do it separately.

hadoop-openmpi is looks better to me.


","30/Aug/11 15:51;vinodkv;This is a great addition! +1 for the simple design.

I guess the client spins till all the containers are allocated (Oh it's HOD all over again :) )

Like MR, the client could continuously ping the AM via RPC to know about the status of container-allocations. Presence/absence of HDFS file sure is a simple beginning.

bq. Also, if any of the container (running orte), exits abnormally, entire virtual MPI cluster is terminated. (This limitation will be removed in the next version.)
How does the mpi client get to know about this? It'd be great if mpi-run automatically detects this.

Security can be postponed for the first-cut.

For now, +1 to hadoop-openmpi module under hadoop-mapreduce-project. Once we have this, we will have a platform(yarn) and two frameworks(MR and MPI) using it. Then we can move out yarn out of mapreduce-project, mapreduce and MPI can move into a frameworks aggregation module (the app-store, gawd, I can't believe I just said that :) )","31/Aug/11 03:26;milindb;Progress report (I wish more people do this on a daily basis, for the Jiras they are working on.):

Had a great conf call with Jeff Squyres and Ralph Castain (both at Cisco, OpenMPI stewards). Both were excited that openMPI is getting hadoop co-existence. They suggested that I base Hamster implementation on the ""direct-launch"" model, such as with slurmd. (Having used slurm in the past, I understand why :-)

So, the design described above has changed. Now, the way to launch MPI jobs on a hadoop cluster is simply:

{code}
hamster -np 32 a.out
{code}

""hamster"" is a client application that connects to the RM, asks it to create an AM with 32 containers, and after all of those are launched, executed a.out inside them, after setting some environment variables for connecting to the AM to get the ""node list"".

That way, there are no security holes (as described above), since the MPI cluster exists only for the duration of the job.

@Vinod, If you remember, I had sent an email on a Y-internal mailing list that HoD will make a comeback. This is it. HoD was loved very much, especially after you and Hemanth took over, in terms of stability. As I had said in that email, once the container abstraction is solidified, HoD will make a comeback. So, here it is. (Afterall, creating a slice of a shared resource has worked for the last 40 years, why won't it work now ?)","31/Aug/11 06:17;acmurthy;{quote}
""hamster"" is a client application that connects to the RM, asks it to create an AM with 32 containers, and after all of those are launched, executed a.out inside them, after setting some environment variables for connecting to the AM to get the ""node list"".
{quote}

Milind, I'm fairly ignorant about OpenMPI - does this mean you don't need a 'unix process' other than the one running a.out itself? Or, do you still need to run some other daemon to then fork a.out?","31/Aug/11 17:44;vicaya;Couple of comments based on what I've seen so far:
# We should not pick a specific MPI implementation (Open-MPI), which is not secure (unlike the current Hadoop RPC, Open-MPI wire protocols are not ""secure"" (with caveats)). The only secure MPI implementation that I'm aware of is a research project called ES-MPICH2, which is of course based on MPICH2.
# Use mpiexec instead mpirun by default as the former is specified by the MPI standard.
# Make environment management commands (mpiexec etc.) configurable.

I'm on vacation, so expect high latencies in response :)","31/Aug/11 18:37;milindb;@Arun, the direct-launch method requires that certain environment variables are set that a.out can access. At a minimum, number of ""nodes"", and the host and port of the head node (i.e. process with rank 0) need to be available to all processes. Thus, we will have a tiny process that sets these environment variables, and launch a.out. When a.out calls MPI_Init(), the MPI library code will read these env vars, wait till all the processes have reported to the head node, and start execution.","31/Aug/11 18:40;milindb;@Luke

1. Communication among processes in Hadoop, i.e. map output that gets consumed by reduce input, is not encrypted. I think un-encrypted communication among MPI processes should be acceptable.
2. mpiexec is used by MPI-2, and OpenMPI supports that.

Can you elaborate on your third point ?","31/Aug/11 18:42;acmurthy;Milind, you can set the environment of a.out directly when launching it (@see ContainerLaunchContext) - so you might not need the extra process to launch a.out?","31/Aug/11 18:54;vicaya;bq. I think un-encrypted communication among MPI processes should be acceptable.

Hadoop RPC is authenticated but not encrypted under the assumption that there is no untrusted root process in the cluster, which is reasonable. Open-MPI doesn't (securely) authenticate connections and messages, which is not acceptable.

bq. Can you elaborate on your third point?

This allow users to pass non-standard options (other than -n) to mpirun/mpiexec.","31/Aug/11 18:59;milindb;@Arun, I do not see how a ContainerLaunchContext can get the hostname and port of the 0'th container (which is the head node). (I remember Jerry had worked around this problem by making the JobClient as a 0th process. But having a gateway execute heavy-duty code is not good.)","31/Aug/11 19:00;vicaya;bq. mpiexec is used by MPI-2, and OpenMPI supports that.

My point was that MPI-2 is the current standard and that if we stick to the standard, the setup can be used with other MPI implementations, such as the popular MPICH2.","31/Aug/11 19:01;milindb;@Luke, how do you prevent map tasks opening sockets, receiving connections, and communicating with each other in Hadoop ? Isn't that the same case here ?","31/Aug/11 19:14;vicaya;bq. how do you prevent map tasks opening sockets, receiving connections, and communicating with each other in Hadoop ? Isn't that the same case here ?

Map tasks that do above is not secure, that's why we have the hadoop security which authenticates all hadoop protocols, so that normal map/reduce tasks are secure. By making the jira work with the MPI standard and making the non-standard portion configurable, you allow users to pick a secure MPI implementation, such as ES-MPICH2.","31/Aug/11 19:49;milindb;@Luke, the problem with mpiexec is it's license:

""Mpiexec is free software and is licensed for use under the GNU General Public License, version 2.""

OpenMPI, on the other hand, is BSD-licensed, and implements the MPI-2 standard.

Re: security, in this implementation, the MPI processes are launched with authentication, they run as the submitting user, and what they do in the user-code is equivalent to what users can do in map-reduce code, so I do not see an issue here.","31/Aug/11 20:08;vicaya;{quote}
the problem with mpiexec is it's license:

""Mpiexec is free software and is licensed for use under the GNU General Public License, version 2.""

OpenMPI, on the other hand, is BSD-licensed, and implements the MPI-2 standard.
{quote}

You're confusing mpiexec the software from OSC with mpiexec the standard specified in MPI standard (MPI-2.2 section 8). OpenMPI includes an mpiexec executable, as many other implementations do.

bq. what they do in the user-code is equivalent to what users can do in map-reduce code, so I do not see an issue here.

They're not equivalent, as normal mapreduce tasks's communication cannot be attack by other users. The Open MPI implementation is somewhat equivalent to a user implementing extra insecure protocols in map/reduce tasks in addition to the standard secure hadoop mapreduce protocols.","09/Sep/11 16:55;milindb;Sorry folks. I got distracted this week by some mind-numbing non-technical stuff. Progress on hamster was slow, as a result. Since I will be travelling next week, hoping to find some time to work on it :-)","25/Nov/11 16:33;rhc;Let me preface my comment by confessing my current ignorance of Hadoop. I'm working on rectifying that situation, but won't claim to be anywhere close to fully understanding it.

That said, I'm wondering if it is possible to simply run the MPI processes as standard Hadoop processes? I confess this was my initial thought. Rather than creating a cluster and using mpirun, just have the user start a standard Hadoop job - but with the processes being part of an overall MPI application. Thus, the processes would all call MPI_Init, execute as an MPI application, call MPI_Finalize, and then exit. If a user wants to integrate that application with MapReduce, more power to them - I can see some cases where that would be of interest.

My point here is that you don't need mpirun at all, nor do you need all the overhead of running OMPI daemons. The Hadoop daemons can start and monitor the state of health of the MPI processes just fine. We might add some capability to the Hadoop daemons to assist (e.g., binding), but those would be of use regardless of whether or not the process is part of an MPI application.

As I said, please forgive the ignorance if my suggestion makes no sense.
","25/Nov/11 16:41;rhc;Ah - my bad. I didn't realize I was looking at the comments in reverse order :-)

After reading the comments in the correct order, I now better understand the thread and see that Milind is following what I had suggested. As to the discussion of secure communications, this is a continuing issue in the MPI community. The problem is that securing at the message level creates considerable overhead and severely impacts MPI performance.

What the community has chosen to do is secure at the user level, and then check socket connections to ensure we are talking to someone from within our own application. Thus, we launch based on ssh-like authentication requirements. During MPI_Init, we wireup socket connections. As each connection is made, we exchange an initial ""ident"" message that checks to ensure that the process on the other end is a member of our application. If it isn't, we drop the connection.

If you want to add further security during the socket formation phase, nobody will object - though we might put it on a configuration basis so others aren't impacted as it will slow down launch times on very large clusters.

HTH
Ralph
","08/Dec/11 01:49;forest520;This is a very helpful feature for scientific algorithms, hope it will release soon","13/Feb/12 17:44;fintler;Although it seems like this project is very interesting, I'd like to draw some attention to Apache Mesos.

http://www.mesosproject.org/

It allows one to run several different versions of Hadoop, Torque, and MPI on the same cluster at the same time. For example, a map-reduce job can be running on the same cluster at the same time an MPI job is.","19/Mar/12 17:04;milindb;Ralph is taken this project over from me, and is very close to a patch.","11/Apr/12 03:11;acmurthy;Milind/Ralph - Any update on Hamster? Tx.","11/Apr/12 10:04;rcastain;I'm afraid our optimism about a near-term patch proved a little too hopeful. Milind's initial prototype (based on advice from me, before I really understood the situation) won't work on multi-tenant systems, so another method had to be developed. After spending a couple of months beating on this, I've pretty much put it on hold for now as I pursue an alternative approach.

Adding MPI support to Yarn has proven to be very difficult, and may not be worth the pain. The problems stem from some basic Yarn architectural decisions that run counter to HPC standards. For example, the linear launch pattern creates scaling behaviors that are objectionable to HPC users, who generally consider anything less than logarithmic to be unacceptable. All HPC RMs meet that requirement, with many of them scaling at better than logarithmic levels. The result is striking: on a 64-node launch, Yarn will take several seconds to start the job - whereas an HPC RM will start the same job in milliseconds.

Similarly, the lack of collective communication support in Yarn means that MPI wireup scales quadratically under Yarn with the number of processes. Contrast this with a typical HPC installation where wireup scales logarithmically, and remember that wireup is the largest time consumer during MPI startup, and you can understand the concern. As a benchmark, we routinely start a 3k-node, 12k-process job (including MPI wireup) in about 5 seconds using Moab/Torque.

Finally, when we compared fault tolerance performance, we didn't see a significant difference between Yarn and the latest HPC RM releases. Both exhibited similar recovery behaviors, and had similar multi-failure race condition issues.

Just to be clear, I'm not criticizing Yarn - the other RMs had similar behaviors at a corresponding point in their development (including my own past efforts in that arena!). Remember, today's HPC RMs each can boast of roughly 50-100 man-years of development behind them, and have undergone several cycles of architectural change to improve scalability, so one would naturally expect them to out-perform Yarn at this point.

There are other issues, including the difficulty of getting a Yarn AM to actually work. However, the impetus behind the ""hold"" really was the above observations, combined with an overwhelmingly negative reaction from the HPC community when I asked about using Yarn on general purpose (Hadoop + non-Hadoop apps) clusters. In contrast, I received a correspondingly positive reaction to the idea of running Hadoop MR on a general purpose cluster, separating that code (plus HDFS) from Yarn.

I have therefore started pursuing this option, which proved to be much easier to do. I expect to have an ""early adopter"" version of MR/HDFS on an HPC cluster sometime in the next week or two, with a general release this summer (aided by other members of the HPC community who have volunteered their help).

Of course, I realize that there will be people out there that decide to run Yarn on their Hadoop systems (as opposed to worrying about general purpose clusters), and that they might also be interested in using MPI. So I'll return to this after I get the HPC problem solved, with the caveat that such users understand that the scaling and performance will not be what they are used to seeing on non-Yarn systems.

HTH
Ralph
","11/Apr/12 16:41;milindb;Thanks for the summary & status, Ralph. It is true that not a lot of Hadoop users so far are concerned with the job startup time, given that the job typically runs for several minutes. (In my experience, the hadoop clusters are heavily oversubscribed, and job might even be pending in the queue for several minutes before even the first task is started.)

So, I think a slow wireup should be acceptable to the hadoop community, and there is a value in having hamster use yarn. (Since your second approach of plugging in a HPC RM can remain modularly separate, one can have both implementations, increasing the choice for users, which is good, IMHO.)","11/Apr/12 18:05;acmurthy;Ralph, thanks for the heads up. It's disappointing it's some way away...

Regarding your observations, but no offense taken, some clarifications:

Foremost, there are a couple of reasons why the container launch on YARN should be more than sufficient:
# People using MPI on Hadoop are generally running very heavy computation models which run for several minutes at least if not for several hours. In fact, I've seen lots of MPI-like jobs run on Hadoop MapReduce which run for several days! In such cases a few seconds is really pure noise. (Also, we've added several features to CapacityScheduler to actually support such apps in hadoop-1.x.)
# Hadoop clusters are also, typically, heavily over-subscribed which means tasks (containers) spend a lot of time in queue waiting for resources to be available.

Having said that, there is no doubt we are focussed on improving YARN. It's currently significantly faster than MapReduce Classic (JobTracker/TaskTracker) in hadoop-1.x for container (task) launch etc. and we have more improvements up our sleeve for the future.

----

A very important factor for container launch in YARN (as with current MR) is that the system itself takes responsibility for 'setting up' the container by copying it's jars/shared-objects/binaries from HDFS. An easy way to speed up container launch is to pre-distributed MPI binaries on all nodes to do a perf-comparision. Is this something you've already tried? 

In contrast you probably, already, did that on Torque/Moab/Maui since they have to be distributed before the MPI 'job' is launched?

----

Regarding 'collective communication support' can you please add more colour? 

With my naive understanding of the MPI world, I don't see why the same mechanisms can't be used in YARN. 

YARN doesn't mandate application communication mechanisms and just leaves it up to the applications. So, I fail to see why this comes up for MPI applications - happy to be educated! *smile*

----

Regarding your current area of focus i.e. running Hadoop MapReduce (and HDFS?) on Torque etc., this is something we in the Hadoop community have already tried (and abandoned) with ""Hadoop On Demand"" i.e. HoD system (HoD jira: HADOOP-1301. HoD removed via HADOOP-7137). There are several reasons why we went away, and too much water under the bridge.

----

Overall, I'm sure there are several people in Hadoop community who would love to use MPI on Hadoop/YARN, they already do that by bending Hadoop MapReduce in myriad & scary ways! I'm sure Milind, like me, has seen several examples! *smile*

I can see why some of the issues you've brought up (milisec latency etc.) matter to folks in MPI world, in some ways it's a clash of the Hadoop and HPC worlds and the fallout is reasonable & expected! *smile*

However, I still believe Hamster, as envisaged here, has a lot of value to the Hadoop community, if you choose to focus on it. Thanks!","11/Apr/12 18:46;milindb;I mostly agree with Arun (except for HoD part). Having a low-latency communication mechanism in Hadoop world is valuable. Applications on this platform will not be typical HPC applications, but mostly iterative machine-learning applications that require crunching more input data, and produce models (small output data). Many are using sub-optimal ways of launching iterative computations on Hadoop anyway, and having even an inefficient MPI launch will be more efficient than what people resort to today.

Arun, HoD is now viable. More later :-)","11/Apr/12 18:50;milindb;One major issue that I faced early on in my Hamster prototype, is that hadoop RPC is still not completely java-independent, even with protobufs, especially when authentication comes into play. Thus, native C clients cannot be used with Yarn RM. This is one of the major roadblocks that Ralph faced as well, and had to go through a lot of trouble to get around that. Arun, what is the roadmap for that ?","11/Apr/12 18:52;rcastain;Hi Arun

No offense taken at all - hopefully both directions. I'm pretty agnostic on these things after working with them all over the years. Every one has its pros/cons. :-)

I'll try to address your points in sequence. I'd be happy to drop by some time (I'm in SF frequently these days) and chat about it (higher bandwidth is often helpful).

------------------------------------------
First, people on HPC clusters also sit for some time in the queue waiting for resources. It is a very odd day when you can just jump onto a system! However, launch time is still a critical issue on such clusters since they typically operate at scale. Seconds add up over time.

MPI jobs typically do run for long times, though surveys report that roughly 70% of them run for one hour or less (but more than a few minutes). Although we might agree that a few seconds shouldn't matter, the fact is that users will loudly protest such delays, especially when many of the MPI jobs are actually run interactively during development. Startup time is a very sensitive issue in HPC - my back has the scars to prove it.

Without changing the basic design of Yarn, I don't see how Yarn can ever really compete in this area. The fact that the RM has to wait to be called via heartbeat by the nodes in order to do the allocation is a bottleneck, and the requirement that we launch one container at a time (and copy binaries et al), will be difficult to overcome.

--------------------------------------

HPC systems are always supported by NFS mounts of home directories as well as system libraries - we never use local disks due to the management headache they create. User requirements for different library versions are handled via the ""modules"" system. We have found ways to make this scalable to over 100k nodes without significant startup time. Of course, the really big clusters do this with sophisticated network design (limiting the number of nodes served by each NFS server, multiple subnets) and TCP-over-IB protocols running at ~50Gbps, so systems connected via lesser networks can't compete. On the other hand, smaller clusters do quite well with appropriately laid out Ethernet support.

Thus, we never pre-distribute binaries, even on the largest clusters (jars aren't an issue as you have to look hard to find any Java applications in HPC). We also never build static libraries as memory is at a premium for HPC jobs, so everything is dynamically linked. On the larger systems, there sometimes are bandwidth restrictions on the NFS channels - in these cases, we disable dlopen so that the MPI libraries are rolled up into one dynamic lib to minimize the number of NFS calls. The launch example I gave was from such a configuration.

--------------------------------------

Clearly, Yarn could provide collective communication support if it chose to do so. However, it would result in a fundamental change in the Yarn architecture.

In the HPC world, the RMs operate in one of two modes. Some, like SLURM, provide their own wireup support. In these cases, each process provides its endpoint information to the local RM daemon. Those daemons in turn share that info across all other daemons hosting processes from that job using collective operations designed for large scale. Application processes are then either passed the resulting info (assembled from all procs in the job), or can query the local RM daemon to obtain the pieces they need.

Some RMs, like Torque, do not offer these services. In those cases, the MPI implementation itself provides it by first launching its own daemons on each node in the job. These daemons then assume the role played by the above SLURM daemon, circulating the endpoint info using similar collective operations. While it might seem that this has to take longer due to the additional daemon launch, it actually is competitive as the startup time (given the scalable launch) for the daemons is really quite small, and the time required to share info across the procs (called the ""modex"" operation) is much longer.

Translating these methods to Yarn would require that the Nodemanagers learn how to communicate with each other, which seems to me to break the Yarn design. I admit I could be wrong here, so please feel free to consider adding node-to-node collective communications. It isn't terribly hard to do, but it does take some work to make it robust.

---------------------------------------

I have looked at HoD and can understand the problems. The mistake (IMHO) was to attempt to run it directly under Torque. This has its limitations. We avoid those by actually using an adaptation of OMPI's mpirun tool, which eliminates most of the problems. So far, it seems to be working quite well, though there is more to be done and always unforeseen problems to resolve.

---------------------------------------

I suspect there are folks out there who will use Yarn and have interest in using MPI. However, my assignment is to focus on creating the ability to dual-use clusters as this has become a limiting issue, so that's where my emphasis currently lies. Once that is complete, and assuming the powers-that-be agree, I will take another look at it. Perhaps by then the pain will have lessened - frankly, I find Yarn to be very difficult to work with at the moment.

HTH
Ralph
","11/Apr/12 20:20;acmurthy;Ralph, thanks again for the discussion.

If I wasn't clear before, I'd like to re-emphasize that I look at Hamster as a way to bring MPI apps to the Hadoop world in a first-class manner (as opposed to the hacks employed so far), not necessarily as a way to bring YARN to the MPI world to which I'm agnostic (too). 

I fully respect that the HPC world has it's own conventions and expectations which isn't really what YARN is geared to solve.

As a result, with some engineering effort I'm confident Hamster could be a really useful addition to the Hadoop toolkit even with some of the current issues you pointed out.

----

bq. MPI jobs typically do run for long times, though surveys report that roughly 70% of them run for one hour or less (but more than a few minutes). 

Like I said, I'm sure it matters in the HPC world, but I'm pretty sure it's much less of an issue in the Hadoop world even as we work to improve container launch etc. As a result, I'm not very worried (at this stage) about this aspect for Hamster on YARN. 

Given your above stat, I'm even more confident Hamster should be fine presently.

bq. the requirement that we launch one container at a time (and copy binaries et al), will be difficult to overcome

I'm not sure I follow. There is no requirement to launch one container at a time, how did that come up? An ApplicationMaster can, and should, launch multiple containers via threads etc. The MR AppMaster does that already.

----

bq. the really big clusters do this with sophisticated network design (limiting the number of nodes served by each NFS server, multiple subnets) and TCP-over-IB protocols running at ~50Gbps, so systems connected via lesser networks can't compete

That's fundamentally the reason Hadoop MapReduce Classic and YARN use HDFS to make the system more scalable/easy-to-manage with the other characteristics it brings along.

----

bq. please feel free to consider adding node-to-node collective communications

In the YARN design, the wireup is facilitate via the application's AppMaster. Thus each worker process just needs to share it's info with it's AppMaster for wireup. Again, the MR AM already does this for MR jobs.

----

Overall, as disappointed I am, I can understand that your priorities lie elsewhere.

OTOH, I'd really appreciate if you could share your still-born code or even a prototype. I'm pretty sure there will be volunteers to take it forward since I know of sufficient interest in running MPI on YARN. Thanks!
","11/Apr/12 20:59;stevel@apache.org;Classic HPC systems contain some expections that aren't present in normal Hadoop clusters

# low-latency IPC & APIs to access it for messaging
# shared location-independent filesystem (e.g. SAN, GPFS, ...) 
# highly calibrated C++ compiler and library systems to guarantee consistent output of FPU- and GPU- intensive code.
# C/C++ code with lots of MPI code scattered through out for synchronization.
# either checkpointing of long-lived work or timespan limits for jobs.

There are some other features that aren't necessarily design goals but have arisen
# lots of smaller per-facility sites with different architectures rather than one or two large-scale datacentres.
# limited local storage (due to cost of storage solution)
# a reliance on zero-costed internet-2/SuperJanet/... interconnect to pull data from shared repositories (for CERN, the national tier-1 sites such as Rutherford Appleton Laboratories)
# high operations cost relative to storage and compute capacity (due to small, heterogenous, multi-site clusters, selected storage solutions)


If you look at the UK university grid [http://pprc.qmul.ac.uk/~lloyd/gridpp/ukgrid.html] you can see that although there are lots of clusters, they are of limited storage capacity -that storage also forces you to choose where to run the work or rely on job preheating to pull it in from RAL or elsewhere. (latency to do this is lower than pulling off tape). You can also see that there are lot of jobs in the queues, including short-lived health tests that verify work reaches the expected answers. I don't know about the duration/needs of the actual work jobs.

When you consider job startup delays you have to look at time to fetch data over long-haul connections, maybe compile code for target cluster, and recognise that without a SAN you can't expect uniform access times to all data.

What you would get from MPI over hadoop is the ability to run MPI work on the cluster -a cluster which, if it also had infiniband on, would have low-latency interconnections. (yes, there is a cost for that, but you may want it for a shared cluster). 

What about an MPI mechanism that has a Grid Scheduler that block-rents a set of machines that an then be used for multiple jobs off the MPI queue, and which aren't released after each job? Once the capacity on the hosts is allocated, health checks can verify the machines work properly, then it can await work. The scheduler can look at the pending queue and flex its set of machines based on expected load?

Job startup would be reduce to the time to push out work to the pre-allocated hosts, which doesn't need to rely on heartbeats and could use Zookeeper or other co-ordination services.

This wouldn't be a drop in replacement for one of the big supercomputing clusters, but it would let people run MPI jobs within a Hadoop cluster. 
","11/Apr/12 21:16;rcastain;Appreciate your thoughts. I think we generally agree on purpose, but maybe not so much on method. In my mind, bringing map-reduce to the MPI world in a first-class manner is a simpler, and ultimately more useful, solution as HPC clusters already exist and organizations know how to manage them. Certainly won't be true at places like Yahoo, but much of the business world has HPC systems (albeit of small size) in various depts.

{quote}MPI jobs typically do run for long times, though surveys report that roughly 70% of them run for one hour or less (but more than a few minutes).

Like I said, I'm sure it matters in the HPC world, but I'm pretty sure it's much less of an issue in the Hadoop world even as we work to improve container launch etc. As a result, I'm not very worried (at this stage) about this aspect for Hamster on YARN.
{quote}

I think this is largely an issue of scale. While Hadoop is deployed on large clusters, it seems to generally run in small jobs - i.e., a job consisting of 100 procs would be considered fairly large. In the HPC world, a 100 proc job is considered tiny. Those 1 hour jobs typically consist of hundreds to thousands of processes. Using the observed scaling behavior, Yarn would take the better part of an hour to launch and wireup an MPI job of that size.

{quote} the requirement that we launch one container at a time (and copy binaries et al), will be difficult to overcome {quote}

{quote} I'm not sure I follow. There is no requirement to launch one container at a time, how did that come up? An ApplicationMaster can, and should, launch multiple containers via threads etc. The MR AppMaster does that already.
{quote}

Yes/no. Unfortunately, MPI procs need to know who is collocated with them on a node at startup - otherwise, you make a major sacrifice in performance. So the AM cannot start launching until ALL nodes have been allocated. While it's true you can then make a threaded launcher to help reduce the time, it's still pretty much a linear launch pattern when you plot it out.

{quote{
the really big clusters do this with sophisticated network design (limiting the number of nodes served by each NFS server, multiple subnets) and TCP-over-IB protocols running at ~50Gbps, so systems connected via lesser networks can't compete

That's fundamentally the reason Hadoop MapReduce Classic and YARN use HDFS to make the system more scalable/easy-to-manage with the other characteristics it brings along.
{quote}

Again, a question of perspective. The HPC world doesn't use the TCP networks for storing data files - only application programs. The data sits on parallel file systems that experiments have shown to be comparable or slightly faster than HDFS, and are easily managed. Again, HPC depts are familiar with these systems and know how to manage them.

Keep in mind that HPC applications consume only kilobytes to a few megabytes of input data - but generate petabytes of output. So the problem is the inverse of what HDFS attempts to address. Where HDFS might come into play in HPC is therefore more in the visualization phase, where those petabytes are consumed to produce megabyte-sized images at video rates.

The interest I've received has come more from the data analysis folks who have large amounts of data, but wish to utilize existing HPC clusters to analyze it with MR techniques. They may or may not use HDFS to do so - depends on the system administration.


{quote}please feel free to consider adding node-to-node collective communications

In the YARN design, the wireup is facilitate via the application's AppMaster. Thus each worker process just needs to share it's info with it's AppMaster for wireup. Again, the MR AM already does this for MR jobs.
{quote}

And therein lies the problem. Each worker process has to contact the AM, thus requiring a socket be opened and the required endpoint info sent to the AM, which must consume it. The AM must then send the aggregated info separately to each process. Result: quadratic scaling. Threading the AM helps a bit, but not a whole lot.

To give you an example, consider again my benchmark system (3k-node, 12k-process, launch and wireup in 5sec). Using Yarn, the launch + wireup time computes to nearly an hour. We are very confident in those numbers because we have tested similar launch methods back in the days before we updated the OMPI architecture. In fact, we updated our architecture in response to those tests.

So for small jobs, Yarn is fine. My concern is to provide an environment that can both run the small Hadoop MR jobs *and* support the larger HPC jobs, all on the same cluster. Creating a medium-performance version of MPI for Hadoop under Yarn is a fairly low priority, to be honest.

Ralph
","11/Apr/12 21:28;rcastain;Hi Steve

{quote}If you look at the UK university grid http://pprc.qmul.ac.uk/~lloyd/gridpp/ukgrid.html you can see that although there are lots of clusters, they are of limited storage capacity -that storage also forces you to choose where to run the work or rely on job preheating to pull it in from RAL or elsewhere. (latency to do this is lower than pulling off tape). You can also see that there are lot of jobs in the queues, including short-lived health tests that verify work reaches the expected answers. I don't know about the duration/needs of the actual work jobs.

When you consider job startup delays you have to look at time to fetch data over long-haul connections, maybe compile code for target cluster, and recognise that without a SAN you can't expect uniform access times to all data.
{quote}

A grid is very different from an HPC cluster, which are far more common (grids have been dying out over the last few years). We never see data pulled over long-haul connections - frankly, you don't see people doing it any more on grids either due to the unreliability and delays in delivery. HPC clusters are almost always homogeneous (I think I've seen two heterogeneous HPC clusters outside of a lab so far), and generally are backed by a parallel file system that actually does provide pretty uniform access times. Remember: MPI jobs use MPI-IO to fetch/write data, and they write a lot more data than they read (as per my prior note).

Thus, once an allocation is given, there is no startup delay like you describe. There is some time required to load binaries and libs onto each node, but that scales well and goes very fast. As per my other note, we figured out how to solve that a while back. :-)


{quote}What you would get from MPI over hadoop is the ability to run MPI work on the cluster -a cluster which, if it also had infiniband on, would have low-latency interconnections. (yes, there is a cost for that, but you may want it for a shared cluster).
{quote}

Agreed - so long as the MPI job is small enough, it should work.

{quote}What about an MPI mechanism that has a Grid Scheduler that block-rents a set of machines that an then be used for multiple jobs off the MPI queue, and which aren't released after each job? Once the capacity on the hosts is allocated, health checks can verify the machines work properly, then it can await work. The scheduler can look at the pending queue and flex its set of machines based on expected load?

Job startup would be reduce to the time to push out work to the pre-allocated hosts, which doesn't need to rely on heartbeats and could use Zookeeper or other co-ordination services.

This wouldn't be a drop in replacement for one of the big supercomputing clusters, but it would let people run MPI jobs within a Hadoop cluster.
{quote}

I'm not sure how that would work - I guess you would have to interface something like OGE/SGE to Yarn so that it could ""rent"" machines from Yarn? As Milind noted, that interface is non-trivial today. I've talked to the GE folks about it (as well as to the other major HPC RM orgs), but they don't have much interest in providing such a capability - they are far more interested in the reverse approach (i.e., running MR on an HPC cluster).

Situation could change as time passes and the interface stabilizes/becomes easier.

HTH
Ralph
","11/Apr/12 21:31;rcastain;Ah heck - fix the quotes. Sorry about that...


Appreciate your thoughts. I think we generally agree on purpose, but maybe not so much on method. In my mind, bringing map-reduce to the MPI world in a first-class manner is a simpler, and ultimately more useful, solution as HPC clusters already exist and organizations know how to manage them. Certainly won't be true at places like Yahoo, but much of the business world has HPC systems (albeit of small size) in various depts.

{quote}MPI jobs typically do run for long times, though surveys report that roughly 70% of them run for one hour or less (but more than a few minutes).

Like I said, I'm sure it matters in the HPC world, but I'm pretty sure it's much less of an issue in the Hadoop world even as we work to improve container launch etc. As a result, I'm not very worried (at this stage) about this aspect for Hamster on YARN.
{quote}

I think this is largely an issue of scale. While Hadoop is deployed on large clusters, it seems to generally run in small jobs - i.e., a job consisting of 100 procs would be considered fairly large. In the HPC world, a 100 proc job is considered tiny. Those 1 hour jobs typically consist of hundreds to thousands of processes. Using the observed scaling behavior, Yarn would take the better part of an hour to launch and wireup an MPI job of that size.

{quote} the requirement that we launch one container at a time (and copy binaries et al), will be difficult to overcome {quote}

{quote} I'm not sure I follow. There is no requirement to launch one container at a time, how did that come up? An ApplicationMaster can, and should, launch multiple containers via threads etc. The MR AppMaster does that already.
{quote}

Yes/no. Unfortunately, MPI procs need to know who is collocated with them on a node at startup - otherwise, you make a major sacrifice in performance. So the AM cannot start launching until ALL nodes have been allocated. While it's true you can then make a threaded launcher to help reduce the time, it's still pretty much a linear launch pattern when you plot it out.

{quote}
the really big clusters do this with sophisticated network design (limiting the number of nodes served by each NFS server, multiple subnets) and TCP-over-IB protocols running at ~50Gbps, so systems connected via lesser networks can't compete

That's fundamentally the reason Hadoop MapReduce Classic and YARN use HDFS to make the system more scalable/easy-to-manage with the other characteristics it brings along.
{quote}

Again, a question of perspective. The HPC world doesn't use the TCP networks for storing data files - only application programs. The data sits on parallel file systems that experiments have shown to be comparable or slightly faster than HDFS, and are easily managed. Again, HPC depts are familiar with these systems and know how to manage them.

Keep in mind that HPC applications consume only kilobytes to a few megabytes of input data - but generate petabytes of output. So the problem is the inverse of what HDFS attempts to address. Where HDFS might come into play in HPC is therefore more in the visualization phase, where those petabytes are consumed to produce megabyte-sized images at video rates.

The interest I've received has come more from the data analysis folks who have large amounts of data, but wish to utilize existing HPC clusters to analyze it with MR techniques. They may or may not use HDFS to do so - depends on the system administration.


{quote}please feel free to consider adding node-to-node collective communications

In the YARN design, the wireup is facilitate via the application's AppMaster. Thus each worker process just needs to share it's info with it's AppMaster for wireup. Again, the MR AM already does this for MR jobs.
{quote}

And therein lies the problem. Each worker process has to contact the AM, thus requiring a socket be opened and the required endpoint info sent to the AM, which must consume it. The AM must then send the aggregated info separately to each process. Result: quadratic scaling. Threading the AM helps a bit, but not a whole lot.

To give you an example, consider again my benchmark system (3k-node, 12k-process, launch and wireup in 5sec). Using Yarn, the launch + wireup time computes to nearly an hour. We are very confident in those numbers because we have tested similar launch methods back in the days before we updated the OMPI architecture. In fact, we updated our architecture in response to those tests.

So for small jobs, Yarn is fine. My concern is to provide an environment that can both run the small Hadoop MR jobs *and* support the larger HPC jobs, all on the same cluster. Creating a medium-performance version of MPI for Hadoop under Yarn is a fairly low priority, to be honest.

Ralph
","07/May/12 08:43;dinkar_s;Hi Milind,

I discussed this with some of the Hadoop contributors (Devaraj Das, Arun Murthy, and Sharad Agarwal), and they suggested that the original approach might be a good project to take forward with a couple of students (I am teaching as a visiting faculty at PESIT, one of the best CS institutes here in Bangalore). Could you please post any code that you have written? I could use that as a base to start.

Regards,
Dinkar","17/May/12 15:59;milindb;I am excited to report that, thanks to great efforts by Ralph Castain and Wangda Tan, Hamster (i.e. OpenMPI on Yarn) now works flawlessly, and is scheduled to be merged to OpenMPI trunk soon. This effort was equivalent to building a second floor on a mobile home while it was hurtling down the freeway at 65 MPH :-) Thanks to both Ralph & Wangda.

According to Ralph:

""Lots of cleanup and documentation to do, and performance sucks per HPC
standards. But at least it works!""

To my knowledge, this is the first application framework implemented in C that uses the multi-lingual protobuf APIs for Yarn. (For secure environments, a small java-based shim is needed.)

Also, it is encouraging that no changes were needed in Yarn to make resource allocation work for MPI. (MPI as a standard came along in 1994, 18 years before Yarn was designed.)

Currently, using MPI-IO functionality in MPI requires a shared posix  file-system mounted on every node. However, this will change in future. For some distributed file systems (*cough*), which offer posix interface, MPI-IO works today.

Once it is decided whether BigTop can include Non-ASF packages, we plan to work with BigTop community to integrate OpenMPI (new BSD-licensed) in the big data stack.

I am closing this issue as fixed.","17/May/12 17:28;acmurthy;Ralph/Milind, this is great to hear!

Do you have pointers to the code for me to play with? Thanks!","17/May/12 17:34;rhc;Hi Arun

I should be committing it later tonight, or perhaps tomorrow. I'll send out a note once I do, including a pointer to the OMPI nightly snapshot link where the code can be obtained.

Ralph




","17/May/12 18:08;acmurthy;Great, thanks Raplh - really look forward to it!

Also, you/Milind might want to send a note out to Hadoop lists (general@) when the code is available so it's widely distributed.","22/May/12 13:18;varun;Hi Ralph,

I was curious to see the code. Which branch has it been committed to?

If there is more work needed on other parts related to Hamster i would love to contribute.","22/May/12 17:36;acmurthy;bq. I was curious to see the code. Which branch has it been committed to?

To clarify, there was no code committed to Hadoop itself.

Ralph - Can you please share information on where it was committed to OpenMPI and how folks can try it out? Thanks.","02/Jul/12 07:27;clarkyzl;I am curious to see the pointer to the OMPI nightly snapshot link. Where the code can be obtained?
Can you please share information on where it was committed to OpenMPI and how folks can try it out?","07/Jan/13 07:38;rahulv89;So where is the code for the same? Any pointers?","07/Mar/13 04:01;cloudeagle_bupt;Where could download the  available version?  thanks.","07/Mar/13 04:14;milindb;The ""community"" just created a huge issue for me to make this available to the community, by naming us ""anti-community"". So, while I am trying to get this available to the community, I have to now a few more obstacles to overcome. Please bear with me, or better still try to stop the ""community"" to stop their bile-spewing against us, so that we can navigate through this mess.



","07/Mar/13 08:37;qwertymaniac;Where exactly did all that naming you refer to, happen? I've not noticed it on the lists and there's been a few asks there as well (IIRC), but no negativism ever came in on its responses. I do not see any 'bile-spewing' on this very ticket either. So what ""community"" are you pointing this onto?

Thanks for still working on getting this available though, there are several people interested in this!","07/Mar/13 13:51;acmurthy;I have the same questions as Harsh.

On May 17, 2012 Ralph said he was close to committing this to OpenMPI, as mentioned on this jira:  http://s.apache.org/uY

Where is this 'bile-spewing' and when did it start? 

I'm still looking forward to playing with this.","13/Mar/13 03:51;vinodkv;I saw a presentation from Ralph a while back but I thought he himself was going to update this JIRA some time soon. It doesn't seem like that is going to happen, so here it goes: What is MR+? - Open MPI -> http://www.open-mpi.org/video/mrplus/Greenplum_RalphCastain-2up.pdf 

Seeing interest on the lists and others on this very JIRA, I do believe there is still merit in having *some* implementation of Mpi on top of YARN. If others agree, I'll go ahead and file a ticket.","30/Apr/13 17:57;kramachandran;Has there been any progress on this ? Any (even rough code) that we could peak at? 
","05/May/13 17:34;rhc;Sorry for the silence - the lawyers have not released the code yet, and I've been uncomfortable providing info in that absence. Now that Greenplum has reorganized into ""Pivotal"", that might change.

I defer all further questions on this matter to Milind as I am leaving Greenplum/Pivotal on May 17th.
","05/May/13 17:35;rhc;I am leaving Greenplum/Pivotal on May 17th and am no longer involved in Hamster.
","03/Jun/13 03:13;clarkyzl;Hi, we've built a demo of MPI on yarn with mpich2.
Any one interested in that please feel free to try and folk.
https://github.com/clarkyzl/mpich2-yarn","03/Jun/13 03:56;acmurthy;[~clarkyzl] Nice!

I've had a number of people ask me about MPI and YARN... this is very exciting, thanks!

Could you please share some more info? 

# What stage is this in? How much usage do you see of this at Taobao?
# The github page for the project doesn't list the license, could you please clarify?
# Would you be interested in contributing this to the ASF? I'm sure you'll see a lot of collaborators here...

Thanks again!","30/Jun/13 03:13;lihui;Hi Clark,

I am recently trying to run your mpich2-yarn jar file with a simple mpi routine, but didn't get success. 
Please can you send a short readme about how to run the mpich2-yarn program. 

Regards,
Hui","30/Jun/13 17:51;clarkyzl;Hi [~acmurthy]

Sorry for late respone, I was on my vocation recently.

1. The stage is that we store almost all the data on the HDFS cluster, all the mpi jobs need these data, so we try to run MPI on it. (Moving the mpi computation to where data stores) There is a 100-node cluster runs MPICH2 on Yarn, and we run simple PLDA/LA application on it.
2. I am not good at license problem, and I think Apache linsence is great and friendly to comercial usage. Should I list that on the README.md?
3. We are really insterested in contributing the project to ASF. The design is simple ,make each nodemanger runs a ""smpd"" daemon and make application master run ""mpiexec"" to submit mpi jobs. The biggest problem here, I think, is the yarn scheduler do not support gang secheduling algorithm, and the scheduling is very very slow. I think if we want to run the MPI fast, we really need collaborators to help us modify the scheduler.

Thank you very much.","30/Jun/13 17:53;clarkyzl;Hi, [~li7hui]

I had updated README.md on https://github.com/clarkyzl/mpich2-yarn, will that help?

Thank you.","30/Jun/13 19:24;lihui;Hi, [~clarkyzl]

Now I can run the mpi program with mpich2-yarn by following the updated README.md. I am interested in this project, and look forward to hearing from you again. 

Thank you very much.","26/Mar/14 10:19;erankimadhu;Hi,

Iam interested in the project Hamster. Please let me know if the sources are available for build and test.
","27/Mar/14 10:44;erankimadhu;Hi,

iam interested in Hamster and want to build and test the sources.
Please let me know f the sources of Hamster are available for build and testing.

thanks,
Madhurima","05/Aug/14 02:33;clarkyzl;Hi, Madhurima

I am not quite sure whether there is a public repo of OPENMPI

There is an alpha version https://github.com/clarkyzl/mpich2-yarn MPICH2 implementation.

I think you may try it on.","29/Apr/15 18:56;Hamza100;Hi Guys,
We have written a Hadoop YARN runtime for MPJ Express [1]. MPJ Express is an open source java based MPI primarily maintained at NUST HPC lab [2]. The code can be downloaded from the MPJ Express website [3].  Currently the MPI processes use the MPJ Express's Ethernet based device driver.

Thanks
--Hamza

[1] http://mpjexpress.org/
[2] http://hpc.seecs.nust.edu.pk/
[3] http://mpjexpress.org/news_5.html",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide facility to users for writting custom MergeManager implementation when custom shuffleconsumerPluggin is used,MAPREDUCE-6332,12823146,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,rohithsharma,rohithsharma,rohithsharma,22/Apr/15 19:33,15/Mar/18 08:49,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"MR provides ability to the user for plugin custom ShuffleConsumerPlugin using *mapreduce.job.reduce.shuffle.consumer.plugin.class*.  When the user is allowed to use this configuration as plugin, user also interest in implementing his own MergeManagerImpl. 

But now , user is forced to use MR provided MergeManagerImpl instead of custom MergeManagerImpl when user is using shuffle.consumer.plugin class. There should be well defined API's in MergeManager that can be used for any implementation without much effort to user for custom implementation.",,Bhupendra,cdouglas,devaraj,jira.shegalov,Naganarasimha,rohithsharma,varun_saxena,vinodkv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/15 20:12;rohithsharma;0001-MAPREDUCE-6332.patch;https://issues.apache.org/jira/secure/attachment/12727322/0001-MAPREDUCE-6332.patch","28/Apr/15 04:42;rohithsharma;0002-MAPREDUCE-6332.patch;https://issues.apache.org/jira/secure/attachment/12728694/0002-MAPREDUCE-6332.patch","08/Jun/15 10:10;rohithsharma;0003-MAPREDUCE-6332.patch;https://issues.apache.org/jira/secure/attachment/12738328/0003-MAPREDUCE-6332.patch","09/Jun/15 05:43;rohithsharma;0004-MAPREDUCE-6332.patch;https://issues.apache.org/jira/secure/attachment/12738504/0004-MAPREDUCE-6332.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2015-04-23 03:11:53.63,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 15 08:49:00 UTC 2018,,,,,,,"0|i2dmbr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"22/Apr/15 20:12;rohithsharma;Attaching the patch, Kindly review the patch","23/Apr/15 03:11;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 43s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 2 new or modified test files. |
| {color:red}-1{color} | whitespace |   0m  0s | The patch has 1  line(s) that end in whitespace. |
| {color:green}+1{color} | javac |   7m 31s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 38s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   4m  1s | The applied patch generated  1  additional checkstyle issues. |
| {color:green}+1{color} | install |   1m 37s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m 14s | The patch does not introduce any new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | mapreduce tests |   1m 47s | Tests passed in hadoop-mapreduce-client-core. |
| | |  41m 26s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12727322/0001-MAPREDUCE-6332.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / a100be6 |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5434/artifact/patchprocess/whitespace.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5434/artifact/patchprocess/checkstyle-result-diff.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5434/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5434/testReport/ |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5434//console |


This message was automatically generated.","27/Apr/15 09:52;rohithsharma;I'd appreciate if any commiter/PMC member comment on the JIRA. ","27/Apr/15 23:20;vinodkv;This almost seems like a new feature to me, at least given that we have to expose more APIs to the outside world. I am going to remove 2.7.1 as target-version, please revert back if you disagree..","27/Apr/15 23:45;vinodkv;Regarding the patch
 - MergeThread.java directly uses the merge-manager impl, that should be fixed too?
 - Add some javadoc to the new methods?","28/Apr/15 04:23;rohithsharma;Thanks [~vinodkv] for sharing your thoughts.. 

bq. This almost seems like a new feature to me, at least given that we have to expose more APIs to the outside world. 
I will mark the JIRA as New Feature.","28/Apr/15 04:43;rohithsharma;bq. MergeThread.java directly uses the merge-manager impl, that should be fixed too?
Agree, I missed it

bq. Add some javadoc to the new methods?
Done.

Updated the patch fixing comments. Kindly review the updated patch.","28/Apr/15 05:35;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 40s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 2 new or modified test files. |
| {color:red}-1{color} | whitespace |   0m  0s | The patch has 4  line(s) that end in whitespace. |
| {color:green}+1{color} | javac |   7m 33s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 32s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 24s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   7m 44s | The applied patch generated  1  additional checkstyle issues. |
| {color:green}+1{color} | install |   1m 34s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m 15s | The patch does not introduce any new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | mapreduce tests |   1m 56s | Tests passed in hadoop-mapreduce-client-core. |
| | |  45m 15s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12728694/0002-MAPREDUCE-6332.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / feb68cb |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5462/artifact/patchprocess/whitespace.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5462/artifact/patchprocess/checkstyle-result-diff.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5462/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5462/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf902.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5462/console |


This message was automatically generated.","12/May/15 11:22;rohithsharma;[~vinodkv] Kindly review the patch ..","08/Jun/15 07:58;devaraj;Sorry for the delay here.

[~rohithsharma], Patch is not getting applied any more, can you rebase the patch?","08/Jun/15 10:11;rohithsharma;Updated the patch rebasing against trunk. Kindly review..","09/Jun/15 04:50;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  16m 12s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 2 new or modified test files. |
| {color:green}+1{color} | javac |   7m 47s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 54s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   0m 46s | The applied patch generated  4 new checkstyle issues (total was 107, now 98). |
| {color:red}-1{color} | whitespace |   0m  1s | The patch has 5  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 35s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m 25s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | mapreduce tests |   1m 39s | Tests passed in hadoop-mapreduce-client-core. |
| | |  40m 18s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12738328/0003-MAPREDUCE-6332.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / fc2ed4a |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5785/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-core.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5785/artifact/patchprocess/whitespace.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5785/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5785/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf903.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5785/console |


This message was automatically generated.","09/Jun/15 05:43;rohithsharma;Updated the patch to fix checkstyle & whitespace warnings.","09/Jun/15 06:35;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  16m  2s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 2 new or modified test files. |
| {color:green}+1{color} | javac |   7m 30s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 29s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 24s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 48s | There were no new checkstyle issues. |
| {color:red}-1{color} | whitespace |   0m  1s | The patch has 1  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 35s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m 23s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | mapreduce tests |   1m 37s | Tests passed in hadoop-mapreduce-client-core. |
| | |  39m 28s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12738504/0004-MAPREDUCE-6332.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / c45784b |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5786/artifact/patchprocess/whitespace.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5786/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5786/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf906.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5786/console |


This message was automatically generated.","10/Jun/15 05:58;rohithsharma;[~devaraj.k] Kindly review the updated patch.
-1 for whitespace, this line is not modified in the patch but still QA reports -1.","10/Jun/15 10:50;devaraj;Thanks [~rohithsharma] for the patch. Here are some of my comments,

1. InMemoryReader still uses MergeManagerImpl, can you update the references to MergeManager.
2. I don't think it is a good idea to have static inner class in the exposing interface. Can we move CompressAwarePath as a separate class instead of having it in MergeManager?
3. 
{code}
void closeInMemoryFile(InMemoryMapOutput<K, V> mapOutput);
{code}
Here we are providing API with  parameter type as InMemoryMapOutput, but InMemoryMapOutput marked as private class and also the visibility is default which cannot be accessed outside of the package.
4. {code:xml}
void closeOnDiskFile(CompressAwarePath compressAwarePath);
{code}
CompressAwarePath is used mostly inside the class MergeManagerImpl except one place in OnDiskMapOutput. Can we avoid giving the argument type as CompressAwarePath here?
5. Can we have detailed java doc for MergeManager at class level, how to implement it for custom merge manager? And also please add detailed description for the newly added API's in MergeManager. 
6. Can we have a test to demonstrate how do we configure and use the custom merge manager?
7. And also there are tests using MergeManagerImpl type for references, please update them as well.
","15/Mar/18 08:49;varun_saxena;[~rohithsharma] , are we planning to get this JIRA in?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backport MR sort plugin(MAPREDUCE-2454) to Hadoop 1.2,MAPREDUCE-4482,12600210,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,masokan,masokan,masokan,25/Jul/12 19:31,04/Mar/18 15:45,12/Jan/21 09:52,04/Mar/18 15:45,1.2.0,,,,,,,,mrv1,,,,,,0,BB2015-05-TBR,,,,,,aajisaka,acmurthy,ahmed.radwan,avnerb,gortsleigh,jdonofrio,lianhuiwang,masokan,sms,ujjwal.wadhawan,zzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-2454,,,,,,,,,,,,,,,,,,,,,"21/Sep/12 16:02;masokan;HadoopSortPlugin.pdf;https://issues.apache.org/jira/secure/attachment/12546054/HadoopSortPlugin.pdf","21/Sep/12 16:06;masokan;mapreduce-4482-release-1.1.0-rc4.patch;https://issues.apache.org/jira/secure/attachment/12546055/mapreduce-4482-release-1.1.0-rc4.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2012-09-21 16:11:18.082,,,false,,,,,,,,,,,,,,,,,,239615,,,,,Sun Mar 04 15:45:18 UTC 2018,,,,,,,"0|i00rpr:",2422,,,,,,,,,,,,,,,,,,,,,"29/Jul/12 12:56;masokan;I think back-porting MAPREDUCE-318(refactoring of shuffle code) to 1.1.x would make it easier to back-port MAPREDUCE-2454.  Will there be any issues or blockers that I should be aware of?  I would like to get the opinions of other developers.
Thanks.
","05/Sep/12 19:54;masokan;At this point, it appears to be a difficult task to back-port MAPREDUCE-318 as there were more changes incorporated on top of MAPREDUCE-318.  So I am opting to refactor {{ReduceCopier}} in {{ReduceTask.java}}.  The shuffle and merge are coupled in this class right now.  I will break it into two classes namely {{ReduceCopier}} which just does the shuffle(will implement {{ShuffleRunner}} discussed in MAPREDUCE-2454) and {{ReduceMerger}} which does the merge.  {{ReduceMerger}} will implement {{ShuffleCallback}} interface.  The {{ShuffleCallback}} interface for 1.x will be slightly different from the one submitted for MAPREDUCE-2454 in order to accommodate subtle differences between 1.x and 2.x in their shuffle code.

I welcome suggestions from other developers.

Thanks.

-- Asokan
","21/Sep/12 16:02;masokan;Uploaded a design document that describes changes related to MAPREDUCE-2454 and MAPREDUCE-4482.","21/Sep/12 16:06;masokan;Submitted a patch on top of branch release-1.1-rc4","21/Sep/12 16:11;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12546055/mapreduce-4482-release-1.1.0-rc4.patch
  against trunk revision .

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2868//console

This message is automatically generated.","22/Sep/12 10:51;masokan;It looks like there is no automated procedure for branch builds.  I will try to run test-patch.sh on my box and post the results here.
 ","23/Sep/12 13:02;masokan;Here is the result of the build:

-1 overall.  

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 12 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 10 new Findbugs (version 1.3.9) warnings.

The findbugs report in newPatchFindbugsWarnings.html points to all warnings not just the new ones.  Is there a base report that I can compare with?  I would appreciate any help from other developers.

The unit tests seem to be running fine. ","14/Oct/12 18:06;acmurthy;Mariappan - I'm very worried about making *MASSIVE* changes in the stable line (hadoop-1.x) right now since it's coming to it's EOL, particularly since the shuffle code is very very brittle in that branch without MAPREDUCE-318... my feeling is that we should stick to doing this work in trunk and more recent versions. Sorry.","22/Oct/12 11:53;avnerb;Hi Asokan,

Just a small thing I thought about...
I think that when MAPREDUCE-4049 will be committed to hadoop-1, it will be possible to use it for loading your plugin too.  For this, you’ll need to change your plugin’s interface and make your patch part of your plugin’s code.  Then, load them together using the plugin interface of MAPREDUCE-4049.

Feel free to let me know if you have any comments.

Avner
","02/May/15 04:22;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12546055/mapreduce-4482-release-1.1.0-rc4.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5500/console |


This message was automatically generated.","04/Mar/18 15:45;aajisaka;branch-1 is EoL. Closing this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mapreduce tasks for YARN Timeline Service v.2: alpha 2,MAPREDUCE-6732,12988356,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,vrushalic,sjlee0,sjlee0,11/Jul/16 17:05,21/Oct/17 06:32,12/Jan/21 09:52,21/Oct/17 06:32,,,,,,2.9.0,3.0.0-beta1,,,,,,,,0,,,,,"This s an umbrella JIRA to capture all mapreduce tasks for YARN Timeline Service v.2 alpha 2.

This is developed on feature branches: {{YARN-5355}} for the trunk-based development and {{YARN-5355-branch-2}} to maintain backports to branch-2. Any subtask work on this JIRA will be committed to those 2 branches.",,agresch,gtcarrera9,haibochen,jrottinghuis,Naganarasimha,rohithsharma,sjlee0,varun_saxena,vinodkv,vrushalic,xinxianyin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-6943,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Jul 11 19:07:14 UTC 2016,,,,,,,"0|i30sun:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"11/Jul/16 19:07;sjlee0;I have just created and pushed branches {{YARN-5355}} and {{YARN-5355-branch-2}}. They are now open for this feature development.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Encryption and Key Protection,MAPREDUCE-4491,12600414,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Do,benoyantony,benoyantony,benoyantony,27/Jul/12 01:24,16/Oct/17 19:44,12/Jan/21 09:52,16/Oct/17 19:44,,,,,,,,,documentation,security,task-controller,tasktracker,,,1,,,,,"When dealing with sensitive data, it is required to keep the data encrypted wherever it is stored. Common use case is to pull encrypted data out of a datasource and store in HDFS for analysis. The keys are stored in an external keystore. 

The feature adds a customizable framework to integrate different types of keystores, support for Java KeyStore, read keys from keystores, and transport keys from JobClient to Tasks.
The feature adds PGP encryption as a codec and additional utilities to perform encryption related steps.


The design document is attached. It explains the requirement, design and use cases.
Kindly review and comment. Collaboration is very much welcome.

I have a tested patch for this for 1.1 and will upload it soon as an initial work for further refinement.

Update: The patches are uploaded to subtasks. 






",,acmurthy,anthonyr,aperepel,atm,avik_dey@yahoo.com,benoyantony,bharathm,cdouglas,chenhsiu,clockfly,daryn,ddas,devaraj,efan,eli,jdonofrio,jerrychenhf,kkambatl,krisden,mingma,rajesh.balamohan,revans2,robw,shv,sseth,tgraves,tlipcon,tomwhite,tucu00,vicaya,zero45,zhihyu@ebaysf.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1814400,1814400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-4554,,,,"09/Aug/12 23:01;benoyantony;Hadoop_Encryption.pdf;https://issues.apache.org/jira/secure/attachment/12540147/Hadoop_Encryption.pdf","27/Jul/12 01:29;benoyantony;Hadoop_Encryption.pdf;https://issues.apache.org/jira/secure/attachment/12538110/Hadoop_Encryption.pdf","17/Oct/12 06:08;jerrychenhf;crypto_abstractions.zip;https://issues.apache.org/jira/secure/attachment/12549454/crypto_abstractions.zip",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2012-07-27 23:39:20.641,,,false,,,,,,,,,,,,,,,,,,249208,,,,,Mon Oct 16 19:44:18 UTC 2017,,,,,,,"0|i0a5qv:",57204,,,,,,,,,,,,,,,,,,,,,"27/Jul/12 23:39;robw;If you want to use different encryption keys for different files (or even for different columns within the same file), how do you identify the right key from the Safe or Keystore, i.e. where is the mapping maintained? Would that be an additional layer on top of this?
","30/Jul/12 21:12;tucu00;Benoy, I've done a quick read to the doc. A couple of initial questions:

* If using compression codec for encryption, are you losing the compression capabilities if doing using encryption or will it work as a composition?
* For the keystores, are you proposing to store them in HDFS use file system permissions to protect them? I'm not sure if I understood this part correctly. If that is the case, then HDFS-3637 would ensure secure transfer.

I'll read the design doc in more detail later this week.

","30/Jul/12 21:52;benoyantony;To Rob's questions :

Different Encryption Keys for Different files:  At this point, the PGPCodec supports only one secret key/Key Pair  for all input files. 
What we need is the ability to specify secret keys/key pair per input file. 
Another enhancement will be to specify secret keys/key pair per each phase like map->output , reduce->output .
As you mentioned, this mapping has to specified via configuration.
I'll try to add these two enhancements. 

Decryption/Encryption of different columns within the same file: This is actually left to the mapreduce programmer as he has to do the Decryption/Encryption of the fields programmatically. The programmer can choose to use different keys  for different fields in the mapreduce program. Multiple keys can be retrieved from the keystore and these keys can be retrieved in the mapper/reducer using the credentials API.  
In a higher level interface like Hive, it may be possible to add additional metadata information to specify the key name. Another reviewer also has recommended to add this capability Hive to identify an encryption field and specify the key (name of the key)  to be used to decrypt/encrypt it.

Thanks for the review and recommendations, Rob. Please let me know if I have not answered the question correctly.","30/Jul/12 22:24;benoyantony;To Alejandro's questions:

1) If using compression codec for encryption, are you losing the compression capabilities if doing using encryption or will it work as a composition?
What I have done is to first compress and then encrypt. I have hardcoded to ZIP. I can expose this as a configuration with a choice of {UNCOMPRESSED, ZIP, ZLIB, BZIP2}. This is an enhancement that I can add.
I have also provided a DistributedSplitter  so that files can be split into smaller files.
I am not aware of an ability to chain multiple compression Codecs, though it was a desirable capability in this case. 

2) For the keystores, are you proposing to store them in HDFS use file system permissions to protect them?

Actually, I am not proposing to store them in HDFS. The keystores themselves are encrypted and a password is required to read keys from them. 

In the use cases that I have encountered, the keystores were external to the cluster. They were either on the CLI machine from where the jobs were submitted or on a separate machine from where the keys were retrieved based on user's credentials. (Alfredo was used in this regard to fetch keys via webservice)
So they were two schemes that I have supported -
  1) reading keys from Java keystore
  2) reading keys from a web Service based keystore  (""Safe"")



","09/Aug/12 23:01;benoyantony;Attaching the initial patches for trunk and branch-1.1. Please review and let me know the comments. 

Did minor updates in the design document.

One of the test cases in the patch depends on a test class which will be part of another jira (yet to be filed due to the ASF Jira problem)","13/Aug/12 16:06;benoyantony;To make the reviewing this patch easier, I am dividing this patch  into smaller patches. I am opening sub tasks under this jira issue and attaching the patches to those liras.","13/Aug/12 17:03;benoyantony;Adding this dependency as one of the tests depends on a test class added in MAPREDUCE-4554's patch","13/Aug/12 18:39;benoyantony;One of the goals of this feature is to achieve encryption of files in transit and at rest(when stored on disk). One way to achieve this goal is to depend on a software/hardware which allows encryption in the local file system plus rely on HDFS-3637  and MR shuffle encryption.

This jira  explores an alternative approach to the problem without depending on s special software to do local file system encryption. 

The key advantages of this approach over the local file system encryption approach are

1)  A file can be decrypted only if the user provides the correct key. So even if someone managed to read the file, he cannot read its contents without key. So user's possession of the key is required in addition to his read permission. So there are two levels of protection. 

There could be cases where a user accidentally set ""read"" permissions for everyone. There could be cases where a superuser reads the file. But  this scheme protects the data.

2) No dependency on local file system encryption software.  This approach allows encryption without such special setup.

3) A file is decrypted/encrypted only during processing and not when it is read.  So this results in a less number of encryption/decryption.


Other key points will be :

1) Encrypted and plain text files can coexist in a normal file system. 

2) Developers can plugin other encryption algorithms/standards - CMS, AES, custom encryption and thus have more flexibility.

3) Allows transporting keys/password/tokens  from JobClient to tasks for use cases other than encryption like connecting to a webservice . MAPREDUCE-4491 adds keyProtection and encryption uses it.

4) Can manage keys in one central location. JobClient  gets on behalf of user like any other application. 

If we look at these two approaches from a higher level, we can see that one local file system approach is an internal approach to encryption and MAPREDUCE-4491 approach is an external approach. These two choices are available in normal (non-distributed) application development also where developers can rely on the file system to provide encryption or do encryption themselves. There are tradeoffs and flexibilities in the both the approaches and we choose it based on our use cases and needs.  So I believe , we should provide  these two alternatives  in Hadoop.

In addition, this feature allows key protection in general, which can be used for purposes other than encryption. The keys also will be encrypted when stored on disk and decrypted only in memory.
","24/Aug/12 17:22;benoyantony;Could I please get a review of this work ?","28/Aug/12 06:13;shv;Benoy. I went over your design document. Pretty comprehensive description. 
Want to clarify couple of things. 
# Do I understand correctly that your approach can be used to securely store (encrypt) data even on non-secure (security=simple) clusters?
# So JobClient uses current user credentials to obtain keys from the KeyStore, encrypts them with cluster-public-key and sends to the cluster along with the user credentials. JobTracker has nothing to do with the keys and passes the encrypted blob over to TaskTrackers scheduled to execute the tasks. TT decrypts the user keys using private-cluster-key and handles them to the local tasks, which is secure as keys don't travel over the wires. Is it right so far?
# TT should be using user credentials to decrypt the blob of keys somehow? Or does it authenticate the user and then decrypts if authentication passes? I did not find it in your document.
# How cluster-private-key is delivered to TTs?
# I think configuration parameters naming need some changes. They should not start with {{mapreduce.job}}. Based on your examples you can just encrypt a HDFS file without spawning any actual jobs. In this case seeing {{mapreduce.job.*}} seems confusing.
My suggestion is to prefix all parameters with simply {{hadoop.crypto.*}} Then you can use e.g. full word ""keystore"" instead of ""ks"".

I plan to get into reviewing the implementation soon.","28/Aug/12 06:29;shv;Edited previous comment. Was: crypto.* Changed to: hadoop.crypto.*
Similar to hadoop.security","29/Aug/12 07:13;benoyantony;1.	Do I understand correctly that your approach can be used to securely store (encrypt) data even on non-secure (security=simple) clusters?   
        
You are right!! If TaskTracker and Task processes are owned by different users, then it is possible to use this approach to encrypt/decrypt data in a non-secure cluster.  This  does not require each task to be run as job owner, instead a fixed user other than TT user is sufficient.  The cluster private key can be made readable/accessible only by TaskTracker user. In this way, the Tasks cannot get hold of the cluster private key. But it requires the use of LinuxTaskController to spawn  tasks as a different user. It also requires some code changes to enable this via configuration. 
            
2.	So JobClient uses current user credentials to obtain keys from the KeyStore, encrypts them with cluster-public-key and sends to the cluster along with the user credentials. JobTracker has nothing to do with the keys and passes the encrypted blob over to TaskTrackers scheduled to execute the tasks. TT decrypts the user keys using private-cluster-key and handles them to the local tasks, which is secure as keys don't travel over the wires. Is it right so far?
	
That is correct. Its a clear and concise  explanation of this straight forward approach.  Please note that though the design is described in terms of TaskTrackers and TaskControllers (1.0 terminology) , the implementation is available for both 1.0 and 2.0 .

3.	TT should be using user credentials to decrypt the blob of keys somehow? Or does it authenticate the user and then decrypts if authentication passes? I did not find it in your document.
		
This is an important point as we do not want Tasktracker to decrypt the blob of keys and blindly hand over to Tasks. The JobClient stores JobId along with keys as part of the encrypted blob. The taskTracker decrypts the encrypted blob, verifies that the JobId in the encrypted blob matches  JobId of the task. The keys are handed over to Tasks only if the JobId verification is successful. This ensures that keys are handed over to the correct tasks.

4.	How cluster-private-key is delivered to TTs?

The TTs can use an implementation of the KeyProvider interface to retrieve keys. The implementation can be configured as a cluster configuration. The default Key provider is Java keystore based key provider in which private key is stored in a Java keystore file on the TT machines. This is the same scheme used by web servers to store their private keys. It is possible to plugin more complex KeyStorage mechanisms via configuration.

5. I think configuration parameters naming need some changes. They should not start with mapreduce.job. Based on your examples you can just encrypt a HDFS file without spawning any actual jobs. In this case seeing mapreduce.job.* seems confusing. My suggestion is to prefix all parameters with simply hadoop.crypto.* Then you can use e.g. full word ""keystore"" instead of ""ks"".

The distributed utility to encrypt/decrypt an HDFS file actually spawns map jobs. Irrespective of that, I think it make perfect sense to rename the configurations as hadoop.crypto  as this approach is useful in non-mapreduce situations. I'll change the configuration names.

I plan to get into reviewing the implementation soon.  

Thanks and please post your comments.","04/Sep/12 19:25;zero45;Great work, Benoy!

This looks like a very neat feature to add. I am all in support. I like your similarity with the compressor / decompressor interfaces and the ease of the implementation to plug-in any keystores.

I am in the midst of applying your patches and doing a small test locally and will reply back with any results I find.","05/Sep/12 23:29;atm;bq. This is an important point as we do not want Tasktracker to decrypt the blob of keys and blindly hand over to Tasks. The JobClient stores JobId along with keys as part of the encrypted blob. The taskTracker decrypts the encrypted blob, verifies that the JobId in the encrypted blob matches JobId of the task. The keys are handed over to Tasks only if the JobId verification is successful. This ensures that keys are handed over to the correct tasks.

Unless I'm missing something, this seems to be insecure unless secure authentication (i.e. Kerberos) is enabled, since someone could connect to the TT from a different task and simply report a different JobId. Or do I misunderstand somehow?","07/Sep/12 22:11;benoyantony;Key Protection is simple to explain.
JobClient retrieves keys from a configured Keystore ,encrypts the keys along with jobId  using cluster public key , submits the encrypted blob 
as part of the job credentials. 
TaskTrackers decrypts the encrypted blob using cluster private key during job localization, verifies that jobId inside the encrypted blob matches the JobId of the task. During Task Launch, the keys are made available to the  child (task) process as an environment variable.

Since the JobId is part of the encrypted blob, the replay attack is prevented with the JobId verification. It is easy to add integrity protection also.

Now, the scheme was designed to be used in a secure cluster. It is good to explore whether it can be used in a non-secure cluster. 

One issue was with the cluster private key. It should be made accessible only to TaskTracker process. If the access is determined by the user's permissions, then tasks should be run as a different user. But it need not be the job owner. It can be a fixed user. 

I believe , you are bringing up another issue in this regard.  
If a rogue task can  make a TT launch another rogue task with a jobId matching the one inside encrypted blob, then the keys area available to the newly launched rogue task.
That's a good point. Basically the rogue task is acting as a JT/AppMaster. I am not sure whether that is possible. Even if its possible, there should be ways to detect it. 



","17/Oct/12 05:44;jerrychenhf;Hi Benoy,
I am Haifeng from Intel. and we was discussing offline as to this feature. And I really apperciate your initiation of this work. And we also see the importance of encryption and decryption in Hadoop when we are deasling with sensitive data. 

Just as you pointed out, the functionalities requirements are more or less same. For hadoop community, we wish to get a high level abstraction that basically provide a foundation for these requirements in different hadoop components (such as HDFS, MapReduce, HBase) while enable different implementations such as different encryption algorithms or different ways of key management of different parts / companies so that not bounding a concept on a specific implementation.  Just as we disuccssed offline, the driving force for such a abstraction is summarized  as following:

1. Encryption and decryption need to be supported in different components and usage models. For example, We may use HDFS Client API and Codec directly to encrypt and decrypt HDFS file; We may use MapReduce to processing a encrypted file and output a encrypted file; And also, the HBase may needs to store its files (such as hfiles) in an encrypted way.

2. The community may have different implemenation of encryption codecs and different ways of providing keys. CompressionCodec provides us a foundation for related work. But CompressionCodec are not enough for encryption and decryption because CompressionCodec assumes to initilize from hadoop Configuration while encryption/decryption may needs a per file crypto context such as the Key. With an abtraction layer of crypto, we can share the common featurs such as ""Provide different keys for different input files of a MapReduce job."" other than each implementation get his own way in MapReduce core and finally becames into a mess.

Based on these driving forces, your work done and our offline discussions, we refined our work and would like to propose the following,

1. For Hadoop common, a new CryptoCodec interface which extends CompressionCodec, which adding the methods of getCryptoContext/setCryptoContext. Just as CompressionCodec, it will initialize its global settings from Configuration. But CryptoCodec will receive its crypto context (the Key, for example) through CryptoContext object setting by setCryptoContext, allowing different usage cases such as ""direct use CryptoCodec to encrypt/decrypt a HDFS file by direct providing the CryptoContext(Key)"" or ""Map Reduce way of using CryptoCodec that a CryptoContext(Key) is choosed per file based on some policy"".

Any specific crypto implementation are under this umbrella and will implement CryptoCodec. The PGPCodec is pretty good fit into a implementation of CryptoCodec. And we also are able to implements our splittable CryptoCodec.

2. For MapReduce, use CryptoContextProvider interface to abstract implementation specific service and allowing the MapReduce core is able to written shared code of retrieveing the CryptoContext of a specific file from a CryptoContextProvider and pass to the CryptoCodec in using. Different CryptoContextProvider implementations can implement different ways of deciding the CryptoContext and different ways of retrieving Keys from different Key Stores. We can provide basic and common implementations of CryptoContextProviders such as ""A CryptoContextProvider provides CryptoContext for a file by regular expression matching the file path and get the key from a java KeyStore"" while not preventing users to implement or extends their own if existing implementation doesn't satisfy their requirements.

CryptoContextProvider configurations are passed by hadoop JobConfig and credentials (credential secret keys) and the implementation of CryptoContextProvider can choose whether or not to encrypt the secret keys stored in job Credentials.

I attched the java files of these interfaces and basic strucutes in Attachments section for demonstrating the concepts and I wish to have a design document for these high level things when we have enough discussion and come to an agreement.

Again, thanks for your patient and time. 
","17/Oct/12 06:08;jerrychenhf;Proposed interfaces and stuctures for crypto abstraction for Hadoop Core and Map Reduce layer.","17/Oct/12 18:48;benoyantony;+1 . I agree. A more generic framework is useful in addressing encryption in components other than MR. Let us work on it together.","28/Jan/13 23:12;benoyantony;I'll continue working on this jira. I'll start by incorporating Jerry's framework changes.","29/Jan/13 06:11;jerrychenhf;[~benoyantony]
Hi Benoy, I am glad that you can working on this again and I was thinking about this too. And one thing that I could think of is that the source code of the encryption feature actually span two hadoop projects: Hadoop Common and Map Reduce. We may need a hadoop common Jira entry to submit the framework and codec implementations which is mostly under the package org.apache.hadoop.io.crypto under hadoop-common; and put map reduce side things at this jira.

Based on these two jira entries, we can further split subtasks if needed. What do you think as to this?

Jerry","31/Jan/13 16:30;benoyantony;Yes, That makes sense.  ","16/Oct/17 19:44;benoyantony;Cleaning up jiras which is not relevant anymore.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for the YARN Shared Cache,MAPREDUCE-5951,12724549,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,ctrezzo,ctrezzo,ctrezzo,30/Jun/14 21:12,12/Oct/17 18:42,12/Jan/21 09:52,12/Oct/17 18:35,,,,,,2.9.0,3.0.0,,,,,,,,0,BB2015-05-TBR,,,,"Implement the necessary changes so that the MapReduce application can leverage the new YARN shared cache (i.e. YARN-1492).

Specifically, allow per-job configuration so that MapReduce jobs can specify which set of resources they would like to cache (i.e. jobjar, libjars, archives, files).",,andrew.wang,ctrezzo,devaraj,erwaman,haibochen,hudson,jianhe,jlowe,junping_du,mingma,mzuehlke,pbacsko,qwertymaniac,rkanter,sjlee0,vrushalic,vvasudev,xkrogen,zhouyejoe,zhz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,YARN-5727,,,,,,,,,,,,,,,,,MAPREDUCE-6267,YARN-3637,YARN-1492,YARN-2217,"14/Jan/17 02:03;ctrezzo;MAPREDUCE-5951-Overview.001.pdf;https://issues.apache.org/jira/secure/attachment/12847468/MAPREDUCE-5951-Overview.001.pdf","03/Oct/17 00:56;ctrezzo;MAPREDUCE-5951-trunk-020.patch;https://issues.apache.org/jira/secure/attachment/12890076/MAPREDUCE-5951-trunk-020.patch","04/Oct/17 08:56;ctrezzo;MAPREDUCE-5951-trunk-021.patch;https://issues.apache.org/jira/secure/attachment/12890320/MAPREDUCE-5951-trunk-021.patch","01/Jul/14 02:40;ctrezzo;MAPREDUCE-5951-trunk-v1.patch;https://issues.apache.org/jira/secure/attachment/12653303/MAPREDUCE-5951-trunk-v1.patch","12/May/15 22:48;ctrezzo;MAPREDUCE-5951-trunk-v10.patch;https://issues.apache.org/jira/secure/attachment/12732403/MAPREDUCE-5951-trunk-v10.patch","13/May/15 01:51;ctrezzo;MAPREDUCE-5951-trunk-v11.patch;https://issues.apache.org/jira/secure/attachment/12732450/MAPREDUCE-5951-trunk-v11.patch","15/May/15 19:35;ctrezzo;MAPREDUCE-5951-trunk-v12.patch;https://issues.apache.org/jira/secure/attachment/12733228/MAPREDUCE-5951-trunk-v12.patch","06/Jul/15 23:36;ctrezzo;MAPREDUCE-5951-trunk-v13.patch;https://issues.apache.org/jira/secure/attachment/12743839/MAPREDUCE-5951-trunk-v13.patch","09/Jul/15 21:44;ctrezzo;MAPREDUCE-5951-trunk-v14.patch;https://issues.apache.org/jira/secure/attachment/12744577/MAPREDUCE-5951-trunk-v14.patch","10/Jul/15 17:40;ctrezzo;MAPREDUCE-5951-trunk-v15.patch;https://issues.apache.org/jira/secure/attachment/12744751/MAPREDUCE-5951-trunk-v15.patch","14/Jul/14 22:11;ctrezzo;MAPREDUCE-5951-trunk-v2.patch;https://issues.apache.org/jira/secure/attachment/12655629/MAPREDUCE-5951-trunk-v2.patch","20/Aug/14 00:03;ctrezzo;MAPREDUCE-5951-trunk-v3.patch;https://issues.apache.org/jira/secure/attachment/12662915/MAPREDUCE-5951-trunk-v3.patch","21/Aug/14 00:29;ctrezzo;MAPREDUCE-5951-trunk-v4.patch;https://issues.apache.org/jira/secure/attachment/12663290/MAPREDUCE-5951-trunk-v4.patch","05/Sep/14 18:42;ctrezzo;MAPREDUCE-5951-trunk-v5.patch;https://issues.apache.org/jira/secure/attachment/12666849/MAPREDUCE-5951-trunk-v5.patch","20/Jan/15 21:35;ctrezzo;MAPREDUCE-5951-trunk-v6.patch;https://issues.apache.org/jira/secure/attachment/12693397/MAPREDUCE-5951-trunk-v6.patch","18/Mar/15 02:45;ctrezzo;MAPREDUCE-5951-trunk-v7.patch;https://issues.apache.org/jira/secure/attachment/12705251/MAPREDUCE-5951-trunk-v7.patch","18/Mar/15 03:55;ctrezzo;MAPREDUCE-5951-trunk-v8.patch;https://issues.apache.org/jira/secure/attachment/12705266/MAPREDUCE-5951-trunk-v8.patch","12/May/15 20:34;ctrezzo;MAPREDUCE-5951-trunk-v9.patch;https://issues.apache.org/jira/secure/attachment/12732362/MAPREDUCE-5951-trunk-v9.patch","13/Dec/16 22:45;ctrezzo;MAPREDUCE-5951-trunk.016.patch;https://issues.apache.org/jira/secure/attachment/12843109/MAPREDUCE-5951-trunk.016.patch","15/Dec/16 02:19;ctrezzo;MAPREDUCE-5951-trunk.017.patch;https://issues.apache.org/jira/secure/attachment/12843339/MAPREDUCE-5951-trunk.017.patch","16/Dec/16 18:36;ctrezzo;MAPREDUCE-5951-trunk.018.patch;https://issues.apache.org/jira/secure/attachment/12843632/MAPREDUCE-5951-trunk.018.patch","19/Jan/17 01:33;ctrezzo;MAPREDUCE-5951-trunk.019.patch;https://issues.apache.org/jira/secure/attachment/12848192/MAPREDUCE-5951-trunk.019.patch",,,,,,,,,,,,,22.0,,,,,,,,,,,,,,,,,,,,2015-01-21 00:10:10.293,,,false,,,,,,,,,,,,,,,,,,402732,,,,,Thu Oct 12 18:42:31 UTC 2017,,,,,,,"0|i1xbhj:",402799,MapReduce support for the YARN shared cache allows MapReduce jobs to take advantage of additional resource caching. This saves network bandwidth between the job submission client as well as within the YARN cluster itself. This will reduce job submission time and overall job runtime.,,,,,,,,,,,,,,,,,,,,"01/Jul/14 02:39;ctrezzo;Attached is a v1 patch based on trunk+YARN-1492 and all of it's subtasks.","14/Jul/14 22:11;ctrezzo;Attached is a v2 patch. This fixes a bug around uploading the jobjar.","20/Aug/14 00:03;ctrezzo;Rebase. Also added two fixes done by [~mingma]:
1. Fix bug in the way the shared cache upload policies are set.
2. Handle local resource symlinks correctly when two distinct resources have the same filename.","20/Aug/14 00:04;ctrezzo;Note: #2 is only in the case where shared cache is enabled. There is still no behavior change when the shared cache is disabled.","21/Aug/14 00:29;ctrezzo;Rebase again.","05/Sep/14 18:42;ctrezzo;Attached v5. Updated to match patch in YARN-1492.","20/Jan/15 21:35;ctrezzo;[~kasha] V6 attached. This patch should be ready for review!","21/Jan/15 00:10;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12693397/MAPREDUCE-5951-trunk-v6.patch
  against trunk revision dd0228b.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 13 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5112//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5112//artifact/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-core.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5112//console

This message is automatically generated.","13/Feb/15 01:21;kasha;Sorry for the delay in getting to this. Getting a continuous chunk of time to look at this somewhat large patch was hard. 

Here are my first round of comments - a combination of high-level and detailed comments. Let us see if we can get some of this in through other JIRAs first, to allow for a more thorough review.
# DistributedCache changes aren’t central to what this JIRA is trying to address. Could we leave them out and address in another JIRA? 
## This has nothing to do with this patch, but it would be nice to make the code around setting CLASSPATH_FILES a little more readable. Could we define another String prefix to hold “” or classpath, based on whether classpath is null. 
# Job
## The new APIs should all be @Unstable
## Let us make the javadoc for the new APIs a little more formal - we don’t need to mention SCMClientProtocol.use, or that the APIs are intended for user use. Even for the return value, I would go with something like “If shared cache is enabled and the resource added successfully, return true. Otherwise, return false.”
## How about renaming the methods to addFileToSharedCache, addArchiveToSharedCache, addFileToSharedCacheAndClasspath? 
## Make both new methods private static instead of static private.
# JobID changes might not be required. Use ConverterUtils#toApplicationId? 
# JobImpl
## cleanupSharedCacheResources - nit: I would check for (checksums == null || checksums.length == 0) and return to save on indentations. 80 chars is already too small.
## cleanupSharedCacheUploadPolicies - javadoc should use block comments. Well, may be a nit.
# JobSubmitter
## Can we do the code moving from JobSumitter to FileUploader (may be, we need a more descriptive name) to another JIRA and look at that first if needed. Otherwise, it is hard to review the changes.
## May be, I am misreading the patch. Is this patch hardcoding MR job submission to always use SharedCache? If yes, we should definitely avoid that. 
# mapred-default.xml: We need a little more fool-proof config. The way the patch currently is, a typo will lead to unexpected behavior without any warnings.
","26/Feb/15 18:56;ctrezzo;Created  MAPREDUCE-6267 to address comment in 5.1.","18/Mar/15 02:45;ctrezzo;[~kasha@cloudera.com] Thanks again for the comments!

Attached is v7 of the patch. This version is rebased and addresses your comments above. I removed the DistributedCache changes, addressed comments about Job, JobID, JobImpl. With respect to comment 5.2, the patch is not hard coding MR job submission to always use SharedCache. See if the new patch improves clarity around that and let me know if you have more questions. There are two changes that will happen even if the shared cache is disabled:
1. The SharedCacheConfig class will be used to parse configuration in JobResourceUploader. If the shared cache config parameters do not exist, then it is a no-op.
2. The MR classpath around job jars has be changed slightly (that is the reason for the MRApps and TestMRApps changes), but should present no behavioral changes to the user. This is to handle the case where the job jar used by a job comes from the shared cache and it is named anything other than job.jar. Note that the current code assumes that whatever is localized in the job.jar directory is a single file named job.jar (i.e. job.jar/job.jar in the classpath). In the case where the job.jar is named something else, it will not get put on the classpath. This change simply puts everything in the job.jar directory (currently only the job jar) on the classpath (i.e. job.jar/*).

With respect to the comment about fool-proof config: did you have anything specific in mind? Currently the config should only recognize disabled, enabled, jobjar, libjars, files, archives. I could split each into a separate boolean config parameter if that seems more safe? Let me know. I was trying to come up with a concise single parameter for all the modes, but maybe splitting them up into separate boolean parameters is better. I can also see the JOBJAR_VISIBILITY parameter being slightly confusing and will think if there is a better way to do that. Again, let me know if you have suggestions.

Also, let me know if you want me to split this patch further. I could see splitting it into the following (although the splits won't be fully functional):
1. JobResourceUploader changes. The diff is still a little wonky with the code restructure from adding shared cache checks.
2. TaskImpl changes.
3. JobImpl changes.
4. job.jar classpath changes","18/Mar/15 03:55;ctrezzo;[~kasha@cloudera.com]
Attached is v8 of the patch. This update did two things:
1. Added the DistributedCache changes back in. On second thought, they are necessary for this patch. The key is the DistributedCache#getPathStringWithoutFragment method. This method is added and used during the construction of the classpath, so that the symlinking during localization of resources works correctly in the case when there are two different resources with the same name. For example, checksum1/job.jar, checksum2/job.jar. If both of these resources are used as libjars in a single job, there will be a naming conflict and one of the resource's container symlink (on the node manager) will get overridden by the other. We use the fragment portion of the URI to control the symlink used by yarn localization. With this, we ensure to avoid naming conflicts at the container level (users can name their jars however they want) and we leverage shared jars coming from the cache.
2. Fixed TestJobResourceUploader unit tests to accommodate using ConverterUtils#toApplicationId.","18/Mar/15 05:19;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12705251/MAPREDUCE-5951-trunk-v7.patch
  against trunk revision fc90bf7.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.mapreduce.TestJobResourceUploader

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5303//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5303//console

This message is automatically generated.","18/Mar/15 06:37;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12705266/MAPREDUCE-5951-trunk-v8.patch
  against trunk revision 3bc72cc.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5304//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5304//console

This message is automatically generated.","12/May/15 18:20;kasha;Sorry again for the delay here. Haven't been able to get a large enough chunk of time to review this. 

Few comments based on the partial review I have been able to get to so far. Will post comments for the remaining review as soon as I can:
# DistributedCache#getPathStringWithoutFragment: Is this to make sure we get the path to the file and not its fields? Can we add comments to describe what the method is intended to do? And, may be a different name? Can't think of a simpler one myself. 
# Job - have some javadoc suggestions, but may be it is simpler to post an updated patch once the patch is ready.
# JobImpl - is the cleanup of upload-policies intended to be in init-transition? Is that because we don't need the policies once we are done uploading the resources? 
# JobResourceUploader
## In isSharedCacheFilesEnabled and co., we should reverse the order of checks. isScmAvailable() should come first as it is cheaper. Also, don't need the outer parentheses there. 
## Rename getFiles to mergeLocalAndCacheResources and make it static? ","12/May/15 18:52;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12705266/MAPREDUCE-5951-trunk-v8.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / fe0df59 |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5716/console |


This message was automatically generated.","12/May/15 20:34;ctrezzo;Rebase.","12/May/15 21:53;ctrezzo;Added YARN-3637 to address sym-linking of resources at the YARN layer. Will remove sym-linking from this patch.","12/May/15 22:48;ctrezzo;V10 Attached.

1. This patch takes out support for symlinking of two cached resources with the same name. [~kasha] and I chatted offline and this seems like something complex enough that it should be handled for all shared cache clients at the YARN layer. Note: Because of this change, this patch will currently not handle resources correctly in certain shared cache scenarios. Please see YARN-3637 for more context.

bq. JobImpl - is the cleanup of upload-policies intended to be in init-transition? Is that because we don't need the policies once we are done uploading the resources?

2. The cleanup of upload-policies happens in the Application Master init-transition to prevent all node managers running tasks for this job from attempting to upload resources to the shared cache. Since all containers in the MapReduce application localize the same resources, we decided that the Application Master is the only container that needs to upload resources. Maybe this needs more fool proofing to prevent redundant resource upload attempts?

3. The order of isScmAvailable() check in isSharedCacheFilesEnabled and co. was changed.

4. Renamed getFiles to mergeLocalAndCacheResources and made it static.","12/May/15 22:49;ctrezzo;Submit patch for qa run.","13/May/15 00:38;kasha;Thanks Chris.

Comments on the latest patch:
# DistributedCache changes are all spurious. Omit it from the diff. 
# Remove Job#addArchiveToSharedCacheAndClasspath altogether? 
# JobImpl#cleanupSharedCacheResources should be always called when the job finishes, irrespective of success.
# JobResourceUploader
## stopSharedCache should set scClient to null
## We don't need a separate boolean to check if SCM is available. We should be able to just use (sClient != null). If we run into any issues talking to SCM, we should just abort and call stopSharedCache to avoid using it for the rest of the dependencies.
## uploadFiles seems to be trying to handle the case where shared-cache goes down after we set all the files that need to be uploaded. We should leave this check to the NM. The SCM might come back up by the time the NM tries uploading files. Or, it could be available at this point and go down later. 
{code}
      // if scm fails in the middle, we will set shared cache upload policies
      // for all resources
      // to be false. The resources that are shared successfully via
      // SharedCacheClient.use will
      // continued to be shared.
      if (scClient != null && !isScmAvailable()) {
{code}
## uploadFiles checks if shared-cache is available before setting the upload policies. I think we should set the upload policies irrespective of SCM's availability when shared-cache is not disabled. Also, we should modify the below code to have the second if inside the first if.
{code}
          if (isSharedCacheFilesEnabled()) {
            newPath = useSharedCache(tmp, conf);
          }

          // need to inform NM to upload the file to shared cache.
          if (newPath == null && isSharedCacheFilesEnabled()) {
            filesSCUploadPolicies[indexOfFilesSCUploadPolicies] = true;
          }
{code}
## useSharedCache should check for (scClient != null)
## uploadFiles has a lot of duplication. Can we file a follow-up JIRA to simplify it? 
## Also, due to the try-finally in uploadFiles, it is kind of hard to see what lines have been changed. Do you think it makes any sense to wrap this in another method call, so the indentations don't show spurious changes? I am assuming most of this code will be touched by the follow-up JIRA.
## useSharedCache javadoc issue
# MRApps - we are using ""*"" instead of ""job.jar"" which should work, but I wonder if that will be an incompatible behavior change. [~jlowe], [~vinodkv] - what do you think? 
# YarnRunner - indentation issue on line 360","13/May/15 01:51;ctrezzo;V11 attached. Addressed all comments from [~kasha]. Filed MAPREDUCE-6365 to refactor JobResourceUploader#uploadFilesInternal.","13/May/15 02:24;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 35s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 4 new or modified test files. |
| {color:green}+1{color} | javac |   7m 28s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 34s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   1m 59s | The applied patch generated  30 new checkstyle issues (total was 583, now 612). |
| {color:red}-1{color} | whitespace |   0m 54s | The patch has 23  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 38s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   3m 52s | The patch does not introduce any new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | mapreduce tests |   9m 21s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   0m 46s | Tests passed in hadoop-mapreduce-client-common. |
| {color:green}+1{color} | mapreduce tests |   1m 36s | Tests passed in hadoop-mapreduce-client-core. |
| {color:red}-1{color} | mapreduce tests | 104m 37s | Tests failed in hadoop-mapreduce-client-jobclient. |
| | | 157m 25s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.mapred.TestLocalJobSubmission |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12732403/MAPREDUCE-5951-trunk-v10.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f24452d |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5718/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-core.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5718/artifact/patchprocess/whitespace.txt |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5718/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-common test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5718/artifact/patchprocess/testrun_hadoop-mapreduce-client-common.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5718/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| hadoop-mapreduce-client-jobclient test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5718/artifact/patchprocess/testrun_hadoop-mapreduce-client-jobclient.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5718/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf906.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5718/console |


This message was automatically generated.","13/May/15 06:04;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 39s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 4 new or modified test files. |
| {color:green}+1{color} | javac |   7m 30s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 36s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   1m 59s | The applied patch generated  27 new checkstyle issues (total was 568, now 595). |
| {color:red}-1{color} | whitespace |   0m 25s | The patch has 18  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 37s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   3m 53s | The patch does not introduce any new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | mapreduce tests |   9m 36s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   0m 45s | Tests passed in hadoop-mapreduce-client-common. |
| {color:green}+1{color} | mapreduce tests |   1m 35s | Tests passed in hadoop-mapreduce-client-core. |
| {color:red}-1{color} | mapreduce tests | 103m 52s | Tests failed in hadoop-mapreduce-client-jobclient. |
| | | 156m 34s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.mapred.TestLocalJobSubmission |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12732450/MAPREDUCE-5951-trunk-v11.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 2463666 |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5720/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-core.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5720/artifact/patchprocess/whitespace.txt |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5720/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-common test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5720/artifact/patchprocess/testrun_hadoop-mapreduce-client-common.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5720/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| hadoop-mapreduce-client-jobclient test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5720/artifact/patchprocess/testrun_hadoop-mapreduce-client-jobclient.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5720/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf901.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5720/console |


This message was automatically generated.","13/May/15 15:02;kasha;The latest patch is definitely easier to understand. 

A couple of high-level comments (sorry for missing these earlier):
# We seem to be using arrays to capture upload policies for individual resources in JobResourceUploader. I feel that is error-prone, and it would be nice to avoid using arrays. 
# Do we need to release the resources? As per our offline discussion, an application calling release doesn't really affect when the resources are actually cleaned up. May be getting rid of it altogether will help us simplify both this patch and the SCM source as well? ","13/May/15 22:23;jlowe;bq. MRApps - we are using ""*"" instead of ""job.jar"" which should work, but I wonder if that will be an incompatible behavior change. Jason Lowe, Vinod Kumar Vavilapalli - what do you think?

I get why the '*' is desirable since the name of the jar being shared and replacing our job jar might not match the job jar name we expect, although I think a comment stating why would help explain why we're not using the more specific job jar name that normally would be expected.  That would also help prevent someone else coming along much later and ""fixing"" that.

I think we're probably OK with using * specifically for the job jar because we explicitly use mapreduce.job.jar.unpack.pattern when we unarchive it.  That means we'll only unarchive the classes and lib portions of the archive and not any other files from it, by default.  So by default we shouldn't be picking up any other jars within the job jar.  In theory someone might have packed other jars into the job jar, modified the unpack pattern property to pick up those jars, then explicitly set their classpath to pick up only a portion of those jars or all of them in a specific order.  '*' does not guarantee order in any way, so that could break that scenario.  I'm not sure that scenario is likely, however.","13/May/15 23:26;ctrezzo;1. [~kasha] I have thought about removing the release api more and also discussed with [~sjlee0]. I think it makes sense from a code simplicity standpoint to remove the release api. This will eliminate the need for multiple arrays and keeping track of the resources you use on the client side. If we feel that it is needed later on, we can always add it back in. The major consequence of not releasing is that the SCM store will have more resource references to keep track of during the cleaner period time (currently defaulting to 1 day). For the InMemorySCMStore, this means that there will be more SharedCacheResourceReference objects in-memory.

Rough hand-wavy calculation for heapsize over a 24 hour period on a large cluster:
* 42k jobs per day x 600 resources per job = 25.2 million resource references
* A resource reference is made up of an ApplicationId and a ShortUserName.
** Let's say the ApplicationId is two longs, so 16 bytes, and the shortUserName is 10 characters, so 20 bytes.
** Let's also multiply this number by 3 to account for Object overhead. So (16 + 20) * 3 = 108 bytes for a single resource reference.
* 25.2 million * 108 bytes = 2.7 GB of total heap space

2.7 GB of extra memory does not strike me of being too crazy. We can also trade off RM load for memory size and run the cleaner at a higher frequency. Thoughts from others? If that sounds reasonable, I will file a YARN jira to make the change.

2. [~jlowe] I will add a comment that explains why we are now using '*' instead of MRJobConfig.JOB_JAR.","14/May/15 01:01;ctrezzo;Note for the above calculation: We can probably get rid of shortUserName from the resource reference if we are not doing release calls. If that is the case, then the additional memory should be even smaller.","14/May/15 01:14;ctrezzo;Another note about removal of the release api: Without the release api, long running applications will not be able to release resources they are no longer using. Their appId will be claiming that resource until the application is finished.","15/May/15 19:35;ctrezzo;V12 attached.

1. Removed calls to the shared cache release API.
2. Fixed whitespace/style-check issues, except for the ""Redundant 'public' modifier."" errors in MRJobConfig.
3. TestLocalJobSubmission test failure seems unrelated to this patch.","15/May/15 22:10;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 44s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 4 new or modified test files. |
| {color:green}+1{color} | javac |   7m 37s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 42s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   1m 58s | The applied patch generated  13 new checkstyle issues (total was 568, now 581). |
| {color:red}-1{color} | whitespace |   0m 21s | The patch has 8  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 37s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   3m 52s | The patch does not introduce any new Findbugs (version 2.0.3) warnings. |
| {color:red}-1{color} | mapreduce tests |   9m  6s | Tests failed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   0m 45s | Tests passed in hadoop-mapreduce-client-common. |
| {color:green}+1{color} | mapreduce tests |   1m 37s | Tests passed in hadoop-mapreduce-client-core. |
| {color:red}-1{color} | mapreduce tests | 101m  6s | Tests failed in hadoop-mapreduce-client-jobclient. |
| | | 153m 31s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.mapreduce.v2.app.webapp.TestAMWebServicesJobConf |
|   | hadoop.mapreduce.v2.app.webapp.TestAMWebServicesJobs |
|   | hadoop.mapreduce.v2.app.webapp.TestAMWebServicesAttempts |
|   | hadoop.mapreduce.v2.app.webapp.TestAMWebServicesAttempt |
|   | hadoop.mapred.TestMiniMRChildTask |
|   | hadoop.mapred.TestTextOutputFormat |
|   | hadoop.mapred.TestLocalJobSubmission |
|   | hadoop.mapred.TestFileOutputFormat |
|   | hadoop.mapred.lib.TestKeyFieldBasedComparator |
|   | hadoop.mapreduce.TestJobResourceUploader |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12733228/MAPREDUCE-5951-trunk-v12.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 03a293a |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5739/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-core.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5739/artifact/patchprocess/whitespace.txt |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5739/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-common test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5739/artifact/patchprocess/testrun_hadoop-mapreduce-client-common.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5739/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| hadoop-mapreduce-client-jobclient test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5739/artifact/patchprocess/testrun_hadoop-mapreduce-client-jobclient.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5739/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf903.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5739/console |


This message was automatically generated.","15/May/15 22:36;ctrezzo;1. All checkstyle errors (except for the unused import) pertain to lines that were not modified by this patch.
2. All of the whitespace errors pertain to lines that were not modified by this patch.
3. It seems like the unit test run was in a bad state as well. There were a bunch of address already in use exceptions on unrelated tests and ClassDefNotFoundExceptions on shared cache tests that pass locally.

[~kasha] I can fix the unused import issue, but will wait to post new version until I hear from you (just in case you have other comments about the patch). Thanks!","15/May/15 23:12;kasha;Nice to see the patch size go down every iteration :)

Looks mostly good. As we discussed offline, Minor comments outside of that:
# Let us fix the use of arrays to capture per-resource upload policies in a follow-up JIRA. Can we add a TODO in the source with a JIRA number please.
# Are the test failures related? 
# pom indentation is broken
","06/Jul/15 23:36;ctrezzo;[~kasha] apologies for the long response time. Attached is v13.

# Rebased.
# Cleaned up imports.
# Fixed POM indentation.
# Added TODO with MAPREDUCE-6365 jira for refactor of uploadFilesInternal and fixing the use of arrays for upload policy tracking.

After a hadoopQA run I will re-look at the unit tests. If I remember correctly, they did not seem related last run. I will double check.","07/Jul/15 03:14;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | pre-patch |  19m 16s | Pre-patch trunk has 1 extant Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 4 new or modified test files. |
| {color:green}+1{color} | javac |   7m 29s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 37s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   1m 59s | The applied patch generated  13 new checkstyle issues (total was 578, now 589). |
| {color:red}-1{color} | whitespace |   0m 31s | The patch has 8  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 36s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   4m 29s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | mapreduce tests |   9m  4s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   0m 46s | Tests passed in hadoop-mapreduce-client-common. |
| {color:green}+1{color} | mapreduce tests |   1m 44s | Tests passed in hadoop-mapreduce-client-core. |
| {color:red}-1{color} | mapreduce tests | 108m 34s | Tests failed in hadoop-mapreduce-client-jobclient. |
| | | 166m 12s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.mapred.TestLocalJobSubmission |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12743839/MAPREDUCE-5951-trunk-v13.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 81f3644 |
| Pre-patch Findbugs warnings | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5870/artifact/patchprocess/trunkFindbugsWarningshadoop-mapreduce-client-app.html |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5870/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-core.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5870/artifact/patchprocess/whitespace.txt |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5870/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-common test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5870/artifact/patchprocess/testrun_hadoop-mapreduce-client-common.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5870/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| hadoop-mapreduce-client-jobclient test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5870/artifact/patchprocess/testrun_hadoop-mapreduce-client-jobclient.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5870/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf906.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5870/console |


This message was automatically generated.","07/Jul/15 04:13;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | pre-patch |  18m 54s | Pre-patch trunk has 1 extant Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 4 new or modified test files. |
| {color:green}+1{color} | javac |   7m 37s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 30s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   1m 58s | The applied patch generated  13 new checkstyle issues (total was 578, now 589). |
| {color:red}-1{color} | whitespace |   0m 21s | The patch has 8  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 38s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   4m 23s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | mapreduce tests |   9m  1s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   0m 45s | Tests passed in hadoop-mapreduce-client-common. |
| {color:green}+1{color} | mapreduce tests |   1m 43s | Tests passed in hadoop-mapreduce-client-core. |
| {color:red}-1{color} | mapreduce tests | 108m 40s | Tests failed in hadoop-mapreduce-client-jobclient. |
| | | 165m 34s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.mapred.TestLocalJobSubmission |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12743839/MAPREDUCE-5951-trunk-v13.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 81f3644 |
| Pre-patch Findbugs warnings | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5871/artifact/patchprocess/trunkFindbugsWarningshadoop-mapreduce-client-app.html |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5871/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-core.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5871/artifact/patchprocess/whitespace.txt |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5871/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-common test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5871/artifact/patchprocess/testrun_hadoop-mapreduce-client-common.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5871/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| hadoop-mapreduce-client-jobclient test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5871/artifact/patchprocess/testrun_hadoop-mapreduce-client-jobclient.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5871/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf901.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5871/console |


This message was automatically generated.","09/Jul/15 00:02;ctrezzo;Investigating TestLocalJobSubmission test failure.","09/Jul/15 21:44;ctrezzo;Attached v14.

Fixed JobResourceUploader so that TestLocalJobSubmission#testLocalJobLibjarsOption passes. Also added two new test cases to TestLocalJobSubmission for files and archives.","10/Jul/15 00:36;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | pre-patch |  18m 46s | Findbugs (version 3.0.0) appears to be broken on trunk. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 5 new or modified test files. |
| {color:green}+1{color} | javac |   7m 51s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 49s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   1m 55s | The applied patch generated  13 new checkstyle issues (total was 578, now 589). |
| {color:red}-1{color} | whitespace |   0m 25s | The patch has 10  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 22s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:red}-1{color} | findbugs |   4m 45s | The patch appears to introduce 1 new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | mapreduce tests |   9m  4s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   0m 47s | Tests passed in hadoop-mapreduce-client-common. |
| {color:green}+1{color} | mapreduce tests |   1m 44s | Tests passed in hadoop-mapreduce-client-core. |
| {color:red}-1{color} | mapreduce tests | 108m 35s | Tests failed in hadoop-mapreduce-client-jobclient. |
| | | 166m 10s | |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-mapreduce-client-app |
| Failed unit tests | hadoop.mapred.TestJobSysDirWithDFS |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12744577/MAPREDUCE-5951-trunk-v14.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f4ca530 |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5879/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-core.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5879/artifact/patchprocess/whitespace.txt |
| Findbugs warnings | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5879/artifact/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-app.html |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5879/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-common test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5879/artifact/patchprocess/testrun_hadoop-mapreduce-client-common.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5879/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| hadoop-mapreduce-client-jobclient test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5879/artifact/patchprocess/testrun_hadoop-mapreduce-client-jobclient.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5879/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf903.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5879/console |


This message was automatically generated.","10/Jul/15 17:40;ctrezzo;v15 attached.
# Fixed whitespace in JobResourceUploader and TestLocalJobSubmission. The rest of the whitespace errors are not in lines of code that this patch touched (they are only in the context supplied by the patch).
# Checkstyle shows several errors, but I did not address any of them. The Method length will be resolved by MAPREDUCE-6365. The rest of the errors are complaining about things that if I were to address I would not be conforming to current project practices. For example the redundant public modifier in MRJobConfig and the TODO comment. [~kasha], let me know if you want me to fix these anyways.
# Findbugs gave a -1, but if you click into the results it did not show any errors or warnings. Weird. I will ignore the -1 then.
# There was 1 test failure, but it passes locally and it looks like it is unrelated (NoClassDefFoundError while shutting down a MiniMRYarnCluster).","10/Jul/15 18:05;sjlee0;I've seen several cases where the findbugs result from jenkins is empty. Yet, I think it'd be a good idea to double check by running {{mvn findbugs:findbugs}} at the said project to see if it is really clean.","10/Jul/15 20:26;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | pre-patch |  18m 33s | Pre-patch trunk has 1 extant Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 5 new or modified test files. |
| {color:green}+1{color} | javac |   7m 36s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 41s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   1m 57s | The applied patch generated  13 new checkstyle issues (total was 578, now 589). |
| {color:red}-1{color} | whitespace |   0m 25s | The patch has 7  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 23s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   4m 30s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | mapreduce tests |   9m  4s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   0m 45s | Tests passed in hadoop-mapreduce-client-common. |
| {color:green}+1{color} | mapreduce tests |   1m 45s | Tests passed in hadoop-mapreduce-client-core. |
| {color:green}+1{color} | mapreduce tests | 108m 54s | Tests passed in hadoop-mapreduce-client-jobclient. |
| | | 165m 38s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12744751/MAPREDUCE-5951-trunk-v15.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 0824426 |
| Pre-patch Findbugs warnings | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5883/artifact/patchprocess/trunkFindbugsWarningshadoop-mapreduce-client-app.html |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5883/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-core.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5883/artifact/patchprocess/whitespace.txt |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5883/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-common test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5883/artifact/patchprocess/testrun_hadoop-mapreduce-client-common.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5883/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| hadoop-mapreduce-client-jobclient test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5883/artifact/patchprocess/testrun_hadoop-mapreduce-client-jobclient.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5883/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf905.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5883/console |


This message was automatically generated.","13/Jul/15 17:14;ctrezzo;Findbugs run was clean this time so I think the previous empty result was just wrong. As perviously mentioned the whitespace errors are not part of this patch and the style errors are remaining consistent with the current code convention.","13/Jul/15 18:15;sjlee0;Thanks for the clarification [~ctrezzo].

There is a recent JIRA that got merged which now enables discovery of whether a certain container is an AM: YARN-3116. It exposes the ""container type"" in the container context. Taking advantage of it might simplify some of the work we do here.","13/Jul/15 20:48;ctrezzo;[~sjlee0] thanks for pointing out YARN-3116. If there isn't a strong objection, for the sake of getting this patch committed I will leverage that work in a follow on patch. I can file a separate jira.","13/Jul/15 21:00;sjlee0;That sounds fine with me. Thanks!","13/Dec/16 22:45;ctrezzo;Attaching v16 for a QA run. This was a non-trivial rebase. I have also modified the patch so that it no longer uses arrays in the JobResourceUploader as requested by [~kasha].","14/Dec/16 01:21;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 16s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 6 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 12s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 7m 23s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 41s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 40s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 55s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 1m 2s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 26s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 11s {color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 9s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 1m 30s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 48s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 1m 48s {color} | {color:green} hadoop-mapreduce-project_hadoop-mapreduce-client generated 0 new + 359 unchanged - 5 fixed = 359 total (was 364) {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 38s {color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client: The patch generated 48 new + 1086 unchanged - 11 fixed = 1134 total (was 1097) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 45s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 51s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red} 0m 1s {color} | {color:red} The patch has 3 line(s) that end in whitespace. Use git apply --whitespace=fix. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green} 0m 2s {color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red} 0m 48s {color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 4s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 3m 0s {color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 0m 44s {color} | {color:green} hadoop-mapreduce-client-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 9m 7s {color} | {color:green} hadoop-mapreduce-client-app in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 110m 26s {color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 28s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 152m 19s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common |
|  |  Boxed value is unboxed and then immediately reboxed in org.apache.hadoop.mapreduce.v2.util.MRApps.parseDistributedCacheArtifacts(Configuration, Map, LocalResourceType, URI[], long[], long[], boolean[], Map)  At MRApps.java:then immediately reboxed in org.apache.hadoop.mapreduce.v2.util.MRApps.parseDistributedCacheArtifacts(Configuration, Map, LocalResourceType, URI[], long[], long[], boolean[], Map)  At MRApps.java:[line 639] |
|  |  java.net.URI is incompatible with expected argument type String in org.apache.hadoop.mapreduce.v2.util.MRApps.parseDistributedCacheArtifacts(Configuration, Map, LocalResourceType, URI[], long[], long[], boolean[], Map)  At MRApps.java:argument type String in org.apache.hadoop.mapreduce.v2.util.MRApps.parseDistributedCacheArtifacts(Configuration, Map, LocalResourceType, URI[], long[], long[], boolean[], Map)  At MRApps.java:[line 637] |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12843109/MAPREDUCE-5951-trunk.016.patch |
| JIRA Issue | MAPREDUCE-5951 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux 7b0e5fd931a9 3.13.0-95-generic #142-Ubuntu SMP Fri Aug 12 17:00:09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / ef34bf2 |
| Default Java | 1.8.0_111 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6843/artifact/patchprocess/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6843/artifact/patchprocess/whitespace-eol.txt |
| findbugs | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6843/artifact/patchprocess/new-findbugs-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-common.html |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6843/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6843/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","15/Dec/16 02:19;ctrezzo;Attaching v17 to address findbugs, checkstyle and whitespace errors.","15/Dec/16 04:55;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 20s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 5 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 51s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 7m 56s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 37s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 40s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 49s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 59s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 21s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 8s {color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 7s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 1m 23s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 34s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 1m 34s {color} | {color:green} hadoop-mapreduce-project_hadoop-mapreduce-client generated 0 new + 359 unchanged - 5 fixed = 359 total (was 364) {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 38s {color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client: The patch generated 4 new + 1086 unchanged - 11 fixed = 1090 total (was 1097) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 40s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 49s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green} 0m 2s {color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 44s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 0s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 2m 54s {color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 0m 42s {color} | {color:green} hadoop-mapreduce-client-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 8m 55s {color} | {color:green} hadoop-mapreduce-client-app in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 104m 24s {color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 32s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 146m 3s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12843339/MAPREDUCE-5951-trunk.017.patch |
| JIRA Issue | MAPREDUCE-5951 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux 563b10e197ee 3.13.0-93-generic #140-Ubuntu SMP Mon Jul 18 21:21:05 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 64a2d5b |
| Default Java | 1.8.0_111 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6845/artifact/patchprocess/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6845/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6845/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","16/Dec/16 18:36;ctrezzo;Attached is v18 to fix the one checkstyle issue. There are three outstanding checkstyle issues that I am leaning towards not fixing as part of the patch. Please let me know your thoughts. They are the following:
bq. ./hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java:752:  private static ContainerLaunchContext createCommonContainerLaunchContext(:3: Method length is 172 lines (max allowed is 150).

This patch barely touches this method so it seems wrong to refactor the method as part of this jira. I can file a separate jira to fix this.

bq. ./hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java:341:  public ApplicationSubmissionContext createApplicationSubmissionContext(:3: Method length is 249 lines (max allowed is 150).

The same reasoning applies to this warning as the previous issue. I can file a separate jira to fix this.

bq. ./hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/util/MRApps.java:572:  private static void parseDistributedCacheArtifacts(:23: More than 7 parameters (found 8).

This issue was caused by this patch adding an additional parameter to this method. I can fix the number of parameter issues, but that forces me to touch three existing calls to the deprecated DistributedCache api, which would fix 1 warning but create 3 new ones. It is a larger change to not use the deprecated api because the existing code is not set up to use it, furthermore use of the deprecated api is currently widespread in this code. My thoughts are that I will leave this warning as is.","16/Dec/16 21:25;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 20s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 5 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 10s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 16m 26s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 16s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 51s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 2m 9s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 1m 17s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 3m 0s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 26s {color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 10s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 1m 48s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 3s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 2m 3s {color} | {color:green} hadoop-mapreduce-project_hadoop-mapreduce-client generated 0 new + 359 unchanged - 5 fixed = 359 total (was 364) {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 42s {color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client: The patch generated 3 new + 1086 unchanged - 11 fixed = 1089 total (was 1097) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 49s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 1m 3s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green} 0m 3s {color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 3m 41s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 18s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 3m 33s {color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 0m 55s {color} | {color:green} hadoop-mapreduce-client-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 9m 52s {color} | {color:green} hadoop-mapreduce-client-app in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 110m 54s {color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 27s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 167m 29s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12843632/MAPREDUCE-5951-trunk.018.patch |
| JIRA Issue | MAPREDUCE-5951 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux 618f3b149fde 3.13.0-95-generic #142-Ubuntu SMP Fri Aug 12 17:00:09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / a956390 |
| Default Java | 1.8.0_111 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6847/artifact/patchprocess/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6847/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6847/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","16/Dec/16 21:38;ctrezzo;I have filed MAPREDUCE-6825 and MAPREDUCE-6824 to address the checkstyle issues around methods that are too long.","11/Jan/17 05:13;sjlee0;I went over the patch with Chris in some detail today, and am posting the review comments here for the record.

(MRJobConfig.java)
- {{mapreduce.job.jobjar.visibility}} and {{mapreduce.job.jobjar.sharedcache.uploadpolicy}} are computed values that are not user-facing; in that case we should not even define the defaults so that there is no confusion that these values are computed

(JobResourceUploader.java)
- l.159-160: I understand lines 161-165 are there to support programmatic use cases of the distributed cache that come in outside of the job submitter code path. Can we make the comments clearer so that the intent of this comes through? We could also annotate them in {{MRJobConfig}}.
- l.171-172: it might be slightly better to use {{LinkedHashMap}}. That way, we'd have a predictable iteration order (the order in which they are specified in the user values).
- l.219: To be fair, this is a bug we need to fix, right? Then can we file a JIRA and add the JIRA id here?
- l.237-240: I would not worry about handling previous values here. Having duplicate paths is not really supported and the worst case scenario here is to reset this upload policy with the same value.
- l.260-263: I think we can improve on this, and reconcile the shared cache with the wildcard feature. We could see if any resource is uploaded to the staging directory, and if so, still preserve the wildcard entry. We also need to consider the case where the shared cache is disabled but the wildcard is enabled.
- l.291-294: same comment as above
- l.348-351: same comment as above
- l.388: Since we're dealing with a local filesystem URI based on l.381-382, the authority check is not meaningful. We should remove this check.
- On a larger note, the path/URI handling between the job jar, libjars, files, and archives is not very consistent, which is an existing behavior. We need to see if they need to get the same consistent treatment for this to work.

(Job.java)
- l.1446: I realized later that passing an empty map has an effect of nulling out the config value; perhaps we could make that more explicit in the javadoc and/or comments/code?
- l.1449-1463: nit: it might be slightly easier to read by using a simple string concatenation with ""+"" (JVM internally uses the {{StringBuilder}})
- l.1490: here also it might be better to use {{LinkedHashMap}}
","11/Jan/17 21:12;ctrezzo;Thanks [~sjlee0] for the review! I will work on v19 of the patch to address your comments.","14/Jan/17 02:03;ctrezzo;Attaching a documentation that gives an overview of MapReduce support for the shared cache. This will hopefully make reviewing the patch easier!

I am also working on an updated patch to address the comments from [~sjlee0]. Thanks!","19/Jan/17 01:33;ctrezzo;Attached is a v19 to address [~sjlee0]'s comments. Thanks!","19/Jan/17 04:53;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 25s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 5 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 1m 13s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 18m 29s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 14s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 1m 1s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 2m 50s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 1m 24s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 3m 29s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 33s {color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 10s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 1m 59s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 37s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 2m 37s {color} | {color:green} hadoop-mapreduce-project_hadoop-mapreduce-client generated 0 new + 341 unchanged - 4 fixed = 341 total (was 345) {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 52s {color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client: The patch generated 3 new + 1086 unchanged - 11 fixed = 1089 total (was 1097) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 2m 18s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 1m 23s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green} 0m 4s {color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 3m 49s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 15s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 3m 30s {color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 0m 50s {color} | {color:green} hadoop-mapreduce-client-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 9m 21s {color} | {color:green} hadoop-mapreduce-client-app in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 110m 54s {color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 34s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 173m 35s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12848192/MAPREDUCE-5951-trunk.019.patch |
| JIRA Issue | MAPREDUCE-5951 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux 3f6e6dcb73b4 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 383aa9c |
| Default Java | 1.8.0_111 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6871/artifact/patchprocess/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6871/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6871/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","19/Jan/17 05:10;ctrezzo;These are the same 3 checkstyle warnings I mentioned in the [comment|https://issues.apache.org/jira/browse/MAPREDUCE-5951?focusedCommentId=15755192&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15755192] above.","19/Jan/17 22:57;ctrezzo;Note: currently this patch depends on how YARN-3637 is implemented. I will adjust this patch once it is committed.","11/Feb/17 00:57;sjlee0;Just to reflect the current status (waiting for an updated patch). Let me know if that is not accurate. Thanks!","29/Mar/17 23:18;ctrezzo;This issue is currently waiting on MAPREDUCE-6862 and MAPREDUCE-6846. I would like to get those two patches in so I don't have to rebase this multiple times.","27/Apr/17 20:21;xkrogen;Hey [~ctrezzo], I have a question about the behavior of this patch. Currently the old logic for resource visibility is used, so if a resource is world-readable, it will be marked as PUBLIC, else PRIVATE. Given my current understanding of this patch's behavior, I see the following scenario:
* Client submits a job with libjar X, which has never been used before. Client contacts SCM to mark X as ""used"", SCM responds that it does not have X.
* Client uploads X to staging directory, which I assume here is _not_ world-readable. X is marked as PRIVATE.
* MR-AM localizes X, then uploads it to the shared cache. Other NMs all localize X as PRIVATE and do not share it with other applications.
* Client then submits the same job with the same X. Client contacts SCM, and SCM responds with a world-readable (755 dirs / 555 file) path inside of the shared cache.
* Client does not upload X, and marks X as PUBLIC, since it is currently in a world-readable location. 
* MR-AM and NMs all localize X as PUBLIC and share it with other applications.

Please correct me if I am wrong on any of these steps. It seems that it is the expected behavior that X is eventually PUBLIC, given that we asked for it to be uploaded to the publicly shared cache, but it seems unnecessary for it to be marked as PRIVATE the first time around. Do we do this just to avoid changing the existing logic for marking a resource as PRIVATE vs PUBLIC, is this an oversight, or is this behavior desired?","27/Apr/17 23:35;ctrezzo;Thanks [~xkrogen] for the comment!

bq. is this an oversight, or is this behavior desired?

Originally we just left it private because we wanted to avoid having to change the staging directory and that portion of how MapReduce uploaded resources. As I am looking more at YARN-5727, I think it makes more sense to do this so that the resources are initially uploaded to a public place and explicitly set with a public visibility by the MapReduce client. I was thinking of potentially adding a public staging directory that is created and cleaned up by the MapReduce client along with the current staging directory. [~xkrogen] would you have any thoughts on this? [~jlowe] would you have any thoughts on this as well?","28/Apr/17 12:38;jlowe;I don't think it really matters whether the jar resource uploaded by the client is public or private.  In both cases the HDFS path to which the client posts the resource will be removed when the job completes.  If any subsequent jobs come along and figure out via the SCM that they can avoid uploading their own, redundant copy of the same resource then they will receive a resource path within the SCM area which is a _different_ path than the one used by the first job.  That means the resource is going to get downloaded to the node again because it's in a different location than the first job's resource.

Even if the first job's client uploads the resource to a public directory, no other job is going to ask for that resource under the same path.  It will be uploaded to a public staging directory which is specific to that app and whose path exists only as long as the app.  The problem with having jobs try to share resources automatically just from the job client is knowing when the resource can be removed, otherwise we could yank it just as another app tries to localize it or never clean it up.  That's why the SCM does the necessary ref counting to know what's being used and when resources can be freed safely.  If we want to avoid the double-download of the resource then the job client will need to upload the resource to the SCM directly and then submit the job _after_ it has received the public resource path from the SCM.
","28/Apr/17 15:44;xkrogen;Ah, excellent point, [~jlowe]... I actually would love to hear the reasoning behind the current strategy of <client uploads resource to HDFS -> AM downloads resource -> AM uploads resource to SCM> rather than the seemingly more obvious/simpler <client uploads resource to SCM>. Is this so that the uploading to SCM can be done by the NM, which is a privileged user, to have more secure control over it?

[~ctrezzo], first off thanks for getting back so quickly! And for the pointer to YARN-5727; that's an interesting issue. The public visibility solution is certainly simpler from the YARN side and seems pretty reasonable from a point of expectation of burden on an application (""you want a publicly shared resource? put it somewhere public""). It  doesn't add _too_ much complexity on the MR side, though having a separate staging directory just for public resources is a bit cumbersome. It also means that other application developers will have to build the same type of logic - in general I would lean towards more logic pushed into the YARN level so that it is easy for application devs to support. I don't have good insight into how difficult your initially proposed solution in YARN-5727 would be to implement, though.","28/Apr/17 22:45;ctrezzo;[~xkrogen]

bq. Is this so that the uploading to SCM can be done by the NM, which is a privileged user, to have more secure control over it?

Yes exactly. We wanted to ensure that only trusted entities (i.e. the SCM and the node manager) were modifying the shared cached directories in HDFS. Additionally, we wanted to make sure that the checksum used when adding a resource to the cache was computed by a trusted entity as well.","03/Oct/17 00:56;ctrezzo;Attached is a v20 trunk patch.","03/Oct/17 03:54;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 45s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 6 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 54s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m 25s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 46s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 44s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 46s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 44s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 17s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m  8s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 41s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 41s{color} | {color:green} hadoop-mapreduce-project_hadoop-mapreduce-client generated 0 new + 354 unchanged - 4 fixed = 354 total (was 358) {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 42s{color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client: The patch generated 3 new + 1062 unchanged - 11 fixed = 1065 total (was 1073) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 43s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  2s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m  1s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  3s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  3m  0s{color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 44s{color} | {color:green} hadoop-mapreduce-client-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  9m 19s{color} | {color:green} hadoop-mapreduce-client-app in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}114m 29s{color} | {color:red} hadoop-mapreduce-client-jobclient in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 28s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}163m 25s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.mapred.TestLocalMRNotification |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:71bbb86 |
| JIRA Issue | MAPREDUCE-5951 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12890076/MAPREDUCE-5951-trunk-020.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux f0ce73fc9e8f 3.13.0-129-generic #178-Ubuntu SMP Fri Aug 11 12:48:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 015abcd |
| Default Java | 1.8.0_144 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7169/artifact/patchprocess/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
| unit | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7169/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7169/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7169/console |
| Powered by | Apache Yetus 0.5.0   http://yetus.apache.org |


This message was automatically generated.

","04/Oct/17 08:57;ctrezzo;Attached is trunk v21. This fixes checkstyle issues and adds documentation.","04/Oct/17 12:41;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 25s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 6 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 58s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 19m 46s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 22m 15s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m 48s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m  3s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m  5s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m  6s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 16s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  2m  5s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m 14s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red} 13m 14s{color} | {color:red} root generated 1 new + 1255 unchanged - 16 fixed = 1256 total (was 1271) {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m 16s{color} | {color:green} root: The patch generated 0 new + 1058 unchanged - 15 fixed = 1058 total (was 1073) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 38s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  3s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m 37s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 57s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 16s{color} | {color:green} hadoop-project in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  3m  2s{color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 53s{color} | {color:green} hadoop-mapreduce-client-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  9m  5s{color} | {color:green} hadoop-mapreduce-client-app in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}114m 18s{color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  1m  2s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}211m 44s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:71bbb86 |
| JIRA Issue | MAPREDUCE-5951 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12890320/MAPREDUCE-5951-trunk-021.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux 11a2b56aaa23 3.13.0-129-generic #178-Ubuntu SMP Fri Aug 11 12:48:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / acf5b88 |
| Default Java | 1.8.0_144 |
| findbugs | v3.1.0-RC1 |
| javac | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7170/artifact/patchprocess/diff-compile-javac-root.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7170/testReport/ |
| modules | C: hadoop-project hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: . |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7170/console |
| Powered by | Apache Yetus 0.5.0   http://yetus.apache.org |


This message was automatically generated.

","05/Oct/17 00:23;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 20s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 6 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 26s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 46s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 14m  9s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m  1s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 17s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 31s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 27s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 16s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 10m 43s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red} 10m 43s{color} | {color:red} root generated 1 new + 1255 unchanged - 16 fixed = 1256 total (was 1271) {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m  1s{color} | {color:green} root: The patch generated 0 new + 1058 unchanged - 15 fixed = 1058 total (was 1073) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  4s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m  6s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 35s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 11s{color} | {color:green} hadoop-project in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  2m 55s{color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 48s{color} | {color:green} hadoop-mapreduce-client-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  9m  3s{color} | {color:green} hadoop-mapreduce-client-app in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}109m 11s{color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 41s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}182m 35s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:71bbb86 |
| JIRA Issue | MAPREDUCE-5951 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12890320/MAPREDUCE-5951-trunk-021.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux 42144958904a 4.4.0-43-generic #63-Ubuntu SMP Wed Oct 12 13:48:03 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 2df1b2a |
| Default Java | 1.8.0_144 |
| findbugs | v3.1.0-RC1 |
| javac | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7172/artifact/patchprocess/diff-compile-javac-root.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7172/testReport/ |
| modules | C: hadoop-project hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: . |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7172/console |
| Powered by | Apache Yetus 0.5.0   http://yetus.apache.org |


This message was automatically generated.

","05/Oct/17 20:16;ctrezzo;This is the javac warning:
bq. [WARNING] /testptch/hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/util/LocalResourceBuilder.java:[34,44] [deprecation] DistributedCache in org.apache.hadoop.mapreduce.filecache has been deprecated

LocalResourceBuilder was a class added to fix a checkstyle warning. I have used {{@SuppressWarnings(""deprecation"")}} to silence the warnings around DistributedCache usage at the class level. This warning is complaining about the import statement. If anyone has an idea for how to apply the annotation to the import statement, please let me know. Furthermore, the LocalResourceBuilder is simply refactoring the MRApps#parseDistributedCacheArtifacts method, so I do not think it makes sense to fix the usage of a deprecated interface in this patch, especially since it is used in a lot of places.","05/Oct/17 22:13;ctrezzo;Hi [~sjlee0], [~kasha], [~mingma], [~jlowe], and [~vrushalic]!

I have made another push to get MapReduce support for the shared cache committed. I have rebased the patch, added documentation and fixed all warnings/issues that I see so far. At this point, I need a reviewer for the final review. I know this patch is a big one, so if there is anything I can do to help with the review process to make it easier to review, or if there is someone else who might be interested in the review, please let me know. Some good news:

# Much of the patch has already been reviewed by [~kasha] [~sjlee0] and [~mingma] during previous iterations.
# I have ensured that the entire feature is behind a switch. As such, when disabled (default) there are no effects for the user.
# I have functionally tested this patch on a pseudo distributed cluster.
# I have deployed this patch to a larger test cluster and ran jobs with the patch.
# There is very similar code running in production that has been working for years at this point.

My main goal is to commit this to trunk and branch-2 (2.9.0). If it can make it into branch-3.0 for GA that would be great as well, but I understand that the beta is already out ([~andrew.wang] please let me know what you think). Once I get a +1 on the patch, I would be happy to do the work to commit.

Thanks in advance for the help and effort. I really do appreciate it!","05/Oct/17 22:17;andrew.wang;If it's going into 2.9.0, I think it's safe for 3.0.0 too. Please include it in branch-3.0 as well, thanks!","06/Oct/17 01:52;mingma;Thanks [~ctrezzo]. The code looks good overall. The only question I have at this point is if any code should be moved from MR to YARN to make it easier for other YARN applications to use shared cache. For example, maybe other applications can benefit from part of  LocalResourceBuilder or the special care when dealing with fragment.","06/Oct/17 18:10;ctrezzo;Thanks for the comment [~mingma]!

bq. Should any code be moved from MR to YARN to make it easier for other YARN applications to use shared cache? For example, maybe other applications can benefit from part of LocalResourceBuilder or the special care when dealing with fragment.

I have thought about this a fair amount. Originally we started pushing more of the fragment code down into the YARN layer (see YARN-3637), but later I realized that the code dealing with fragments is purely at the MapReduce layer. YARN's api does not use fragments. Instead the ContainerLaunchContext expects a Map<String, LocalResource> localResources, where the strings are the destination file names (i.e. symlinks). We wound up pulling the fragment portion back out of YARN (see YARN-7250) because it was not consistent with the rest of the YARN api. Additionally, I think that the way MapReduce uses fragments right now is very brittle and prone to bugs. Within MapReduce, resources with fragments are converted between paths, URIs and URLs multiple times throughout the code and each of these three classes supports fragments in different ways. If you are not very careful, one could easily drop a fragment.

I also thought about moving LocalResourceBuilder to YARN, but it has a fair amount of MapReduce specific things that would need to change. For example:
# All of the parameters are array based due to how MapReduce currently handles resources. We could change this, but then that would need additional refactoring at the MapReduce level.
# Components from the MapReduce wildcard feature are in this class. We would need to figure out if that makes sense at the yarn layer.
# LocalResourceBuilder currently handles fragments, which we would also need to figure out if it makes sense at the yarn layer.

At the end of the day, it would not be simply dropping the LocalResourceBuilder into YARN and being done. We would have to think about it more. It does seem like something YARN could benefit from, along with a resource uploader. I can file another jira to cover these topics, but I think it is probably out of scope for this jira.

I think in reality the complexity in this jira is due to the way MapReduce itself handles resources and the above mentioned issues with fragments. If we wanted to implement a generic yarn resource uploader, I think it could be much simpler. For example, this is a slightly simplified version of the code devoted to using something in the shared cache:
{noformat}
String localPathChecksum = sharedCacheClient.getFileChecksum(localPath);
URL cachedResource = sharedCacheClient.use(appId, localPathChecksum);
LocalResource resource = LocalResource.newInstance(cachedResource,
      LocalResourceType.FILE, LocalResourceVisibility.PUBLIC
      size, timestamp, null, true);
{noformat}

That LocalResource can then be passed directly to the ContainerLaunchContext where a symlink can be specified as a String. As you can see, there is no innate need for fragments at the YARN layer.

Please let me know if that makes sense or if I have missed something! Thanks.","10/Oct/17 05:51;mingma;+1.","10/Oct/17 17:20;ctrezzo;Thank you [~mingma] for the review! I will wait until Thursday to commit in case there are any other comments. Otherwise, I plan to commit to trunk, branch-3.0 and branch-2.","10/Oct/17 23:15;sjlee0;+1. Thanks for the great work and taking it to completion [~ctrezzo]!","12/Oct/17 18:33;ctrezzo;Committed to trunk, branch-3.0 and branch-2. Thanks for all the help with reviews [~mingma], [~sjlee0], and [~kasha]!","12/Oct/17 18:42;hudson;SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #13078 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/13078/])
MAPREDUCE-5951. Add support for the YARN Shared Cache. (ctrezzo: rev e46d5bb962b0c942f993afc505b165b1cd96e51b)
* (add) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJobResourceUploaderWithSharedCache.java
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapred/TestLocalDistributedCacheManager.java
* (add) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/markdown/SharedCacheSupport.md
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobResourceUploader.java
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/util/MRApps.java
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/pom.xml
* (add) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/util/LocalResourceBuilder.java
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJobResourceUploader.java
* (add) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/SharedCacheConfig.java
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/JobImpl.java
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestMRJobs.java
* (edit) hadoop-project/src/site/site.xml
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/v2/util/TestMRApps.java
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestLocalJobSubmission.java
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a JSP page for RaidNode,MAPREDUCE-2442,12504500,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,schen,schen,schen,17/Apr/11 20:01,01/Aug/17 17:12,12/Jan/21 09:52,01/Aug/17 17:12,,,,,,,,,contrib/raid,,,,,,0,,,,,,,aw,rvadali,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-08-01 17:12:52.997,,,false,,,,,,,,,,,,,,,,,,150196,,,,,Tue Aug 01 17:12:52 UTC 2017,,,,,,,"0|i0e7hz:",80977,,,,,,,,,,,,,,,,,,,,,"01/Aug/17 17:12;aw;MR RAID has been replaced by HDFS EC in modern versions of Hadoop.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow raid to use Reed-Solomon erasure codes,MAPREDUCE-1969,12470213,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,rvadali,schen,schen,26/Jul/10 22:23,01/Aug/17 17:12,12/Jan/21 09:52,01/Aug/17 17:12,,,,,,,,,contrib/raid,,,,,,1,,,,,"Currently raid uses one parity block per stripe which corrects one missing block on one stripe.
Using Reed-Solomon code, we can add any number of parity blocks to tolerate more missing blocks.
This way we can get a good file corrupt probability even if we set the replication to 1.

Here are some simple comparisons:
1. No raid, replication = 3:
File corruption probability = O(p^3), Storage space = 3x

2. Single parity raid with stripe size = 10, replication = 2:
File corruption probability = O(p^4), Storage space = 2.2x 

3. Reed-Solomon raid with parity size = 4 and stripe size = 10, replication = 1:
File corruption probability = O(p^5), Storage space = 1.4x

where p is the missing block probability.
Reed-Solomon code can save lots of space without compromising the corruption probability.

To achieve this, we need some changes to raid:
1. Add a block placement policy that knows about raid logic and do not put blocks on the same stripe on the same node.
2. Add an automatic block fixing mechanism. The block fixing will replace the replication of under replicated blocks.
3. Allow raid to use general erasure code. It is now hard coded using Xor.
4. Add a Reed-Solomon code implementation

We are planing to use it on the older data only.
Because setting replication = 1 hurts the data locality.
",,anty,atm,aw,baggioss,celinasam,dhruba,guoleitao,hammer,herberts,lianhuiwang,luoli,rschmidt,rvadali,srivas,steffeng,szetszwo,tdunning,tomwhite,vicaya,wtantisi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-2169,MAPREDUCE-1831,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-08-23 21:42:02.266,,,false,,,,,,,,,,,,,,,,,,149896,,,,,Tue Aug 01 17:12:52 UTC 2017,,,,,,,"0|i0e8a7:",81104,,,,,,,,,,,,,,,,,,,,,"23/Aug/10 21:42;dking;Proposal 3 would have to be applied only to data that essentially never gets deleted, because deleting a block would affect four parity blocks.","24/Aug/10 02:23;dhruba;for all these proposals, the unwritten assumption is that all the blocks in a stripe belong to the same hdfs file. In that case, when the data file is deleted, the parity file can be deleted too.","30/Aug/10 20:30;wtantisi;How fast this RS implementation encode per sec? In case we need a faster encoder, I am thinking about porting Cauchy Reed-Solomon as described @ http://www.cs.utk.edu/~plank/plank/papers/FAST-2009.pdf to Java. James S. Plank, the author, has already given me a permission to release it with Apache License. ","30/Aug/10 20:41;rvadali;Our feeling is that IO costs will dominate CPU cost, but we do not have experimental results yet.","12/Oct/10 20:17;schen;Wittawat:

The RS implementation has a complexity of
O(n^2) where n is the parity length.

In our case, the parity length is really small (we pick 4).
So we think the efficiency should not be a problem here.","01/Aug/17 17:12;aw;MR RAID has been replaced by HDFS EC in modern versions of Hadoop.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Benchmarking random reads with DFSIO,MAPREDUCE-4651,12607196,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,shv,shv,shv,12/Sep/12 07:48,08/Jun/17 12:10,12/Jan/21 09:52,25/Sep/12 21:40,1.0.0,,,,,0.23.4,,,benchmarks,test,,,,,0,,,,,"TestDFSIO measures throughput of HDFS write, read, and append operations. It will be useful to have an option to use it for benchmarking random reads.",,adi2,benoyantony,cdouglas,eli,erik.fang,hudson,jghoman,raviprak,revans2,shv,szetszwo,vincent he,zero45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-14508,,,,,,,,"25/Sep/12 19:34;shv;randomDFSIO.patch;https://issues.apache.org/jira/secure/attachment/12546568/randomDFSIO.patch","14/Sep/12 03:57;shv;randomDFSIO.patch;https://issues.apache.org/jira/secure/attachment/12545095/randomDFSIO.patch","12/Sep/12 08:42;shv;randomDFSIO.patch;https://issues.apache.org/jira/secure/attachment/12544790/randomDFSIO.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2012-09-13 17:18:43.84,,,false,,,,,,,,,,,,,,,,,,247034,Reviewed,,,,Fri Sep 28 13:45:13 UTC 2012,,,,,,,"0|i07y3b:",44290,,,,,,,,,,,,,,,,,,,,,"12/Sep/12 08:11;shv;The idea is to utilize HDFS positional read, which is defined by {{PositionedReadable}} and allows to read a segment of data from a given position.
I propose three variants of such benchmarks:
# *Random read*. Randomly choose an offset in the range [0, fileSize] and read one buffer of data from that random position. Repeat operation until a specified number of bytes is read. 
Random read can occasionally read the same bytes twice.
# *Backward read* reads file in reverse order.
This is intended to read all bytes of the given file, but avoid reading any of them twice.
# *Skip read*. Starting from the beginning read one buffer of data, then jump ahead, and read again. Repeat until either the specified number of bytes is read or the end of file is reached.
Skip read allows to avoid read-ahead. With sequential read data mostly comes from the system block cache. Jumping ahead far enough will ensure that bytes are actually read from the storage device.","12/Sep/12 08:42;shv;The patch
# Introduces the three types of random reads.
# It also adds getIOStream() method, which excludes stream construction from the timed part of the execution. This is important for small writes and reads.
And this let me move all compression functionality from IOMapperBase to TestDFSIO, where it truly belongs.
# Converted to JUnit 4 format. Finally. And added test cases for new benchmarks.
# Fixed couple of warnings and removed unnecessary generic parameters.","13/Sep/12 17:18;raviprak;Hi Konstantin,

Thanks for this initiative. I like the idea of benchmarking random reads. Some comments:
1. Why not label IOMapperBase.getIOStream() abstract rather than return null?
2. Some extra whitespaces.
3. TestDFSIO:doIO, @Override //IOMapperBase
4. In doIO(), would it make sense to do?
      if( this.stream instanceof InputStream) InputStream in = (InputStream)this.stream;
   Similarly for PositionedReadable
5. public RandomReadMapper()  you can use new Random(), to seed it with a distinct seed. You don't need a call to System.nanoTime().

Oh, and could you please review MAPREDUCE-4645? =D","13/Sep/12 18:52;shv;(1) I did want to make IOMapperBase.getIOStream() abstract, but there are other tests that are based on IOMapperBase. A dummy implementation of getIOStream() will avoid changing them.
(2) Do you want me to add spaces or you see unnecessary spaces in my patch?
(3) So you suggest to add comments to @Override specifying the base class, right? Agreed, should have done that for the new method, will also do it for doIO().
(4) Sure you can check the instance before casting, but what would you do in the else clause - throw exception. So one way or another an exception will be thrown saying the type of stream is not right. And this is sort of an assert, because it means there is a bug in DFSIO, not user's fault.
(5) Yep, you are right nanoTime() is already in the default constructor. I believe it wasn't there before.

Thanks for the review. I'll check SLive jira.","14/Sep/12 03:57;shv;Removed nanoTime(), added comments to @Override.","14/Sep/12 16:15;raviprak;Thanks Konstantin! I applied the patch and ran the random and backward read tests on my single node dev box.

{noformat}
$HADOOP_PREFIX/bin/hadoop org.apache.hadoop.fs.TestDFSIO -read -random -fileSize 10MB 
Average IO rate mb/sec: 134.43310546875
IO rate std deviation: 0.00896365222201456

$HADOOP_PREFIX/bin/hadoop org.apache.hadoop.fs.TestDFSIO -read -backward -fileSize 10MB
Average IO rate mb/sec: 134.49253845214844
IO rate std deviation: 0.026679629420752023

$HADOOP_PREFIX/bin/hadoop org.apache.hadoop.fs.TestDFSIO -read -random -fileSize 1GB
Average IO rate mb/sec: 249.47183227539062
IO rate std deviation: 0.014617091655162118

$HADOOP_PREFIX/bin/hadoop org.apache.hadoop.fs.TestDFSIO -read -backward -fileSize 1GB
Average IO rate mb/sec: 295.8538818359375
IO rate std deviation: 0.061419808441541615

$HADOOP_PREFIX/bin/hadoop org.apache.hadoop.fs.TestDFSIO -read -random -fileSize 10GB
Average IO rate mb/sec: 320.3417663574219
IO rate std deviation: 0.05935480659067817

$HADOOP_PREFIX/bin/hadoop org.apache.hadoop.fs.TestDFSIO -read -backward -fileSize 10GB
Average IO rate mb/sec: 323.28045654296875
IO rate std deviation: 0.0598550775330073

$HADOOP_PREFIX/bin/hadoop org.apache.hadoop.fs.TestDFSIO -read -backward -fileSize 30GB
Average IO rate mb/sec: 390.9880065917969
IO rate std deviation: 0.06083891027478396

$HADOOP_PREFIX/bin/hadoop org.apache.hadoop.fs.TestDFSIO -read -random -fileSize 30GB
Average IO rate mb/sec: 369.2136535644531
IO rate std deviation: 0.056819116587427144
{noformat}

Could you please post recommended usage? And at what sizes do we expect to achieve stable IO rates?
","25/Sep/12 19:09;jghoman;+1. Good refactoring as well.  nit: bit of wonky spacing in TestDFSIO::AppendMapper::getIOStream.","25/Sep/12 19:34;shv;Fixed wonky space in AppendMapper.getIOStream()","25/Sep/12 19:55;shv;Ravi, 
- even with single node you can specify more -nrFiles if you have multiple drives on the node. I usually setup number of map slots equal to the number of drives on a node.
- I don't know how big was the file that you created with -write prior to reads. If it was 10 MB than the actual size of reads was not more than that. Check the DFSIO summary it prints how much data was read.
- You probably ran reads right after creating the file. So the the data was in buffer cache. I usually clean the cache before each test run. (On linux 'echo 1 > /proc/sys/vm/drop_caches')
- Also -fileSize is replaced by -size in my patch. It says how much data you want to read/write/append, rather than specifying the size of a file. Initially (read/write) it was the same.","25/Sep/12 20:29;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12546568/randomDFSIO.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2878//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2878//console

This message is automatically generated.","25/Sep/12 21:40;shv;I just committed this to trunk, branch-2, and branch 0.23.4","25/Sep/12 21:55;hudson;Integrated in Hadoop-Hdfs-trunk-Commit #2835 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2835/])
    MAPREDUCE-4651. Benchmarking random reads with DFSIO. (Revision 1390159)

     Result = SUCCESS
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1390159
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/IOMapperBase.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java
","25/Sep/12 21:58;hudson;Integrated in Hadoop-Common-trunk-Commit #2772 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2772/])
    MAPREDUCE-4651. Benchmarking random reads with DFSIO. (Revision 1390159)

     Result = SUCCESS
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1390159
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/IOMapperBase.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java
","25/Sep/12 22:40;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #2793 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2793/])
    MAPREDUCE-4651. Benchmarking random reads with DFSIO. (Revision 1390159)

     Result = FAILURE
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1390159
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/IOMapperBase.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java
","26/Sep/12 12:42;hudson;Integrated in Hadoop-Hdfs-0.23-Build #386 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/386/])
    MAPREDUCE-4651. Benchmarking random reads with DFSIO. Contributed by Konstantin Shvachko. (Revision 1390164)

     Result = UNSTABLE
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1390164
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/IOMapperBase.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java
","26/Sep/12 12:58;hudson;Integrated in Hadoop-Hdfs-trunk #1177 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1177/])
    MAPREDUCE-4651. Benchmarking random reads with DFSIO. (Revision 1390159)

     Result = SUCCESS
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1390159
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/IOMapperBase.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java
","26/Sep/12 13:59;hudson;Integrated in Hadoop-Mapreduce-trunk #1208 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1208/])
    MAPREDUCE-4651. Benchmarking random reads with DFSIO. (Revision 1390159)

     Result = SUCCESS
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1390159
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/IOMapperBase.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java
","27/Sep/12 14:54;szetszwo;Hi Konstantin, the patch no longer applies to trunk.  Could you update it?","28/Sep/12 07:56;shv;That is because I committed it. Jakob reviewed.","28/Sep/12 13:45;szetszwo;Oops, I forgot to reload the page.  It is great that Jakob has reviewed it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backport MAPREDUCE-6304 to branch 2.7: Specifying node labels when submitting MR jobs,MAPREDUCE-6890,13072984,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,,redvine,redvine,18/May/17 02:30,18/May/17 07:44,12/Jan/21 09:52,18/May/17 07:36,,,,,,,,,,,,,,,0,,,,,"As per discussussion in [mailling list|http://mail-archives.apache.org/mod_mbox/hadoop-mapreduce-dev/201705.mbox/browser] backport MAPREDUCE-6304 to branch-2.7. ",,redvine,shv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-05-18 07:36:31.823,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 18 07:44:14 UTC 2017,,,,,,,"0|i3f59j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"18/May/17 02:33;redvine;[~shv] or [~zhz] I would like to assign this to myself and attach a patch but don't seem to have permission. Can you take a look?","18/May/17 07:36;shv;[~redvine] ooks like [~elgoiri] beat you.
Closing as Duplicate since the patch went into original MAPREDUCE-6304.","18/May/17 07:44;shv;I added you to MAPREDUCE jira contributors. You should be able to assign now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow users to specify racks and nodes for strict locality for AMs,MAPREDUCE-6871,13060097,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,rkanter,rkanter,rkanter,29/Mar/17 18:02,02/May/17 12:01,12/Jan/21 09:52,21/Apr/17 23:17,,,,,,2.9.0,3.0.0-alpha4,,client,,,,,,0,,,,,"YARN-6050 fixed the YARN API to allow multiple {{ResourceRequest}}'s when submitting an AM so that you can actually do rack or node locality.  We should allow MapReduce users to take advantage of this by exposing this functionality in some way.  The raw YARN API allows for a lot of flexibility (e.g. different resources per request, etc), but we don't necessarily want to allow the user to do too much here so they don't shoot themselves in the foot and we don't make this overly complicated.  

I propose we allow users to specify racks and nodes for strict locality.  This would allow users to restrict an MR AM to specific racks and/or nodes.  We could add a new property, {{mapreduce.job.am.resource-request.strict.locality}}, which takes a comma-separated list of entries like:
- {{/<rack>}}
- {{/<rack>/<node>}}
- {{<node>}} (assumes /default-rack)

MapReduce would then use this information to create the corresponding {{ResourceRequest}}'s.  

For example, {{mapreduce.job.am.resource-request.strict.locality=/rack1/node1}} would create the following {{ResourceRequest}}'s:
- resourceName=ANY, relaxLocality=false, capability=<X,Y>
- resourceName=/rack1, relaxLocality=false, capability=<X,Y>
- resourceName=node1, relaxLocality=true, capability=<X,Y>

By default, the property would be unset, and you'd get the normal {{ANY}} {{ResourceRequest}}.",,haibochen,hudson,kasha,rkanter,zhz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,OOZIE-2874,,,,,,,,,,,,,,,,,,,,,"29/Mar/17 18:07;rkanter;MAPREDUCE-6871.001.patch;https://issues.apache.org/jira/secure/attachment/12861073/MAPREDUCE-6871.001.patch","06/Apr/17 22:53;rkanter;MAPREDUCE-6871.002.patch;https://issues.apache.org/jira/secure/attachment/12862389/MAPREDUCE-6871.002.patch","13/Apr/17 01:52;rkanter;MAPREDUCE-6871.003.patch;https://issues.apache.org/jira/secure/attachment/12863188/MAPREDUCE-6871.003.patch","20/Apr/17 00:29;rkanter;MAPREDUCE-6871.004.patch;https://issues.apache.org/jira/secure/attachment/12864170/MAPREDUCE-6871.004.patch","20/Apr/17 16:35;rkanter;MAPREDUCE-6871.005.patch;https://issues.apache.org/jira/secure/attachment/12864311/MAPREDUCE-6871.005.patch","20/Apr/17 16:35;rkanter;MAPREDUCE-6871.005.patch;https://issues.apache.org/jira/secure/attachment/12864310/MAPREDUCE-6871.005.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,2017-03-29 20:41:54.759,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Apr 21 23:56:55 UTC 2017,,,,,,,"0|i3cyfj:",9223372036854775807,,,,,,,,,,,,,2.9.0,3.0.0-alpha4,,,,,,,"29/Mar/17 18:07;rkanter;The 001 patch:
- Adds the new property with the behavior as described above
- Adds unit tests
- I also verified that it behaves correctly in a cluster","29/Mar/17 20:41;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 20s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 51s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 16m 25s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 20s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 40s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 18s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 35s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 34s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 45s {color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 10s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 1m 12s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 27s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 2m 27s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 38s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 7s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 32s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 59s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red} 0m 33s {color} | {color:red} hadoop-mapreduce-client-core in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 3m 21s {color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 114m 45s {color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 25s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 153m 28s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12861073/MAPREDUCE-6871.001.patch |
| JIRA Issue | MAPREDUCE-6871 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 64f5fe4d508b 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 15e3873 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| javadoc | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6934/artifact/patchprocess/patch-javadoc-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6934/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6934/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","05/Apr/17 19:27;haibochen;Thanks [~rkanter] for the patch! A few comments
1) Do you think having regular expression for the configuration is more robust? rack1/node2 will be treated as a node name with current implementation. If this will cause application failure later, better we catch it early before submission.
2) Maybe rename mapreduce.job.am.resource-request.strict.locality to mapreduce.job.am.resource-request-strict-locality ?
3) The different test cases can be broken into different test methods individually, so that one test case failure won't disguise failures in following test cases.","06/Apr/17 22:53;rkanter;Thanks [~haibochen] for taking a look.  I've addressed your feedback.

The 002 patch:
- Uses a regex for parsing the config.  I think this makes the code much clearer and more readable because I'm also using named captured groups.  
- Renames the config property as [~haibochen] suggested.
- Refactored the test code to split it up into different tests and added a few more cases.","07/Apr/17 01:27;hadoopqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 28s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 59s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 18m 3s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 28s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 39s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 11s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 37s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 39s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 48s {color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 10s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 59s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 18s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 2m 18s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 34s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 7s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 33s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 54s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 42s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 3m 22s {color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 108m 19s {color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 31s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 148m 29s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12862389/MAPREDUCE-6871.002.patch |
| JIRA Issue | MAPREDUCE-6871 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux a9c964f291ee 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / a49fac5 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6949/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6949/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","08/Apr/17 04:36;kasha;Thanks for working on this, Robert. The patch looks mostly good. The tests in particular are quite comprehensive. Few minor comments:
# findResourceRequest, in its current form, iterates through all ResourceRequests. Instead, would it be more efficient to track the set of racks that have been included so far? 
# Can we move the AM container's ResourceRequest construction into its own method for better readability?","13/Apr/17 01:52;rkanter;Thanks for taking a look [~kasha].  I've addressed your feedback.

The 003 patch:
- Moves the code to it's own method
- Replaces the iterating lookup with a hashmap","13/Apr/17 04:23;hadoopqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 18s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 45s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 15m 54s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 9s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 35s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 8s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 40s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 46s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 45s {color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 10s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 1m 7s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 23s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 2m 23s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 36s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 10s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 38s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 8s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 41s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 3m 1s {color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 104m 1s {color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 29s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 141m 24s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:612578f |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12863188/MAPREDUCE-6871.003.patch |
| JIRA Issue | MAPREDUCE-6871 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 5271f9ca5c19 3.13.0-105-generic #152-Ubuntu SMP Fri Dec 2 15:37:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 0cab572 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6954/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6954/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","18/Apr/17 02:10;kasha;Thanks for updating the patch, Robert. Few minor comments:
# For the config, the resource-request part might not be adding much information. How about we call it just mapreduce.job.am.strict-locality and update the corresponding field in MRJobConfig as well? Sorry for not pointing it out in my previous review. 
# YARNRunner.generateResourceRequests: 
## Let us wrap the debug log in a isDebugEnabled guard.
{code}LOG.debug(""AppMaster capability = "" + capability);{code}
## rackRequests need not be a map. It could be a set as well.
# In the tests, verifyResourceRequestLocality sets node label expression multiple times. Is that required?","20/Apr/17 00:29;rkanter;The 004 patch:
- Renames the config to {{mapreduce.job.am.strict-locality}}
- Adds a debug logging check
- Removes the redundant label setting code in a test (I think that's a holdover from an earlier version)

[~kasha], for your feedback about the {{rackRequests}}: it does need to be a map.  If someone specifies {{/rack1,/rack1/node1}}, we'd need to update the already created request for rack1 to have the strict locality set to false.","20/Apr/17 02:51;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 2m 1s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 54s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 13m 0s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 33s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 26s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 48s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 34s {color} | {color:green} trunk passed {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red} 0m 54s {color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core in trunk has 3 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 32s {color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 6s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 45s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 41s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 1m 41s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 25s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 49s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 32s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 20s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 27s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 2m 42s {color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 101m 40s {color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 40s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 132m 59s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:0ac17dc |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12864170/MAPREDUCE-6871.004.patch |
| JIRA Issue | MAPREDUCE-6871 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 8fdccacbb863 4.4.0-43-generic #63-Ubuntu SMP Wed Oct 12 13:48:03 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / c154935 |
| Default Java | 1.8.0_121 |
| findbugs | v3.1.0-RC1 |
| findbugs | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6961/artifact/patchprocess/branch-findbugs-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core-warnings.html |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6961/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6961/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","20/Apr/17 16:35;rkanter;I was talking to [~templedf] about regexes and he suggested using {{String.format}} to make the regex as one String, which makes it easier to read.  The 005 patch does that.","20/Apr/17 16:35;rkanter;I was talking to [~templedf] about regexes and he suggested using {{String.format}} to make the regex as one String, which makes it easier to read.  The 005 patch does that.","20/Apr/17 19:07;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 21s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 45s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 58s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 34s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 28s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 53s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 34s {color} | {color:green} trunk passed {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red} 0m 48s {color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core in trunk has 3 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 35s {color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 7s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 40s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 31s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 1m 31s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 25s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 46s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 28s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 20s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 29s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 2m 47s {color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 110m 9s {color} | {color:red} hadoop-mapreduce-client-jobclient in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 27s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 139m 18s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.mapred.TestLocalMRNotification |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:0ac17dc |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12864311/MAPREDUCE-6871.005.patch |
| JIRA Issue | MAPREDUCE-6871 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 797fc76746c3 4.4.0-43-generic #63-Ubuntu SMP Wed Oct 12 13:48:03 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / c0cf11e |
| Default Java | 1.8.0_121 |
| findbugs | v3.1.0-RC1 |
| findbugs | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6962/artifact/patchprocess/branch-findbugs-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core-warnings.html |
| unit | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6962/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6962/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6962/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6962/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","20/Apr/17 19:25;rkanter;Test failure unrelated","21/Apr/17 22:39;kasha;bq. it does need to be a map. If someone specifies /rack1,/rack1/node1, we'd need to update the already created request for rack1 to have the strict locality set to false.
Gotcha. That makes sense.

String.format does make it more readable.

+1","21/Apr/17 23:17;rkanter;Thanks [~haibochen] and [~kasha] for reviews.  Committed to branch-2 and trunk!","21/Apr/17 23:56;hudson;SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #11622 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/11622/])
MAPREDUCE-6871. Allow users to specify racks and nodes for strict (rkanter: rev 3721cfe1fbd98c5b6aa46aefdfcf62276c28c4a4)
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make DistributedCache check if the content of a directory has changed,MAPREDUCE-6874,13061086,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,,asasvari,asasvari,03/Apr/17 10:37,03/Apr/17 13:45,12/Jan/21 09:52,03/Apr/17 13:45,,,,,,,,,,,,,,,0,,,,,"DistributedCache does not check recursively if the content a directory has changed when adding files to it with {{DistributedCache.addCacheFile()}}. 

h5. Background
I have an Oozie workflow on HDFS:
{code}
example_workflow
├── job.properties
├── lib
│   ├── components
│   │   ├── sub-component.sh
│   │   └── subsub
│   │       └── subsub.sh
│   ├── main.sh
│   └── sub.sh
└── workflow.xml
{code}
Executed the workflow; then made some changes in {{subsub.sh}}. Replaced the file on HDFS. When I re-ran the workflow, DistributedCache did not notice the changes as the timestamp on the {{components}} directory did not change. As a result, the old script was materialized.

This behaviour might be related to [determineTimestamps() |https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/filecache/ClientDistributedCacheManager.java#L84].
In order to use the new script during workflow execution, I had to update the whole {{components}} directory.


h6. Some more info:
In Oozie, [DistributedCache.addCacheFile() |https://github.com/apache/oozie/blob/master/core/src/main/java/org/apache/oozie/action/hadoop/JavaActionExecutor.java#L625] is used to add files to the distributed cache.",,asasvari,jlowe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-04-03 13:07:45.041,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 03 13:44:56 UTC 2017,,,,,,,"0|i3d4jb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"03/Apr/17 13:07;jlowe;This is a limitation with distributed cache.  It can be very expensive to do a full-depth traversal of a directory tree, and the API only supports one timestamp for a distributed cache entry.  Not only is it expensive to perform the stats of the tree in order to see if it is changed, it's also expensive to localize the files.  There's RPC overhead for each file in the tree.

It is much more efficient, and safer, for an archive (e.g.: .tar.gz, .zip, etc.) to be used instead of a directory.  Then there's only one timestamp we need to check to know if anything in the ""tree"" has changed.  Arguably directory trees shouldn't be supported in the distributed cache at all, but I believe they were added way back when to support use cases where a chain of MapReduce jobs needed the output of a previous job (i.e.: a directory) to be used as a cache file for the next job (e.g.: a map-side join).","03/Apr/17 13:44;asasvari;[~jlowe] Thanks for the detailed explanation. I was wondering why distributed cache accepted directory as input, but now I understand this is because of legacy reasons. Negative impact on performance is also clear. Closing this with won't fix.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit the number of resources a single map reduce job can submit for localization,MAPREDUCE-6690,12965711,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,ctrezzo,ctrezzo,ctrezzo,06/May/16 20:14,09/Mar/17 23:30,12/Jan/21 09:52,17/Aug/16 16:25,,,,,,2.9.0,3.0.0-alpha1,,,,,,,,0,,,,,"Users will sometimes submit a large amount of resources to be localized as part of a single map reduce job. This can cause issues with YARN localization that destabilize the cluster and potentially impact other user jobs. These resources are specified via the files, libjars, archives and jobjar command line arguments or directly through the configuration (i.e. distributed cache api). The resources specified could be too large in multiple dimensions:
# Total size
# Number of files
# Size of an individual resource (i.e. a large fat jar)

We would like to encourage good behavior on the client side by having the option of enforcing resource limits along the above dimensions.

There should be a separate effort to enforce limits at the YARN layer on the server side, but this jira is only covering the map reduce layer on the client side. In practice, having these client side limits will get us a long way towards preventing these localization anti-patterns.",,ctrezzo,hudson,jlowe,kihwal,mingma,sjlee0,templedf,vvasudev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-6862,,,,,,,,"06/May/16 20:29;ctrezzo;MAPREDUCE-6690-trunk-v1.patch;https://issues.apache.org/jira/secure/attachment/12802735/MAPREDUCE-6690-trunk-v1.patch","10/May/16 20:50;ctrezzo;MAPREDUCE-6690-trunk-v2.patch;https://issues.apache.org/jira/secure/attachment/12803314/MAPREDUCE-6690-trunk-v2.patch","02/Jun/16 02:32;ctrezzo;MAPREDUCE-6690-trunk-v3.patch;https://issues.apache.org/jira/secure/attachment/12807595/MAPREDUCE-6690-trunk-v3.patch","09/Jun/16 03:01;ctrezzo;MAPREDUCE-6690-trunk-v4.patch;https://issues.apache.org/jira/secure/attachment/12809100/MAPREDUCE-6690-trunk-v4.patch","09/Jun/16 22:22;ctrezzo;MAPREDUCE-6690-trunk-v5.patch;https://issues.apache.org/jira/secure/attachment/12809313/MAPREDUCE-6690-trunk-v5.patch","04/Aug/16 22:40;ctrezzo;MAPREDUCE-6690-trunk-v6.patch;https://issues.apache.org/jira/secure/attachment/12822196/MAPREDUCE-6690-trunk-v6.patch","16/Aug/16 23:32;ctrezzo;MAPREDUCE-6690-trunk-v7.patch;https://issues.apache.org/jira/secure/attachment/12824023/MAPREDUCE-6690-trunk-v7.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,2016-05-07 02:01:58.473,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Aug 17 19:08:29 UTC 2016,,,,,,,"0|i2xa7z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"06/May/16 20:29;ctrezzo;Attached is a v1 patch for this jira. A few notes:
# This patch enforces the Total size and Number of files dimensions.
# This patch does not support symlinks. They are broken in other places and it adds to the complexity of the patch if we want to support them (i.e. handling symlink cycles). If the community thinks symlink support is a blocker for this patch, please let me know.

Thanks!","06/May/16 21:10;ctrezzo;Also note that this isn't perfect in that jobs could be of varying sizes and this is really only limiting the per-container amount of localization. At least, in the map reduce case, all of the containers localize the same amount, not considering differences in what is already in the YARN local cache on various node managers.","07/May/16 02:01;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 11s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 9s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 6m 49s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 30s {color} | {color:green} trunk passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 42s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 29s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 58s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 27s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 30s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 32s {color} | {color:green} trunk passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 39s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 10s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 47s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 32s {color} | {color:green} the patch passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 1m 32s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 41s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 1m 41s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 27s {color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client: patch generated 8 new + 537 unchanged - 0 fixed = 545 total (was 537) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 52s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 22s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red} 0m 0s {color} | {color:red} The patch has 1 line(s) that end in whitespace. Use git apply --whitespace=fix. {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red} 0m 0s {color} | {color:red} The patch has 1 line(s) with tabs. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green} 0m 1s {color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 53s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red} 0m 22s {color} | {color:red} hadoop-mapreduce-client-core in the patch failed with JDK v1.8.0_91. {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red} 2m 18s {color} | {color:red} hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core-jdk1.7.0_95 with JDK v1.7.0_95 generated 1 new + 25 unchanged - 0 fixed = 26 total (was 25) {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 36s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 2m 9s {color} | {color:green} hadoop-mapreduce-client-core in the patch passed with JDK v1.8.0_91. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 110m 55s {color} | {color:red} hadoop-mapreduce-client-jobclient in the patch failed with JDK v1.8.0_91. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 2m 31s {color} | {color:green} hadoop-mapreduce-client-core in the patch passed with JDK v1.7.0_95. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 111m 15s {color} | {color:red} hadoop-mapreduce-client-jobclient in the patch failed with JDK v1.7.0_95. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 28s {color} | {color:green} Patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 252m 20s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| JDK v1.8.0_91 Failed junit tests | hadoop.mapred.TestMiniMRChildTask |
| JDK v1.7.0_95 Failed junit tests | hadoop.mapred.TestMiniMRChildTask |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:cf2ee45 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12802735/MAPREDUCE-6690-trunk-v1.patch |
| JIRA Issue | MAPREDUCE-6690 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux 7652e62211cc 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 1c5bbf6 |
| Default Java | 1.7.0_95 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_91 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_95 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6485/artifact/patchprocess/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6485/artifact/patchprocess/whitespace-eol.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6485/artifact/patchprocess/whitespace-tabs.txt |
| javadoc | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6485/artifact/patchprocess/patch-javadoc-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core-jdk1.8.0_91.txt |
| javadoc | hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core-jdk1.7.0_95: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6485/artifact/patchprocess/diff-javadoc-javadoc-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core-jdk1.7.0_95.txt |
| unit | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6485/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient-jdk1.8.0_91.txt |
| unit | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6485/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient-jdk1.7.0_95.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6485/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient-jdk1.8.0_91.txt https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6485/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient-jdk1.7.0_95.txt |
| JDK v1.7.0_95  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6485/testReport/ |
| modules | C:  hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core   hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient  U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6485/console |
| Powered by | Apache Yetus 0.2.0   http://yetus.apache.org |


This message was automatically generated.

","10/May/16 20:50;ctrezzo;Attached is a v2 patch. Here are the updates:
# Fixed checkstyle and whitespace issues.
# Added check for limiting the size of an individual resource.
# Test failures from v1 were unrelated.","11/May/16 03:09;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 13s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 9s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 6m 37s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 29s {color} | {color:green} trunk passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 42s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 35s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 57s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 27s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 30s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 32s {color} | {color:green} trunk passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 38s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 9s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 46s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 25s {color} | {color:green} the patch passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 1m 25s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 37s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 1m 37s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 33s {color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client: patch generated 1 new + 577 unchanged - 0 fixed = 578 total (was 577) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 52s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 23s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green} 0m 0s {color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 49s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 30s {color} | {color:green} the patch passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 34s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 1m 55s {color} | {color:red} hadoop-mapreduce-client-core in the patch failed with JDK v1.8.0_91. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 109m 9s {color} | {color:red} hadoop-mapreduce-client-jobclient in the patch failed with JDK v1.8.0_91. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 2m 36s {color} | {color:green} hadoop-mapreduce-client-core in the patch passed with JDK v1.7.0_95. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 130m 41s {color} | {color:red} hadoop-mapreduce-client-jobclient in the patch failed with JDK v1.7.0_95. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 25s {color} | {color:green} Patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 269m 24s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| JDK v1.8.0_91 Failed junit tests | hadoop.mapreduce.tools.TestCLI |
|   | hadoop.mapred.TestMRCJCFileOutputCommitter |
|   | hadoop.mapred.TestMiniMRChildTask |
| JDK v1.7.0_95 Failed junit tests | hadoop.mapred.TestMRCJCFileOutputCommitter |
|   | hadoop.mapred.TestMiniMRChildTask |
| JDK v1.7.0_95 Timed out junit tests | org.apache.hadoop.mapreduce.lib.jobcontrol.TestMapReduceJobControl |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:cf2ee45 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12803314/MAPREDUCE-6690-trunk-v2.patch |
| JIRA Issue | MAPREDUCE-6690 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux 480670560c43 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 27242f2 |
| Default Java | 1.7.0_95 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_91 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_95 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6492/artifact/patchprocess/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
| unit | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6492/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core-jdk1.8.0_91.txt |
| unit | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6492/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient-jdk1.8.0_91.txt |
| unit | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6492/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient-jdk1.7.0_95.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6492/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core-jdk1.8.0_91.txt https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6492/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient-jdk1.8.0_91.txt https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6492/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient-jdk1.7.0_95.txt |
| JDK v1.7.0_95  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6492/testReport/ |
| modules | C:  hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core   hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient  U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6492/console |
| Powered by | Apache Yetus 0.2.0   http://yetus.apache.org |


This message was automatically generated.

","24/May/16 21:45;jlowe;Thanks for the patch, Chris!  Initial comments:

Is this intended to apply to all distributed cache items or only those that need to be uploaded during job submission?  Some comments in the JIRA and the property descriptions imply it also should apply to items in the distributed cache that already reside in HDFS, but it doesn't look like the patch does that.  The changes are to JobResourceUploader which AFAIK only gets involved on files that potentially need to be copied to the staging area before job submission.  I'm not seeing how this affects items already in HDFS elsewhere before job submission (i.e.: items already in mapreduce.job.cache.*)

Speaking of mapreduce.job.cache.*, it would be nice if the properties used that same prefix since it's related to the distributed cache.  Also I'd personally prefer something like mapreduce.job.cache.limit.max-files, mapreduce.job.cache.limit.max-file-mb, and mapreduce.job.cache.limit.max-total-mb if it's supposed to apply to the entire distributed cache.

The TotalNumberOfFilesAndSize API is verbose and error-prone -- is there ever a valid reason to call incrementTotalSize without also calling incrementTotalNumberOfFiles and findMaxFileSize?  Probably does the wrong thing if the client doesn't call them all for each file.  IMHO there should just be two APIs, addFile(long filesize) and checkLimit().  Or maybe just one if it's OK to throw during addFile() directly.

Suggestion: TotalNumberOfFilesAndSize might be easier to comprehend (and type) if named something like LimitsChecker.  Also its constructor can just be passed a Configuration.  Then it can hide all the confs and other implementation details related to the dist cache limits, and a predicate function like hasLimits() can be used to do the early-out checks.  Or maybe we just pass it the files directly and it can decide internally whether to visit the paths or early-out.

I think it would be very helpful if the file path was shown in the error message when something exceeds the single-file limit, otherwise the user has to manually track it down among all the files involved.

Nit: Javadocs listing the parameters to a method but no description for any of those parameters isn't useful.
","01/Jun/16 00:14;ctrezzo;Thanks for the review [~jlowe]!

bq. Is this intended to apply to all distributed cache items or only those that need to be uploaded during job submission?

Yes, it is intended to apply to all distributed cache items as well. Good catch! I will add in the DC items to the check. As a side note: the reasoning for including DC items is that even though the DC items are in an accessible place, they could still cause a significant amount of localization to the YARN local cache. The amount of localization is affected by the local cache size and the hit rate in the cache, but I chose to go with the most conservative approach.

I will also address your other comments.","01/Jun/16 22:55;jlowe;bq.  I will add in the DC items to the check.

This reminds me: should there be a corresponding YARN feature to reject applications that are asking for too much localization?  Admins could then configure a cluster so it still rejects bad apps from frameworks that do not support this type of self-checking or from users who are overriding configs.  This check in MapReduce would still be useful from the standpoint of avoiding large copies to HDFS for staging if we know it's not going to work anyway, but the YARN check could catch any type of application before the distributed cache bomb hits the cluster nodes when the bad app runs.
","02/Jun/16 02:30;ctrezzo;bq. should there be a corresponding YARN feature to reject applications that are asking for too much localization?

Yes, totally agree. As per the description, I was thinking of creating a follow up jira for a more complete server-side YARN solution. I am thinking of something where we can leverage the container launch context and the node manager can be smart about not launching containers that will cause too much localization. I haven't thought too much about this yet, but I will definitely file the jira.","02/Jun/16 02:32;ctrezzo;V3 attached.

# Addressed comments from [~jlowe].
# Added more unit tests.","02/Jun/16 02:33;ctrezzo;Note: I was getting some InvocationTargetException failures during TestMRJobs on my local machine. I am submitting the patch anyways to get a run.","02/Jun/16 02:40;ctrezzo;I filled YARN-5192 to address the server-side YARN feature.","02/Jun/16 05:03;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 15s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 20s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 6m 14s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 30s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 32s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 51s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 23s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 5s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 32s {color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 7s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 40s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 28s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 1m 28s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 30s {color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client: The patch generated 1 new + 581 unchanged - 0 fixed = 582 total (was 581) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 46s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 18s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green} 0m 1s {color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 16s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red} 0m 19s {color} | {color:red} hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core generated 1 new + 2509 unchanged - 0 fixed = 2510 total (was 2509) {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 1m 56s {color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 124m 23s {color} | {color:red} hadoop-mapreduce-client-jobclient in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 30s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 144m 49s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.mapred.TestMRCJCFileOutputCommitter |
|   | hadoop.mapreduce.v2.TestMRJobs |
|   | hadoop.mapred.TestMiniMRChildTask |
|   | hadoop.mapreduce.v2.TestUberAM |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:2c91fd8 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12807595/MAPREDUCE-6690-trunk-v3.patch |
| JIRA Issue | MAPREDUCE-6690 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux e93aef851bf8 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 16b1cc7 |
| Default Java | 1.8.0_91 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6532/artifact/patchprocess/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
| javadoc | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6532/artifact/patchprocess/diff-javadoc-javadoc-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core.txt |
| unit | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6532/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6532/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6532/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6532/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","08/Jun/16 18:47;templedf;Please suffer me a dumb question: assuming that YARN-5192 is implemented, why do we also need this JIRA?  Doesn't having two settings to do the same thing from different ends make the system needlessly confusing?","08/Jun/16 19:33;jlowe;Implementing the check in MapReduce allows for fast-failure and more accurate/informative errors to the client.  The check in MapReduce can prevent an unnecessary upload of one or more resources to the staging area in HDFS because the client knows the job is going to fail anyway.  Also YARN-5192 will only be able to detect the error when a container starts to localize on a node that asks for a resource set that violate the limits.  Since MapReduce localizes everything for all containers (including the AM) it will fail under YARN-5192 as soon as the AM tries to run on a node, but it might take a while for the AM to get scheduled.  As for error reporting, if the violation comes from one or more files that were submitted locally then the paths via a YARN-5192 check will be for HDFS staging directories rather than the local path the client originally specified.  The error also will not be reported to the job client submitting the job unless it hangs around to monitor the job after submission.  With this check the job client will get the error directly when it tries to submit.

If we don't care much about these differences then we can just go with the YARN-5192 implementation.","08/Jun/16 19:43;templedf;Thanks for the clarification, [~jlowe].  I assumed that YARN-5192 would implement the check as part of the submit call so that the client gets immediate feedback.  The point that I forgot about, though, is that regardless the submit only happens after the resources have been uploaded to HDFS.  Given that this check specifically targets wide loads, the cases where the server-side check would reject the submit are exactly the ones that will waste the most time with the upload.

I now see the light.  I would like to find a way, however, to try to keep the two settings in sync if possible.  I've seen cases, such as the number of concurrent moves in the HDFS mover, where the limit is set on both the client and server sides, and it ends up confusing customers.  What about having the RM offer up its resource limits through a call?  The client could then query the RM's limits and apply those.","08/Jun/16 19:58;jlowe;bq. I assumed that YARN-5192 would implement the check as part of the submit call so that the client gets immediate feedback.

Note that YARN-5192 cannot do the check on application submit.  An application submit only requires the resources necessary to get the ApplicationMaster localized.  Subsequent containers for the application could have a completely different set of resources, and they won't be available in the application submission context for validation at submit time.  MapReduce is an app framework that happens to localize all resources for all containers, but other application frameworks do not always do this.

bq.  I would like to find a way, however, to try to keep the two settings in sync if possible.

Agreed it would be annoying for admins to have to keep these in sync, assuming nobody would ever want to configure the YARN limit higher than the MapReduce limit.

bq. What about having the RM offer up its resource limits through a call?

That would be one way to tackle it.  There have been cases in the past where it would have been nice for clients to be able to query config settings via the central daemons (i.e.: namenode, resourcemanager, etc.) rather than assume the local settings in hdfs-site.xml or yarn-site.xml are the same as what the central daemon is using.  That's a somewhat open-ended API change for YARN with backwards-compatibility concerns going forward, but maybe it's time we hammered out whether or not we're going to do it on a YARN JIRA and if not, what clients/users are supposed to do to better keep the client and the server in sync.

","09/Jun/16 03:01;ctrezzo;V4 attached.
# Fixed checkstyle/javadoc.
# Fixed TestMRJobs failures (test only changes).","09/Jun/16 03:09;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 0s {color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} docker {color} | {color:red} 0m 4s {color} | {color:red} Docker failed to build yetus/hadoop:2c91fd8. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12809100/MAPREDUCE-6690-trunk-v4.patch |
| JIRA Issue | MAPREDUCE-6690 |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6543/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","09/Jun/16 05:42;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 30s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 18s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 6m 20s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 30s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 32s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 51s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 23s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 8s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 35s {color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 7s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 44s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 30s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 1m 30s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 31s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 47s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 19s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green} 0m 1s {color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 20s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red} 0m 19s {color} | {color:red} hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core generated 2 new + 2508 unchanged - 1 fixed = 2510 total (was 2509) {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 2m 1s {color} | {color:red} hadoop-mapreduce-client-core in the patch failed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 114m 22s {color} | {color:red} hadoop-mapreduce-client-jobclient in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 24s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 135m 26s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.mapreduce.tools.TestCLI |
|   | hadoop.mapred.TestMRCJCFileOutputCommitter |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:2c91fd8 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12809100/MAPREDUCE-6690-trunk-v4.patch |
| JIRA Issue | MAPREDUCE-6690 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux f6e7eb5194a4 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 1500a0a |
| Default Java | 1.8.0_91 |
| findbugs | v3.0.0 |
| javadoc | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6544/artifact/patchprocess/diff-javadoc-javadoc-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core.txt |
| unit | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6544/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core.txt |
| unit | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6544/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6544/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core.txt https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6544/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6544/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6544/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","09/Jun/16 21:29;ctrezzo;V5 attached.
# Fixed javadoc.
# Test failures are unrelated.","10/Jun/16 00:54;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 16s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 7s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 7m 47s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 52s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 33s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 59s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 25s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 31s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 35s {color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 7s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 48s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 45s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 1m 45s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 30s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 55s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 22s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red} 0m 0s {color} | {color:red} The patch has 20 line(s) that end in whitespace. Use git apply --whitespace=fix. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green} 0m 2s {color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 25s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 29s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 2m 1s {color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 122m 4s {color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 26s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 145m 48s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:2c91fd8 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12809313/MAPREDUCE-6690-trunk-v5.patch |
| JIRA Issue | MAPREDUCE-6690 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux 22ced402a5f8 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 9581fb7 |
| Default Java | 1.8.0_91 |
| findbugs | v3.0.0 |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6546/artifact/patchprocess/whitespace-eol.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6546/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6546/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","10/Jun/16 20:22;ctrezzo;The whitespace errors were for lines that this patch did not touch. I am not sure why they appeared during the run. [~jlowe] the patch should be good as is, unless you have additional comments. Thanks!","04/Aug/16 22:40;ctrezzo;V6 Attached. Re-based to trunk.","05/Aug/16 01:25;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 17s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 11s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 8m 28s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 4s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 36s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 9s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 33s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 24s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 38s {color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 8s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 52s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 45s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 1m 45s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 31s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 4s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 29s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green} 0m 1s {color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 42s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 34s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 2m 15s {color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 130m 39s {color} | {color:red} hadoop-mapreduce-client-jobclient in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 29s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 156m 39s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Timed out junit tests | org.apache.hadoop.mapreduce.lib.jobcontrol.TestMapReduceJobControl |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12822196/MAPREDUCE-6690-trunk-v6.patch |
| JIRA Issue | MAPREDUCE-6690 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux b7ef7637fc03 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 438a9f0 |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
| unit | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6659/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6659/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6659/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6659/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","05/Aug/16 20:02;ctrezzo;TestMapReduceJobControl#testJobControlWithKillJob times out in trunk without this patch. The broken test is unrelated. I will file another jira to fix the test, but this patch should be ready for review. Thanks!","05/Aug/16 20:12;ctrezzo;Filed jira to fix unrelated test failure: MAPREDUCE-6747","09/Aug/16 20:30;jlowe;Thanks for updating the patch!  Looks good overall with just a few nits:

I think the code would be cleaner if we leveraged Configuration#getStringCollection to get the conf values rather than checking for null and splitting on comma directly.  That method will return an empty collection if there are no values for the property, so then we can just remove some of the null checks and just loop over the items for each property.  Some of the null checks would change to !isEmpty checks to avoid doing unnecessary mkdirs, etc., during upload methods, but they could be completely removed in the limit checking code.

The totalConfigSize* variables are essentially loop-invariants, so they should be computed once in the Limits constructor rather than each addFile call.

Both TestMRJobs and TestJobResourceUploader assume any IOException is OK if the job submission is supposed to fail.  The unit tests should verify that the expected exception that failed the job submission was related to limits, otherwise we could be failing the job submission for the wrong reasons and the test would still pass.  I'm thinking something along the lines of checking the exception message for limits-related wording, but maybe there's a cleaner way.

","16/Aug/16 23:34;ctrezzo;Thanks for the review [~jlowe]! Attached is a v7 patch. Here are the major changes:
# Changes to address your comments around getStringCollection, totalConfigSize* and ensuring tests failed in the intended way.
# Changes to make the usage of the word resource vs file consistent throughout the patch (i.e. a file is a type of resource).","17/Aug/16 01:59;hadoopqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 17s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 7s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 6m 41s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 35s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 33s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 58s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 29s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 11s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 34s {color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 8s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 44s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 33s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 1m 33s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 31s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 53s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 24s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green} 0m 1s {color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 19s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 30s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 2m 11s {color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 118m 49s {color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 25s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 140m 41s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12824023/MAPREDUCE-6690-trunk-v7.patch |
| JIRA Issue | MAPREDUCE-6690 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux 2bad52d01f13 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 27a6e09 |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6673/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6673/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","17/Aug/16 16:17;jlowe;+1 lgtm.  Committing this.","17/Aug/16 16:25;jlowe;Thanks, Chris!  I committed this to trunk and branch-2.","17/Aug/16 16:40;hudson;SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #10291 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/10291/])
MAPREDUCE-6690. Limit the number of resources a single map reduce job (jlowe: rev f80a7298325a4626638ee24467e2012442e480d4)
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/filecache/ClientDistributedCacheManager.java
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestMRJobs.java
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobResourceUploader.java
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* (add) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJobResourceUploader.java
","17/Aug/16 19:08;ctrezzo;Thanks [~jlowe] for the review and commit!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Locality hints for Reduce,MAPREDUCE-199,12352877,New Feature,Reopened,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,breed,breed,10/Oct/06 14:21,26/Oct/16 14:49,12/Jan/21 09:52,,,,,,,,,,applicationmaster,mrv2,,,,,6,,,,,It would be nice if we could add method to OutputFormat that would allow a job to indicate where a reducer for a given partition should should run. This is similar to the getSplits() method on InputFormat. In our application the reducer is using other data in addition to the map outputs during processing and data accesses could be made more efficient if the JobTracker scheduled the reducers to run on specific hosts.,,acmurthy,beneguo,bpodgursky,davidparks21,devaraj,diegov,drboscolo,eric14,gemini5201314,jmspaggi,kkambatl,nemon,nroberts,ozawa,qwertymaniac,ravidotg,sandyr,sseth,stack,un_brice,vicaya,viper799,wdavid,xiaokang,yhemanth,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-1199,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/12 09:34;qwertymaniac;MAPREDUCE-199.patch;https://issues.apache.org/jira/secure/attachment/12544391/MAPREDUCE-199.patch","09/Sep/12 09:07;qwertymaniac;MAPREDUCE-199.patch;https://issues.apache.org/jira/secure/attachment/12544389/MAPREDUCE-199.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2012-01-16 10:06:17.791,,,false,,,,,,,,,,,,,,,,,,148589,,,,,Mon Oct 21 13:35:48 UTC 2013,,,,,,,"0|i0fulr:",90556,,,,,,,,,,,,,,,,,,,,,"16/Jan/12 10:06;qwertymaniac;This has been debated many times and the end reason has always been that there just isn't any merit in trying to schedule reducers with given locality (or hints of).

Resolving this old old one but am sure there were more recent tickets closed with the same result.","16/Jan/12 10:07;qwertymaniac;But please reopen or open a new issue if need be (/me looks at HBase).","17/Jan/12 03:57;acmurthy;This should be much more feasible with MR2 and should yield significant benefits for small jobs.","09/Sep/12 09:07;qwertymaniac;Here's a first shot it. Adds support for locality hints via Job configuration (rather than an API). Supports one host hint per partition at the moment.

For those who feel this would benefit: Would you like to have multi-host locality hinting support for each partition, akin to maps?

Patch still needs some work, but hopefully the approach is right. Yet to test with a built cluster to observe the locality pick-up, but the framework around these areas is pretty nice (compared to MR1's JIP classes and such).","09/Sep/12 09:34;qwertymaniac;- Fixed a blooper in test (Reduce task attempt impl. constructor had some args switched, failing the test)
- Added a precondition check to the static method of extracting reducer hints, just as a completeness check. Thanks to Sho for this.","09/Sep/12 14:49;yuzhihong@gmail.com;Shall we consider extending support, syntactically, for multi-host locality hint ?
Meaning some kind of delimiter needs to be defined which should be different from ',' used by StringUtils#getStringCollection()
","09/Sep/12 15:50;qwertymaniac;It may make more sense to have a separate serialized structure for multi-hosts (for partitions), rather than leveraging the config object? Since the host list can grow and there are limits on the config object's serialization size.","09/Sep/12 15:54;yuzhihong@gmail.com;That's more considerate. +1.","10/Sep/12 05:39;qwertymaniac;Thanks for the feedback Ted, I'll come up with another patch that does it in the same way as the InputSplit structures as soon as I get some more free cycles.","10/Sep/12 13:18;acmurthy;I'm not sure I see the value in user-specifying 'hints', the way to get this to work is to 'figure' where the map-outputs are (the AM knows it) and then try to pick the right hosts/racks. 
","10/Sep/12 15:28;qwertymaniac;bq. I'm not sure I see the value in user-specifying 'hints', the way to get this to work is to 'figure' where the map-outputs are (the AM knows it) and then try to pick the right hosts/racks.

This is good too, as an auto-optimization of regular MR apps. However, in HBase-land we would benefit by direct control if the reducer can be scheduled directly onto the RegionServer that hosts the sorted area of keys the reducer is going to process, or even fetch.

Seems like we can go for two things:

# Auto-optimize by default, so that all users benefit somehow.
# Provide a way to override the automation via API supplied partition->host mappings, to allow those who want to control for other odd purposes.

Arun - Would this be good?","10/Sep/12 18:28;acmurthy;Harsh - I'm not familiar with the HBase case; can you please add more colour?

bq.  in HBase-land we would benefit by direct control if the reducer can be scheduled directly onto the RegionServer that hosts the sorted area of keys the reducer is going to process, or even fetch.

In this case, won't it be sufficient to schedule maps on the RS? If the data is already sorted, but would you try schedule reduces instead? 

My concern adding apis/config is that it becomes part of the user interface and I'd like to think through it's implications, and whether it's really necessary, before we commit to it. Makes sense?","10/Sep/12 18:41;qwertymaniac;bq. Harsh - I'm not familiar with the HBase case; can you please add more colour?

Surely!

bq. In this case, won't it be sufficient to schedule maps on the RS? If the data is already sorted, but would you try schedule reduces instead?

We have this concept of bulkloads, for example, in HBase, where the Maps read in data from a raw source (such as a delimited text file) and passes it to a reducer (partitioned by TotalOrderPartitioner based on the region distribution of the table in HBase). The sorted data is then written onto a file on HDFS and later, injected into the /hbase directory structure for serving.

There's cheap gains (but gains nevertheless) if the data written by the reducer is local to the RegionServer hosting that specific partition (region) itself, before we bulkload it in.

Likewise, if people have HBase jobs doing a reduce phase for whatever reason, and wish to achieve locality such that the reducer task (which emit the keys) are local to the regionserver serving the same region for those keys, they can do so via a pre-configured job.

There are some use-cases out of HBase as well (I'll let those who've desired this comment), but maybe YARN can change those to be outside of MR today.

Or maybe HBase can get a custom AM to do their work in more efficient manner than the current MR (MR is easy to use though) - in the long term.

I just think using YARN to write a new app for everything is a slightly longer path to take if MR can be harmlessly tweaked a bit more to do the same thing along with the other good things it already does.

bq. My concern adding apis/config is that it becomes part of the user interface and I'd like to think through it's implications, and whether it's really necessary, before we commit to it. Makes sense?

Yes, makes sense on the API side. Partly why I went with a simple config-based option on doing this.","10/Sep/12 18:47;kkambatl;This might not be the use case Harsh was thinking of, but here is a use case from my summer internship a couple of years ago:

Our use case: We were building a topic-based pub/sub system. The published events were in one HBase table, and the subscriptions were in another table. While the published events were stored by their published time-stamp, the subscriptions were stored by <Topic ID: Subscription ID> as the key. Matching the published events to subscriptions required a join of the two tables on the topic.

Approach: The map phase reads all the published events and emits (topic, event) pairs. The reduce's input essentially is all events for a topic - the reduce reads all the subscriptions of that topic and matches. Now, it would save a lot of communication if the reduce (for topic A) were scheduled on the same node that had the subscriptions for the same topic A. Hence, the need for reduce  data-locality.

We achieved this data locality through ugly hacks to the JT to store HBase region (key-range): host mapping and overloading the partitioner to push each <key, value> pair to appropriate reducers. I don't remember the exact speedups, but it was quite significant. (if my memory is not wrong ~2x) ","11/Sep/12 03:16;eric14;I can see the value of matching reduce outputs to region servers.  This does seem like a compelling use case.

That said, the MR interface is already very broad.  Let's let any extensions to the API bake for a while to make sure we are doing the right thing.  Its a lot easier to add thing to the config or API than take them out.  Using the same abstractions / API as the Map would be nice if doable.","18/Oct/13 15:19;bpodgursky;Doesn't seem like there's been any progress on this recently, but this functionality would be really helpful to us (we've been hunting for a way to do exactly this.)

Our use-case is somewhat similar to the HBase one--we have a number of stores which we keep sorted on the same keys and partitioned identically (ex, partitioned into partfiles 0000- 0599).  When we need to join these stores, instead of running a full map + reduce, we can just run a map task for each file which reads in the partfiles for each side of the join.  Since we are reading these stores many times, it saves us a lot of cluster time to only sort the files once.  

These files are each produced by a normal reduce task.  It would be great if we were able to give hadoop a hint that part-0123 of store A and part-0123 of store B should end up on the same host, so any job joining the two files will be reading purely local data.  Ideally we could accomplish this by giving hadoop a hint about where to run each reduce task so we don't have to shuffle the data around later.","20/Oct/13 03:46;qwertymaniac;Hey Ben,

The previously posted patch ""works"" for the hints, but uses a manual config based hint approach instead of an API based one. See the tests in the patch for an example.

I'll be glad to implement an API wrapper/alternate approach for this as well, can you post what you may have in mind? We can refine it up from there.","21/Oct/13 13:35;bpodgursky;Hey Harsh.  Delay was because I've only worked with MR1 so far (cloudera hadoop 4) and all of my source suggestions were in the context of MR1, so I spent a bit of time checking out what in the source changed between MR1 and MR2.   

After looking around your patch seems like a pretty nice way of enabling this functionality without baking anything else into the API or complicating the code (since it bootstraps on locality logic which already exists.)  

The other alternative I was thinking about was making the logic pluggable via the JobConf, similar to how partitions are set, eg

conf.setReduceTaskLocalizer(MyLocalityLogic.class);

Where MyLocalityLogic would have logic for assigning task -> host.  I'm not really sure how it would work though since (1) I'm not sure whether user-code is on the classpath at the time tasks are assigned to nodes and (2) the locality logic would need to be presented with a whole network topology to be able to do anything intelligent, and I'm not sure where that would come from...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add ability to send ""signals"" to jobs and tasks",MAPREDUCE-205,12348873,New Feature,Reopened,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,ab,ab,29/Aug/06 20:06,22/Oct/16 22:20,12/Jan/21 09:52,,,,,,,,,,,,,,,,1,,,,,"In some cases it would be useful to be able to ""signal"" a job and its tasks about some external condition, or to broadcast a specific message to all tasks in a job. Currently we can only send  a single pseudo-signal, that is to kill a job.

Example 1: some jobs may be gracefully terminated even if they didn't complete all their work, e.g. Fetcher in Nutch may be running for a very long time if it blocks on relatively few sites left over from the fetchlist. In such case it would be very useful to send it a message requesting that it discards the rest of its input and gracefully completes its map tasks.

Example 2: available bandwidth for fetching may be different at different times of day, e.g. daytime vs. nighttime, or total external link usage by other applications. Fetcher jobs often run for several hours. It would be good to be able to send a ""signal"" to the Fetcher to throttle or un-throttle its bandwidth usage depending on external conditions.

Job implementations could react to these messages either by implementing a method, or by registering a listener, whichever seems more natural.

I'm not quite sure how to go about implementing it, I guess this would have to be a part of  TaskUmbilicalProtocol but my knowledge here is a bit fuzzy ... ;) Comments are welcome.",,brianmackay,eric14,md87,qwertymaniac,svdixit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2006-08-29 20:16:37.0,,,false,,,,,,,,,,,,,,,,,,148595,,,,,Sat Oct 22 22:20:55 UTC 2016,,,,,,,"0|i0irdz:",107531,,,,,,,,,,,,,,,,,,,,,"29/Aug/06 20:16;cutting;The mapper could periodically poll a server for new messages.  For example, a DFS directory could be used per job with a message per file, named with a timestamp.  This would not require changes to the MapReduce system.  Would this be impractical for the fetcher application?","29/Aug/06 20:24;ab;It could address this particular problem, yes. However, each time application writers would have to design their own way to do this - it would be better if the framework provided some support for this.","29/Aug/06 21:09;cutting;> each time application writers would have to design their own way to do this

I prefer to wait until a few application writers have done it, then generalize, rather than try to guess what is universal.  Otherwise the framework gets bloated with features that are only used by one application.

Are there other folks who need to send messages to running map and reduce tasks?","15/Sep/06 21:16;ab;This implementation is not Nutch specific, and can be easily moved to Hadoop if users find it useful.","16/Sep/06 11:21;ab;(Oops, I thought JIRA would include the link in the comment).

NUTCH-368 provides an implementation of a filesystem-based message queue system. This implementation is not Nutch specific, and can be easily moved to Hadoop if users find it useful.","19/Sep/06 05:41;eric14;-1

Unless I'm reading this wrong, a file per message would kill the name node at any scale.  Also, in a large task, the cost of having every mapper/task scan all the messages could be fairly prohibitive.

I'd suggest making it available in contrib or some other mechanism until we see how much uptake it gets.  This would leave specific applications free to use it.  Perhaps if this gains wide acceptance we could explore moving the concepts into core, but we would need to address the scaling issues to make a general facility.  

A very interesting set of ideas here, but very complicated if you want to make it work in large general cases.
","19/Sep/06 07:47;ab;Re: namenode issue: yes, that's a good point - I didn't think of that, mainly because I'm working with smaller clusters (dozens machines at most).

Re: cost of scanning: that's true as well, although tasks don't have to poll so often, in some cases you could configure the poll interval to be in the range of minutes. However, this points back to a deficiency in the current framework, namely that there is no support for sending arbitrary messages to tasks. If there were a way to do this (well, then the issue would be solved and we wouldn't need this MQ api ... ;) then we could run a separate filesystem monitor, which would dispatch events to all listening tasks concerning filesystem changes such as file/dir creation/deletion/update.

Overall, I'm aware that this is a less than ideal solution to the problem - IMHO my original proposal explained in this issue would be better. ;)","23/Feb/07 20:50;ab;I'd like to call for re-evaluation of this issue. With the introduction of TaskTrackerAction it seems to me that signals could be accommodated easier than before, simply by sending yet another type of TaskTrackerAction. The original reasons for this issue are still valid - the need to pass bits of information to all tasks in a job.

The message queue approach mentioned before has been tested in practice, and found useful for small-scale clusters and infrequent (control-type) messages. However, it's not scalable due to the heavy load it puts on the namenode.","16/Jan/12 09:58;qwertymaniac;This never seemed to generate any demand and the discussion has also grown stale. Should be doable in user-land in MR2 though, so closing out for now.","18/Jan/12 14:39;qwertymaniac;In hindsight, I think this was valid but it still has some stale information and the discussion would be better moved to a new ticket so that it fits in with the MR2 model we have in trunk/0.23 now, 'stead of the older protocols.","22/Oct/16 22:20;svdixit;I could use this feature, I have one use case similar to [~ab].  Devoid of that I am thinking of using a external KV store where I can poll for new instructions. Polling DFS @ higher frequency may not be scalable in very large cluster with thousands of tasks. <sidenote> I haven't tried DFS based soln. but I have bumped into namenode issues where thousands of mappers try to access same sufficiently replicated avro schema file </sidenote>



",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for HDFS erasure code policy to TestDFSIO,MAPREDUCE-6774,13002910,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,Sammi,Sammi,Sammi,06/Sep/16 13:02,18/Sep/16 07:39,12/Jan/21 09:52,17/Sep/16 01:07,,,,,,3.0.0-alpha2,,,,,,,,,0,,,,,HDFS erasure code policy allows user to store directory and file to predefined erasure code policies. Currently only 3x replication is supported in TestDFSIO implementation. This is going to add an new option to enable tests of files with erasure code policy enabled. ,,drankye,haibochen,hudson,Sammi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/16 13:21;Sammi;MAPREDUCE-6774-v1.patch;https://issues.apache.org/jira/secure/attachment/12827186/MAPREDUCE-6774-v1.patch","08/Sep/16 08:04;Sammi;MAPREDUCE-6774-v2.patch;https://issues.apache.org/jira/secure/attachment/12827513/MAPREDUCE-6774-v2.patch","08/Sep/16 11:39;Sammi;MAPREDUCE-6774-v3.patch;https://issues.apache.org/jira/secure/attachment/12827546/MAPREDUCE-6774-v3.patch","13/Sep/16 10:26;Sammi;MAPREDUCE-6774-v4.patch;https://issues.apache.org/jira/secure/attachment/12828201/MAPREDUCE-6774-v4.patch","14/Sep/16 07:30;Sammi;MAPREDUCE-6774-v5.patch;https://issues.apache.org/jira/secure/attachment/12828406/MAPREDUCE-6774-v5.patch","14/Sep/16 11:17;Sammi;MAPREDUCE-6774-v6.patch;https://issues.apache.org/jira/secure/attachment/12828433/MAPREDUCE-6774-v6.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,2016-09-06 16:08:20.482,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sun Sep 18 07:39:33 UTC 2016,,,,,,,"0|i33a4f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"06/Sep/16 16:08;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 17s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 8m 47s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 27s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 18s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 33s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 18s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 28s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 14s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 27s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 26s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 26s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 17s {color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient: The patch generated 5 new + 50 unchanged - 0 fixed = 55 total (was 50) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 32s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 15s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 38s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 11s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 143m 25s {color} | {color:red} hadoop-mapreduce-client-jobclient in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 28s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 158m 46s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.mapred.TestMROpportunisticMaps |
|   | hadoop.mapred.TestReduceFetch |
|   | hadoop.mapred.TestMerge |
|   | hadoop.mapreduce.TestMapReduceLazyOutput |
|   | hadoop.mapred.TestMRIntermediateDataEncryption |
|   | hadoop.mapred.TestLazyOutput |
|   | hadoop.mapreduce.TestLargeSort |
|   | hadoop.mapred.TestReduceFetchFromPartialMem |
|   | hadoop.mapreduce.v2.TestMRJobsWithProfiler |
|   | hadoop.mapreduce.lib.output.TestJobOutputCommitter |
|   | hadoop.mapreduce.security.ssl.TestEncryptedShuffle |
|   | hadoop.mapreduce.v2.TestMROldApiJobs |
|   | hadoop.mapred.TestJobCleanup |
|   | hadoop.mapreduce.v2.TestSpeculativeExecution |
|   | hadoop.mapred.TestClusterMRNotification |
|   | hadoop.mapreduce.security.TestUmbilicalProtocolWithJobToken |
|   | hadoop.mapreduce.v2.TestMRAMWithNonNormalizedCapabilities |
|   | hadoop.mapreduce.v2.TestMRJobs |
|   | hadoop.mapred.TestJobName |
|   | hadoop.mapreduce.TestMRJobClient |
|   | hadoop.mapred.TestClusterMapReduceTestCase |
|   | hadoop.mapred.TestAuditLogger |
|   | hadoop.mapreduce.security.TestMRCredentials |
|   | hadoop.mapred.TestMRTimelineEventHandling |
|   | hadoop.mapreduce.v2.TestMiniMRProxyUser |
|   | hadoop.mapreduce.v2.TestMRJobsWithHistoryService |
|   | hadoop.mapred.TestMiniMRClientCluster |
|   | hadoop.mapred.TestMiniMRChildTask |
|   | hadoop.mapreduce.TestChild |
|   | hadoop.mapreduce.security.TestBinaryTokenFile |
|   | hadoop.mapred.TestJobCounters |
| Timed out junit tests | org.apache.hadoop.mapred.TestMiniMRClasspath |
|   | org.apache.hadoop.mapred.TestJobSysDirWithDFS |
|   | org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12827186/MAPREDUCE-6774-v1.patch |
| JIRA Issue | MAPREDUCE-6774 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 28e213582d6e 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 62a9667 |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6706/artifact/patchprocess/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
| unit | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6706/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6706/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6706/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6706/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","08/Sep/16 08:04;Sammi;1. improve the patch, only io_data directory is mattered when apply EC policy
2. fix checkstyle reported issue","08/Sep/16 10:25;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 11s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 7m 22s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 23s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 17s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 30s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 15s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 24s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 12s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 21s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 21s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 21s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 14s {color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient: The patch generated 5 new + 45 unchanged - 5 fixed = 50 total (was 50) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 25s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 12s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 30s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 10s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 123m 7s {color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 29s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 136m 0s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12827513/MAPREDUCE-6774-v2.patch |
| JIRA Issue | MAPREDUCE-6774 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 3b6c36917bf8 3.13.0-93-generic #140-Ubuntu SMP Mon Jul 18 21:21:05 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 63f5948 |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6708/artifact/patchprocess/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6708/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6708/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","08/Sep/16 11:39;Sammi;fix style issues","08/Sep/16 14:16;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 15s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 6m 45s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 24s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 16s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 30s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 15s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 25s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 13s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 22s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 21s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 21s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 13s {color} | {color:green} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient: The patch generated 0 new + 45 unchanged - 5 fixed = 45 total (was 50) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 26s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 13s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 28s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 10s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 135m 32s {color} | {color:red} hadoop-mapreduce-client-jobclient in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 23s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 147m 51s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Timed out junit tests | org.apache.hadoop.mapreduce.lib.jobcontrol.TestMapReduceJobControl |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12827546/MAPREDUCE-6774-v3.patch |
| JIRA Issue | MAPREDUCE-6774 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux cf3bca10b7c8 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 63f5948 |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
| unit | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6709/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6709/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6709/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6709/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","13/Sep/16 08:03;drankye;Thanks [~Sammi] for the work!

It looks good overall. Suggestions:

{code}
boolean isStoragePolicyValid(String storagePolicy, FileSystem fs)
boolean isErasureCodePolicyValid(String erasureCodePolicyName, FileSystem fs, TestType testType) throws IOException
{code}
would be good to:
{code}
void checkStoragePolicy(String storagePolicy, FileSystem fs) throws Exception
void checkErasureCodePolicy(String erasureCodePolicyName, FileSystem fs, TestType testType) throws Exception
{code}
","13/Sep/16 08:05;drankye;So given above, the following can then be:
{code}
+    if (storagePolicy != null) {
+      if (!isStoragePolicyValid(storagePolicy, fs)) {
+        return -1;
+      }
{code}
=>
{code}
+    if (storagePolicy != null) {
+      checkStoragePolicy(storagePolicy, fs);
+    }
{code}
","13/Sep/16 10:26;Sammi;
Thanks Kai, for take time review the patch and give very good suggestion!
The name 'checkStoragePolicy' sounds better than 'isStoragePolicyValid'. So does the name 'checkErasureCodePolicy'. I uploaded Patch v4 which contains the improvement. One the other hand, I would suggest keep the return value {{boolean}} of each check function. Because the {{main}} function depends on the return value, not the exception, to handle the invalid parameter case. 


","13/Sep/16 12:39;hadoopqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 18s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 6m 57s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 23s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 17s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 30s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 16s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 23s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 13s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 22s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 20s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 20s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 14s {color} | {color:green} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient: The patch generated 0 new + 45 unchanged - 5 fixed = 45 total (was 50) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 27s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 13s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 28s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 10s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 117m 35s {color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 26s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 130m 9s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12828201/MAPREDUCE-6774-v4.patch |
| JIRA Issue | MAPREDUCE-6774 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux d30c2069690d 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 729de3e |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6716/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6716/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","13/Sep/16 22:50;drankye;Thanks Sammi for the update! It looks good and only some minors now.

1. Could you define constants for: {{test.io.erasure.code.policy}}, {{test.io.block.storage.policy}}?
2. Would like to see minor refinements for {{checkErasureCodePolicy}}. For example, having some line breaks, avoiding the {{else}}.

{code}
+  private boolean checkErasureCodePolicy(String erasureCodePolicyName,
+      FileSystem fs, TestType testType) throws IOException {
+    Collection<ErasureCodingPolicy> list =
+        ((DistributedFileSystem) fs).getAllErasureCodingPolicies();
+    boolean isValid = false;
+    int i = 0;
+    for (ErasureCodingPolicy ec : list) {
+      if (erasureCodePolicyName.equals(ec.getName())) {
+        isValid = true;
+        break;
+      }
+    }
+    if (!isValid) {
+      System.out.println(""Invalid erasure code policy: "" +
+          erasureCodePolicyName);
+      System.out.println(""Current supported erasure code policy list: "");
+      for (ErasureCodingPolicy ec : list) {
+        System.out.println(ec.getName());
+      }
+      return false;
+    } else {
+      if (testType == TestType.TEST_TYPE_APPEND ||
+          testType == TestType.TEST_TYPE_TRUNCATE) {
+        System.out.println(""So far append or truncate operation"" +
+            "" with erasureCodePolicy enabled is not supported"");
+        return false;
+      }
+    }
+    config.set(""test.io.erasure.code.policy"", erasureCodePolicyName);
+    LOG.info(""erasureCodePolicy = "" + erasureCodePolicyName);
+    return true;
+  }
{code}","14/Sep/16 07:30;Sammi;Improved patch based on Kai's suggestion","14/Sep/16 09:58;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 10s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 7m 41s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 23s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 18s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 29s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 16s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 24s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 13s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 21s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 19s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 19s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 14s {color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient: The patch generated 2 new + 45 unchanged - 5 fixed = 47 total (was 50) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 26s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 13s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 27s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 10s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 126m 30s {color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 27s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 139m 35s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12828406/MAPREDUCE-6774-v5.patch |
| JIRA Issue | MAPREDUCE-6774 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 746e781eb2ed 3.13.0-93-generic #140-Ubuntu SMP Mon Jul 18 21:21:05 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / ea0c2b8 |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6717/artifact/patchprocess/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6717/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6717/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","14/Sep/16 11:17;Sammi;Fix 2 style issues","14/Sep/16 13:33;hadoopqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 26s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 8m 44s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 25s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 18s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 32s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 16s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 25s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 15s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 22s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 21s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 21s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 15s {color} | {color:green} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient: The patch generated 0 new + 45 unchanged - 5 fixed = 45 total (was 50) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 28s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 13s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 29s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 11s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 119m 42s {color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 27s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 134m 30s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12828433/MAPREDUCE-6774-v6.patch |
| JIRA Issue | MAPREDUCE-6774 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux aaaf6aff956f 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / ea0c2b8 |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6718/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6718/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","14/Sep/16 22:35;drankye;Thanks Sammi for the update! The latest patch LGTM and +1.","17/Sep/16 01:07;drankye;Committed to trunk. Thanks Sammi for the contribution!","17/Sep/16 01:20;hudson;SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #10452 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/10452/])
MAPREDUCE-6774. Add support for HDFS erasure code policy to TestDFSIO. (kai.zheng: rev 501a77856d6b6edfb261547117e719da7a9cd221)
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java
","18/Sep/16 07:39;Sammi;Thanks Kai for review the patch and help commit to trunk!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for HDFS heterogeneous storage testing to TestDFSIO,MAPREDUCE-6578,12922714,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,zhouwei,zhouwei,zhouwei,17/Dec/15 13:40,24/Aug/16 14:40,12/Jan/21 09:52,24/Aug/16 14:26,,,,,,3.0.0-alpha1,,,,,,,,,0,,,,,HDFS heterogeneous storage allows user to store data blocks to different storage medias according to predefined storage policies. Only 'Default' policy is supported in current TestDFSIO implementation. This is going to add an new option to enable tests of other storage polices.,,drankye,hudson,Sammi,zhouwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/15 13:59;zhouwei;MAPREDUCE-6578.00.patch;https://issues.apache.org/jira/secure/attachment/12778248/MAPREDUCE-6578.00.patch","23/Aug/16 03:35;drankye;MAPREDUCE-6578.01.patch;https://issues.apache.org/jira/secure/attachment/12824979/MAPREDUCE-6578.01.patch","23/Aug/16 07:51;drankye;MAPREDUCE-6578.02.patch;https://issues.apache.org/jira/secure/attachment/12825006/MAPREDUCE-6578.02.patch","24/Aug/16 10:02;drankye;MAPREDUCE-6578.03.patch;https://issues.apache.org/jira/secure/attachment/12825239/MAPREDUCE-6578.03.patch","24/Aug/16 14:10;drankye;MAPREDUCE-6578.04.patch;https://issues.apache.org/jira/secure/attachment/12825269/MAPREDUCE-6578.04.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,2016-08-23 03:35:53.523,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Aug 24 14:40:16 UTC 2016,,,,,,,"0|i2q1cn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"17/Dec/15 13:59;zhouwei;Add a new option ""-storagePolicy"" to specify the storage policy. If not specified default policy is used as previous. Thanks!","23/Aug/16 03:35;drankye;Rebased the patch.","23/Aug/16 06:11;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 18s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 6m 41s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 23s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 17s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 29s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 17s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 24s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 13s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 21s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 21s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 21s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 14s {color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient: The patch generated 2 new + 50 unchanged - 0 fixed = 52 total (was 50) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 27s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 13s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 28s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 11s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 130m 16s {color} | {color:red} hadoop-mapreduce-client-jobclient in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 26s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 142m 37s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Timed out junit tests | org.apache.hadoop.mapreduce.lib.jobcontrol.TestMapReduceJobControl |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12824979/MAPREDUCE-6578.01.patch |
| JIRA Issue | MAPREDUCE-6578 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 7e12e4cb9730 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / c49333b |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6688/artifact/patchprocess/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
| unit | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6688/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6688/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6688/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6688/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","23/Aug/16 07:51;drankye;[~Sammi] helped updating the patch. Uploading it for her.","23/Aug/16 10:10;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 25s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 6m 39s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 23s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 17s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 29s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 16s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 24s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 12s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 21s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 20s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 20s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 14s {color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient: The patch generated 1 new + 50 unchanged - 0 fixed = 51 total (was 50) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 28s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 15s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 34s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 12s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 118m 48s {color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 24s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 131m 22s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12825006/MAPREDUCE-6578.02.patch |
| JIRA Issue | MAPREDUCE-6578 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux f12d68659583 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / c49333b |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6689/artifact/patchprocess/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6689/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6689/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","24/Aug/16 02:08;drankye;Thanks [~zhouwei] for the work and [~Sammi] for the update.

It's a good idea to support the {{storagePolicy}} option to test with some storage policy. The patch looks good overall. One comment is, could we print in the usage the list of all the supported policies? Otherwise it's hard for user to use the option.","24/Aug/16 10:02;drankye;Uploading the updated patch provided by [~Sammi]. Thanks!","24/Aug/16 10:14;Sammi;Thanks Kai's suggestion. If the user input {{storagePolicy}} parameter is incorrect or invalid, it's better provide user the knowledge that what's are current supported storage policy list. 
Update the patch based on above thoughts. ","24/Aug/16 12:19;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 19s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 6m 57s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 25s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 16s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 30s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 16s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 27s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 15s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 23s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 20s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 20s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 16s {color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient: The patch generated 2 new + 49 unchanged - 1 fixed = 51 total (was 50) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 28s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 29s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 10s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 117m 30s {color} | {color:red} hadoop-mapreduce-client-jobclient in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 25s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 130m 19s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.fs.slive.TestSlive |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12825239/MAPREDUCE-6578.03.patch |
| JIRA Issue | MAPREDUCE-6578 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux aecca326fa35 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 092b4d5 |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6692/artifact/patchprocess/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
| unit | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6692/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6692/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6692/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6692/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","24/Aug/16 14:10;drankye;This fixed the minor style.","24/Aug/16 14:13;drankye;+1 on the updated patch except the minor style. Will commit the latest one that solved the style.","24/Aug/16 14:18;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 0s {color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red} 0m 7s {color} | {color:red} MAPREDUCE-6578 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12825269/MAPREDUCE-6578.04.patch |
| JIRA Issue | MAPREDUCE-6578 |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6693/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","24/Aug/16 14:26;drankye;Committed to 3.0.0 and trunk branches. Thanks [~zhouwei] and [~Sammi] for the contribution.","24/Aug/16 14:30;drankye;Please ignore the above messy building message. It happened after the patch committed already. I fixed the minor check style (line too long) just before committing it.","24/Aug/16 14:40;hudson;SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #10336 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/10336/])
MAPREDUCE-6578. Add support for HDFS heterogeneous storage testing to (kai.zheng: rev 0ce1ab95cc1178f9ea763fd1f5a65a890b23b0de)
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rehashing partitioner for better distribution,MAPREDUCE-4887,12624377,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,hsn,hsn,hsn,18/Dec/12 00:12,12/May/16 18:23,12/Jan/21 09:52,19/Dec/12 21:50,,,,,,3.0.0-alpha1,,,,,,,,,0,,,,,rehash value returned by Object.hashCode() to get better distribution,,cutting,hsn,hudson,jlowe,tgraves,vicaya,wind5shy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/12 00:13;hsn;rehash1.txt;https://issues.apache.org/jira/secure/attachment/12561375/rehash1.txt","18/Dec/12 02:42;hsn;rehash2.txt;https://issues.apache.org/jira/secure/attachment/12561407/rehash2.txt","18/Dec/12 19:50;hsn;rehash3.txt;https://issues.apache.org/jira/secure/attachment/12561554/rehash3.txt","19/Dec/12 02:41;hsn;rehash4.txt;https://issues.apache.org/jira/secure/attachment/12561627/rehash4.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2012-12-18 00:44:33.573,,,false,,,,,,,,,,,,,,,,,,299216,,,,,Fri Dec 21 14:05:13 UTC 2012,,,,,,,"0|i160bb:",243183,,,,,,,,,,,,,,,,,,,,,"18/Dec/12 00:44;cutting;This looks like a good addition.  The javadoc might provide more detail, e.g., that a smoother partitioning may improve reduce time in some cases and should harm things in no cases, that this is suggested with Integer and Long keys with simple patterns in their distributions.","18/Dec/12 03:08;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12561407/rehash2.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3130//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3130//console

This message is automatically generated.","18/Dec/12 17:24;cutting;This patch needs some unit tests.","18/Dec/12 17:39;hsn;HashPartitioner do not have unit tests either.","18/Dec/12 19:50;hsn;fixed javadoc comment.","19/Dec/12 02:35;hsn;Very smooth distribution for pattern. If you were not defending people depending on undocumented behavior, you would make it default. 

Dumping buckets distribution: min=902 avg=1043 max=1184
bucket 0 964 items, variance -0.07574304889741132
bucket 1 1042 items, variance -9.587727708533077E-4
bucket 2 1101 items, variance 0.05560882070949185
bucket 3 1039 items, variance -0.003835091083413231
bucket 4 1099 items, variance 0.053691275167785234
bucket 5 1044 items, variance 9.587727708533077E-4
bucket 6 998 items, variance -0.04314477468839885
bucket 7 1040 items, variance -0.0028763183125599234
bucket 8 1184 items, variance 0.13518696069031638
bucket 9 976 items, variance -0.06423777564717162
bucket 10 902 items, variance -0.13518696069031638
bucket 11 1124 items, variance 0.07766059443911794
bucket 12 931 items, variance -0.10738255033557047
bucket 13 1094 items, variance 0.0488974113135187
bucket 14 1152 items, variance 0.10450623202301054
bucket 15 977 items, variance -0.06327900287631831
bucket 16 1057 items, variance 0.013422818791946308
bucket 17 1048 items, variance 0.004793863854266539
bucket 18 1052 items, variance 0.00862895493767977
bucket 19 1042 items, variance -9.587727708533077E-4
bucket 20 1028 items, variance -0.014381591562799617
bucket 21 1038 items, variance -0.004793863854266539
bucket 22 1037 items, variance -0.005752636625119847
bucket 23 1040 items, variance -0.0028763183125599234
bucket 24 1084 items, variance 0.039309683604985615
bucket 25 974 items, variance -0.06615532118887824
bucket 26 954 items, variance -0.08533077660594439
bucket 27 1122 items, variance 0.07574304889741132
bucket 28 1009 items, variance -0.032598274209012464
bucket 29 1095 items, variance 0.04985618408437201
bucket 30 1109 items, variance 0.06327900287631831
bucket 31 978 items, variance -0.062320230105465
0 of 32 are too small or large buckets
","19/Dec/12 02:41;hsn;unit test added - test if hash function returns smooth distribution for pattern input.","19/Dec/12 03:04;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12561627/rehash4.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3139//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3139//console

This message is automatically generated.","19/Dec/12 21:50;cutting;I committed this.","19/Dec/12 21:55;hudson;Integrated in Hadoop-trunk-Commit #3143 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3143/])
    MAPREDUCE-4887. Add RehashPartitioner, to smooth distributions with poor implementations of Object#hashCode(). Contributed by Radim Kolar. (Revision 1424158)

     Result = SUCCESS
cutting : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1424158
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/RehashPartitioner.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/partition
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/partition/TestRehashPartitioner.java
","20/Dec/12 11:11;hudson;Integrated in Hadoop-Yarn-trunk #71 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/71/])
    MAPREDUCE-4887. Add RehashPartitioner, to smooth distributions with poor implementations of Object#hashCode(). Contributed by Radim Kolar. (Revision 1424158)

     Result = SUCCESS
cutting : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1424158
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/RehashPartitioner.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/partition
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/partition/TestRehashPartitioner.java
","20/Dec/12 13:12;hudson;Integrated in Hadoop-Hdfs-trunk #1260 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1260/])
    MAPREDUCE-4887. Add RehashPartitioner, to smooth distributions with poor implementations of Object#hashCode(). Contributed by Radim Kolar. (Revision 1424158)

     Result = FAILURE
cutting : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1424158
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/RehashPartitioner.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/partition
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/partition/TestRehashPartitioner.java
","21/Dec/12 14:05;hudson;Integrated in Hadoop-Mapreduce-trunk #1291 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1291/])
    MAPREDUCE-4887. Add RehashPartitioner, to smooth distributions with poor implementations of Object#hashCode(). Contributed by Radim Kolar. (Revision 1424158)

     Result = SUCCESS
cutting : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1424158
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/RehashPartitioner.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/partition
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/partition/TestRehashPartitioner.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pluggable merge at reduce side,MAPREDUCE-4891,12624569,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,,jerrychenhf,jerrychenhf,19/Dec/12 03:05,12/May/16 18:22,12/Jan/21 09:52,19/Dec/12 06:32,3.0.0-alpha1,,,,,,,,,,,,,,0,,,,,"The current implementation of sort in MapReduce is cooperated by Map side sort and Reduce side merge.  MAPREDUCE-2454 provided pluggable sort at the Map side currently and pluggable shuffle at Reduce side, while no pluggable merger provided.

Considering a general need of hash grouping and join, we may need to replace both the Map Sort and Reduce merge with a more light weight hash grouping alorithm. A general pluggable merge would help support this need.",,acmurthy,jerrychenhf,tgraves,tucu00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1814400,1814400,,0%,1814400,1814400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-12-19 03:35:08.799,,,false,,,,,,,,,,,,,,,,,,300390,,,,,Mon Dec 24 02:52:32 UTC 2012,,,,,,,"0|i167mv:",244369,,,,,,,,,,,,,,,,,,,,,"19/Dec/12 03:35;tucu00;Jerry, unless I'm missing something from your description, this is a duplicate of MAPREDUCE-4808. Would you please check if that is the case and close this this one as dup of MAPREDUCE-4808 is so?. Thx","19/Dec/12 06:32;acmurthy;Duplicate of MAPREDUCE-4808","24/Dec/12 02:52;jerrychenhf;Yes, it should be a duplication of MAPREDUCE-4808. While MAPREDUCE-4808 was originally as subtask of MAPREDCUE-2454. But MAPREDUCE-2454 was resolved without MAPREDUCE-4808. So I mistakenly think that the future was gone. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobSubmitter with unmanaged MRApplicationMaster  ,MAPREDUCE-6653,12951443,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,jira.shegalov,jira.shegalov,18/Mar/16 06:10,18/Mar/16 20:07,12/Jan/21 09:52,,,,,,,,,,applicationmaster,client,,,,,0,,,,,"Long living DAG applications (pig, hive, etc) may launch many mapreduce jobs. Since we already have to allocate resources for these clients, we could reduce the latency of monitoring AM's, the overall number of network RPC's, the number of containers running on YARN cluster by embedding the MRAppMaster with the submitter. ",,haibochen,jira.shegalov,jlowe,Naganarasimha,qwertymaniac,vvasudev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 18 20:07:53 UTC 2016,,,,,,,"0|i2uv9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"18/Mar/16 20:07;jira.shegalov;In terms of user API we can introduce a new value for mapreduce.framework.name: yarn.local or yarn.unmanaged.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - Mumak: Map-Reduce Simulator,MAPREDUCE-6531,12909171,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,hong.tang,habesha,habesha,30/Oct/15 10:28,04/Nov/15 20:25,12/Jan/21 09:52,04/Nov/15 20:25,0.21.0,,,,,0.21.0,,,,,,,,,0,,,,,"h3. Vision:

We want to build a Simulator to simulate large-scale Hadoop clusters, applications and workloads. This would be invaluable in furthering Hadoop by providing a tool for researchers and developers to prototype features (e.g. pluggable block-placement for HDFS, Map-Reduce schedulers etc.) and predict their behaviour and performance with reasonable amount of confidence, there-by aiding rapid innovation.

----

h3. First Cut: Simulator for the Map-Reduce Scheduler

The Map-Reduce Scheduler is a fertile area of interest with at least four schedulers, each with their own set of features, currently in existence: Default Scheduler, Capacity Scheduler, Fairshare Scheduler & Priority Scheduler.

Each scheduler's scheduling decisions are driven by many factors, such as fairness, capacity guarantee, resource availability, data-locality etc.

Given that, it is non-trivial to accurately choose a single scheduler or even a set of desired features to predict the right scheduler (or features) for a given workload. Hence a simulator which can predict how well a particular scheduler works for some specific workload by quickly iterating over schedulers and/or scheduler features would be quite useful.

So, the first cut is to implement a simulator for the Map-Reduce scheduler which take as input a job trace derived from production workload and a cluster definition, and simulates the execution of the jobs in as defined in the trace in this virtual cluster. As output, the detailed job execution trace (recorded in relation to virtual simulated time) could then be analyzed to understand various traits of individual schedulers (individual jobs turn around time, throughput, faireness, capacity guarantee, etc). To support this, we would need a simulator which could accurately model the conditions of the actual system which would affect a schedulers decisions. These include very large-scale clusters (thousands of nodes), the detailed characteristics of the workload thrown at the clusters, job or task failures, data locality, and cluster hardware (cpu, memory, disk i/o, network i/o, network topology) etc.


",,kasha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-728,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/15 10:28;habesha;19-jobs.topology.json.gz;https://issues.apache.org/jira/secure/attachment/12769741/19-jobs.topology.json.gz","30/Oct/15 10:28;habesha;19-jobs.trace.json.gz;https://issues.apache.org/jira/secure/attachment/12769742/19-jobs.trace.json.gz","30/Oct/15 10:28;habesha;mapreduce-728-20090917-3.patch;https://issues.apache.org/jira/secure/attachment/12769743/mapreduce-728-20090917-3.patch","30/Oct/15 10:28;habesha;mapreduce-728-20090917-4.patch;https://issues.apache.org/jira/secure/attachment/12769744/mapreduce-728-20090917-4.patch","30/Oct/15 10:28;habesha;mapreduce-728-20090917.patch;https://issues.apache.org/jira/secure/attachment/12769745/mapreduce-728-20090917.patch","30/Oct/15 10:28;habesha;mapreduce-728-20090918-2.patch;https://issues.apache.org/jira/secure/attachment/12769746/mapreduce-728-20090918-2.patch","30/Oct/15 10:28;habesha;mapreduce-728-20090918-3.patch;https://issues.apache.org/jira/secure/attachment/12769747/mapreduce-728-20090918-3.patch","30/Oct/15 10:28;habesha;mapreduce-728-20090918-5.patch;https://issues.apache.org/jira/secure/attachment/12769748/mapreduce-728-20090918-5.patch","30/Oct/15 10:28;habesha;mapreduce-728-20090918-6.patch;https://issues.apache.org/jira/secure/attachment/12769749/mapreduce-728-20090918-6.patch","30/Oct/15 10:28;habesha;mapreduce-728-20090918.patch;https://issues.apache.org/jira/secure/attachment/12769750/mapreduce-728-20090918.patch","30/Oct/15 10:28;habesha;mumak.png;https://issues.apache.org/jira/secure/attachment/12769751/mumak.png",,,,,,,,,,,,,,,,,,,,,,,,11.0,,,,,,,,,,,,,,,,,,,,2015-11-04 20:25:07.694,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 04 20:25:31 UTC 2015,,,,,,,"0|i2nqj3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"04/Nov/15 20:25;kasha;Looks like this was committed to 0.21.0. What is the expectation here? ","04/Nov/15 20:25;kasha;Resolving as ""Won't Fix"". ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mumak: Map-Reduce Simulator,MAPREDUCE-728,12429759,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,hong.tang,acmurthy,acmurthy,07/Jul/09 21:45,30/Oct/15 10:28,12/Jan/21 09:52,25/Sep/09 00:26,0.21.0,,,,,0.21.0,,,,,,,,,0,,,,,"h3. Vision:

We want to build a Simulator to simulate large-scale Hadoop clusters, applications and workloads. This would be invaluable in furthering Hadoop by providing a tool for researchers and developers to prototype features (e.g. pluggable block-placement for HDFS, Map-Reduce schedulers etc.) and predict their behaviour and performance with reasonable amount of confidence, there-by aiding rapid innovation.

----

h3. First Cut: Simulator for the Map-Reduce Scheduler

The Map-Reduce Scheduler is a fertile area of interest with at least four schedulers, each with their own set of features, currently in existence: Default Scheduler, Capacity Scheduler, Fairshare Scheduler & Priority Scheduler.

Each scheduler's scheduling decisions are driven by many factors, such as fairness, capacity guarantee, resource availability, data-locality etc.

Given that, it is non-trivial to accurately choose a single scheduler or even a set of desired features to predict the right scheduler (or features) for a given workload. Hence a simulator which can predict how well a particular scheduler works for some specific workload by quickly iterating over schedulers and/or scheduler features would be quite useful.

So, the first cut is to implement a simulator for the Map-Reduce scheduler which take as input a job trace derived from production workload and a cluster definition, and simulates the execution of the jobs in as defined in the trace in this virtual cluster. As output, the detailed job execution trace (recorded in relation to virtual simulated time) could then be analyzed to understand various traits of individual schedulers (individual jobs turn around time, throughput, faireness, capacity guarantee, etc). To support this, we would need a simulator which could accurately model the conditions of the actual system which would affect a schedulers decisions. These include very large-scale clusters (thousands of nodes), the detailed characteristics of the workload thrown at the clusters, job or task failures, data locality, and cluster hardware (cpu, memory, disk i/o, network i/o, network topology) etc.


",,aah,andyk,anirban.dasgupta,cdouglas,cos,ddas,devaraj,dma1982,guanying,guilinsun,hammer,he yongqiang,hong.tang,iyappans,jenvor,jorda,kimballa,lianhuiwang,matei,njshah.ce,ntolia,pfxuan,rangadi,rksingh,shawnh,snehal,stamas,tanjiaqi,tomwhite,yhemanth,zhong,zzningxp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-729,MAPREDUCE-1001,MAPREDUCE-1006,,,,,,,,,,,,,,,,,,,"17/Sep/09 14:56;hong.tang;19-jobs.topology.json.gz;https://issues.apache.org/jira/secure/attachment/12419895/19-jobs.topology.json.gz","17/Sep/09 14:56;hong.tang;19-jobs.trace.json.gz;https://issues.apache.org/jira/secure/attachment/12419896/19-jobs.trace.json.gz","18/Sep/09 02:14;hong.tang;mapreduce-728-20090917-3.patch;https://issues.apache.org/jira/secure/attachment/12419958/mapreduce-728-20090917-3.patch","18/Sep/09 03:17;hong.tang;mapreduce-728-20090917-4.patch;https://issues.apache.org/jira/secure/attachment/12419962/mapreduce-728-20090917-4.patch","17/Sep/09 20:13;hong.tang;mapreduce-728-20090917.patch;https://issues.apache.org/jira/secure/attachment/12419924/mapreduce-728-20090917.patch","18/Sep/09 21:43;hong.tang;mapreduce-728-20090918-2.patch;https://issues.apache.org/jira/secure/attachment/12420084/mapreduce-728-20090918-2.patch","18/Sep/09 22:16;hong.tang;mapreduce-728-20090918-3.patch;https://issues.apache.org/jira/secure/attachment/12420088/mapreduce-728-20090918-3.patch","18/Sep/09 22:31;hong.tang;mapreduce-728-20090918-5.patch;https://issues.apache.org/jira/secure/attachment/12420093/mapreduce-728-20090918-5.patch","19/Sep/09 05:54;hong.tang;mapreduce-728-20090918-6.patch;https://issues.apache.org/jira/secure/attachment/12420122/mapreduce-728-20090918-6.patch","18/Sep/09 21:00;hong.tang;mapreduce-728-20090918.patch;https://issues.apache.org/jira/secure/attachment/12420077/mapreduce-728-20090918.patch","07/Jul/09 22:01;acmurthy;mumak.png;https://issues.apache.org/jira/secure/attachment/12412795/mumak.png",,,,,,,,,,,,,,,,,,,,,,,,11.0,,,,,,,,,,,,,,,,,,,,2009-07-07 21:57:31.606,,,false,,,,,,,,,,,,,,,,,,149003,Reviewed,,,,Mon Apr 13 10:50:18 UTC 2015,,,,,,,"0|i0jedr:",111260,,,,,,,,,,,,,,,,,,,,,"07/Jul/09 21:50;acmurthy;h2. Mumak 1.0

The goal is to build a discrete event simulator to simulate conditions under which a Hadoop Map-Reduce Scheduler performs on a large-scale Map-Reduce cluster running a specific workload.

Mumak takes as input a reasonably large workload (e.g. a month's worth of jobs from production cluster(s)) and simulates them in a matter of hours if not minutes on very few machines.

h4. What is it not?

It is a non-goal to simulate the actual map/reduce tasks themselves.

The scope of Version 1.0 does not include specifics of trying to simulate the actual workload itself. It will merely take a digest of the Hadoop Map-Reduce JobHistory of all jobs in the workload, and faithfully assume the actual run-time of individual tasks from the digest without simulating the tasks themselves. Clearly this will not try and simulate resources and their utilization on the actual tasktrackers, interaction between running tasks on the tasktrackers etc. The simulation of individual tasks is left for future versions.

Some other simplifications are also made (mainly due to the lacking of such information from the job trace):

    * No job dependency. Jobs are faithfully submitted to the cluster as defined in the job trace.
    * No modeling of failure correlations (eg a few task attempts fail due to a node failure, but in the simulation run, the same set of task attempts may run on different nodes). 

h4. What goes in? What comes out?

The 'workload' alluded to in the previous sections needs elaboration. The proposal is to use the job-history for all jobs which are part of the workload. The Hadoop Map-Reduce per-job job-history is a very detailed log of each component task with run-times, counters etc. We can use this to generate a per-job digest with all relevant information. Thus, it is quite sufficient and feasible to collect workload from different clusters (research, production etc.) to be then used during simulation.

More specifically, the following is a list of details it simulates:

    * It would simulate a cluster of the same size and network topology as where the source trace comes from. The reason for this restriction is because data locality is an important factor to the scheduler decisions and scaling the job traces obtained from cluster A and try to simulate it on cluster B with different diameters require a much thorough understanding.
    * It would simulate failures faithfully as recorded in the job trace. Namely, if a particular task attempt fails in the trace, it would fail in the simulation.
    * It would replay the same number of map tasks and reduce tasks as specified in the job digest.
    * It would use the inputsplit locations as are recorded in the job trace. 

The simulator will generate the same job-history for each of the simulated jobs. Thus we can use the same tools for slicing and dicing the output of the simulator.

h4. Design & Architecture

Design Goals

An overarching design goal for Mumak is that we should be able to use the exact same Map-Reduce Schedulers (listed above) as-is without any changes. This implies that we use the same interfaces used by Hadoop Map-Reduce so that it is trivial to plug-in the Scheduler of interest.

Along the same lines it is a legitimate goal to use all relevant Hadoop Map-Reduce interfaces between various components so that it is trivial to replace each by the appropriate Hadoop Map-Reduce component (e.g. run the simulator in a emulation mode with real Map-Reduce clusters etc. in future).

Architecture

Mumak consists of the following components:

    * Discrete Event Simulator Engine with an event-queue
    * Simulated JobTracker
    * Simulated Cluster (set of tasktrackers)
    * Client for handling job-submission 

Engine

The Simulator Engine is the heart of Mumak. It manages all the discrete events in virtual time and fires the appropriate handlers (JobClient, TaskTracker) when the events occur. Typically each event responded to by a component results in a new set of events to be fired in the future (virtual time).

Some of the various event-types are:

    * HeartbeatEvent - An event which instructs a specific Tasktracker to send a heartbeat to the JobTracker.
    * TaskCompletionEvent - An event which denotes the completion (success/failure) of a specific map or reduce task which is sent to the TaskTracker.
    * JobSubmissionEvent - An event which instructs the JobClient to submit a specific job to the JobTracker 

Simulated JobTracker

The JobTracker is driver for the Map-Reduce Scheduler. On receipt of heartbeats from various TaskTrackers it 'tracks' progress of the current jobs and forwards the appropriate information to the Scheduler to allow it to make the task-scheduling decisions. The simulated JobTracker uses the virtual time to allow the scheduler to make scheduling decisions.

The JobTracker also uses the per-job digest to fill-in information about expected runtime for each of the tasks scheduled by the Scheduler to get Mumakil to simulate run-times for each task.

The JobTracker is purely reactive in the sense that it only reacts to hearbeats sent by TaskTrackers. Further more it does not directly handle any events from the Engine, it only responds to the InterTrackerProtocol.heartbeat calls as in the real-world.

Simulated Cluster

The simulated cluster consists of an appropriate number of simulated TaskTrackers which respond to events generated by Engine. Each simulated TaskTracker maintains state about currently running tasks (all tasks are 'running' till an appropriate TaskCompletionEvent fires) and sends periodic status updates to the JobTracker on receipt of HeartbeatEvent.

HeartbeatEvent

When a HeartbeatEvent fires, the appropriate TaskTracker build status-reports for each of the running tasks and sends a hearbeat to the JobTracker (InterTrackerProtocol.heartbeat). The JobTracker updates its data-structures (JobInProgress, TaskInProgress etc.) to refect the latest state and forwards information to the Scheduler. If any new tasks are be to scheduled on this TaskTracker the JobTracker also fills in expected run-times for each via information gleaned from the job-digest. The TaskTracker then processes the instructions to launch the new tasks and responds to the Engine by inserting a set of new TaskCompletionEvents for the new tasks into the EventQueue.

TaskCompletionEvent

When a TaskCompletionEvent fires, the appropriate TaskTracker marks the relevant task as complete and forwards that information to the JobTracker on the next HeartbeatEvent.

Simulated JobClient

The JobClient responds to JobSubmissionEvents sent by the Engine and submits the appropriate jobs to the JobTracker via the standard JobSubmissionProtocol.

h4. Relevant Details

Job Summary for Simulation

The following can be derived from job history file by rumen:

    * Detailed job trace with properties and counters of each task attempt (of each task of each job in a workload).
    * Digest of jobs in a workload. From the jobs in the workload, we can derive statistical information of tasks to build a model which can help us fabricate tasks which not even scheduled to run (e.g. tasks of a failed job which were never run since the job was declared as FAILED soon after submission). Along the same lines, the digest will also have statistical details for helping modelling run-times for data-local maps, rack-local maps and off-rack maps based on data in the job-history logs. This is necessary for simulating tasks which might be scheduled on different nodes in the simulation run by the scheduler. 

How to deal with failure in workload?

We will try to faithfully model task failures by replaying failed task-attempts by using information in the detailed job-traces.

We also plan to build a simple statistical model of task failures which can then be used to simulate tasks which were never scheduled since the job failed early etc.

Simulating Reduce Tasks

In Mumak 1.0 we do not plan to simulate the running of the actual map/reduce tasks. Given that it is not possible to simulate the implicit dependency between completion of maps, the shuffle phase and the start of the reduce phase of the reduce tasks. Hence, we have decided to use a special AllMapsFinished event generated by the SimulatedJobTracker to trigger the start of the reduce-phase. For the same reasons, we have to model the total runtime of the reduce task as the summation of the time taken for completion of all maps and the time taken for individual task to complete the reduce-phase by itself. Thus, we are not going to try modelling the shuffle phase accurately.

Furthermore, we will ignore map-task failures due to failed shuffles since we are not simulating the shuffle-phase.

----

Thoughts?
","07/Jul/09 21:57;matei;Arun, is this something you are developing / have developed at Yahoo, or is it more of a wish?

I have a simple Ruby simulator for MR, and we are building a more detailed one for Nexus. However, neither of these runs existing Hadoop schedulers. If you think a Ruby simulator would be useful though, perhaps we can contribute one of these separately.","07/Jul/09 22:01;acmurthy;Proposed architecture for Mumak.","07/Jul/09 22:03;acmurthy;We propose to develop/contribute Mumak as a contrib module.","07/Jul/09 22:04;acmurthy;Matei, we are proposing Mumak as a contrib module. We would love to collaborate. An explicit goal of Mumak is to work simulate using existing Hadoop simulators.","07/Jul/09 22:06;acmurthy;I hasten to add that we are also working on a tool (internally we call it *rumen*) which will be tasked with gleaning information required for Mumak given a workload (i.e. a list of job-history files for starters).","07/Jul/09 22:07;matei;I'll take a look at the design in more detail. I think this would be really great to have for unit tests too.","07/Jul/09 23:08;matei;This is looking good!

I have one item of high-level feedback. It looks like Mumak has two components - a simulator and a trace-driven workload generator. It would be nice if the workload generator was pluggable so that the simulator could be used on synthetic workloads without requiring a trace. For example, one should be able to create a simulated cluster where some given node is always slow, or fails partway through, etc. Then the simulator could be used in unit tests, simplifying a lot of the testing code in various schedulers.

Also, some questions about things that will be difficult to simulate:
* What will be done about speculative tasks? The trace currently shows a second attempt being started and a first being killed. One option would be to make the first attempt take forever, but then you'd have to decide when to mark the task as speculatable in the simulated JobInProgress. Another option might be to always use the time of the fastest non-killed task attempt and forget about simulation in V1.
* Will Mumak simulate high-memory jobs? That's one of the more interesting scheduling problems.
* The schedulers and the JobTracker currently have some threads that perform an operation periodically and sleep in-between doing so. To make these work in a simulator, I think we have to make these pieces of code not use threads, and include an API in the JobTracker such as schedulePeriodically(Runnable runnable, long interval) so that these threads can run in simulated time.
* Calls to System.currentTimeMillis will have to be replaced by use of Clock throughout the schedulers.","07/Jul/09 23:25;tanjiaqi;Clarification question: If the per-job digest from the ""source"" workload is generated by a real Hadoop cluster, then that workload would be an artifact of all the pluggable components used in the cluster used to generate it? For instance, if I had a cluster running the default scheduler, and I took those job digests and throw those into Mumak, then the question that Mumak is supposed to answer is, given a workload of a set of jobs under the Default Scheduler, how different would the execution times be under some different set of pluggable components? I'm trying to understand what Mumak is meant to do.
","07/Jul/09 23:35;acmurthy;bq. I have one item of high-level feedback. It looks like Mumak has two components - a simulator and a trace-driven workload generator. It would be nice if the workload generator was pluggable so that the simulator could be used on synthetic workloads without requiring a trace.
The proposal is for Mumak to work with Rumen (whose jira is coming along soon) which expose necessary apis to let us 'query' Rumen for characteristics of the workload (e.g. for a given job j how long did a data-local map-task take? or a off-rack one take?). So, yes you could seed Rumen with a synthetic trace and run Mumak against it.

bq. What will be done about speculative tasks? 
For V1 we plan to ignore speculation. It is considerably harder to simulate per-task progress and thus the plan is to push it to a future release.

bq. Will Mumak simulate high-memory jobs? 
Yes!

bq. The schedulers and the JobTracker currently have some threads that perform an operation periodically and sleep in-between doing so. To make these work in a simulator, I think we have to make these pieces of code not use threads [...]
Agreed. I know for sure that neither the default or capacity-scheduler use threads, what about fair-share? How hard is it to stop using threads there?

bq. Calls to System.currentTimeMillis will have to be replaced by use of Clock throughout the schedulers.
+1
As you'll see when we put up our work we use a 'virtual time' throughout Mumak which we will use to seed JobTracker.clock.","07/Jul/09 23:41;acmurthy;bq. Clarification question: If the per-job digest from the ""source"" workload is generated by a real Hadoop cluster, then that workload would be an artifact of all the pluggable components used in the cluster used to generate it?
The characteristics of the workload (e.g. for a given job j, runtime for data-local maps, off-rack maps etc.) are reasonably independent of the Scheduler in question.

bq.  For instance, if I had a cluster running the default scheduler, and I took those job digests and throw those into Mumak, then the question that Mumak is supposed to answer is, given a workload of a set of jobs under the Default Scheduler, how different would the execution times be under some different set of pluggable components?

Yes. With 'pluggable components' limited to the Scheduler.

Crucially, an equally important (if not more so) role of Mumak is to help us answer the question: What will be turnaround of the the same workload be if we added a feature 'X' to the same scheduler 'Y'?","08/Jul/09 00:55;matei;bq. Agreed. I know for sure that neither the default or capacity-scheduler use threads, what about fair-share? How hard is it to stop using threads there?

The capacity scheduler has a thread for reclaiming capacity (at least in my version of trunk that I've checked out).

It would be easy to get rid of this thread and the ones in the fair scheduler by either having a schedulePeriodicTask API in the MapReduceMaster interface or perhaps by checking the current time in assignTasks() and running any computations whose interval has passed. I'm fine with either solution.","08/Jul/09 00:57;matei;Forgot to add, both schedulers also have a thread or a thread pool for initializing jobs, but those should be easy to mock out.","08/Jul/09 03:55;yhemanth;Arun / Matei,

The trunk version of Capacity scheduler does not have a thread to reclaim capacities. But, as Matei pointed out, we do have a thread for job initialization.","08/Jul/09 05:13;matei;Sorry, I just did a svn update and I see a reclaimCapacityThread created at line 1145 of CapacityTaskScheduler.java. Is this change just not committed yet?","08/Jul/09 05:17;matei;Oh, never mind. I was looking at a CapacityScheduler from an older Hadoop version in Eclipse for some reason.","17/Sep/09 14:55;hong.tang;Ok, this is the patch that is close to the final form. The unit tests currently would fail due to MAPREDUCE-995.

Also, the end-to-end unit test requires two trace files in gzip format. I will upload them separately.","17/Sep/09 14:56;hong.tang;These two trace files should go to src/contrib/mumak/src/test/data.","17/Sep/09 18:44;hong.tang;After applying the patch uploaded on Jira MAPREDUCE-995, all Mumak unit tests pass. Also ran findbugs and run-commit-tests, both pass.","17/Sep/09 20:21;hong.tang;To review/commit the patch, follow the steps below:
- apply patch http://issues.apache.org/jira/secure/attachment/12419875/mapred-995-v1.patch
- apply patch mapreduce-728-20090917.patch
- download the two json.gz files and store them under src/contrib/mumak/src/test/data
- ""ant jar tools""
- ""cd src/contrib/mumak && ant test""
","17/Sep/09 20:49;cos;I'd like to point out that AspectJ related modifications of the build file are likely to be modified significantly as soon as HADOOP-6204 is  completed and committed.
+1 on using AspectJ smartly!","18/Sep/09 02:12;cdouglas;(unit tests, etc. being prepared)
* Since this exposes some private JobTracker/JobInProgress fields to subclasses, any fields that can be made final, should be. Also, there doesn't seem to be a consistent protected/package-private strategy for these members. Since SimulatorJobTracker is in the same package, most should be package-private.
* The copied JobTracker constructor is regrettable, but reworking the code to support this is a large undertaking that can be postponed. The same can be said of SimulatorJobInProgress. A JIRA tracking a cleanup these APIs would be appropriate.

I have not stepped through the details of the simluator code.","18/Sep/09 02:14;hong.tang;Incorporated comments from Chris. Also used ""git diff --text"" trick to include the binary .gz files int eh patch.","18/Sep/09 02:15;hong.tang;Need to apply patch from MAPREDUCE-995 before testing this patch.","18/Sep/09 03:17;hong.tang;Fixed release audit warnings.","18/Sep/09 03:53;hong.tang;test-patch passed on my local machine  now:
{noformat}
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 30 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{noformat}","18/Sep/09 08:58;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419962/mapreduce-728-20090917-4.patch
  against trunk revision 816476.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 30 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/102/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/102/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/102/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/102/console

This message is automatically generated.","18/Sep/09 16:13;hong.tang;The attached patch is the first step toward many things that it could enable us to do. The following is a follow-up of previous comments in this jira.

bq. I have one item of high-level feedback. It looks like Mumak has two components - a simulator and a trace-driven workload generator. It would be nice if the workload generator was pluggable so that the simulator could be used on synthetic workloads without requiring a trace. For example, one should be able to create a simulated cluster where some given node is always slow, or fails partway through, etc. Then the simulator could be used in unit tests, simplifying a lot of the testing code in various schedulers.

In the patch, this is very close to what we did (after MAPREDUCE-966). The dependency between Mumak and Rumen (the load generator) comes down to four interfaces: JobStory, JobStoryProducer, ClusterStory. And currently JobStoryProducer maps to SimulatorJobStory, and ClusterStory maps to ZombieCluster. It should be very easy to make them plugable, and I will create a Jira to track this.

bq. Then the simulator could be used in unit tests, simplifying a lot of the testing code in various schedulers.
Yes, the unit tests included were written in this way. And it showed two possible bugs in recent changes in JobHistory by MAPREDUCE-157 (MAPREDUCE-995, and MAPREDUCE-1000), this is actually a pleasant surprise to me, we were thinking of using Mumak for design proof or performance validation, but our design choice to use the actual scheduler code and JT code also makes it a JT debugger.

bq. What will be done about speculative tasks? • Will Mumak simulate high-memory jobs? 
Neither is done in this patch. But I agree these are things we should simulator in follow-up improvements.

bq. The schedulers and the JobTracker currently have some threads that perform an operation periodically and sleep in-between doing so. 
We (partially) solve the problem by using aspectJ so that the threads would become no-op. The reason I say it is a partial solution is that the threads are still active, and that the interception logic is not something you can mechanically determine, but in most cases are very straightforward to identify.","18/Sep/09 16:32;cos;bq. Since this exposes some private JobTracker/JobInProgress fields to subclasses, any fields that can be made final, should be. bq. The copied JobTracker constructor is regrettable

I believe both issues could be solve through appropriate use of AspectJ. For the first case one might use {{privileged}} aspects to access private members via generated getters.

The second one might be possible to resolve by aspect generated constructor in the appropriate class. ","18/Sep/09 21:00;hong.tang;Patch broken due to MAPREDUCE-777.","18/Sep/09 21:00;hong.tang;Patch that fixes problems caused by API changes in MAPREDUCE-777.","18/Sep/09 21:43;hong.tang;New patch that resolves a conflict in src/contrib/build-contrib.xml.","18/Sep/09 21:54;cdouglas;{noformat}
     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 30 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     -1 release audit.  The applied patch generated 180 release audit warnings (more than the trunk's current 176 warnings).
{noformat}

These files need license headers:
{noformat}
src/contrib/mumak/src/java/org/apache/hadoop/mapred/SimulatorClock.java
src/contrib/mumak/src/java/org/apache/hadoop/mapred/SimulatorJobStory.java
{noformat}

The two .gz files don't need license headers, of course.","18/Sep/09 22:16;hong.tang;Patch added some changes that were lost btw 20090918 and 20090917-4","18/Sep/09 22:38;cdouglas;MAPREDUCE-980 broke the patch in an unrecoverable way. This cannot make feature freeze.","19/Sep/09 04:26;hong.tang;Given that the reason it was broken is due to the very late check in of MAPREDUCE-980, I'd like to request an extension of this patch to go in to 21.","19/Sep/09 05:05;hong.tang;Fixed problem introduced by MAPREDUCE-980.","19/Sep/09 05:56;hong.tang;test-patch, contrib-tests passed on my local machine.","23/Sep/09 03:36;hong.tang;test-patch passed on both trunk and hadoop-0.21 branch on my local machine.","23/Sep/09 07:30;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12420122/mapreduce-728-20090918-6.patch
  against trunk revision 817740.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 30 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/124/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/124/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/124/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/124/console

This message is automatically generated.","24/Sep/09 23:22;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12420122/mapreduce-728-20090918-6.patch
  against trunk revision 818577.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 30 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/129/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/129/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/129/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/129/console

This message is automatically generated.","24/Sep/09 23:51;cdouglas;The failing core test, TestCopyFiles.testHftpAccessControl also fails on trunk ( MAPREDUCE-1029 )","24/Sep/09 23:59;hong.tang;The failed test org.apache.hadoop.tools.TestCopyFiles.testHftpAccessControl  (from TestCopyFiles) is not related.","25/Sep/09 00:26;cdouglas;+1

I committed this to trunk and the 0.21 branch, per the vote on mapreduce-dev.

Thanks to Arun Murthy, Tamas Sarlos, Anirban Dasgupta, Guanying Wang, and Hong Tang.","25/Sep/09 01:00;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #64 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/64/])
    . Add Mumak, a Hadoop map/reduce simulator. Contributed by Arun C Murthy,
Tamas Sarlos, Anirban Dasgupta, Guanying Wang, and Hong Tang
","05/Oct/09 01:13;matei;Is there any documentation on Mumak, or is this JIRA the current place to find out about it?","05/Oct/09 02:12;cdouglas;Documentation is being added in MAPREDUCE-1056","15/Nov/11 14:36;arun_kumar;Few Clarification Questions :
Q>How does mumak place the per job data on the simulated nodes ? I am interested in controlling the placement of data of every job from the Job trace.
Q>Which classes do i need to modify and what has to be done for this ?","22/Nov/11 00:46;stamas;Hi Arun,

We do not control or simulate anything about the placement of a job's _output_ data.  As for the effects of task placement and the _input_ data of a job, we rely on the input split locations recorded in the Hadoop job logs. The locality of the map task's input plays a role in determining the run time of the map task in the simulation. We differentiate among 3 levels of locality based on the closest input split to the task tracker: same node, same rack, and cross rack. We parse the Hadoop job logs using org.apache.hadoop.tools.rumen.

In details, with pointers to the code:
SimulatorTaskTracker.java:738 sets the run time of the task in the
simulation; the relevant
SimulatorTaskTracker.SimulatorTaskInProgress.userSpaceRunTime comes from
org.apache.hadoop.tools.rumen.TaskAttemptInfo,
org.apache.hadoop.tools.rumen.MapTaskAttemptInfo, and
org.apache.hadoop.tools.rumen.ReduceTaskAttemptInfo

These are set by the SimulatorJobTracker.java:443
using SimulatorJobInProgress.getTaskAttemptInfo(taskTracker, taskAttemptID)
which uses SimulatorJobStory, which is just a wrapper around the
org.apache.hadoop.tools.rumen.JobStory interface.

The latter is actually implemented by org.apache.hadoop.tools.rumen.ZombieJob, which uses the logged runtime of task attempts and simple heuristics based on the locality of the taskTracker to the input splits to make up a run time for the task.

If you wanted to alter the effect of task placement on run time, we suggest to modify the ZombieJob class.

Please let us know if we understood your question correctly.

Best,
  Tamas and Anirban
","26/Nov/11 15:07;arun_kumar;Tamas and Anirban,thanks for the Clarification.
you got my question's right.

I was also interested in knowing the following :
> Are there any benchmarks for Mumak ?
> How do i change the preferred locations(apart from manually) in the job trace obtained from Rumen? 
> Can i add some fields to the job trace generated using Rumen ?

Thanks,
Arun


","13/Apr/15 10:50;dma1982;Hi team,

Is there any tutorial for Mumak? I'm a new player for this component.

Thanks
Da Ma",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add REST API for getting all attempts for all the tasks,MAPREDUCE-6422,12841963,New Feature,Patch Available,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,lavkesh,lavkesh,lavkesh,01/Jul/15 11:24,23/Jul/15 14:56,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"Web UI has the feature where one can get all attempts for all map tasks or reduce tasks. 
REST api seems to be missing it. 
Should we add this in both HsWebService and AMWebService ?
{code}
  @GET
  @Path(""/mapreduce/jobs/{jobid}/tasks/attempts"")
  @Produces({ MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML })
  public JobTaskAttemptsInfo getAllJobTaskAttempts(@Context HttpServletRequest hsr,
   @PathParam(""jobid"") String jid, @QueryParam(""type"") String type) {
}
{code}

We might also add queryparam on state to filter by succeeded attempts etc.
Thoughts ?",,lavkesh,srikanth.sampath,vvasudev,zxu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/15 13:46;lavkesh;MAPREDUCE-6422.2.patch;https://issues.apache.org/jira/secure/attachment/12746804/MAPREDUCE-6422.2.patch","10/Jul/15 11:17;lavkesh;MAPREDUCE-6422.patch;https://issues.apache.org/jira/secure/attachment/12744694/MAPREDUCE-6422.patch","07/Jul/15 10:30;lavkesh;MAPREDUCE-6422.patch;https://issues.apache.org/jira/secure/attachment/12743930/MAPREDUCE-6422.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2015-07-01 16:07:07.118,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 23 14:56:36 UTC 2015,,,,,,,"0|i2gqe7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"01/Jul/15 16:07;rchiang;For all job/task related queries, I'd recommend being able to specify a range of what to return (as already recommended in MAPREDUCE-3971).  This has two benefits:

1) You could break queries into smaller chunks of data.  Some of the queries would take too long otherwise.

2) This could also open the door for a more responsive web UI that isn't part of the particular server (e.g. RM, JHS)","07/Jul/15 11:33;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | pre-patch |  16m 28s | Pre-patch trunk has 1 extant Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 2 new or modified test files. |
| {color:green}+1{color} | javac |   7m 34s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 36s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 24s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   0m 49s | The applied patch generated  5 new checkstyle issues (total was 25, now 24). |
| {color:red}-1{color} | whitespace |   0m  3s | The patch has 2  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 36s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 34s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m 59s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | mapreduce tests |   9m  4s | Tests passed in hadoop-mapreduce-client-app. |
| {color:red}-1{color} | mapreduce tests |   4m 59s | Tests failed in hadoop-mapreduce-client-hs. |
| | |  53m 17s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.mapreduce.v2.hs.TestJobHistoryParsing |
|   | hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesJobs |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12743930/MAPREDUCE-6422.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / af63427 |
| Pre-patch Findbugs warnings | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5872/artifact/patchprocess/trunkFindbugsWarningshadoop-mapreduce-client-app.html |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5872/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-app.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5872/artifact/patchprocess/whitespace.txt |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5872/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-hs test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5872/artifact/patchprocess/testrun_hadoop-mapreduce-client-hs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5872/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf903.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5872/console |


This message was automatically generated.","10/Jul/15 12:14;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | pre-patch |  16m 19s | Pre-patch trunk has 1 extant Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 2 new or modified test files. |
| {color:green}+1{color} | javac |   7m 38s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 37s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   0m 48s | The applied patch generated  2 new checkstyle issues (total was 26, now 22). |
| {color:green}+1{color} | whitespace |   0m  3s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 21s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 34s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m 57s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | mapreduce tests |   9m  4s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   5m 55s | Tests passed in hadoop-mapreduce-client-hs. |
| | |  53m 49s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12744694/MAPREDUCE-6422.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / b489080 |
| Pre-patch Findbugs warnings | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5882/artifact/patchprocess/trunkFindbugsWarningshadoop-mapreduce-client-app.html |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5882/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5882/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-hs test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5882/artifact/patchprocess/testrun_hadoop-mapreduce-client-hs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5882/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf905.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5882/console |


This message was automatically generated.","23/Jul/15 13:40;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  19m 41s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 2 new or modified test files. |
| {color:red}-1{color} | javac |   5m 17s | The patch appears to cause the build to fail. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12746797/MAPREDUCE-6422.1.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / ee98d63 |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5904/console |


This message was automatically generated.","23/Jul/15 14:56;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  20m 39s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 2 new or modified test files. |
| {color:green}+1{color} | javac |   9m 52s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |  12m 12s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 28s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   1m  9s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  3s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 47s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 43s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   2m 29s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | mapreduce tests |  10m 15s | Tests passed in hadoop-mapreduce-client-app. |
| {color:red}-1{color} | mapreduce tests |   6m 15s | Tests failed in hadoop-mapreduce-client-hs. |
| | |  65m 55s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesJobs |
|   | hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesTasks |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12746804/MAPREDUCE-6422.2.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / ee98d63 |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5905/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-hs test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5905/artifact/patchprocess/testrun_hadoop-mapreduce-client-hs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5905/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf904.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5905/console |


This message was automatically generated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integrating Hadoop Vaidya with Job History UI in Hadoop 2.0 ,MAPREDUCE-3202,12527636,New Feature,Patch Available,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,vitthal_gogate,vitthal_gogate,vitthal_gogate,18/Oct/11 18:21,21/Jul/15 18:47,12/Jan/21 09:52,,2.0.0-alpha,,,,,,,,jobhistoryserver,,,,,,3,,,,,Hadoop Vaidya provides a detailed analysis of the M/R job in terms of various execution inefficiencies and the associated remedies that user can easily understand and fix. This Jira patch integrates it with Job History UI under Hadoop 2.0 branch.,,devaraj,erik.fang,jaoki,judes,junping_du,lianhuiwang,revans2,rvs,sseth,stanley_shi,varun_saxena,viraj,vitthal_gogate,xinwei,zjshen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/13 00:03;vitthal_gogate;MAPREDUCE-3202.patch;https://issues.apache.org/jira/secure/attachment/12607698/MAPREDUCE-3202.patch","30/Sep/13 22:21;vitthal_gogate;MAPREDUCE-3202.patch;https://issues.apache.org/jira/secure/attachment/12605989/MAPREDUCE-3202.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2013-09-30 16:11:28.721,,,false,,,,,,,,,,,,,,,,,,88887,,,,,Tue Jul 21 18:47:47 UTC 2015,,,,,,,"0|i0e6sv:",80864,Hadoop Vaidya analysis available using Job History Server UI in 2.0 branch. Also ability for users to add new rules is provided. ,,,,,,,,,,,,,,,,,,,,"30/Sep/13 16:03;vitthal_gogate;Submitting the patch to make Hadoop Vaidya work with 2.0 branch and also integrate it with Job History server providing UI interface analyze M/R jobs using Vaidya tool. 

Thanks to Evan Meng @ Pivotal Inc [~evanmeng] make this happen. Submitting the patch on his behalf. ","30/Sep/13 16:04;vitthal_gogate;Will submit the patch for trunk soon.","30/Sep/13 16:11;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12605914/vaidya-2.0.6-alpha.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4062//console

This message is automatically generated.","30/Sep/13 22:21;vitthal_gogate;Submitting patch for trunk.","30/Sep/13 22:57;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12605989/MAPREDUCE-3202.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 15 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 1536 javac compiler warnings (more than the trunk's current 1534 warnings).

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 12 new Findbugs (version 1.3.9) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-tools/hadoop-tools-dist hadoop-tools/hadoop-vaidya:

                  org.apache.hadoop.mapreduce.v2.hs.webapp.dao.TestJobInfo

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4066//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4066//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4066//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-vaidya.html
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4066//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4066//console

This message is automatically generated.","01/Oct/13 18:56;vitthal_gogate;attaching new patch fixing Hadoop QA warnings","01/Oct/13 19:18;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12606157/MAPREDUCE-3202.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 15 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 3 new Findbugs (version 1.3.9) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-tools/hadoop-tools-dist hadoop-tools/hadoop-vaidya:

                  org.apache.hadoop.mapreduce.v2.hs.webapp.dao.TestJobInfo

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4074//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4074//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4074//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-vaidya.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4074//console

This message is automatically generated.","09/Oct/13 21:35;vitthal_gogate;Fixing unit-test failures. ","09/Oct/13 21:58;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12607659/MAPREDUCE-3202.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 15 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 3 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-tools/hadoop-tools-dist hadoop-tools/hadoop-vaidya.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4109//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4109//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-vaidya.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4109//console

This message is automatically generated.","10/Oct/13 00:28;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12607698/MAPREDUCE-3202.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 15 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-tools/hadoop-tools-dist hadoop-tools/hadoop-vaidya.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4111//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4111//console

This message is automatically generated.","10/Oct/13 00:44;vitthal_gogate;Can I request one of the Yarn/MapReduce committers to review and commit the vaidya patch?  It is a contrib project. Thanks in advance!   --Suhas","03/Jan/14 18:26;rvs;[~vitthal_gogate] I did the first pass over the patch and it looks reasonable. Here's a few bits of feedback I've got so far:
   * I would like to suggest that an extra JIRA (perhaps a subjira of this one) gets created to track *just* the changes required in existing HS webUI to support Vaidya. One half of the patch will go there.
   * please make sure to add unit tests for that first half of the patch
   * on this JIRA we shall keep track of things under hadoop-tools/hadoop-vaidya
   * we would also need to make sure that hadoop-tools/hadoop-vaidya bits end up in maven assembly","23/Oct/14 17:11;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12607698/MAPREDUCE-3202.patch
  against trunk revision d71d40a.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4975//console

This message is automatically generated.","10/Mar/15 05:41;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12607698/MAPREDUCE-3202.patch
  against trunk revision 7711049.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5271//console

This message is automatically generated.","21/Jul/15 18:47;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12607698/MAPREDUCE-3202.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 5137b38 |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5897/console |


This message was automatically generated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support to distcp to preserve raw.* namespace extended attributes,MAPREDUCE-6007,12729752,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,clamb,clamb,clamb,25/Jul/14 16:04,30/Jun/15 07:19,12/Jan/21 09:52,08/Aug/14 01:33,fs-encryption,,,,,2.6.0,fs-encryption,,distcp,,,,,,0,,,,,"As part of the Data at Rest Encryption work (HDFS-6134), we need to add support to distcp which preserves raw.* namespace extended attributes when both the src and target pathnames are in the /.reserved/raw directory hierarchy. See the doc in HDFS-6509 for details.",,andrew.wang,clamb,vinodkv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HDFS-6509,HDFS-6134,,,,,,,,,,,,,,,,,,,,"31/Jul/14 22:04;clamb;MAPREDUCE-6007.001.patch;https://issues.apache.org/jira/secure/attachment/12659017/MAPREDUCE-6007.001.patch","05/Aug/14 16:50;clamb;MAPREDUCE-6007.002.patch;https://issues.apache.org/jira/secure/attachment/12659910/MAPREDUCE-6007.002.patch","07/Aug/14 17:16;clamb;MAPREDUCE-6007.003.patch;https://issues.apache.org/jira/secure/attachment/12660408/MAPREDUCE-6007.003.patch","08/Aug/14 01:25;clamb;MAPREDUCE-6007.004.patch;https://issues.apache.org/jira/secure/attachment/12660525/MAPREDUCE-6007.004.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2014-08-05 00:14:54.744,,,false,,,,,,,,,,,,,,,,,,407826,,,,,Tue Jun 30 07:19:00 UTC 2015,,,,,,,"0|i1y6cf:",407835,,,,,,,,,,,,,fs-encryption,,,,,,,,"31/Jul/14 22:04;clamb;A patch is attached which adds a ""d"" option to distcp -p. ""d"" is Disable raw.* namespace xattr preservation. Refer to the document attached to HDFS-6509 for more details on the motivation for this.

I've also taken the liberty to create a new DistCpTestUtils file to hold common methods for distcp tests.

Please have a look.","05/Aug/14 00:14;andrew.wang;Hi Charles, thanks for the patch,

I had to take notes while reviewing this patch, the behavior is kind of complicated. We have a variety of flags that can be specified, and the destination FS can have different levels of support. It'd be very useful to specify this behavior in gory detail in the DistCp documentation.

Check me on this though:

Options:

{noformat}
-px : preserve raw and non-raw xattrs
-pr : no xattrs are preserved
-p  : preserve raw xattrs
-pxr: preserve non-raw xattrs
    : no xattrs are preserved
{noformat}

Behavior with a given src and dst, varying levels of dst support:

* raw src, raw dst: the options apply as specified above
* raw src, not-raw dst, dst supports xattrs but no {{/reserved/.raw}}: we will fail to set raw xattrs at runtime.
* raw src, dst doesn't support xattrs: if {{-pX}} is specified, throws an exception. Else, silently discards raw xattrs.

Some discussion on the above:
* If the src is {{/reserved/.raw}}, the user is expecting preservation of raw xattrs when {{-p}} or {{-pX}} is specified. In this scenario, we should test that the dest is {{/.reserved/raw}} and that it's present on the dstFS.
* There might be other weird cases, haven't thought through all of them

Some code review comments:

Misc:
- We have both {{noPreserveRaw}} and {{preserveRaw}} booleans, can we standardize on one everywhere? I'd like a negative one, call it {{disableRaw}} or {{excludeRaw}} since it better captures the meaning of the flag. {{exclude}} feels a bit better IMO, but it looks like {{-pe}} is taken.
- What's the expected behavior when the dest doesn't support xattrs or reserved raw, or supports xattrs but not reserved raw?
- CopyListing, this is where we'd also test to see if the destFS has a /.reserved/raw directory
- CopyMapper, two periods in the block comment

Documentation:
- I don't want to tie raw preservation just to encryption since we might also use it for compression, how about this instead:
{quote}
d: disable preservation of raw namespace extended attributes
...
raw namespace extended attributes are preserved by default if supported. Specifying -pd disables preservation of these xattrs.
{quote}
- As noted above, it'd be good to have the expected preservation behavior laid out in the distcp documentation.

DistCp:
{code}
    if (!Path.getPathWithoutSchemeAndAuthority(target).toString().
{code}
What if the target is a relative path here?

Test:
- Any reason this isn't part of the existing XAttr test? They seem pretty similar, and you also added a PXD test to the existing test.
- Don't need to do makeFilesAndDirs inO the BeforeClass
- Doesn't there need to be a non-raw attribute set so you can test some of these combinations?
- Can we test what happens when the dest FS doesn't support xattrs or raw xattrs?","05/Aug/14 16:50;clamb;{code}

-px : preserve raw and non-raw xattrs
-pr : no xattrs are preserved
-p  : preserve raw xattrs
-pxr: preserve non-raw xattrs
    : no xattrs are preserved

{code}

Yes, this is almost all correct. Assuming that -pr (above) is the same as -pd in the patch, then the only typo above is the last line which should be ""no raw xattrs are preserved"", but I think you had the ""raw"" part implied so I throw that in just to be crystal clear.

{quote}
    raw src, raw dst: the options apply as specified above
    raw src, not-raw dst, dst supports xattrs but no /reserved/.raw: we will fail to set raw xattrs at runtime.
    raw src, dst doesn't support xattrs: if -pX is specified, throws an exception. Else, silently discards raw xattrs.
{quote}

bq. If the src is /reserved/.raw, the user is expecting preservation of raw xattrs when -p or -pX is specified. In this scenario, we should test that the dest is /.reserved/raw and that it's present on the dstFS. There might be other weird cases, haven't thought through all of them

Good idea. I've cleaned this up so that anytime there's a /.reserved/raw src path present, and either pd or a non- /.reserved/raw target path is specified, an exception is thrown (actually a DistCpConstants.INVALID_ARGUMENT exit code is returned and a message logged). This will presumably help admins from shooting themselves in the foot.

bq. We have both noPreserveRaw and preserveRaw booleans, can we standardize on one everywhere? I'd like a negative one, call it disableRaw or excludeRaw since it better captures the meaning of the flag. exclude feels a bit better IMO, but it looks like -pe is taken.
 
I changed all of the booleans to be ""disableRawXattrs"" (vs disableRaw). This way it matches the ""preserveXattrs"" siblings in the code. However, in SimpleCopyList#doBuildListing, the boolean is still named copyRawXAttrs to mirror its copyAcls and copyXAttrs siblings. It felt wrong to change that to a disableRawXattrs. Ditto #traverseNonEmptyDirectory. Not to mention that if we negated the sense of the boolean from copyRawXattrs to disableRawXattrs, it would hair up the calls to toCopyListFileStatus when they are &&'d with child.isDirectory(). It feels like it's cleaner to leave all three booleans in the positive in these two cases.

I left the name of the enum the same (DISABLERAWXATTRS) since (1) it is an available letter (""d""), and (2) I think you were only suggesting changing the name of the booleans in the code. I can change it to DISABLERAW if you prefer.

bq.  What's the expected behavior when the dest doesn't support xattrs or reserved raw, or supports xattrs but not reserved raw?

If the dest doesn't support xattrs or raw, then (assuming -px was specified) an exception is thrown (per current behavior). If the dest supports xattrs but not reserved raw, then it depends on whether reserved raw was specified in the source or not. If it was, then an INVALID_ARGUMENT (-1) is returned as the exit code. If reserved/raw was not specified on any source path, then silence.

bq.    CopyListing, this is where we'd also test to see if the destFS has a /.reserved/raw directory

Yes, I've added a test for src/dst reserved/raw mismatch here.

I've updated the documentation per your suggestions.

{quote}

    if (!Path.getPathWithoutSchemeAndAuthority(target).toString().

What if the target is a relative path here?
{quote}

Check me on this. I convinced myself that a relative path could never be relative to /.reserved/raw since you can't set your working directory to that.

bq. Any reason this isn't part of the existing XAttr test? They seem pretty similar, and you also added a PXD test to the existing test.

I started out with a combined test, but during development, they diverged enough in terms of structure, @BeforeClass, files that they operated on, initializing files, directories, xattrs, etc. that it felt cleaner to leave them split into two tests.
","06/Aug/14 02:16;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12659910/MAPREDUCE-6007.002.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4789//console

This message is automatically generated.","06/Aug/14 21:33;andrew.wang;bq. the only typo above is the last line which should be ""no raw xattrs are preserved""

If none of these flags are specified, AFAIK neither non-raw or raw xattrs are preserved, i.e. no xattrs. Yes?

bq. I convinced myself that a relative path could never be relative to /.reserved/raw since you can't set your working directory to that.

AFAIK you can set your wd to whatever you want, and you can have "".."" in absolute paths too. We need to make sure that this path is fully normalized if we're doing a prefix check. Paths from a FileStatus are normalized, but paths coming from the user (like the ones coming out of a DistCpOptions) are suspect. setTargetPathExists has one of these suspect checks.

Doc
* This is hard to read, could we expand this into a separate section and a new table? I'd particularly like to see a fuller explanation of what happens with different dst options.

CopyListing
* Let's improve the InvalidInputException message. Paths don't really ""specify"" something, you could say ""starts with"" or something instead. We should also print the target path.
* I don't quite understand this error either, why is a {{/.r/r}} src and {{-pd}} not okay? The exception also mentions the target not starting with {{/.r/r}}, but that's not part of the if check.
* Line longer than 80chars
* I expected to see a check that was ""if (-p || -px) && !-pd && src is /.r/r, then also check that the dst supports xattrs and is /.r/r"". I wish there was a way to test that it's HDFS too, but looking for dest having /.r/r is probably good enough.

CopyMapper
* Can we expand the block comment to say that toCopyListingFileStatus is used to filter xattrs, and passing copyXAttrs in twice is okay because we already did it earlier? The double passing looks weird, though logically correct.

DistCp:
* I really don't like setting the DISABLERAWXATTRS flag in setTargetPathExists, since the expectation is that Options flags are set by the user. This method is also not named such that doing this there makes sense. We have the target path via the DistCpOptions, so let's be explicit and verbose with the checks instead. This is quite possibly why the CopyListing check is confusing to me.
* To expand on the above, -px means preserving all xattrs, while -pxd means preserving non-raw xattrs. Then we have {{toCopyListingFileStatus}} where the {{preserveXAttrs}} parameter actually means ""preserve non-raw xattrs"". This is also definitely confusing...

DistCpOptionSwitch:
* XATTR is not a standard capitalization style, let's lower case it as xattr here. XAttr isn't standard either, but that ship has sailed.

Test
* I'd like tests for weird src and dst paths, i.e. relative or containing ""..""s
* We could also test the ""no preserve flags"" behavior, that no xattrs at all are preserved.","07/Aug/14 17:16;clamb;[~andrew.wang],

This patch eliminates the confusing -pd flag and instead preserves raw.* xattrs if all pathnames have /.reserved/raw. If none of the pathnames have /.reserved/raw then no raw.* xattrs are preserved. If a subset of the src and target pathnames have /.reserved/raw, then an error is thrown and a non-0 return code passed back.
","07/Aug/14 21:26;andrew.wang;Nice work, this is way simpler. I think we're pretty close.

* In the md.vm file, let's scratch the change to the table, I think the section is enough by itself.
* Not sure we're fully qualifying relative paths correctly. I wrote a small test which I expected to work. Could you confirm? I think we just need to qualify the src paths with the src FileSystem first.

{code}
  @Test
  public void testWorkingDir() throws Exception {
    final Path wd = fs.getWorkingDirectory();
    try {
      fs.setWorkingDirectory(new Path(""/.reserved/raw/""));
      doTestPreserveRawXAttrs(""raw/src"", ""raw/dest"", ""-px"", true, true, 
          DistCpConstants.SUCCESS);
    } finally {
      fs.setWorkingDirectory(wd);
    }
  }
{code}","07/Aug/14 21:28;andrew.wang;Eh, I looked at the test output, and it's complaining about ""raw/src doesn't exist"". I guess distcp doesn't support relative paths?

In that case, +1 pending the doc change.","07/Aug/14 21:42;andrew.wang;Okay, so I need to stop rushing this :) If you fix my above test by removing the ""raw"" path components, you'll see that the target path isn't being qualified before being checked. Try adding this near the top of SimpleCopyListing#validatePaths:

{code}
    # Qualify the target path before checking
    targetPath = targetFS.makeQualified(targetPath);
    final boolean targetIsReservedRaw =
        Path.getPathWithoutSchemeAndAuthority(targetPath).toString().
            startsWith(HDFS_RESERVED_RAW_DIRECTORY_NAME);
{code}","07/Aug/14 21:48;clamb;Thanks!

So, I'll

. Add a test like the one above
. fix the doc as you mentioned
. Add a makeQualified call
. Put quotes around the paths in the exception messages
","08/Aug/14 01:25;clamb;Thanks for the review @andrew.wang!

This .004 patch incorporates the four items that I indicated in my last comment.

I'll commit this shortly to fs-encryption.","08/Aug/14 01:33;clamb;I've committed this to fs-encryption.","30/Jun/15 07:19;vinodkv;Closing old tickets that are already part of a release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve Solaris support in Hadoop Map/Reduce,MAPREDUCE-6390,12836128,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,alanburlison,alanburlison,alanburlison,08/Jun/15 12:27,20/Jun/15 09:31,12/Jan/21 09:52,,,,,,,,,,build,,,,,,0,,,,,At present the Hadoop Map/Reduce native components aren't fully supported on Solaris primarily due to differences between Linux and Solaris. This top-level task will be used to group together both existing and new issues related to this work. A second goal is to improve Hadoop Map/Reduce performance on Solaris wherever possible.,"Solaris x86, Solaris sparc",alanburlison,cdouglas,srikanth.malligeswaran@gmail.com,varun_saxena,vvasudev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-11985,YARN-3719,HDFS-8478,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Jun 20 09:22:44 UTC 2015,,,,,,,"0|i2frcf:",9223372036854775807,,,,,,,,,,,Solaris Native C x86 sparc ,,,,,,,,,,"20/Jun/15 09:22;alanburlison;Patch to migrate Map/Reduce over to the new CMake infrastructure. Requires HADOOP-12036",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Learning Scheduler,MAPREDUCE-1439,12455029,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,jaideep,jaideep,jaideep,02/Feb/10 09:44,13/May/15 21:21,12/Jan/21 09:52,13/May/15 21:21,,,,,,,,,jobtracker,,,,,,0,,,,,"I would like to contribute the scheduler I have written to the MapReduce project. Presently the scheduler source code is available on http://code.google.com/p/learnsched/. It has been tested to work with Hadoop 0.20, although the code available at the URL had been modified to build with trunk and needs testing. Currently the scheduler is in experimental stages, and any feedback for improvement will be extremely useful.",,acmurthy,ashutoshc,aw,chaitk,cichaolee,devaraj,hammer,hong.tang,jaideep,kasha,leftnoteasy,lianhuiwang,matei,ravidotg,raviteja,schen,sreekanth,vicaya,xinxianyin,yhemanth,zjshen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/10 10:34;jaideep;learning-scheduler-description.pdf;https://issues.apache.org/jira/secure/attachment/12434517/learning-scheduler-description.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2015-05-12 02:32:12.274,,,false,,,,,,,,,,,,,,,,,,72471,,,,,Wed May 13 21:21:10 UTC 2015,,,,,,,"0|i0e9an:",81268,,,,,,,,,,,,,,,,,,,,,"02/Feb/10 10:34;jaideep;PDF describing important features and functioning of the scheduler.","12/May/15 02:32;cichaolee;I find it very intresting and useful, but I can not open the  source code link, can you share me?
my email: cichaolee_1@163.com
Thank you very much","13/May/15 20:20;leftnoteasy;[~jaideep], thanks for sharing this, but JIRA is to track issues or feature proposals. I suggest you can share it via mail list. Closing as not a problem.","13/May/15 20:32;leftnoteasy;My bad, this should be a new feature, reopen and assign to [~jaideep].","13/May/15 21:21;kasha;Given the lack of activity for 5 years and that there haven't been any releases on Hadoop 1, I think it is fair to close this as ""Won't Fix"".

If there is interest in building something along these lines for Yarn, please open a JIRA with a design doc. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add support to back up JHS files from application master,MAPREDUCE-6258,12774741,New Feature,Patch Available,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,john.jian.fang,john.jian.fang,12/Feb/15 23:44,06/May/15 03:26,12/Jan/21 09:52,,2.4.1,,,,,,,,applicationmaster,,,,,,0,BB2015-05-TBR,,,,"In hadoop two, job history files are stored on HDFS with a default retention period of one week. In a cloud environment, these HDFS files are actually stored on the disks of ephemeral instances that could go away once the instances are terminated. Users may want to back up the job history files for issue investigation and performance analysis before and after the cluster is terminated. 


A centralized backup mechanism could have a scalability issue for big and busy Hadoop clusters where there are probably tens of thousands of jobs every day. As a result, it is preferred to have a distributed way to back up the job history files in this case. To achieve this goal, we could add a new feature to back up the job history files in Application master. More specifically, we could copy the job history files to a backup path when they are moved from the temporary staging directory to the intermediate_done path in application master. Since application masters could run on any slave nodes on a Hadoop cluster, we could achieve a better scalability by backing up the job history files in a distributed fashion.

Please be aware, the backup path should be managed by the Hadoop users based on their needs. For example, some Hadoop users may copy the job history files to a cloud storage directly and keep them there forever. While some other users may want to store the job history files on local disks and clean them up from time to time.",,jlowe,john.jian.fang,varun_saxena,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/15 23:46;john.jian.fang;MAPREDUCE-6258.patch;https://issues.apache.org/jira/secure/attachment/12698571/MAPREDUCE-6258.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2015-02-13 01:30:23.769,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 13 19:45:44 UTC 2015,,,,,,,"0|i25kmn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"13/Feb/15 01:30;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12698571/MAPREDUCE-6258.patch
  against trunk revision 99f6bd4.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:red}-1 eclipse:eclipse{color}.  The patch failed to build with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5191//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5191//console

This message is automatically generated.","13/Feb/15 14:21;jlowe;If I understand correctly this seems like an uncommon, specific use-case, and there are simple alternatives that don't require adding features to MapReduce to solve it.  distcp is a very effective way to copy files in a distributed manner, and one could simply run distcp before shutting down the cluster.  Or even easier, we could configure the job intermediate and/or history directory to have the history files dumped somewhere that is more persistent which has the benefit of avoiding an extra copy.

","13/Feb/15 19:45;john.jian.fang;It is not uncommon, all users run hadoop clusters in cloud should face the same issue. For example, we have to write a specific progress to dump out the JHS files to local disks continuously and then upload to multiple places such as s3. As we observed, the single process did not scale well for a big and busy cluster and the overhead to synchronize the local JHS files and the files on HDFS is nontrivial. Furthermore, we need to have the JHS files available once a Job is finished that rules out distcp. 

As far as I understand, the current JHS files are stored on HDFS only by looking at its internal implementation. I think the reason is that they have to be remotely accessible if the job history server runs in another node after the JHS server is separated out from the job tracker in Hadoop one and the job tracker is split into multiple distributed components in hadoop two. You cannot really just dump the JHS files to somewhere. The ""somewhere"" must be reliable and accessible by the JHS server. As a result, I think this feature is the easy way to achieve our goal. Furthermore, this feature is off by default, users turn it on only when they need it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor MapOutput and MergeManager to facilitate reuse by Shuffle implementations,MAPREDUCE-4808,12616788,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,masokan,acmurthy,acmurthy,19/Nov/12 19:28,27/Apr/15 23:45,12/Jan/21 09:52,22/Jan/13 14:12,,,,,,2.0.3-alpha,,,,,,,,,0,,,,,"Now that Shuffle is pluggable (MAPREDUCE-4049), it would be convenient for alternate implementations to be able to reuse portions of the default implementation. 

This would come with the strong caveat that these classes are LimitedPrivate and Unstable.
",,acmurthy,avnerb,cdouglas,cutting,gortsleigh,hudson,jerrychenhf,jlowe,lakshman,lianhuiwang,masokan,revans2,sandyr,tenduy,tgraves,tomwhite,tucu00,ujjwal.wadhawan,vicaya,zzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-6332,,,,,,,,,,,,,,,,,,,,,"26/Nov/12 19:38;masokan;COMBO-mapreduce-4809-4812-4808.patch;https://issues.apache.org/jira/secure/attachment/12554898/COMBO-mapreduce-4809-4812-4808.patch","18/Jan/13 23:28;cdouglas;M4808-0.patch;https://issues.apache.org/jira/secure/attachment/12565578/M4808-0.patch","19/Jan/13 01:03;cdouglas;M4808-1.patch;https://issues.apache.org/jira/secure/attachment/12565595/M4808-1.patch","17/Jan/13 23:10;tucu00;MR-4808.patch;https://issues.apache.org/jira/secure/attachment/12565391/MR-4808.patch","21/Dec/12 03:06;masokan;MergeManagerPlugin.pdf;https://issues.apache.org/jira/secure/attachment/12562030/MergeManagerPlugin.pdf","18/Jan/13 20:19;masokan;mapreduce-4808.patch;https://issues.apache.org/jira/secure/attachment/12565547/mapreduce-4808.patch","15/Jan/13 21:55;masokan;mapreduce-4808.patch;https://issues.apache.org/jira/secure/attachment/12565016/mapreduce-4808.patch","15/Jan/13 21:05;masokan;mapreduce-4808.patch;https://issues.apache.org/jira/secure/attachment/12564997/mapreduce-4808.patch","23/Dec/12 22:41;masokan;mapreduce-4808.patch;https://issues.apache.org/jira/secure/attachment/12562292/mapreduce-4808.patch","21/Dec/12 03:45;masokan;mapreduce-4808.patch;https://issues.apache.org/jira/secure/attachment/12562034/mapreduce-4808.patch","18/Dec/12 17:49;masokan;mapreduce-4808.patch;https://issues.apache.org/jira/secure/attachment/12561532/mapreduce-4808.patch","17/Dec/12 23:28;masokan;mapreduce-4808.patch;https://issues.apache.org/jira/secure/attachment/12561370/mapreduce-4808.patch","25/Nov/12 05:03;masokan;mapreduce-4808.patch;https://issues.apache.org/jira/secure/attachment/12554786/mapreduce-4808.patch",,,,,,,,,,,,,,,,,,,,,,13.0,,,,,,,,,,,,,,,,,,,,2012-11-20 23:56:42.745,,,false,,,,,,,,,,,,,,,,,,258658,Reviewed,,,,Thu Jan 24 17:05:15 UTC 2013,,,,,,,"0|i0l207:",120975,,,,,,,,,,,,,,,,,,,,,"20/Nov/12 23:56;masokan;Hi Arun and Alejandro,
  Please take a look at the bare-bones version of the patch in MAPREDUCE-4812.  I will come back to this one once that is committed.  Arun, as per your suggestion {{ReduceInputMerger}} plugin will be passed to {{Shuffle.}}  However, to implement the sort avoidance plugin I need MAPREDUCE-4049(shuffle plugin) to be committed.

Thanks.
-- Asokan
","25/Nov/12 05:03;masokan;Hi Arun and Alejandro,
  I have uploaded the incremental patch as well as the combo patch.  The combo patch still has a mock plugin test.  I will attach the original end-to-end test once we have MAPREDUCE-4809, 4807, 4812, 4049, and 4048 are committed in that order.

Thanks.
-- Asokan
","25/Nov/12 05:06;masokan;I meant 4808 not 4048 in the above comment.
","26/Nov/12 20:36;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12554898/COMBO-mapreduce-4809-4812-4808.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3070//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3070//console

This message is automatically generated.","26/Nov/12 23:35;masokan;Hi Arun and Alejandro,
  I picked up Avner's patch for MAPREDUCE-4049, made some minor changes and merged it with MAPREDUCE-4809, MAPREDUCE-4807, MAPREDUCE-4812, and MAPREDUCE-4808 to build an experimental version.  I modified the original end-to-end test(which does a copy operation both on the map and reduce sides) and it ran successfully with this experimental version.

-- Asokan
","07/Dec/12 06:32;acmurthy;Asokan, apologies for the delay.

I'm very confused about this patch.

Can you please explain why you need ReduceInputMerger and why it has only some of the interfaces in MergeManager?

How do you intend to use this for SyncSort?

Thanks.","07/Dec/12 06:37;acmurthy;To be clear: my preference would be to just use MergeManager as the interface. 

IAC, I feel I just don't understand the requirements - can you pls elaborate?","07/Dec/12 16:26;acmurthy;On second thoughts, a simpler solution: why don't we use a simplified version of Merger as the interface?

This way Syncsort can just implement that, why bother with trying to deal with memory reservation etc.?","07/Dec/12 21:39;tucu00;Arun, what do you exactly mean?

Both Merger and MergeManager are classes. 

The reducer side of logic in the Merger class is used only in 2 places, within the MergeManager (used by the Shuffle) and in the ReduceTask (for the local case).

This patch is moving  the later use into the MergeManager as well. By doing this all reduce merge logic, for the local and the distributed case, is encapsulated in the MergerManager.

Then, when an alternate implementation is provided, it can handle both cases, local and distributed case.

The resulting interface being introduced as ReduceInputMerger is quite simple:

{code}
  public void init(Context<K, V> reduceMergerContext);
  public void waitForResource() throws InterruptedException;
  public MapOutput<K, V> reserve(TaskAttemptID mapId, long requestedSize,
                                 int fetcher) throws IOException;
  public RawKeyValueIterator close() throws Throwable;

  // To merge files created for a local job.
  public RawKeyValueIterator mergeLocalFiles(Path localFiles[])
    throws IOException;
{code}

I think this is much simpler than trying modify things in the Merger, given that the merger is not directly used by the Shuffle, but through the MergeManager.
","09/Dec/12 16:41;masokan;Hi Aun,
   Thanks for your feedback.  Perhaps I should mention some use cases of a MergeManager plugin in addition to the technical details of the design mentioned here as well as in MAPREDUCE-4812.

MergeManager plugin would allow us and any implementer of the plugin to do variety of additional transformations like copy, limit-N query(MAPREDUCE-1928), full join, and hashed aggregation more efficiently.  Since shuffle code is available in the framework, we want to make use of it.  In my opinion, the framework shuffle code seems to be stable in MRv2.

Making Merger to be pluggable will not add much value.  If I understand correctly, it allows plugin implementers to implement only a single pass of the merge.  The overall merge is still driven by MergeManager.  Also, there is only merge operation possible.  Any additional transformation has to be done in the Reducer only.  A lot of times this is not very efficient.

Hope I clarified the usefulness of allowing MergeManager to be pluggable.  Please feel free if you any questions.

Thanks.

-- Asokan","10/Dec/12 21:57;acmurthy;Asokan, thanks for the clarification.

However, I'm still trying to understand what you are trying to achieve here.

The original goals of the parent task (MAPREDUCE-2454) was to make 'sort pluggable'.

We've accomplished that with MAPREDUCE-4807 and MAPREDUCE-4809.

Now, are we done? If not, what else is remaining to achieve that? Do you need some special hook in the Reducer's merge for Syncsort?

As I've told you in person, when making sweeping changes to framework it's better to focus on the 'goal' and make as minimal changes to get there.

We can always do more work and add more features, but let's do one thing at a time. We can add limit-N etc. separately, it just delays this jira - why do that?
","10/Dec/12 22:09;acmurthy;To be clear, I'm not against newer features - I just want them done independently so we can close this out and be done with.","13/Dec/12 19:40;acmurthy;Asokan? Alejandro?

Can you pls confirm if we need this for MAPREDUCE-2454? If not, we can decouple this and do it separately after merging MAPREDUCE-2454 to trunk. 

Thoughts?","13/Dec/12 19:57;cdouglas;bq. Can you pls confirm if we need this for MAPREDUCE-2454? If not, we can decouple this and do it separately after merging MAPREDUCE-2454 to trunk.

From his [comment|https://issues.apache.org/jira/browse/MAPREDUCE-4049?focusedCommentId=13529004&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13529004] on MAPREDUCE-4049, I think that's the consensus approach.","13/Dec/12 20:33;tucu00;The idea is to merge what we have in branch MR-2454 (MAPREDUCE-4809 & MAPREDUCE-4807) in trunk and continue the remaining work in trunk.","13/Dec/12 23:35;acmurthy;I'll ask again - is there any remaining work required on reduce-side for Syncsort or are we done?

For e.g. MAPREDUCE-4809 made changes to reduce-side (MapHost etc.) - are they now deemed unnecessary?","13/Dec/12 23:40;tucu00;Yes, there is work pending on the reducer side. Making the MergeManager pluggable (MAPREDUCE-4812), the refactoring of MapOuput into an abstract class and 2 concrete impls MEM & DISK and enabling the local case to use the plugable MergerManager (this JIRA). The changes in MAPREDUCE-4809 were of visibility only.","17/Dec/12 23:28;masokan;I will briefly outline what the patch does and the rationale.
* It makes the {{MergeManager}} pluggable.  Rationale: {{MergeManager}} does the merge sorting on the reduce side and is part of the overall sorting that happens in MR data flow.
* It makes  {{MapOutput}} class overridable.
Rationale: {{MergeManager}} plugin implementations can make efficient use of JVM memory for data shuffling and provide their own implementation of {{MapOutput.}}
* It makes local job runs use {{MergeManager}} or plugin implementations to do the merge sort instead of {{Merger.}}
Rationale: Local job runs should also be able to use the sort plugin on the reduce side just like they can make use of the sort plugin on the map side.","18/Dec/12 01:06;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12561370/mapreduce-4808.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 2018 javac compiler warnings (more than the trunk's current 2012 warnings).

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3129//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3129//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3129//console

This message is automatically generated.","18/Dec/12 17:49;masokan;Fixed java compiler warnings in the tests.
","18/Dec/12 18:32;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12561532/mapreduce-4808.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3131//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3131//console

This message is automatically generated.","20/Dec/12 15:15;tomwhite;These changes look reasonable to me. The changes in MapOutput means Fetcher doesn't know if the shuffle is in-memory or on-disk - it's hidden behind the MapOutput shuffle() method - which is an improved separation of concerns. MergeManagerPlugin is then free to return whatever implementations of MapOutput that it likes.

* It wasn't immediately obvious to me that returning null in MergeManager#reserve() means ""wait"" (since the ""wait"" type of MapOutput has gone). It would be good to have a comment to that effect.
* It would be good to have javadoc for the methods on MergeManagerPlugin.
* In TestMergeManagerPlugin the try/catch blocks can be avoided by making the tests throw the relevant exception. 
","20/Dec/12 18:40;acmurthy;Mariappan, can you pls put up a design doc on how you see all these apis being used?

Rather than debate code upfront, let's get to same page on our goals and use-cases. Tx.","21/Dec/12 03:06;masokan;Hi Tom,
  Thanks for your comments.  I have made the changes you suggested and uploaded a new patch.

Hi Arun,
  I created a design document that contains both the use cases and the rationale for the new interface for {{MergeManagerPlugin.}}

Thanks to both of you for reviewing the patch.

-- Asokan","21/Dec/12 03:12;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12562030/MergeManagerPlugin.pdf
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3155//console

This message is automatically generated.","21/Dec/12 04:18;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12562034/mapreduce-4808.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3156//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3156//console

This message is automatically generated.","23/Dec/12 22:41;masokan;Picked up changes I made to MergeManager.java in MAPREDUCE-4842 to fix a severe bug and uploaded a new patch.
","23/Dec/12 22:44;tenduy;Thank you for your message. I am out of the office with limited access to e-mail, returning on Monday Jan 7th. Should you need immediate assistance, please contact Fernanda Tavares at ftavares@syncsort.com<mailto:ftavares@syncsort.com>. Wishing you a Happy Holiday Season and a Happy New Year!



Regards,
","23/Dec/12 22:55;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12562292/mapreduce-4808.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3171//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3171//console

This message is automatically generated.","27/Dec/12 18:25;tucu00;+1, to the patch and design doc. 

I'll add that having this pluggability in place it would have allowed to have a fix for MAPREDUCE-4842 avail for jobs hitting the problem until the fix is delivered in a Hadoop release. 

I'll wait until after the New Year to give the opportunity to others to review/comment the patch and design doc.
","27/Dec/12 18:28;tenduy;Thank you for your message. I am out of the office with limited access to e-mail, returning on Monday Jan 7th. Should you need immediate assistance, please contact Fernanda Tavares at ftavares@syncsort.com<mailto:ftavares@syncsort.com>. Wishing you a Happy Holiday Season and a Happy New Year!



Regards,
","09/Jan/13 17:39;tomwhite;Asokan, thanks for addressing my feedback. The latest patch looks good to me.","10/Jan/13 04:37;acmurthy;Asokan, sorry I've been away traveling home during the holidays and hence the delay.

I have more comments, but I'll put some here to keep the discussion going.

Thanks for the design doc, but I was looking for thoughts on *how* the plugin was going used for use-cases you've mentioned (hash-join etc.), alternatives on design etc. 

IAC, taking a step back, the 'goal' here is to make the 'merge' pluggable.

Reduce-side has 2 pieces:
# Shuffle - Move data from maps to the reduce.
# Merge - Merge already sorted map-outputs.

The rest (MergeManager etc.) are merely implementation details to manage memory etc., which are irrelevant in several scenarios as soon as we consider alternatives to the current HTTP-based shuffle (several alternatives exist such RDMA etc.).

Your current approach tries to encapsulate and enshrine the current implementation of the reduce task, which I'm not wild about. By this I mean, you are focussing too much on the current state and trying to make interfaces which are unnecessary for now and might not suffice for the future.

I really don't think we should be tying Shuffle & Merge as you have done by introducing yet another new interface (regardless of whether it's public or not).


As I've noted above, adding a simple 'Merge' interface with one 'merge' call will address all of the use-cases you have outlined. If not, let's discuss.
","10/Jan/13 04:57;acmurthy;bq. As I've noted above, adding a simple 'Merge' interface with one 'merge' call will address all of the use-cases you have outlined. If not, let's discuss.

Forgot to add - if you don't think this suffices, please help me understand why. We can discuss alternatives then. Thanks.","10/Jan/13 05:02;acmurthy;As an example - with RDMA-based shuffle we don't need MergeManager at all since we can just do very-wide, network merges directly.","10/Jan/13 21:02;masokan;Hi Arun,
  Thanks for your comments.  The design for the use cases like hash-join, limit-N query, and so on is left to the creativity of the implementer of the plugin.  I did not want to mention any specific designs.  For the case of limit-N query, I created a test which contains one implementation of the plugin.

You mentioned RDMA shuffle as one of the alternative shuffle implementations.  The RDMA shuffle requires special hardware(infiniband card) which may not be present in all Hadoop installations.  RDMA based shuffle does not require {{MergeManager}} because it is a combination of shuffle and merge implemented in native code.  There is no clear separation of shuffle and merge.

The current HTTP shuffle has been around in Hadoop for a long time and functionally it will continue to work even with infiniband cards by using IP over Infiniband(IPoIB) without requiring any native code.

I consider RDMA shuffle as a special case and it is not going to be very common to warrant obsoleting the current separation of shuffle and merge. Besides, a merge plugin does not preclude RDMA merge.  A shuffle plugin can be used for that purpose.

The {{MergeManager}} not only manages memory, it also coordinates with the shuffle and manages mulitple merge passes.  The interface I have defined captures the methods needed for the above purposes.  A single {{merge()}} method will not suffice to take care of merging shuffled data.

-- Asokan","11/Jan/13 05:52;jerrychenhf;Hi Arun and Asokan,
I am trying an implemenation of hash based reduce other than the global sort and merge. And first of all, Asokan's work is valuable for making this implemenation possible. And I have tried to use interfaces in this patch to plugin the HashMergeManager. Mostly, the current interface can satisfy the hash merge manager needs. But I am not sure whether the interface is ""suffice for the future"" for others just as Arun point out.

While the problem I encountered during the implementation is the reusing of the common code of the MergeManager. HashMergeManager shares a lot of common feature and code from the MergeManager such as OnDiskMapOutput, InMemoryMapOutput, InMemoryReader and MergeThread (PartitionThread actually). But the current code base of these classes make references to MergeManager and thus can not be reused by a HashMergeManager. I have to make another copy of these classes and modify them to refer to HashMergeManager. These would generate duplicated code and would not be preferred.

We would best either make MergeManager inheritable (by making some private member protected) or abstract out OnDiskMapOutput and InMemoryMapoutput and others to be utility classes that can be shared by some ""light weight"" merge managers which share a lot of common with MergeManager and only modify some small aspects.

Jerry


","11/Jan/13 15:45;masokan;Hi Jerry,
  Thanks for your feedback and comments.  There may be several ways you can implement a {{MergeManager}} plugin.  Inheriting from current {{MergeManager}} is one of them that you tried for your implementation.

I would suggest you take a look at the test that I created for the patch.  It contains a plugin implementation that does not do any sorting both on the map and reduce sides.  Although it is a very simple implementation, it is a good starting point for your hash aggregation.  I attached it to some of the previous patches I posted for MAPREDUCE-2454.  If you want the latest copy, please shoot an e-mail to me.  I will send it to you.

-- Asokan
","11/Jan/13 22:04;tucu00;Jerry,

Would your suggested changes be on the MergeManager implementation only? Without affecting the interfaces this JIRA defines? If that is the case, I'd suggest having a different JIRA to modify the visibility of the methods/classes from MergeManager; by doing that we keep this JIRA focused on making MergeManager pluggable and the follow up JIRA to enable reusing it.

Regarding your comments on if this interface will 'suffice for the future', that is why all these classes/interfaces are considered UNSTABLE and LIMITED_PRIVATE. In other words, this is an initial step that enables the current needs but we could change it in the future.

","13/Jan/13 01:19;cdouglas;I like many of these changes anyway- the refactoring from tagged unions to a type hierarchy for {{MapOutput}} is nice- though I'm still not groking why the new API is necessary. The design doc reads like a summary of the patches and a motivation for pluggability generally, without directly motivating this particular moving part. [~masokan], is there code from MAPREDUCE-2454 reviewers should look at to better understand why this is required?

While much of the shuffle code can be reused between implementations- as Jerry points out- a common base for all implementations would be overly-ambitious. To rephrase what Arun has been saying: adding a ""plugin"" for tweaking the existing code is not necessary when the whole {{Shuffle}} can be swapped out after MAPREDUCE-4049.

The only exception I can think of is for {{LocalRunner}}, where the {{Merger}}- not the {{MergeManager}}- needs to be pluggable. The current patch introduces a dependency on the {{MergeManager}} for the local case, using only the subset that applies in that context. Without a use case (e.g., useful for testing), it's hard to justify the patch's abstraction if all implementations of {{MergeManager}} are exposed to this special invocation. To be fair, it may be required for the ""ubertask"" code to work seamlessly, but that case needs to be made. Even so: the {{Merger}} seems like the right cut point.

To be concrete, take the RDMA/network merge case as an example. Assume the overhead of a reader requires one to limit the number of clients/open connections in the reduce. Implementing a {{Merger}} that keeps a total order on its segments and only keeps the top _k_ remote segments open would loosely track that resource. But since the {{Fetcher}} is essentially just packaging the remote segment and translating the TCE into a {{MapOutput}} (i.e., doesn't open a connection), it's not obvious why a {{MergeManager}} would coordinate resource use between them. Even with a hybrid scheme- where a set of {{Fetcher}} and {{Merger}} instances pull segments locally based on some cost function, so they *do* share resources- aren't the changes extensive enough to justify a separate {{Shuffle}} implementation?

The Merger *does* handle multiple merge passes; intermediate merges of segments are part of its contract. While you may be referring to _continuous_ merging of segments arriving concurrently with a merge, again: why would it be preferable to configure a {{MergeManager}} instead of a wholly new {{Shuffle}} impl?","14/Jan/13 20:14;tucu00;Arun, Chris, Asokan,

It seem the contention point here is adding the MergerManager as public API.

How about the following alternative?

* 1. Refactoring of the MapOutput into abstract class and introduce the Memory and Disk subclasses.
* 2. Make the MergerManager pluggable in the Shuffle class via ReflectionUtils or a protected createMergeManager() method.
* 3. Move the thread creation from the MergeManager constructor to an init() method.

This would keep the MergerManager specific to the Shuffle implementation but it would allow alternate implementations to reuse parts of it.

The only thing left would be how to handle the local case which currently is done by the Merger via a static method.
","14/Jan/13 20:20;cdouglas;[~tucu00]: that sounds reasonable; it should also simplify the API by eliminating the {{mergeLocalFiles}} method.","15/Jan/13 21:05;masokan;Hi Chris and Alejandro,
  Thanks to both of you for your comments and suggestions.  I spent some time reflecting on them.  I totally agree with Chris that the RDMA/network merge will be different from the way the merge is done by the current {{MergeManager.}} In that case a shuffle plugin is the way to go.

When the current HTTP based {{Shuffle}} is used, it makes sense to define a {{MergeManagerPlugin}} to work with that.  So based on Alejandro's suggestion, I updated the patch to make {{MergeManagerPlugin}} work only with {{Shuffle.}}  I removed the method {{mergeLocalFiles()}} from the interface as Chris suggested.  The {{Shuffle}} class would instantiate the {{MergeManagerPlugin}} object in its {{init()}} method.

By the end of the day, Hadoop users will be offered more choices: a shuffle plugin for RDMA/network merge when the cluster nodes have hardware support and a merge plugin that works with the current shuffle when the cluster does not have special network hardware.

Please provide any feedback on the updated patch.

Once again, thanks to both of you for the time you are spending on this Jira.

-- Asokan
","15/Jan/13 21:24;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12564997/mapreduce-4808.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3242//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3242//console

This message is automatically generated.","15/Jan/13 21:25;tucu00;Thanks Asokan, LGTM. Chris, is this what you had in mind? Are we good to go? Thx","15/Jan/13 21:55;masokan;Added the call to {{init()}} method of {{MergeManagerPlugin}} in {{Shuffle}}'s {{init().}}  Surprised that the testing did not catch this.

-- Asokan
","15/Jan/13 22:14;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12565016/mapreduce-4808.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3244//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3244//console

This message is automatically generated.","15/Jan/13 23:22;acmurthy;This is better - but it still makes the MergeManager a public api, which I'm not wild about.

We aren't we considering the Merger? It would be simple and would easily solve the limit-N case by aborting the merge *after* N+1 items are seen.
","15/Jan/13 23:23;acmurthy;Would you prefer to have me implement a prototype of the Merger?","15/Jan/13 23:25;acmurthy;As I've said before MergeManager is an unholy alliance of Shuffle & Merge phases, we really should separate them - not enshrine them further by making MergeManager a public api.","15/Jan/13 23:52;acmurthy;I'll admit I'm really confused - the jira says 'make reduce-side merge pluggable' and then the patch does changes to MapOutput, tries to make MergeManager a resolver etc.

OTOH, there is a concrete alternative to introduce a Merger abstraction which solves the use-cases described i.e. limit-N, hash-join etc.

Why are we not doing the straight-fwd thing of solving what the jira intends to do i.e. make merge pluggable?

Are we trying to solve something else here? If so, can we at least articulate what we are trying to solve? Thanks.","16/Jan/13 00:21;tucu00;Arun, thanks for pointing out the wrong summary/description. I think we ended up with it when creating the subtasks for MAPREDUCE-2454 based on previous ideas.

I've just updated it reflect the scope of the current patch.","16/Jan/13 14:33;acmurthy;[~tucu00] Thanks for clarifying that this is indeed a different endeavor.

Can you please help me understand what the goals now are? What are we hoping to achieve with this re-factor? Is this something specific to Syncsort's needs? I'm not against it, I just want to understand why we are doing this re-factor and how it will be used by Syncsort. Thanks.","16/Jan/13 17:02;tucu00;The goal is to be able to write alternate implementations of the Shuffle, like the ones Asokan and Jerry are trying to do, while reusing functionality provided by the Hadoop default implementation. For example, being able to leverage the logic in the default shuffle (ie fetchers), while replacing the merge logic and merge resources allocation logic driven by the MergeManager. While some of this logic replacement could be done at Merge level as you suggested, other, like MapOutput allocation cannot be done there as this is driven by the MergeManager. The refactoring of MapOutput allocation from struct to  classes permits alternate implementations that can reuse memory buffers thus reducing JVM heap allocation.","16/Jan/13 23:27;tucu00;Assuming all concerns/questions have been addressed, can we move forward and commit the latest patch?","17/Jan/13 15:02;acmurthy;bq. The goal is to be able to write alternate implementations of the Shuffle

Alejandro - it seems like you understand something about the use-case that I don't. Maybe you & Asokan have had a private chat? 

What are the use-cases for alternate implementations of the Shuffle? Like Chris also mentioned with MAPREDUCE-4049 we already allow alternate implementations of Shuffle, is this redundant then?

bq. While some of this logic replacement could be done at Merge level as you suggested, other, like MapOutput allocation cannot be done there as this is driven by the MergeManager. 

So, a combination of MapOutput re-factor and Merger interface should suffice?

IAC, what are the use-cases for alternate implementations of MapOutput? Or, is it the MapOutput re-factor merely a code-hygiene issue?

----

I'm not trying to be difficult here. But, I feel like I just don't understand the use-case. So, I'd appreciate if we could focus on concrete use-cases for the plugin. I admit I still am having a hard time understanding why we need this complexity.

Thanks.","17/Jan/13 17:47;masokan;Hi Arun,
  MAPREDUCE-4049 expects the plugin implementer to implement the shuffle from scratch.  With the default implementation of HTTP shuffle being robust and secure it is possible to reuse it in majority of the situations.

The alternate implementation of MapOutput can be left to the plugin implementer.  For example, it can be optimized to use less JVM memory and minimize Java garbage collection.

Some of the concrete use cases for the plugin are: hash aggregation, hash join, limit-N query, etc.

Thanks.

-- Asokan
","17/Jan/13 20:01;cdouglas;Asokan, the concern is that even breaking an API, even if it's marked unstable, is an incompatible change. Since the pluggable shuffle is particularly useful for frameworks, breaking this contract could require patching/validation/rewrite of plugin and optimizer code in projects that invest in it (Hive, Pig, etc.). Moreover, if we wanted to change the default {{Shuffle}} to a different implementation, then user/framework code would perform badly- or break- unless we exposed this implementation-specific mechanism in the _new_ impl. So it's fair to press for use cases, to ensure it's _sufficient_ and that the abstraction could apply to most {{Shuffle}} implementations.

Personally, I'm ambivalent about exposing this as an API and am +1 on the patch overall (mostly because I like the {{MapOutput}} refactoring). The user can always configure the current {{Shuffle}}, which is exactly how frameworks would handle this until they port/specialize their efficient {{MergeManager}} plugin.

As a compromise, would it make sense to just add a protected {{createMergeManager}} method to the {{Shuffle}}? The user still needs to configure their custom {{Shuffle}} impl now, but that's better than the inevitable future where they configure both. It also makes its tie to this implementation explicit.","17/Jan/13 20:51;tucu00;Chris, are you suggesting?

* remove the MergeManagerPlugin interface
* introduce a protected createMergerManager() in the Shuffle class to instantiate (via new) & initialize the existing MergerManager.


","17/Jan/13 21:11;masokan;Hi Arun,
  I will think about your suggestion to make the Merger class pluggable and post my findings for different use cases.

-- Asokan
","17/Jan/13 21:13;masokan;Hi Chris,
  I will work on creating a real working plugin for the use cases to show that the proposed API is sufficient to handle them.

-- Asokan","17/Jan/13 21:15;masokan;Hi Alejandro,
  If the MergeManagerPlugin is to be removed, it should be possible to extend the framework's MergeManager by an external implementation.

-- Asokan
","17/Jan/13 21:19;masokan;Hi Alejandro,
  I meant to ask whether it is okay to make the existing MergeManager to be extendable?

-- Asokan","17/Jan/13 23:03;masokan;Hi Arun,
  I will try to explain a simple use case of an external implementation of merge on the reduce side.  Let us say this merge implementation has some fixed area of memory (Java byte array) allocated to store the shuffled data.  This may be done to avoid frequent garbage collection by JVM or for better processor cache efficiency.

Looking at the methods in the {{Merge}} class, they either accept input to the merge in disk files(array of {{Path}} objects) or memory segments(list of {{Segment}} objects.)  The former is not suitable since merge is done in memory first and any intermediate merged output file is under the control of the plugin implementation.  The latter is not suitable because memory for the shuffled data is not under the control of the plugin implementation.

Ideally, if an {{InputStream}} object is available, the external implementation can read shuffled data from the stream to the fixed area of memory at a specific offset in the byte array.

With the {{MergeManagerPlugin,}} the external implementation will get the HTTP connection's {{InputStream}} object via the {{shuffle()}} method in {{MapOutput}} object.  In addition, if merge goes though multiple passes because the memory area is limited in size, there should be some way for the {{Shuffle}} to wait until memory is released by a merge pass.  There is no method in {{Merge}} for that either.

I find that it is possible to define the interaction points between current {{Shuffle}} and {{MergeManager}} using the {{MergeManagerPlugin}} interface.  The plugin interface has only three methods and it allows the external plugin to have a lot of freedom in its implementation.  As a side effect, the {{MapOutput}} is also refactored.

Hope I explained this well.  If you have any questions, please let me know.

-- Asokan
","17/Jan/13 23:10;tucu00;I've taken the liberty to tweak the patch a bit based the last comments in the JIRA.

* removed pluggability via config of the MergeManager
* Shuffle has a protected createMergeManager() method
* MergeManager is annotated as Private
* Kept MergeManagerPlugin interface
* Removed MergeManagerPlugin.Context
* MergeManagerPlugin interface annotated as Private

These changes avoid having an extra know (the MergeManager class) in the config. Keep the MergeManager owned by the Shuffle class. The interface allows, for impls like Jerry's and Asokan's, for alternate implementations.

Asokan, Arun, Chris?","17/Jan/13 23:26;cdouglas;+1 Looked through it; the latest patch lgtm. Asokan, is that sufficient for your use cases? Arun?

_Very_ minor, optional nit: {{s/MergeManager/MergeManagerImpl/}} and {{s/MergeManagerPlugin/MergeManager/}}. There's an argument to be made for doing the same with the {{ShuffleScheduler}} while we're at it, but neither of these are blocking, IMO.","17/Jan/13 23:36;masokan;Hi Chris,
  Thanks for your quick feedback.  I looked at the patch.  It has one minor nit.  The {{createMergeManager}} method should take {{ShuffleConsumerPlugin.Context}} object. I will go over it one more time, work out the change, run tests, and post the patch shortly.

Thanks.

-- Asokan
","18/Jan/13 19:24;acmurthy;bq. I will try to explain a simple use case of an external implementation of merge on the reduce side. Let us say this merge implementation has some fixed area of memory (Java byte array) allocated to store the shuffled data. This may be done to avoid frequent garbage collection by JVM or for better processor cache efficiency.

Asokan - this is the first time I've heard this use case which seems something Syncsort can take advantage of, and, as a consequence, I've been viewing from the lens of 'limit-N/hash-join' merge etc.

In future, being clear and upfront about use-cases will obviously prevent further such confusion.

----

Having said that, I still feel a better approach would be to use a custom shuffle via MAPREDUCE-4049 and friends since you get more control - for e.g. you might want to defer shuffle based on memory on the heap (byte[]) and memory outside heap (JNI or DirectBuffers) for Syncsort plugin - and clearly, the current MergeManager will not suffice for such.

However, if this unblocks you in the short run I think the approach is fine. Thanks for the clarification. I'll take another look at the details on the patch once you upload it, but seem mostly fine to me. Thanks.","18/Jan/13 20:19;masokan;I have uploaded the latest patch.

Arun, thanks for your feedback.  Your points are well taken.  I realized that I should have posted the details of an implementation of a plugin to justify the design.  Please take a look at the latest patch.

Chris, I thought about your suggestion on naming.  I came up with the following conclusions:
* Changing the name of {{MergeManager}} at this point may cause problems if this Jira is ported to other branches(like 0.23.)
* The existing test {{TestMergeManager}} also has to be renamed.

So, I renamed {{MergeManagerPlugin}} to {{MergeManagerI}} since it is just an interface.  Is that okay?

Thanks.

-- Asokan
","18/Jan/13 21:39;cdouglas;bq. So, I renamed MergeManagerPlugin to MergeManagerI since it is just an interface. Is that okay?

That's not a naming convention used anywhere else in Hadoop. I'd rather not introduce it for this case. Renaming test classes and problems with backporting(?) are not issues.","18/Jan/13 22:17;tucu00;Mhhh, I would prefer not rename MergeManager class as fixes in trunk (after the rename) will require manual patching in branch 0.23 and maintenance releases of branch 2. Why not leave the original MergerManagerPlugin name?","18/Jan/13 23:28;cdouglas;I'd argue:

# {{sed}} is not brain surgery and these issues will likely be backported to branch-2 anyway
# Taking the good names for interfaces, even if they're mostly private, is usually better than reserving them for implementations
# It shows that the {{MapOutput}} types are bound to the impl, e.g., {{InMemoryMapOutput}} calls {{closeInMemoryFile}} on the {{MergeManager}} instance, which is only part of that impl's API.

This does the naive rename, and also renames the findbugs exclude rule for the {{MergeManager}}. I'd be OK with the patch as-is, but would prefer this.","19/Jan/13 00:10;tucu00;Sure, me good. thx","19/Jan/13 00:33;masokan;Hi Chris,
  Thanks for posting the proper patch.  I am ashamed of myself:) I should have done it myself without coming up with excuses.  I ran all mapreduce tests and verified your patch.

-- Asokan
","19/Jan/13 00:54;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12565578/M4808-0.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 3 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3252//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3252//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-core.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3252//console

This message is automatically generated.","19/Jan/13 01:03;cdouglas;Wrong patch; meant to change the target of the findbugs suppression, not delete it. However, since the reasoning includes ""not likely to be subclassed""- which is exactly what this issue does- maybe we should move this to init()","19/Jan/13 01:24;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12565595/M4808-1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3254//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3254//console

This message is automatically generated.","19/Jan/13 01:29;masokan;Chris,
   Any shuffle plugin can subclass {{Shuffle}} but should implement its own {{MergeManager.}}  Allowing one to subclass {{MergeManagerImpl}} is not within scope of this Jira.  So, I am okay to suppress the warning for now.

Thanks.
-- Asokan
","21/Jan/13 07:20;cdouglas;Fair point.

This looks ready to commit. Unless someone would like additional changes, I'll plan to push it in tomorrow.","22/Jan/13 14:12;tucu00;Thanks Asokan and everybody. Committed to trunk.","22/Jan/13 14:15;hudson;Integrated in Hadoop-trunk-Commit #3266 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3266/])
    MAPREDUCE-4808. Refactor MapOutput and MergeManager to facilitate reuse by Shuffle implementations. (masokan via tucu) (Revision 1436936)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1436936
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/dev-support/findbugs-exclude.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/InMemoryMapOutput.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/InMemoryReader.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MapOutput.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeThread.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/OnDiskMapOutput.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Shuffle.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestFetcher.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestMergeManager.java
","22/Jan/13 17:28;masokan;Alejandro, thanks for committing this.

Alejandro, Arun, Chris, and Tom, thanks to all of you for providing valuable comments and feedback.

-- Asokan
","23/Jan/13 10:47;hudson;Integrated in Hadoop-Yarn-trunk #105 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/105/])
    MAPREDUCE-4808. Refactor MapOutput and MergeManager to facilitate reuse by Shuffle implementations. (masokan via tucu) (Revision 1436936)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1436936
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/dev-support/findbugs-exclude.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/InMemoryMapOutput.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/InMemoryReader.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MapOutput.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeThread.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/OnDiskMapOutput.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Shuffle.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestFetcher.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestMergeManager.java
","23/Jan/13 12:56;hudson;Integrated in Hadoop-Hdfs-trunk #1294 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1294/])
    MAPREDUCE-4808. Refactor MapOutput and MergeManager to facilitate reuse by Shuffle implementations. (masokan via tucu) (Revision 1436936)

     Result = FAILURE
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1436936
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/dev-support/findbugs-exclude.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/InMemoryMapOutput.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/InMemoryReader.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MapOutput.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeThread.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/OnDiskMapOutput.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Shuffle.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestFetcher.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestMergeManager.java
","23/Jan/13 14:04;hudson;Integrated in Hadoop-Mapreduce-trunk #1322 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1322/])
    MAPREDUCE-4808. Refactor MapOutput and MergeManager to facilitate reuse by Shuffle implementations. (masokan via tucu) (Revision 1436936)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1436936
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/dev-support/findbugs-exclude.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/InMemoryMapOutput.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/InMemoryReader.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MapOutput.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeThread.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/OnDiskMapOutput.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Shuffle.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestFetcher.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestMergeManager.java
","24/Jan/13 17:05;masokan;Hi Alejandro, Arun, Chris, and Tom,
  Now that MAPREDUCE-4807, MAPREDUCE-4809, and MAPREDUCE-4808 were committed, I would like to work on MAPREDUCCE-4039(sort avoidance) as two contributed plugins.  One will be to avoid sorting on the map side and the other on the reduce side.  These plugins will open up more use cases.  For example, Jerry can implement hash aggregation in the Combiner and/or in the Reducer without any overhead of the sort.

If there is no objection from you, I will post a brief description of a design in MAPREDUCE-4039 soon.  Please let me know.

Thanks.

-- Asokan
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tasks to run on a different jvm version than the TaskTracker,MAPREDUCE-217,12409659,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,amar_kamat,knoguchi,knoguchi,02/Dec/08 16:42,26/Apr/15 01:39,12/Jan/21 09:52,26/Apr/15 01:39,,,,,,,,,,,,,,,1,,,,,"We use 32-bit jvm for TaskTrackers. 
Sometimes our users want to call 64-bit JNI libraries from their tasks.
This requires tasks to be running on 64-bit jvm.
On Solaris, you can simply use -d32/-d64 to choose, but on Linux, it's on a completely different package.

So far, tasks run on the same jvm version as the TaskTracker.
{noformat}
// use same jvm as parent
File jvm =   new File(new File(System.getProperty(""java.home""), ""bin""), ""java"");
{noformat}

Is it possible to let users provide a java home path 
or let them choose from a pre-selected list of paths?

",linux,acmurthy,aw,cdouglas,iyappans,jothipn,mridulm80,revans2,romainr,vicaya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/09 10:58;amar_kamat;mapreduce-217-v1.0.patch;https://issues.apache.org/jira/secure/attachment/12423007/mapreduce-217-v1.0.patch","30/Oct/09 10:54;amar_kamat;mapreduce-217-v1.1.patch;https://issues.apache.org/jira/secure/attachment/12423679/mapreduce-217-v1.1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2009-10-23 10:58:04.567,,,false,,,,,,,,,,,,,,,,,,148605,,,,,Sun Apr 26 01:39:53 UTC 2015,,,,,,,"0|i0ivjj:",108204,,,,,,,,,,,,,,,,,,,,,"28/Sep/09 02:54;knoguchi;This wasn't as simple as setting a java path.
At least it also needs to set the new classpath for the native libraries and probably there's more that I'm missing.","23/Oct/09 10:58;amar_kamat;Attaching a patch that 
# allows the child jvm to be configured via mapreduce.task.jvm
# gives preference to child classpath over tasktracker (parent) classpath entries

Result of test-patch
[exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Testing the patch. Mainly I will test some 64bit jvm apps which includes 64 bit libs etc.

The testcase requires the system-property mapreduce.task.jvm to be set to an alternate jvm.","26/Oct/09 06:47;amar_kamat;Had a discussion with Sharad on this. As he rightly pointed out that giving preference to user defined classpath entries over (tt's) inherited classpath entries can lead to security issues where a malicious user can define its own Task.java or ReduceTask.java. I think we should keep the classpath ordering as is.

bq. At least it also needs to set the new classpath for the native libraries and probably there's more that I'm missing.
Koji, as of today users can add their libraries which is given preference over the inherited ones.

Currently this is what is done
child.jvm : tt.jvm
child.libraries : user-defined-libraries+tt.libraries
child.classpath : tt.classpath+job-jar.classpath+dist-cache-entries+current.wor,dir+user-defined.classpath

Changes are 
child.jvm : user-defined.jvm else tt.jvm
// since user is specifying the jvm, the user is responsible for add the the required libs too
","30/Oct/09 10:54;amar_kamat;Attaching a patch that simply allows users to configure the jvm for the tasks. Result of test-patch
[exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Running ant tests. ","03/Nov/09 07:19;amar_kamat;Except TestTaskTrackerMemoryManager (timeout), all tests have passed. Manually testing the patch.","03/Nov/09 11:26;amar_kamat;Manually tested the patch and checked that the configured jvm's libs/classes are loaded correctly. ","04/Nov/09 08:50;amar_kamat;bin/hadoop-config.sh by defaults adds JAVA_HOME/lib/tools.jar to the tasktracker's classpath which will be inherited by the child. Probably we should fix this to point to the configured java.home's tools.jar.","26/Apr/15 01:39;aw;This is possible to do in YARN.  Closing as fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DFSIO for truncate,MAPREDUCE-6227,12770187,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,shv,shv,shv,26/Jan/15 21:03,10/Apr/15 20:19,12/Jan/21 09:52,08/Feb/15 03:41,2.7.0,,,,,2.7.0,,,benchmarks,test,,,,,0,,,,,Create a benchmark and a test for truncate within the framework of TestDFSIO.,,hitliuyi,hudson,junping_du,milandesai,shv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/15 02:31;shv;DFSIO-truncate-00.patch;https://issues.apache.org/jira/secure/attachment/12696938/DFSIO-truncate-00.patch","06/Feb/15 07:42;shv;DFSIO-truncate-01.patch;https://issues.apache.org/jira/secure/attachment/12696986/DFSIO-truncate-01.patch","06/Feb/15 20:25;shv;DFSIO-truncate-02.patch;https://issues.apache.org/jira/secure/attachment/12697116/DFSIO-truncate-02.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2015-02-06 04:56:04.313,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Feb 09 00:15:43 UTC 2015,,,,,,,"0|i24td3:",9223372036854775807,,,,,,,,,,,,,2.7.0,,,,,,,,"06/Feb/15 02:31;shv;Adding truncate to DFSIO.","06/Feb/15 04:56;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12696938/DFSIO-truncate-00.patch
  against trunk revision 6583ad1.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.conf.TestJobConf

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5170//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5170//console

This message is automatically generated.","06/Feb/15 07:42;shv;Moved TestDFSIO_results.log under {{target/test-dir}} for tests.","06/Feb/15 08:47;hitliuyi;Thanks [~shv] for the patch.
Overall looks very good, just few comments, +1 after addressing.
# . 
{code}
isClosed = (status.getLen() == newLength);
{code}
If {{isClosed}} is true, we should break the loop and no need for an additional sleep, right?
# .
{code}
reporter.setStatus(""truncate recover for "" + name + "" to newLength "" + 
            newLength + "" attempt "" + (i++) + "" ::host = "" + hostName);
{code}
{{i++}} is duplicated with the one in {{for}}, we should only keep one.
# .
{code}
try { Thread.sleep(DELAY); } catch (InterruptedException ignored) {}
{code}
I would like to use apache code style, but if you want to keep this, it's OK for me.
# This comment is unrelated to your patch, but if you can fix it, it would be great. in {{createControlFile}}, could you format the {{finally}} closure.","06/Feb/15 10:07;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12696986/DFSIO-truncate-01.patch
  against trunk revision 18b2507.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.conf.TestJobConf

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5171//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5171//console

This message is automatically generated.","06/Feb/15 20:25;shv;Fixed all 4.
BTW, TestJobConf is failing due to MAPREDUCE-6223.","06/Feb/15 23:07;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12697116/DFSIO-truncate-02.patch
  against trunk revision 4c48432.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.conf.TestJobConf

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5174//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5174//console

This message is automatically generated.","06/Feb/15 23:22;hitliuyi;+1, Thanks [~shv]","07/Feb/15 01:55;milandesai;I tested this on a standalone cluster and can confirm it works properly.","08/Feb/15 03:41;hitliuyi;Committed to trunk and branch-2.
Thanks [~shv] for contribution and [~milandesai] for verification.","08/Feb/15 05:55;hudson;FAILURE: Integrated in Hadoop-trunk-Commit #7050 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7050/])
MAPREDUCE-6227. DFSIO for truncate. (shv via yliu) (yliu: rev ef01768333ec0e59e7d747864183835e756a7bf6)
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java
","08/Feb/15 15:21;hudson;FAILURE: Integrated in Hadoop-Yarn-trunk #832 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/832/])
MAPREDUCE-6227. DFSIO for truncate. (shv via yliu) (yliu: rev ef01768333ec0e59e7d747864183835e756a7bf6)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java
* hadoop-mapreduce-project/CHANGES.txt
","08/Feb/15 16:24;hudson;FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #98 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/98/])
MAPREDUCE-6227. DFSIO for truncate. (shv via yliu) (yliu: rev ef01768333ec0e59e7d747864183835e756a7bf6)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java
* hadoop-mapreduce-project/CHANGES.txt
","08/Feb/15 20:30;hudson;SUCCESS: Integrated in Hadoop-Hdfs-trunk #2030 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2030/])
MAPREDUCE-6227. DFSIO for truncate. (shv via yliu) (yliu: rev ef01768333ec0e59e7d747864183835e756a7bf6)
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java
","08/Feb/15 21:50;hudson;FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #95 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/95/])
MAPREDUCE-6227. DFSIO for truncate. (shv via yliu) (yliu: rev ef01768333ec0e59e7d747864183835e756a7bf6)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java
* hadoop-mapreduce-project/CHANGES.txt
","09/Feb/15 00:14;hudson;FAILURE: Integrated in Hadoop-Mapreduce-trunk #2049 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2049/])
MAPREDUCE-6227. DFSIO for truncate. (shv via yliu) (yliu: rev ef01768333ec0e59e7d747864183835e756a7bf6)
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java
","09/Feb/15 00:15;hudson;FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #99 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/99/])
MAPREDUCE-6227. DFSIO for truncate. (shv via yliu) (yliu: rev ef01768333ec0e59e7d747864183835e756a7bf6)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java
* hadoop-mapreduce-project/CHANGES.txt
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add truncate operation to SLive,MAPREDUCE-6228,12770189,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,zero45,shv,shv,26/Jan/15 21:06,10/Apr/15 20:19,12/Jan/21 09:52,19/Feb/15 08:20,,,,,,2.7.0,,,benchmarks,test,,,,,0,,,,,Add truncate into the mix of operations for SLive test.,,hudson,milandesai,shv,varun_saxena,zero45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/15 08:10;shv;MAPREDUCE-6228-trunk.patch;https://issues.apache.org/jira/secure/attachment/12699626/MAPREDUCE-6228-trunk.patch","13/Feb/15 01:07;zero45;MAPREDUCE-6228.patch;https://issues.apache.org/jira/secure/attachment/12698588/MAPREDUCE-6228.patch","12/Feb/15 03:19;zero45;MAPREDUCE-6228.patch;https://issues.apache.org/jira/secure/attachment/12698296/MAPREDUCE-6228.patch","12/Feb/15 00:48;zero45;MAPREDUCE-6228.patch;https://issues.apache.org/jira/secure/attachment/12698255/MAPREDUCE-6228.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2015-02-12 00:48:39.374,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Feb 19 15:21:50 UTC 2015,,,,,,,"0|i24tdj:",9223372036854775807,,,,,,,,,,,,,2.7.0,,,,,,,,"12/Feb/15 00:48;zero45;Attaching ""git diff --no-prefix"" generated patch file.","12/Feb/15 01:15;shv;* ""Error extracting & merging append size range"" should be ""... truncate ...""
* Running org.apache.hadoop.fs.slive.TestSlive
Tests run: 18, Failures: 3,
* {{TruncateOp}} has a bunch of unused imports.
* The default truncate size range should probably be something like {{\[0, 1M\]}}. If truncate range is the same as the append's then actual truncation may occur too rare.
* Should we make {{waitForRecovery}} configurable? It would make sense to run a bunch of truncates with recoveries in-progress and see the behaviour when they are mixed with appends","12/Feb/15 03:19;zero45;Attaching new patch with Konstantin's requests addressed.

# Done.
# It now passes all but TestMRFlow. TestMRFlow was failing on my machine even before my patch however, with the same reason after my patch as well.
# Done.
# I added a WAIT_ON_TRUNCATE boolean option via ""-truncateWait"".","12/Feb/15 03:50;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12698255/MAPREDUCE-6228.patch
  against trunk revision 67efab9.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 7 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.yarn.server.resourcemanager.recovery.TestFSRMStateStore
                  org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5189//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5189//console

This message is automatically generated.","12/Feb/15 07:14;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12698296/MAPREDUCE-6228.patch
  against trunk revision 8a54384.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 8 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.conf.TestJobConf

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5190//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5190//console

This message is automatically generated.","12/Feb/15 09:01;shv;Few minor things:
# Long line defining {{ConfigOption<Boolean> WAIT_ON_TRUNCATE}}
# Lets increment {{PROG_VERSION = ""0.0.2"";}} to {{0.1.0}}. I missed it last time.
#  {{0M}} and {{Constants.MEGABYTES * 0}} is just {{0}}
# The unit test is passing for me.
# Could you please confirm that it has been tried on a cluster with truncate on.","13/Feb/15 01:07;zero45;Attaching patch with Konstantin's points addressed.

# Done.
# Done.
# Done.
# Okay.
# I do not have access to a cluster at the moment but I will confirm once I do.","13/Feb/15 03:33;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12698588/MAPREDUCE-6228.patch
  against trunk revision 99f6bd4.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 8 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.conf.TestJobConf

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5192//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5192//console

This message is automatically generated.","13/Feb/15 04:32;zero45;Test failure appears to be unrelated to my changes.

I saw that it also failed for build #5190 on Jenkins: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5190/, a build previous to my latest, which was #5192.","13/Feb/15 05:16;shv;TestJobConf is failing due to MAPREDUCE-6223.
+1 - pending cluster test confirmation.","18/Feb/15 22:43;milandesai;Tested on a standalone cluster and can confirm the truncate operations on the slive test work properly. +1","19/Feb/15 08:10;shv;Trunk has diverged due to HADOOP-11602. Minor difference, but still attaching new patch. Plamen's latest patch is applying to branch-2 as is.

[~zero45] it would be good to comply with the patch naming convention in the future.

[~milandesai] thanks for testing on the cluster.","19/Feb/15 08:20;shv;I just committed this. Thank you Plamen.","19/Feb/15 08:22;hudson;FAILURE: Integrated in Hadoop-trunk-Commit #7153 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7153/])
MAPREDUCE-6228. Add truncate operation to SLive. Constributed by Plamen Jeliazkov. (shv: rev a19820f2fb2000a789a114f8ed55cb7e071723c8)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/TruncateOp.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigOption.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/Constants.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ArgumentParser.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/TestSlive.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/OperationFactory.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigMerger.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigExtractor.java
","19/Feb/15 11:30;hudson;FAILURE: Integrated in Hadoop-Yarn-trunk #843 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/843/])
MAPREDUCE-6228. Add truncate operation to SLive. Constributed by Plamen Jeliazkov. (shv: rev a19820f2fb2000a789a114f8ed55cb7e071723c8)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigOption.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/Constants.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ArgumentParser.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/TestSlive.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/OperationFactory.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigExtractor.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigMerger.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/TruncateOp.java
","19/Feb/15 11:42;hudson;FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #109 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/109/])
MAPREDUCE-6228. Add truncate operation to SLive. Constributed by Plamen Jeliazkov. (shv: rev a19820f2fb2000a789a114f8ed55cb7e071723c8)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/Constants.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigOption.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/TruncateOp.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigMerger.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigExtractor.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/TestSlive.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ArgumentParser.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/OperationFactory.java
","19/Feb/15 14:10;hudson;FAILURE: Integrated in Hadoop-Hdfs-trunk #2041 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2041/])
MAPREDUCE-6228. Add truncate operation to SLive. Constributed by Plamen Jeliazkov. (shv: rev a19820f2fb2000a789a114f8ed55cb7e071723c8)
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigOption.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/TruncateOp.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/Constants.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigExtractor.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/OperationFactory.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ArgumentParser.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/TestSlive.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigMerger.java
","19/Feb/15 14:18;hudson;FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #100 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/100/])
MAPREDUCE-6228. Add truncate operation to SLive. Constributed by Plamen Jeliazkov. (shv: rev a19820f2fb2000a789a114f8ed55cb7e071723c8)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/OperationFactory.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/TestSlive.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigMerger.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigOption.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigExtractor.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ArgumentParser.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/TruncateOp.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/Constants.java
","19/Feb/15 14:21;hudson;FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #110 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/110/])
MAPREDUCE-6228. Add truncate operation to SLive. Constributed by Plamen Jeliazkov. (shv: rev a19820f2fb2000a789a114f8ed55cb7e071723c8)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigExtractor.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigMerger.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/Constants.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigOption.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ArgumentParser.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/TruncateOp.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/OperationFactory.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/TestSlive.java
","19/Feb/15 15:21;hudson;FAILURE: Integrated in Hadoop-Mapreduce-trunk #2060 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2060/])
MAPREDUCE-6228. Add truncate operation to SLive. Constributed by Plamen Jeliazkov. (shv: rev a19820f2fb2000a789a114f8ed55cb7e071723c8)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigMerger.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigExtractor.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/TestSlive.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigOption.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ArgumentParser.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/TruncateOp.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/Constants.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/OperationFactory.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
To limit the map task number or reduce task number of an application,MAPREDUCE-6176,12758172,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,yanghaogn,yanghaogn,yanghaogn,27/Nov/14 08:41,09/Apr/15 09:16,12/Jan/21 09:52,09/Apr/15 09:16,2.4.0,2.4.1,2.5.0,2.5.1,2.5.2,,,,mr-am,mrv2,,,,,0,patch,,,,"As MapReduce is a batch framework of calculation, so people may want to run application A as well as application B 、C, and a limit resource be put on A. A good way to do so is that we can limit the number of application's map task or reduce task. If we set mapreduce.map.num.max as M, then the map task number will not exceed M. At the same time, if we set mapreduce.map.num.max as R, then the reduce task number will not exceed R",,cdouglas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-5583,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/14 05:33;yanghaogn;MAPREDUCE-6176-branch2.4.0.patch;https://issues.apache.org/jira/secure/attachment/12684817/MAPREDUCE-6176-branch2.4.0.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Dec 01 08:44:48 UTC 2014,,,,,,,"0|i22ucv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"01/Dec/14 08:44;yanghaogn;add test",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
want InputFormat for zip files,MAPREDUCE-210,12377264,New Feature,In Progress,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,indrajeetapache,cutting,cutting,31/Aug/07 19:01,11/Mar/15 12:14,12/Jan/21 09:52,,,,,,,,,,,,,,,,11,,,,,"HDFS is inefficient with large numbers of small files.  Thus one might pack many small files into large, compressed, archives.  But, for efficient map-reduce operation, it is desireable to be able to split inputs into smaller chunks, with one or more small original file per split.  The zip format, unlike tar, permits enumeration of files in the archive without scanning the entire archive.  Thus a zip InputFormat could efficiently permit splitting large archives into splits that contain one or more archived files.",,acmurthy,adi,alexvk,anjackson,ankur,aw,cdouglas,chutium,cutting,cwensel,ddas,devaraj,forest520,harisekhon,iholsman,indrajeetapache,jmspaggi,jwarren,kweiner,lawr.eskin,lianhuiwang,mfernest,nicaiseeric,ozawa,patrickangeles,philip,qwertymaniac,rem120,shalin,tomwhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/08 10:49;ankur;ZipInputFormat_fixed.patch;https://issues.apache.org/jira/secure/attachment/12373816/ZipInputFormat_fixed.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2007-12-27 08:23:21.768,,,false,,,,,,,,,,,,,,,,,,148599,,,,,Wed Mar 11 12:14:27 UTC 2015,,,,,,,"0|i0is9r:",107674,,,,,,,,,,,,,,,,,,,,,"27/Dec/07 08:23;ankur; Proposed Implementation Approach
--------------------------------------------------

1. Implement class ZipInputFormat to extend FileInputFormat.

2. Override the getSplits() method to read each file's
   InputStream and construct a ZipInputStream out of it.

3. Create FileSplits in a way that each file split has the following
   properties
	*  FileSplit.start = start index of a zip entry.
      *  FileSplit.length = end index of a zip entry.
      *  fileSplit.file = Zip file.
      *  Sum of compressed size of zip entries <= splitSize

   For e.g. start = 3, length = 6 signifies that zip entries 3 to 6 
   will be read from the zip file of this split.

4. Implement class ZipRecordReader to read each zip entry in its split
   Using LineRecordReader. 

5. Each zip entry will be treated as a text file.

6. Implement the necessary unit test case classes.

Questions: 
=========
1. Is there a need to implement a ZipCodec (like GzipCodec and DefaultCodec) ?
2. Should the ZipRecordReader be flexible enough to treat the individual zip entries in a 
     FileSplit as being a text file or a sequence file ?

Please feel free to comment on anything that I missed which might be required.
Also any suggestions/recommendation to make the implementation better will be greatly
appreciated.

-Ankur","08/Jan/08 22:47;cutting;> 2. Override the getSplits() method to read each file's InputStream

I think getSplits() should construct a split for each element of java.util.zip.ZipFile#entries().

> 3. Create FileSplits [ ... ]

We should probably extend FileSplit or InputSplit specifically for zip files.  The fields needed per split are the archive file's path and the path of the file within the archive.  I don't think there's much point in supporting splits smaller than a file within the zip archive, so start and end offsets are not required here.

> 4. Implement class ZipRecordReader to read each zip entry in its split
Using LineRecordReader.

We should be able to use LineRecordReader directly, passing its constructor the result of ZipFile#getInputStream().

","21/Jan/08 15:00;ankur;* This patch does not modify any existing source file and adds 3 new files
                1. ZipInputFormat.java
                2. ZipSplit.java
                3. TestZipInputFormat.java

* The ZipInputFormat simply creates one split for each zip entry in an input zip file.
*  Each split is of type ZipSplit and is read using a LineRecordReader.
*  TestZipInputFormat is the unit test code that tests the ZipInputFormat with different zip files
    having different number of entries.
* More information is available in the javadoc","21/Jan/08 15:02;ankur;Attaching the patch file","22/Jan/08 15:50;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12373681/ZipInputFormat.patch
against trunk revision r614192.

    @author +1.  The patch does not contain any @author tags.

    javadoc -1.  The javadoc tool appears to have generated  messages.

    javac +1.  The applied patch does not generate any new compiler warnings.

    findbugs -1.  The patch appears to introduce 2 new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests -1.  The patch failed contrib unit tests.

Test results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1673/testReport/
Findbugs warnings: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1673/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1673/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1673/console

This message is automatically generated.","23/Jan/08 10:49;ankur;Following issues reported by QA were fixed

1. Findbugs errors in ZipInputFormat.java were fixed. The streams are now closed properly in isSplitable() and getSplits() methods.
2.  Javadoc comments fixed and verified that no new javadoc warnings are generated after applying the patch.
3.  Fixed formatting in the code.
4.  core-tests and contrib-tests are now passing after the above changes.

Kindly verify.","24/Jan/08 19:44;cutting;Some comments:
- isSplittable throws an exception when an empty zip archive is passed.  Instead, an empty zip file should just provide no keys and values, but not throw exceptions.
- in getSplits, there's no need to explicitly test that each file exists.  Instead, we can rely on open() throwing an exception if a file does not exist.
- getRecordReader should not loop calling getNextEntry(), but instead just call getEntry(String).

Oh, wait.  On that last point, it looks like getEntry() is only available on ZipFile, and we cannot create a ZipFile except from a File.  Wih an InputStream we must use ZipInputStream, which does not support getEntry(), since InputStream doesn't support random access.  Sigh.  This considerably reduces the utility of this InputFormat.  GNU Classpath's implementation of java.io.zip.ZipFile use a RandomAccessFile, which we could implement, but, alas, we can't use GNU's code at Apache because it is under the GPL.

Zlib includes a zip file parser (minizip) that's under a BSD-like license and that permits random access to zip file entries from a user-supplied input stream.  So we could do it in C.  Sigh.
","24/Jan/08 19:59;cutting;So, while implementing a zip InputFormat based on native code would be a lot more work, it would also have some distinct advantages:
- it could handle archives greater than 2GB;
- it would know where within the archive each split resides, so that splits could be properly localized;
- once HDFS implements append, it could provide appendable archives.

None of these are possible with java.io.zip.

","25/Jan/08 15:56;ankur;Some questions.
1. How is a  java.io.InputStream passed and used in native code. The header file represents it as a jobject which I tried casting to FILE * and reading, it did not work as expected. 

2. Can a native method call return structures that can be converted to java objects ? If so how ?
   Basically I want to be able to return an array of C structure where each element holds the following information
                 - The path of the entry
                 - The number of the entry
                 - Offset of the entry in the zip file
So that this info can be converted to an array of ZipSplit.

I am new to JNI so things are less than obvious for me, a little help will be greatly appreciated on JNI.","25/Jan/08 19:52;acmurthy;bq. How is a java.io.InputStream passed and used in native code. The header file represents it as a jobject which I tried casting to FILE * and reading, it did not work as expected.

I'm not sure what exactly you are trying, but the way I implemented the native codecs was to read data from the InputStream in the Java layer, put the data into a direct-buffer and then pass it to the native zlib library.

The stream you are talking about is the handle to the zlib stream, which is zlib specific. That just represents the state of the zlib stream.

bq. Can a native method call return structures that can be converted to java objects ? If so how ?

I'm sure that can be done via some hoops, but would be quite involved (I think).

Some details here: http://java.sun.com/docs/books/jni/html/other.html#30942

JNI Documentation from Sun: http://java.sun.com/docs/books/jni/html/jniTOC.html

Hope that helps.","25/Jan/08 20:05;cutting;> I'm not sure what exactly you are trying [ ... ]

The need here is to read from an FSInputStream, returned from an arbitrary FileSystem implementation, from C.  In particular, we need to be able to make callbacks from C to Java for read() and seek().  (I think open() and close() can be handled entirely in Java, and tell() can be implemented entirely in C.)
","29/Jan/08 13:18;ankur;> The need here is to ...
Callback from C to Java is fine for read(). But seek() might be an issue since for true random access we need to be able to seek forward and backwards from  
1. start of the stream
2. current pos of the stream
3. end of the stream

After taking a deep dive into the minizip code and implementing some POC code I am not sure how a seek() callback from C to java might be implemented in way that can be leveraged from existing minizip parser code. Any suggestions ?

Just to give an idea, here is a some sample code for read() that I implemented. 

// including zlib & minizip libraries
#include ""unzip.h""

// including java library
#include <jni.h>
#include ""ZipInputFormat.h""

//defining read() and seek() IO APIs

uLong ZCALLBACK fread_file_func
( voidpf opaque, voidpf stream, void* buf, uLong size)

{

    jlong bytesRead;
    JNIEnv *env = (JNIEnv *) opaque;
    jobject javaStream = (jobject) stream;
    jclass dataInputStream = (*env)->GetObjectClass(env, stream);
    jmethodID MID_read = (*env)->GetMethodID(env, dataInputStream, ""read"", ""([BII)I"");
    if(MID_read == NULL)    {
	printf(""\nfread_file_func(): read() method not found"");
    }
    else    {
	jbyteArray byteArray = (*env)->NewByteArray(env, size);
        bytesRead = (*env)->CallIntMethod(env, javaStream, MID_read, byteArray, 0, size);
        (*env)->GetByteArrayRegion(env, byteArray, 0, bytesRead, buf);
	printf(""\nNumber of bytes read: %u\n"", bytesRead );	
    }

    return bytesRead;

}

// the native function exposed to Java, declared as a static method
// dataStream is of type java.io.DataInputStream.
// zipClass is of type ZipInputformat

JNIEXPORT void JNICALL Java_ZipInputFormat_display
  (JNIEnv *env, jclass zipClass, jobject dataStream)
{
  unsigned char * buf = (unsigned char *) malloc( sizeof (unsigned char) * 1024 * 64);
  fread_file_func(env, dataStream, buf, 64*1024);
}","29/Jan/08 18:05;cutting;Right, you can't seek a DataInputStream.  Instead use FSDataInputStream, which is seekable.","30/Jan/08 10:19;ankur;Ok, But I should be able to change the offset to the end of the stream since central directory structure of zip file is at the end.
Presently the FSDataInputStream.seek() throws IOExeption and doe not change the stream  position if I try to position it past the
end of stream which is unlike fseek() which positions the offset to end of stream.

Is there a workaround to this or is it a functionality that needs to be added ?","30/Jan/08 19:25;cutting;Since the file is being accessed read-only, we can call FileSystem#getStatus(Path).getLen() and pass the file length from Java to C with the FSInputStream when we open the archive.  Would that work?

Arguably we should add a method to Seekable that returns the length, or perhaps adopt the convention that attempts to seek past EOF leave the pointer at EOF, but I don't think that's required for this issue.","01/Feb/08 13:36;ankur;Here is what I did
1. Implemented  JNI callbacks in C that callback Java for open, read, close, seek and tell on a FSDataInputStream.
2. Implemented some JAVA test code to verify that callbacks work correctly.
3. Made changes to existing Makefile of minizip to compile and build my C code as shared object.
4. Placed the "".so"" file in $LD_LIBRARY_PATH directory.

The integration was successful and worked beautifully. The callbacks worked perfectly to ensure that zip file opened
as FSDataInputStream was opened ad read correclty :-)

However, Sigh. I found that the minizip parsing code did'nt work correctly for Zip file > 2 GB.  :-(

The code uses uLong (unsigned long 4 bytes) instead of jlong (signed long long 8 bytes).
Replacing uLong with jlong would'nt work as code performs a lot of bit shifting operations. (I tried this.)

Also the parsing code relies on directory structure entries being in 32 - bit format and will require RE-WORK
based upon knowledge of 64-bit entries keeping in mind backward compatibility with 32 bit entries.

Note:- The IO callback APIs implemented by me make use of jlong in read(), seek() and tell().

QUESTION:  Is the RE-WORK really required or is there a workaround that I am missing ???
    ","01/Feb/08 15:32;ankur;Small correction: 
=============
                The parsing code works for zip archives UPTO 4 GB (Not 2 GB) 
It fails to process zip files of SIZE > 4 GB correctly. 

After a little more research I figured that the Zip64 format support (for files > 4 GB) 
is not implemented presently in the minizip code.

So looks like if we need support for files > 4GB, then minizip parsing and reading code
would definitely require re-work. In other words the minizip code would need to be ""extended""
to support Zip64 format.

This in turn further increases the scope of work.

Any suggestion or recommendations ?
               ","01/Feb/08 21:09;cutting;It looks like minizip is out, then.  The unzip code is based on a file descriptor, but there are only 35 lines that touch that file descriptor, so it might not be too hard to modify it to read from something else.  But then we have to maintain a branched version of that.  Sigh.","03/Feb/08 12:07;ankur;> ...so it might not be too hard to modify it to read from something else.
Actually! I already spent sufficient time setting things up, adding new code (I/O apis that can be plugged to the unzip code) to make it work in a manner that a Zip file name is passed to a native C call from Java which then uses unzip APIs to do open/read/seek operations on it. 

What's different is that my custom implemented I/O APIs are used to construct the I/O function pointer structure and this structure is passed to unzip APIs. The custom I/O APIs are responsible for making Java callbacks whenever unzip APIs request an I/O operation via them.

My concern is not that part, but the APIs of unzip.c that are ZIP format agnostic and does all the low level bit shifting operations, directory parsing, reading and uncompressing stuff since it is that part which fails for file > 4GB. Now modifying that part would mean 2 things.

1. We would be extending unzip code in minizip to support ZIP64 format for our needs and we would be required to maintain it.
2. Any modification would require decent knowledge of the format and would need to ensure backward compatibility with older ZIP format.

So the question here is, do we go ahead and extend the minizip code for ZIP64 format? (This would be quite involved I think)
Or do we stick with the present limitation of 4GB and schedule it for later ?","04/Feb/08 18:07;cutting;Sorry, I wasn't clear.  I was thinking we might try using, instead of minizip, the source code for the unzip command line executable, http://www.info-zip.org/UnZip.html, which uses file-io directly, but not in too many places.","05/Feb/08 07:32;ankur;Thanks for clarifying :-). But even Unzip in its present release 5.52 does not serve our purpose of supporting large files ( >  4GB) since it does not take care of extra headers in Zip64 format that are used specifically for supporting large archives. 

This is well documented and clearly stated in the FAQ,  http://www.info-zip.org/FAQ.html#limits.
Given below is an excerpt from the page :-

""Also note that in August 2001, PKWARE released PKZIP 4.50 with support for large files and archives via a pair of new header types, ""PK\x06\x06"" and ""PK\x06\x07"". So far these headers are undocumented, but most of their fields are fairly obvious. We don't yet know when Zip and UnZip will support this extension to the format. In the short term, it is possible to improve Zip and UnZip's capabilities slightly on certain Linux systems (and probably other Unix-like systems) by recompiling with the -DLARGEFILE_SOURCE -D_FILE_OFFSET_BITS=64  options. This will allow the utilities to handle uncompressed data files greater than 2 GB in size, as long as the total size of the archive containing them is less than 2 GB.""
                                                                                             =======================================================

This leaves us with little options. Either we look for something else that implements zip64 extension and whose license is such that we can include it in our code or we ourselves implement these extensions in Minizip code which we will have to test extensively and maintain. Sigh.
","05/Feb/08 12:13;ankur;Or as another option we can have our implementation of ZipInputStream purely in Java (no native code) that is based upon Sun's Java.io.zip.ZipInputStream with some additions and modifications to :-

1. Work with a Seekable stream (like FSDataInputStream).
2. Read only central directory structure to obtain file information instead of sequentially 
    reading the whole archive (Sun's implementation).
3. Make sure Zip64 headers are processed correctly.

This way we will have the following advantages.

1. A pure Java Zip stream parser supporting Zip64 format (No native code).
2. Support for Random as well as Sequential access.
3. No dependency on any external components.
4. Ease of modification for adding append when HDFS provides this facility.
5. Possibility of donating our parser as a Zip64 compliant java zip parser to open source in future.

The above of course require a lot of work but looking at the advantages I feel its worth it.

Your opinion ?
","05/Feb/08 17:29;cutting;One of the major attractions of the zip format for Hadoop is that it provides interoperability with standard tools.  But if we generate >4GB archives that shell tools cannot access, interoperability is broken.  Folks might as well then use SequenceFile or some other Hadoop-specific format.  So, until standard shell tools support access to >4GB zip archives, I see little motivation for Hadoop to support this.
","06/Feb/08 05:45;ankur;So do we wait for standard tools to support files > 4 GB before making a Zip InputFormat available in HADOOP ?","06/Feb/08 12:09;ankur;Also it would be nice and I shall be thankful if you can recommend other bugs/issues that I can fix to make useful contributions :-)","13/Mar/08 08:54;ddas;Cancelling the patch since this work is not complete yet and maybe requires further discussions..","30/Oct/08 23:59;grant;I have use for this canceled patch as of present. Are there bits of the code that need to modified in order for it to run properly on hadoop 0.17, or should I be able to pop them into the mapred directory and go?","03/Nov/08 09:24;ankur;There are 2 problems with this patch.

1. It does not split the zip files efficiently. This is because there is no way in Java to construct a zip input stream that permits random seeks given a zip entry name.
2. Java's handling of large zip file is not robust.

The plan was to modify the code to make use of an external zip parsing library that is compatible with Apache license. It was decided to use zip/unzip (standard shell tools) code via JNI but support for large zip files if still missing from unzip (Zip 3.0 is out with large zip file support). 

So at the moment, just waiting for Unzip 6.0 to come out and modify the code accrodingly.","03/Nov/08 11:18;steve_l;The most tested/stable Apache-licensed Java unzip code is in Ant's codebase; you can either take/fork that or try and get the changes back in, which, with suitable tests, I am sure will be happily accepted. ","25/Sep/09 16:38;patrickangeles;Any updates on this issue? What's the current thinking on shell tools + JNI versus Ant's unzip code? Anything I can do to contribute? Regards...","17/Jul/14 17:47;aw;Ping!

","06/Jan/15 17:25;chutium;ping again, in 2015... -_-","11/Mar/15 12:14;harisekhon;There is 3rd party zip inputformat here:

http://cotdp.com/2012/07/hadoop-processing-zip-files-in-mapreduce/

I think it's important for the zip inputformat to be natively supported because traditional enterprises where Hadoop is now starting to penetrate use zip a lot, especially in large corporates which are Windows heavy and don't realize the problems they are causing by having so many things in zip files that Hadoop currently can't read.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow for metrics to be exchanged between maps and reduces,MAPREDUCE-3520,12534357,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,revans2,revans2,08/Dec/11 16:13,10/Mar/15 04:32,12/Jan/21 09:52,,2.0.0-alpha,,,,,,,,mrv2,,,,,,2,,,,,"Some people on the mailing lists have requested having read access in reducers to counters that were set by mappers.  I propose that we provide a new interface for doing this.  The counter data in addition to being set periodically to the Application Master would be put into the intermediate output of the mapper for each reducer and be specially marked as such.  When the reducers fetch that data they can combine it together and provide a read only interface to it.

This allows users to potentially optimize their code on the reducer side if they know some special metadata before hand.",,acmurthy,cdouglas,frankluo,johnvijoe,kam_iitkgp,lewismc,qwertymaniac,raviteja,revans2,sseth,tlipcon,tomwhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-08-17 16:00:08.869,,,false,,,,,,,,,,,,,,,,,,220079,,,,,Mon Aug 19 13:36:17 UTC 2013,,,,,,,"0|i0e6g7:",80807,,,,,,,,,,,,,,,,,,,,,"17/Aug/13 16:00;frankluo;any progress on this ticket, or there is a better way to do it now?","19/Aug/13 13:36;revans2;No better way to do it.  i just have not found the time to actually make it happen.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integrate the Twitter Storm to work with Next Gen MapReduce,MAPREDUCE-3046,12523655,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,venug,venug,venug,20/Sep/11 05:32,10/Mar/15 04:32,12/Jan/21 09:52,,2.0.0-alpha,,,,,,,,mrv2,,,,,,4,,,,,Twitter has open-sourced storm (https://github.com/nathanmarz/storm). it will be good to integrate the Next Gen Mapreduce to storm.,,aah,atm,cdouglas,cutting,devaraj,eli,hitesh,johnvijoe,lianhuiwang,qwertymaniac,revans2,rozemarys,sharadag,subrotosanyal,tlipcon,udanax,vicaya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,60137,,,,,2011-09-20 05:32:28.0,,,,,,,"0|i0e6xz:",80887,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Rumen] Need a standalone JobHistory log anonymizer,MAPREDUCE-778,12431001,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,amar_kamat,hong.tang,hong.tang,21/Jul/09 07:42,10/Mar/15 04:31,12/Jan/21 09:52,16/Dec/11 14:26,0.23.0,2.0.0-alpha,,,,0.23.1,,,tools/rumen,,,,,,0,anonymization,rumen,,,"Job history logs contain a rich set of information that can help understand and characterize cluster workload and individual job execution. Examples of work that parses or utilizes job history include HADOOP-3585, MAPREDUCE-534, HDFS-459, MAPREDUCE-728, and MAPREDUCE-776. Some of the parsing tools developed in previous work already contains a component to anonymize the logs. It would be nice to combine these effort and have a common standalone tool that can anonymizes job history logs and preserve much of the structure of the files so that existing tools on top of job history logs continue work with no modification.",,aaa,acmurthy,cdouglas,guanying,luoli,ravidotg,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-3581,MAPREDUCE-3580,,,,,,,,,,,,,,,,,,,,"15/Apr/10 18:35;guanying;ASF.LICENSE.NOT.GRANTED--anonymizer.patch;https://issues.apache.org/jira/secure/attachment/12441863/ASF.LICENSE.NOT.GRANTED--anonymizer.patch","20/Jan/12 18:54;tucu00;MAPREDUCE-778_branch0.23.patch;https://issues.apache.org/jira/secure/attachment/12511305/MAPREDUCE-778_branch0.23.patch","01/Apr/10 03:20;guanying;anonymizer.py;https://issues.apache.org/jira/secure/attachment/12440436/anonymizer.py","12/Dec/11 18:36;amar_kamat;mapreduce-778-v1.14-12.patch;https://issues.apache.org/jira/secure/attachment/12507028/mapreduce-778-v1.14-12.patch","15/Dec/11 14:23;amar_kamat;mapreduce-778-v1.14-14.patch;https://issues.apache.org/jira/secure/attachment/12507529/mapreduce-778-v1.14-14.patch","12/Jul/11 11:47;amar_kamat;mapreduce-778-v1.2-2.patch;https://issues.apache.org/jira/secure/attachment/12486169/mapreduce-778-v1.2-2.patch","01/Apr/10 03:20;guanying;same.py;https://issues.apache.org/jira/secure/attachment/12440437/same.py",,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,2010-04-01 03:20:41.653,,,false,,,,,,,,,,,,,,,,,,68324,Reviewed,,,,Sat Jan 21 13:16:27 UTC 2012,,,,,,,"0|i09emn:",52803,Added an anonymizer tool to Rumen. Anonymizer takes a Rumen trace file and/or topology as input. It supports persistence and plugins to override the default behavior.,,,,,,,,,,rumen anonymization,,,,,,,,,,"01/Apr/10 03:20;guanying;An anonymizer implemented in Python attached. This anonymizer can work with v20, v22, or rumen log files. On doing anonymization, a private file with tables is created, and can be used to de-anonymize the anonymized trace. The tables file can be used in two ways, either grown incrementally or stand alone, when working with multiple traces.

Another file attached same.py is a simple Python script to compare two json-based trace files. It works similar to diff. Because json objects can be semantically equivalent even if keys in dictionaries are in different orders, so running diff directly on two files may not work as desired. It outputs nothing if the two files represent the same trace, otherwise print the objects (which can be big anyway) that are different in the two files. v22 and rumen log files can be compared using this script. Keys in v20 script have fixed orders so v20 log files can be compared using diff directly.

Known issues:

1. In v22 and rumen-trace log files, multiple json objects are in one file, and separate by white spaces. Without the power of Java Jackson package, the Python json module can only load a json object from a string or a file.  Currently, the scripts rely on detecting ""}\n"" as a whole line to determine ending of a json object. That may fail if the particular pattern occurs in a string object. A better implementation is similar to what Java Jackson does. An object should be found from a file, leaving the rest of the file still operational for further operations.

2. Sample rumen-trace and rumen-topology files are got from hadoop-mapreduce/src/test/tools/data/rumen/. These sample files seem to be generated from v20 log files, since ""."" are escaped as ""\."" in many fields. I'm not sure if rumen works with v22 log files, and if there are differences between rumen files generated from v22 log files and rumen files generated from v20 log files.
","02/Apr/10 09:05;hong.tang;Guanying, thanks for taking the effort.

Although it seems versatile to have the tool to parse all types of formats, I am concerned the effort of maintaining such versatility may outweight its potential usefulness. I think it is more preferable to implement the tool on top of Rumen API (and probably as part of Rumen). There are a number of reasons why this makes sense:

- As we discovered in Rumen development, parsing job history is not trivial and the format could continue evolving in the near future (the data model is not cleanly defined as-is IMO, see MAPREDUCE-1175). So it is advantageous to let Rumen be the only module to interface with different variations of job history format and present the common abstraction of job history.
- Job history contains more than the basic information about job execution, it also contains things like status string, and counters, etc and we have lesser control of what fields may be added into job history logs over time. So it would be a challenging task to keep the anonymizer up to date with high confidence that it would not leak any private  info. On the other side, since Rumen only extracts a subset of info from the job history logs, we can easily enumerate every fields of Rumen json objects to be sure any sensitive fields are anonymized.
- Currently some info we want wrt job execution are only available in jobconf xml file (such as queue name), rumen does the job of combining them together, and building the anonymizer on top of rumen saves the effort of having to have another configuration parser.

Other comments:
- I like the idea of storing a private ""lookup table"" and keep the capability of ""de-anonymize"" the trace if we choose to.
- The coverage of anonymization fields and the way how they are anonymized looks good to me. (Need to add the ""queue"" entity and I do not think we need ""/path"" type.)
","02/Apr/10 09:09;hong.tang;The implementation on top of Rumen seems pretty straightforward. On the highest level, we obviously need an interface called Anonymizable to all the LoggedXXX classes.

{code}
interface Anonymizable {
 void anonymize(TranslationTable table);
 void deanonymize(TranslationTable table);
}
{code}

and the rough definition of TranslationTable:

{code}
class TranslationTable {
 enum Type { HOST, RACK, JOB, USER, GROUP, PATH, QUEUE };
 EnumMap<Type, String> prefixes;

 static class Tablet {
   int seq;
   Map<String, String> fwdTbl;
   Map<String, String> revTbl;
 };

 EnumMap<Type, Tablet> tablets;

 String fwdTranslate(Type type, String val);
 String revTranslate(Type type, String val);
}
{code}","15/Apr/10 18:35;guanying;With the attached patch, an anonymizer is implemented integrated with Rumen. The way it works is similar as how the Python script works. Please look at src/test/mapred/org/apache/hadoop/tools/rumen/TestAnonymizer.java for examples of how to run the anonymizer.","12/Jul/11 11:47;amar_kamat;Attaching a patch that does a very basic level of anonymization of Rumen job traces and cluster topologies. This patch adds a new option in Rumen called 'Anonymizer' which anonymizes the specified Rumen job trace and/or cluster topology. The approach exploits Jackson's ability to accept custom object serializers.  

TODO:
1. Remove code duplication in Folder and HadoopLogsAnalyzer (i.e use DefaultOutputter)
2. Extract and anonymize important job configuration parameters from job properties. The current patch (v1.2-2) simply filters out (hides) all the job configuration parameters. The biggest challenge here it handle classnames (mapper/reducer/combiner etc) and paths (job input/output etc). Classnames that are open sourced or publicly available (eg. org.apache.*) can be filtered in without anonymization.
3. Strings like job-names can be intelligently masked to retain certain key characteristics. For example, job-names having keywords like 'monthly/daily/weekly/hourly' etc can be retained as they represent job characteristics.","12/Dec/11 18:36;amar_kamat;Attaching a patch that adds the remaining features to the Anonymizer. 

Some newly added features:
1. Note that Gridmix runs with the anonymized trace.

2. Classname now have a filter which allows the user to specify which packages to pass through.

3. Job config parsing and filtering is done. Only MR (framework-level) configs are parsed and allowed. There is a config to extend this functionality and allow users to handle other keys. E.g. Pig etc.

Read the Rumen manual for details on the Anonymizer and its configuration parameters. 

Testing:
test-patch and ant tests passed. Also tested on 2 days worth of job history data. 

Todos:
1. Currently, the job properties only consideres MR (framework-level) properties. Add job config parsers for Input/Output file formats, pig, etc.
2. Chunking of data (esp. job names) to preserve some useful stats like daily/weekly/monthly etc.","13/Dec/11 04:43;amar_kamat;Running the latest patch (v1.14-12) through Hudson.","13/Dec/11 05:32;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12507028/mapreduce-778-v1.14-12.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 44 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in .

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1432//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1432//console

This message is automatically generated.","14/Dec/11 11:03;amar_kamat;I ran test-patch on my local box for MapReduce and it passed. 
{noformat}
+1 overall.  

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 44 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{noformat}

All the Rumen JUnit tests except 'TestRumenJobTraces' passed. 'TestRumenJobTraces' failed due to MAPREDUCE-3462. 11 out of 16 Gridmix JUnit tests passed. 5 Gridmix tests failed due to MAPREDUCE-3462 and MAPREDUCE-3168.","15/Dec/11 14:23;amar_kamat;Attaching a patch that removes the hack for 'mapred.child.java.opts'. Lets revisit the unsupported job-conf key later.

The diff with the latest patch can be seen here http://pastebin.com/Zf9fuJv0.","16/Dec/11 04:12;amar_kamat;test-patch passed on my local box.
{noformat}
+1 overall.  

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 44 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{noformat}

All the Rumen JUnit tests except 'TestRumenJobTraces' passed. 'TestRumenJobTraces' failed due to MAPREDUCE-3462. 12 out of 16 Gridmix JUnit tests passed. 4 Gridmix tests failed due to MAPREDUCE-3462 and MAPREDUCE-3168.","16/Dec/11 11:42;cdouglas;+1","16/Dec/11 14:26;amar_kamat;I just committed this to trunk. Thanks Chris!","16/Dec/11 14:59;hudson;Integrated in Hadoop-Hdfs-trunk-Commit #1516 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/1516/])
    MAPREDUCE-778. Rumen Anonymizer. (Amar Kamat and Chris Douglas via amarrk)

amarrk : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1215141
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/ivy.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/ivy/libraries.properties
* /hadoop/common/trunk/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/docs/src/documentation/content/xdocs/rumen.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenAnonymization.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenFolder.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Anonymizer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Folder.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JsonObjectMapperWriter.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedLocation.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedNetworkTopology.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTask.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTaskAttempt.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ParsedHost.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieCluster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/DataAnonymizer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordList.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordListAnonymizerUtility.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/AnonymizableDataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/ClassName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultAnonymizableDataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultDataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/FileName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobProperties.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/NodeName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/QueueName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/UserName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/DefaultJobPropertiesParser.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/JobPropertyParser.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/MapReduceJobPropertiesParser.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/BlockingSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultAnonymizingRumenSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultRumenSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/ObjectStringSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/State.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StateDeserializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StatePool.java
","16/Dec/11 15:01;hudson;Integrated in Hadoop-Common-trunk-Commit #1443 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/1443/])
    MAPREDUCE-778. Rumen Anonymizer. (Amar Kamat and Chris Douglas via amarrk)

amarrk : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1215141
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/ivy.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/ivy/libraries.properties
* /hadoop/common/trunk/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/docs/src/documentation/content/xdocs/rumen.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenAnonymization.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenFolder.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Anonymizer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Folder.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JsonObjectMapperWriter.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedLocation.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedNetworkTopology.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTask.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTaskAttempt.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ParsedHost.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieCluster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/DataAnonymizer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordList.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordListAnonymizerUtility.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/AnonymizableDataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/ClassName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultAnonymizableDataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultDataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/FileName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobProperties.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/NodeName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/QueueName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/UserName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/DefaultJobPropertiesParser.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/JobPropertyParser.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/MapReduceJobPropertiesParser.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/BlockingSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultAnonymizingRumenSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultRumenSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/ObjectStringSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/State.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StateDeserializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StatePool.java
","16/Dec/11 15:03;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #1468 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/1468/])
    MAPREDUCE-778. Rumen Anonymizer. (Amar Kamat and Chris Douglas via amarrk)

amarrk : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1215141
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/ivy.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/ivy/libraries.properties
* /hadoop/common/trunk/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/docs/src/documentation/content/xdocs/rumen.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenAnonymization.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenFolder.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Anonymizer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Folder.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JsonObjectMapperWriter.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedLocation.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedNetworkTopology.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTask.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTaskAttempt.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ParsedHost.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieCluster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/DataAnonymizer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordList.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordListAnonymizerUtility.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/AnonymizableDataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/ClassName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultAnonymizableDataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultDataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/FileName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobProperties.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/NodeName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/QueueName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/UserName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/DefaultJobPropertiesParser.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/JobPropertyParser.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/MapReduceJobPropertiesParser.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/BlockingSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultAnonymizingRumenSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultRumenSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/ObjectStringSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/State.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StateDeserializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StatePool.java
","17/Dec/11 12:41;hudson;Integrated in Hadoop-Hdfs-trunk #897 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/897/])
    MAPREDUCE-778. Rumen Anonymizer. (Amar Kamat and Chris Douglas via amarrk)

amarrk : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1215141
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/ivy.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/ivy/libraries.properties
* /hadoop/common/trunk/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/docs/src/documentation/content/xdocs/rumen.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenAnonymization.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenFolder.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Anonymizer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Folder.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JsonObjectMapperWriter.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedLocation.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedNetworkTopology.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTask.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTaskAttempt.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ParsedHost.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieCluster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/DataAnonymizer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordList.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordListAnonymizerUtility.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/AnonymizableDataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/ClassName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultAnonymizableDataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultDataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/FileName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobProperties.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/NodeName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/QueueName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/UserName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/DefaultJobPropertiesParser.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/JobPropertyParser.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/MapReduceJobPropertiesParser.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/BlockingSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultAnonymizingRumenSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultRumenSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/ObjectStringSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/State.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StateDeserializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StatePool.java
","17/Dec/11 13:42;hudson;Integrated in Hadoop-Mapreduce-trunk #930 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/930/])
    MAPREDUCE-778. Rumen Anonymizer. (Amar Kamat and Chris Douglas via amarrk)

amarrk : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1215141
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/ivy.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/ivy/libraries.properties
* /hadoop/common/trunk/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/docs/src/documentation/content/xdocs/rumen.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenAnonymization.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenFolder.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Anonymizer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Folder.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JsonObjectMapperWriter.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedLocation.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedNetworkTopology.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTask.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTaskAttempt.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ParsedHost.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieCluster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/DataAnonymizer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordList.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordListAnonymizerUtility.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/AnonymizableDataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/ClassName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultAnonymizableDataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultDataType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/FileName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobProperties.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/NodeName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/QueueName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/UserName.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/DefaultJobPropertiesParser.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/JobPropertyParser.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/MapReduceJobPropertiesParser.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/BlockingSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultAnonymizingRumenSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultRumenSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/ObjectStringSerializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/State.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StateDeserializer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StatePool.java
","20/Jan/12 18:54;tucu00;patch for branch-0.23 by Ahmed (originally posted in MAPREDUCE-3582)","20/Jan/12 18:57;tucu00;committed to branch-0.23","20/Jan/12 19:06;hudson;Integrated in Hadoop-Hdfs-0.23-Commit #384 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Commit/384/])
    Merge -r 1215140:1215141 from trunk to branch. FIXES: MAPREDUCE-778

tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1234070
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/ivy.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/ivy/libraries.properties
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/docs/src/documentation/content/xdocs/rumen.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenAnonymization.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenFolder.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Anonymizer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Folder.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JsonObjectMapperWriter.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedJob.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedLocation.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedNetworkTopology.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTask.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTaskAttempt.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ParsedHost.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieCluster.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieJob.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/DataAnonymizer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordList.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordListAnonymizerUtility.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/AnonymizableDataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/ClassName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultAnonymizableDataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultDataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/FileName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobProperties.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/NodeName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/QueueName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/UserName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/DefaultJobPropertiesParser.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/JobPropertyParser.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/MapReduceJobPropertiesParser.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/BlockingSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultAnonymizingRumenSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultRumenSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/ObjectStringSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/State.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StateDeserializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StatePool.java
","20/Jan/12 19:11;hudson;Integrated in Hadoop-Common-0.23-Commit #393 (See [https://builds.apache.org/job/Hadoop-Common-0.23-Commit/393/])
    Merge -r 1215140:1215141 from trunk to branch. FIXES: MAPREDUCE-778

tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1234070
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/ivy.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/ivy/libraries.properties
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/docs/src/documentation/content/xdocs/rumen.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenAnonymization.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenFolder.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Anonymizer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Folder.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JsonObjectMapperWriter.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedJob.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedLocation.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedNetworkTopology.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTask.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTaskAttempt.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ParsedHost.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieCluster.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieJob.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/DataAnonymizer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordList.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordListAnonymizerUtility.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/AnonymizableDataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/ClassName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultAnonymizableDataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultDataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/FileName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobProperties.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/NodeName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/QueueName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/UserName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/DefaultJobPropertiesParser.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/JobPropertyParser.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/MapReduceJobPropertiesParser.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/BlockingSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultAnonymizingRumenSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultRumenSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/ObjectStringSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/State.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StateDeserializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StatePool.java
","20/Jan/12 19:14;hudson;Integrated in Hadoop-Mapreduce-0.23-Commit #407 (See [https://builds.apache.org/job/Hadoop-Mapreduce-0.23-Commit/407/])
    Merge -r 1215140:1215141 from trunk to branch. FIXES: MAPREDUCE-778

tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1234070
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/ivy.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/ivy/libraries.properties
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/docs/src/documentation/content/xdocs/rumen.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenAnonymization.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenFolder.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Anonymizer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Folder.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JsonObjectMapperWriter.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedJob.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedLocation.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedNetworkTopology.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTask.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTaskAttempt.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ParsedHost.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieCluster.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieJob.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/DataAnonymizer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordList.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordListAnonymizerUtility.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/AnonymizableDataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/ClassName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultAnonymizableDataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultDataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/FileName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobProperties.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/NodeName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/QueueName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/UserName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/DefaultJobPropertiesParser.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/JobPropertyParser.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/MapReduceJobPropertiesParser.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/BlockingSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultAnonymizingRumenSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultRumenSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/ObjectStringSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/State.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StateDeserializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StatePool.java
","21/Jan/12 12:47;hudson;Integrated in Hadoop-Hdfs-0.23-Build #145 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/145/])
    Merge -r 1215140:1215141 from trunk to branch. FIXES: MAPREDUCE-778

tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1234070
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/ivy.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/ivy/libraries.properties
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/docs/src/documentation/content/xdocs/rumen.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenAnonymization.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenFolder.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Anonymizer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Folder.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JsonObjectMapperWriter.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedJob.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedLocation.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedNetworkTopology.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTask.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTaskAttempt.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ParsedHost.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieCluster.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieJob.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/DataAnonymizer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordList.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordListAnonymizerUtility.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/AnonymizableDataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/ClassName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultAnonymizableDataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultDataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/FileName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobProperties.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/NodeName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/QueueName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/UserName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/DefaultJobPropertiesParser.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/JobPropertyParser.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/MapReduceJobPropertiesParser.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/BlockingSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultAnonymizingRumenSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultRumenSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/ObjectStringSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/State.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StateDeserializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StatePool.java
","21/Jan/12 13:16;hudson;Integrated in Hadoop-Mapreduce-0.23-Build #167 (See [https://builds.apache.org/job/Hadoop-Mapreduce-0.23-Build/167/])
    Merge -r 1215140:1215141 from trunk to branch. FIXES: MAPREDUCE-778

tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1234070
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/ivy.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/ivy/libraries.properties
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/docs/src/documentation/content/xdocs/rumen.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenAnonymization.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenFolder.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Anonymizer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Folder.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JsonObjectMapperWriter.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedJob.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedLocation.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedNetworkTopology.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTask.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/LoggedTaskAttempt.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ParsedHost.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieCluster.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/ZombieJob.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/DataAnonymizer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordList.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/anonymization/WordListAnonymizerUtility.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/AnonymizableDataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/ClassName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultAnonymizableDataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/DefaultDataType.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/FileName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/JobProperties.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/NodeName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/QueueName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/UserName.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/DefaultJobPropertiesParser.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/JobPropertyParser.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/datatypes/util/MapReduceJobPropertiesParser.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/BlockingSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultAnonymizingRumenSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/DefaultRumenSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/serializers/ObjectStringSerializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/State.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StateDeserializer.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/state/StatePool.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hash-based MapReduce,MAPREDUCE-5494,12666821,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,jerrychenhf,jerrychenhf,jerrychenhf,04/Sep/13 03:15,10/Mar/15 04:30,12/Jan/21 09:52,,,,,,,,,,mrv2,,,,,,0,,,,,"To support parallel processing, the MapReduce computation model essentially implements group data by key, then apply the reduce function to each group. The currently implementation of MapReduce framework uses sort-merge to guarantee the computation model. While “sort-merge” is relatively expensive when only grouping is needed. And this is what hash capable to do.
 
We propose to implement Hash-based MapReduce which utilizes hash to guarantee the computation model instead of the sort merge. This technique will works as following:
 
1. At map side, the hash map output collector will collect the map output directly to the corresponding partition buffer (for each reduces) without sorting (first level partition). Each partition buffer can be flushed to disk if the buffer is full or close to full. To handling disk IO efficiently when there are too many partitions (reduces), the map side can be optimized by using a shared buffer for different partitions. Counting sort on partition number can be performed when flushing the shared buffer. 

2. At reduce side, the hash shuffle will fetch its own partitions from maps as usually. While fetching, the records will be further partitioned (secondary level partition) by a universal hash function. By properly choosing the number of the partitions, every single partition should be able to fit into the memory. For cases such as much skewed distribution of the keys, the size of a partition may be too large to fit into the memory. When this happens, a parameter can be used to control whether we simply choose to fail the job or to try further partition the large partition into smaller ones using another hash function. 

3. Once all the data are fetched and partitioned at reduce side, it starts iterating. A RawKeyValueIterator will be wrapped to process and iterating the partitions one by one. The processing for each partition is to load the partition into memory and a hash table can be built. And an iterator will be wrapped on the hash table to feed reduce the groups of keys and values in the hash table. 

Although there are some JIRAs related in using hash in MapReduce, the process proposed here has some fundamental differences with them. MAPREDUCE-1639 (Grouping using hashing instead of sorting) is described to be replacement of map side sort only. MAPREDUCE-3247 (Add hash aggregation style data flow and/or new API) and MAPREDUCE-4039 (Sort Avoidance) are mostly focused on no sort map reduce and not trying to guarantee the computation model at the framework level. From the above process, this work is a complete hash based approach. Sort at map side and merge at reduce side are completely replaced by hash and guarantee the computation model of MapReduce. 
 
While one potential affect to use hash without sorting is that MapReduce users should not depends on the order of different keys. The order of the keys are implied by the sort-merge process but will no longer implied when using hash for grouping keys. 
 
This work is implemented based on the pluggable MapOutputCollector (Map side) and ShuffleConsumerPlugin (Reduce side) provided by MAPREDUCE-2454. There are no modifications to the existing MapReduce code and so keep the affect to the original implementation to minimum. The hash-based MapReduce is not used by default. To enable Hash-based MapReduce, set “mapreduce.job.map.output.collector.class” to HashMapOutputCollector class and “mapreduce.job.reduce.shuffle.consumer.plugin.class” to HashShuffle class.
",,anoop.hbase,apurtell,cdouglas,cutting,daisuke.kobayashi,devaraj,drankye,jerrychenhf,jerrylead,jlowe,karthik govindaraj,kkambatl,mmccline,nroberts,qwertymaniac,sandyr,shv,sseth,vicaya,yvesbastos,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4838400,4838400,,0%,4838400,4838400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-09-09 05:07:55.604,,,false,,,,,,,,,,,,,,,,,,346759,,,,,Mon Sep 09 05:07:55 UTC 2013,,,,,,,"0|i1nsmf:",347059,,,,,,,,,,,,,,,,,,,,,"09/Sep/13 05:07;jerrylead;Cool, Hadoop's sort-based and disk-based MapReduce framework should be dramatically improved to better job's performance.

As for this hash-based MapReduce, Apache Spark uses a similar hash-based GroupBy-Aggregate approach. Compared with Spark's implementation, this proposal may have some problems. 

1) At map side, the only problem is that Combine() cannot be done easily. For jobs with Combine(), Spark uses a hashmap to collect the <K,V> pairs emitted by map() directly without a buffer. Combine() can be done when the <K,V> pairs are put into the hashmap one by one. But it requires that the semantic meaning of this Combine() is similar with fold() in functional language. After map(), hashmap outputs all the <K,V> into different buffers (one for each reducer) according to each pair's partition id. Then, the <K,V> are reserved in the buffers or spilled onto disk for further shuffling. The defect of Spark's approach is that hashmap is memory-consuming. So it is a trade-off between performance and fault-tolerance. For jobs without Combine(), Spark uses your proposed method. 

2) At reduce side, firstly, I want to argue the sort-based MapReduce in current Hadoop versions. Why should we shuffle all the map outputs into a reducer before doing reduce()? Since all the <K,V> pairs in remote map outputs are sorted, reduce() can perform as same as Merge() on the fly. Maybe the reason is that it is not easy to iterating <K,V> pairs when map outputs are compressed.
   Secondly, return to the proposed approach. ""the hash shuffle will fetch its own partitions from maps as usually"" is not very clear. Maybe you want to do shuffle in this way: Firstly, copying remote map outputs and store the shuffled segments into memory as usual. When a threshold is achieved, you iterate every record in every in-memory segment. During iterating, you choose a pretty good hash function to partition each record and store it onto related disk files. After all the records are stored onto disk, reduce() begins to load the partition file into hashmap. After all the records are cached into hashmap, Reduce() begins to perform on each <K, UnsortedList<V>>. The secondary level partition guarantees the hashmap can be kept in memory.
   I think this approach is applicable. However, how to choose the secondary partition function is tricky and Combine() is discarded too. Another performance issue is that this memory-to-disk and disk-to-memory back-and-forth copy is not efficient. Spark uses a large hashmap to do ""shuffle and combine()"" simultaneously as descried in mapper's combine(), but it requires memory is enough to hold the hashmap. We can enlarge the reduce number to lower down this requirement.

Above all, if we only change sort-based to hash-based, the job's performance may not be promoted. A better way is to combine the hash-based and memory-based approaches together as Spark does. We can use memory aggressively (e.g., use large hashmap instead of back-and-forth copy). If there is a memory error, we can change the job's running style back into sort-based. 



",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Distributed simulator for stressing JobTracker and NameNode,MAPREDUCE-2425,12503695,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,,coderplay,coderplay,08/Apr/11 04:00,10/Mar/15 01:58,12/Jan/21 09:52,10/Mar/15 01:58,,,,,,,,,contrib/mumak,,,,,,0,benchmark,hadoop,,,"Hadoop need a tool for stressing JobTracker and NameNode. Mumak introduced a simulated JobTracker, whose behavior doesn't exactly like that of the real JobTracker. Even more, mumak can't simulate a large cluster with quite a lot of jobs run on it. On the other hand, Gridmix v3 need hundreds of physical nodes to replay job stories. 

You can think this tool a complementation of mumak and gridmix v3. We successfully used this tool to simulate a 12000 nodes cluster through 4 real machines. 
I've talk to Hong Tang and Scott Chen offline, they suggested me contributing this tool to the hadoop community.",,aah,atm,aw,clarkyzl,cutting,devaraj,hong.tang,lianhuiwang,ravidotg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/11 04:05;coderplay;ASF.LICENSE.NOT.GRANTED--.jpg;https://issues.apache.org/jira/secure/attachment/12475780/ASF.LICENSE.NOT.GRANTED--.jpg","08/Apr/11 04:07;coderplay;ASF.LICENSE.NOT.GRANTED--screenshot-1.jpg;https://issues.apache.org/jira/secure/attachment/12475781/ASF.LICENSE.NOT.GRANTED--screenshot-1.jpg",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-04-08 04:12:27.682,,,false,,,,,,,,,,,,,,,,,,59900,,,,,Tue Mar 10 01:58:19 UTC 2015,,,,,,,"0|i0e7j3:",80982,,,,,,,,,,,,,,,,,,,,,"08/Apr/11 04:05;coderplay;A screenshot of this tool. we are using hadoop 0.19.1","08/Apr/11 04:07;coderplay;screenshot","08/Apr/11 04:12;amar_kamat;Min,
Can you kindly post some more details as to what this new simulation intends to do (as opposed to Mumak) and the basic design? Also is it possible to modify Mumak to do what your tool is doing. If the only difference is about scalability then I would prefer fixing Mumak and making it better. Thoughts?","08/Apr/11 04:42;coderplay;Amar,

This simulator is developed  majorly for stressing JT and NN. It can also verify JT's runtime behavior as mumak does. Actually, we use v0.19.1 where rumen and mumak havenot been introduced at that time, so I developed this tool independent from them.  Now that I am planning to merge my code into mumak. But before that, I must do 2 things list below 

1. Mumak uses a simulated JT for telling TT some informations of a task-attempt reproduced by rumen through heartbeat. I perfer using the real JT when stressing it. 
2. I should uses new MR API before merging into mumak.","08/Apr/11 05:42;amar_kamat;Min,
Thanks for the quick reply. I guess testing (stress/functional) JT is also the primary goal of Mumak. Hence I see this as an enhancement (maybe rewrite) of Mumak. So the goal is to support existing functionality of Mumak and add new features to support the use cases that you are interested in. 

bq. Mumak uses a simulated JT.. 
I guess Mumak only wanted to work with certain features of JT and hence the design. I believe we can enhance Mumak to instantiate the ""real"" JT if needed.

bq. I should uses new MR API before merging into mumak.
Yes.

For now, can you quickly point out the major highlights of the simulator that you are planning to contribute? It would be nice to also compare/contrast it with Mumak.
","08/Apr/11 06:32;coderplay;Amar,

see MAPREDUCE-1261. 
1. Mumak couldn't stress JT.  It's JobClient, SimulatorJobTracker & SimulatorTaskTrackers are all run in the same JVM.
2. Mumak doesn't use the real JT. ","08/Apr/11 06:38;coderplay;btw, this tool can stress RPC as well. ","21/Sep/11 16:57;shv;New feature. Unassigning from 0.22","10/Mar/15 01:58;aw;closing as won't fix",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Forward port MapReduce server MXBeans,MAPREDUCE-2330,12498692,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,,vicaya,vicaya,15/Feb/11 21:37,10/Mar/15 01:27,12/Jan/21 09:52,10/Mar/15 01:27,,,,,,,,,,,,,,,0,,,,,"Some JMX classes e.g., JobTrackerMXBean and TaskTrackerMXBean in 0.20.100~ needs to be forward ported to 0.23 in some fashion, depending on how MapReduce 2.0 emerges.

Note, similar item for HDFS, HDFS-1318 is already in 0.22.",,atm,aw,hyunsik.choi,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-03-10 01:27:55.446,,,false,,,,,,,,,,,,,,,,,,150139,,,,,Tue Mar 10 01:27:55 UTC 2015,,,,,,,"0|i0e7nz:",81004,,,,,,,,,,,,,,,,,,,,,"10/Mar/15 01:27;aw;i'm just going to close this as won't fix at this point.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Gridmix] Gridmix should provide a tool to compare and analyze Gridmix runs,MAPREDUCE-2617,12511372,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,amar_kamat,amar_kamat,23/Jun/11 15:05,09/Mar/15 23:15,12/Jan/21 09:52,,0.23.0,,,,,,,,contrib/gridmix,,,,,,0,analysis,gridmix,reporting,,"After running Gridmix, users manually process simulated job history logs (probably using Rumen) and then use some external tools to analyze it, compare it with either the original job history files or with simulated jobhistory files of other Gridmix runs and generate reports. It would be nice if Gridmix comes bundled with a set of tools to do this transparently.",,aah,sekikn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,70165,,,,,2011-06-23 15:05:22.0,,,,,,,"0|i0e7an:",80944,,,,,,,,,,,gridmix reporting analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add ""teraread"" example",MAPREDUCE-2853,12519224,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,tlipcon,tlipcon,tlipcon,18/Aug/11 01:31,09/Mar/15 23:12,12/Jan/21 09:52,,0.23.0,,,,,,,,benchmarks,examples,,,,,0,,,,,"Teragen is a good benchmark of raw DFS write throughput. Terasort is a good benchmark of the whole MR system (input, shuffle, output). I've added a simple ""teraread"" example which reads through the terasort input data without performing any processing: this acts as a good benchmark of a read-only workload (similar to real-life ""find a needle in a haystack"" MR jobs)",,atm,cutting,eli,kihwal,pfxuan,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/11 01:33;tlipcon;mapreduce-2853.txt;https://issues.apache.org/jira/secure/attachment/12490730/mapreduce-2853.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2011-08-18 01:50:40.752,,,false,,,,,,,,,,,,,,,,,,64751,,,,,Thu Aug 18 01:50:40 UTC 2011,,,,,,,"0|i0e71z:",80905,,,,,,,,,,,,,,,,,,,,,"18/Aug/11 01:50;atm;Patch looks pretty good, Todd. The only thing I notice is that it looks like there are a few unused imports in the file. +1 pending removal of those.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Gridmix] Support addons in Gridmix,MAPREDUCE-3439,12532109,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,amar_kamat,amar_kamat,amar_kamat,21/Nov/11 04:11,09/Mar/15 22:06,12/Jan/21 09:52,,,,,,,,,,contrib/gridmix,,,,,,0,addons,gridmix,gridmix3,,"At times there is a need to benchmark certain Hadoop client APIs. Often, this is done by running simple & standard sort-like programs on Hadoop and then using an external utility to benchmark the APIs. But then the benchmarking results tend to be off from reality as the load on the cluster doesn't match the actual load. We believe that Gridmix3 - which is a Hadoop workload simulator - can prove useful here. Gridmix3 already provides a mechanism to load the cluster - often called as a 'test cluster' - using a real trace thus mimicking the real-life workload.

Currently, Gridmix3 consumes a representative workload trace and loads the Hadoop cluster to match what is seen in the trace. Gridmix3 can be enhanced to also support user scripts (hereby referred as 'addons') which will be loaded within Gridmix3 and will get updates like
1. Job submission
2. Job completion
3. Cluster status

These addons can also ping/access a live, close-to-real-life Hadoop cluster. This will allow users to benchmark the Hadoop cluster while it is running. ",,hong.tang,ravidotg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,217845,,,,,2011-11-21 04:11:16.0,,,,,,,"0|i0e6kn:",80827,,,,,,,,,,,gridmix3 addons gridmix,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Rumen] Rumen should provide simple trace filtering capabilities,MAPREDUCE-3508,12533855,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,amar_kamat,amar_kamat,amar_kamat,05/Dec/11 09:21,09/Mar/15 22:06,12/Jan/21 09:52,,,,,,,,,,tools/rumen,,,,,,0,job-filter,rumen,,,"Rumen should provide inbuilt tools to filter jobs from a given trace. Following are the usecases:
1. Select only first k jobs.
2. Select jobs with certain configuration keys set or available
3. Select jobs where the original job id matches the specified list
4. Select jobs which have at-least/at-most x map tasks and/or at-least/at-most y reduce tasks.
5. Select jobs belonging to a specific user(s)
6. Select jobs having specific name(s)
and so on.",,ravidotg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,219581,,,,,2011-12-05 09:21:22.0,,,,,,,"0|i0e6h3:",80811,,,,,,,,,,,rumen job-filter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support multiple network interfaces,MAPREDUCE-4168,12551820,New Feature,Reopened,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,tomwhite,tomwhite,20/Apr/12 17:22,18/Feb/15 09:55,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,Umbrella jira to track the MapReduce side of HADOOP-8198.,,atm,avnerb,aw,eli2,hammer,jayf,kkambatl,mbertozzi,minjikim,phunt,qwertymaniac,rajive,revans2,sekikn,sseth,stevel@apache.org,tgraves,tlipcon,varun_saxena,vinithra,vvasudev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-07-30 09:38:38.915,,,false,,,,,,,,,,,,,,,,,,236627,,,,,Tue Jan 06 20:09:36 UTC 2015,,,,,,,"0|i0e5rj:",80696,,,,,,,,,,,,,,,,,,,,,"30/Jul/13 09:38;kkambatl;The RM configs all listen on 0.0.0.0 to support multiple network interfaces. However, submitting the job from the RM node (using the RM config) leads to the AM trying to connect to the RM at 0.0.0.0 which doesn't exist. Other than that, MR works with multiple interfaces.","31/Jul/13 00:28;kkambatl;Could run jobs successfully by setting yarn.resourcemanager.scheduler.address explicitly to the host:port, would be nice if this is not required.","28/Dec/14 18:29;kasha;It is expected that an appropriate client config is used to submit jobs. Don't have to support submitting jobs with the RM config. ","29/Dec/14 02:35;aw;I'm re-opening this issue based upon the resolution it was closed.

It has not and never will be acceptable to require different configs on different nodes for the software stack.  (Hardware, specifically file system and memory configuration is obviously a different problem.)  Making this a requirement to fix such a ho-hum issue greatly increases operational overhead.","29/Dec/14 08:24;avnerb;Thanks for re-opening.
I also suffered from the bias of hadoop towards the main interface (that resolves the hostname) regardless of the interface I configured.","29/Dec/14 17:54;kasha;Thanks for following up, Allen and Avner. I was under the impression that it was common practice to have a client configuration to submit jobs. 

Marking this unassigned so someone else can work on it.

[~avnerb] - not sure if it is straight-forward to let the AMs talk to RM on all interfaces. IIUC, the AM will talk to either the primary interface (hostname) or a specified interface. 

","30/Dec/14 04:05;aw;bq.  I was under the impression that it was common practice to have a client configuration to submit jobs. 

If users are creating special *-site.xml files for clients for what are effectively server-side configurations, we need to find out why and fix it.","30/Dec/14 09:35;avnerb;Karthik - I only need hadoop to talk on my *secondary* interface and not on all interfaces.
Still, when I tested hadoop/yarn and configured slaves file that defines my cluster (and also enabled passwordless ssh), I noticed that a major part of the traffic goes over the main interface and not over the secondary interface that I defined in the slaves file.
I once tracked it in the code and saw that Hadoop did reverse DNS lookup for the IP that I configured in the slaves file and got the hostname of the peer machine.  Then it communicated with that machine based on the hostname that it resolved; hence, traffic went over the main interface instead of my secondary interface.
","30/Dec/14 16:42;aw;bq.  I only need hadoop to talk on my secondary interface and not on all interfaces.

If one only needs to listen on one interface, then only that interface should be configured in the *-site.xml files.

bq. I noticed that a major part of the traffic goes over the main interface and not over the secondary interface that I defined in the slaves file.

The slaves files has absolutely zero to do with what the Java interfaces configure.  The etc/hadoop/slaves file is only for using the shell code and even then only for the ssh bits.  (FWIW, I'm working on fixing the documentation via HADOOP-10908 to help clarify this, since this is a big point of confusion.) Almost all TCP/IP stacks by default will send traffic outbound on any network interface where the other side of the pipe is reachable.  This is *usually* a good thing, but there are times where this is problematic. :)

The biggest problem that we have in Hadoop is that we use the server-side network configuration as the same information we pass to or expect clients to use for connection information.  In multi-home situations, these may not be (and usually aren't) the same.  So if a service is configured to use 0.0.0.0 (to bind to all interfaces), the client can't use that to connect to it since that doesn't resolve to anything usable.  

Karthik's suggestion of using a custom set of *-site.xml files installed on client nodes to specify that the RM is actually at 10.1.2.3 (or whatever) is a viable workaround. But it effectively tosses the problem over the wall to ""let ops deal with it.""  It's friction in the system we really need to fix rather than passing the buck to the poor schmuck down the line.

There's also the problem of being able to pick which interfaces: if I only want 2 out of 3, there is no way to do it since the network configuration only supports one entry. So it's all or nothing.  The parent of this JIRA is geared towards fixing these issues. We should probably take a fresh look at it and fix it in 3.x, since it will likely be an incompatible change to do this correctly.
","30/Dec/14 16:57;kasha;bq. If users are creating special *-site.xml files for clients for what are effectively server-side configurations, we need to find out why and fix it.

The RM address the client (and AM) should talk to is a client-side configuration, no? ","30/Dec/14 17:25;stevel@apache.org;bq. The biggest problem that we have in Hadoop is that we use the server-side network configuration as the same information we pass to or expect clients to use for connection information. In multi-home situations, these may not be (and usually aren't) the same. So if a service is configured to use 0.0.0.0 (to bind to all interfaces), the client can't use that to connect to it since that doesn't resolve to anything usable.

The issue here is the expectation that clients will magically get valid -site.xml files, where the definition of valid is : valid at time {{t}} for client {{c}} to submit jobs to cluster {{h}}. The configurations used within cluster {{h}} may not be valid for client {{c}} even for the same time value {{t}}. This is why, although you can point your browser at http://resourcemanager:8090/config, you have no guarantees of getting a usable config.

Allen, if you look closely at YARN-896 you'll see that the registry can live standalone, and has special space reserved {{/services}} for us to publish service records for system-wide services, service records that could include the URL to an API which lets you enum configs and retrieve them. Which is precisely what we offer in Slider for pushing out dynamically generated hbase-site.xml, accumulo-site.xml, etc. 

The fact that there is effectively a placeholder for publishing core-site XML &c is not a coincidence. It's something we need.

","30/Dec/14 18:02;avnerb;{quote}
If one only needs to listen on one interface, then only that interface should be configured in the *-site.xml files.
{quote}
Thanks for the correction.  you can replace slaves file with *-site.xml files in my previous comment.

{quote}
using a custom set of *-site.xml files installed on client nodes to specify that the RM is actually at 10.1.2.3 (or whatever) is a viable workaround. But it effectively tosses the problem over the wall...
{quote}
FWIW, In hadoop-1 I used 
{code}
<property> <name>slave.host.name </name> <value>${MYDESIREDIP}</value> </property> 
{code}
and added to hadoop-env.sh something like: 
{code}
export HADOOP_OPTS=""-DMYDESIREDIP=`hostanme`.domain.of.my.secondary.interface ${HADOOP_OPTS}"" 
{code}
(notice the back tick around `hostanme`. This was resolved uniquely to my specific secondary interface on each machine).  Unfortunately, this trick doesn't work in yarn.  Perhaps it is simple to restore it.
","30/Dec/14 19:41;rajive;Trying to understand actual need for multihomed systems. With all popular specialty networks and standard ethernet support IP, which could be used for management and application traffic, it seems unnecessary to make current stack handle multiple addresses.

1. Bandwidth - 10G is commodity. If this is not sufficient, L2 trunks is an simpler solution (If your vendor doesn't support LACP, probably should consider alternative, its pretty standard now). Cabling and port density requirements would be same.

2. Client and cluster networks are in different subnets: routing?","30/Dec/14 21:15;stevel@apache.org;One use case is actually cloud environments where there's a public IPaddr and a private one. Use the public/external one and you get billed per GB.","30/Dec/14 21:31;kasha;The latter is definitely a usecase for some of our customers. ","30/Dec/14 22:20;rajive;Clients that would need RPC connectivity to compute nodes, would be within cluster network. Are there use cases to have clients outside of hosted cloud environment need to RPC ports hadoop clusters? I would expect external clients would go via Knox or equivalent gateway.","30/Dec/14 22:21;rajive;Aren't client and cluster networks routable?","31/Dec/14 00:58;aw;Why would one use multiple nics?

The easy and obvious reason is security.  It's an extremely desirable config to have compute nodes be 'outbound only'. If YARN is providing compute power to an automated system, there is no reason for the client to talk to anything other than the RM and maybe the proxy server.  Input is fetched via some other system and output is pushed as the last part of the pipeline. 

On the same token, in some networks there is a separate admin network that is used for operational processes.  That network is trusted, the user facing one is not.

Hadoop supports multiple file systems.  There are many filesystem designs where it's reasonable to configure another nic acting as a backhaul to the backup infrastructure.  The last thing you'd want going across that pipe is user network traffic.

... and those are just the ones off the top of my head. 

bq. Clients that would need RPC connectivity to compute nodes, would be within cluster network.

This seems to be too narrow of a view of the potential operating environment.  In other words, who says that the multiple nics are there because Hadoop needs them?  What if Hadoop is going into a DC that is brown field or has other custom needs?  Of course, as pointed above, it's trivial to come up with a realistic use case where clients only need RPC access to master nodes which are, unfortunately, the key problem bits.","31/Dec/14 10:23;rajive;Restricting access to nodes managed by YARN by using proxies for outbound access:

Cant this restriction be enforced via firewall, limiting external clients to only reach RM? Having separate physical network for RM to NM vs RM to clients assumes one can easily protect RM host instead of using a firewall to restrict access to nodes running nodemanagers. 

Reading through other comments and Hadoop-8198, requirement for multi homed network support is for redundancy and performance. This can't just be implemented by adding multiple IP's.

Admin/oob vs cluster network:

YARN and HDFS could function with out being aware of admin network.

Non HDFS access:

Changes proposed in this Jira are for core hadoop services. Network requirements for no hdfs access is up to respective client implementation. Compute node can have a Luster configured to use OFED infinband. This doesn't require changes to hdfs or yarn. Masters can be bound to one network. 
YARN could add support to manage interconnect as a resource.




  






 

","31/Dec/14 17:54;kasha;Too many discussion threads here, it is getting harder to follow. 

Just so we are all on the same page, MR/YARN already support multiple network interfaces. I left this JIRA open initially to address one inconvenience when submitting jobs using the RM conf. 

[~aw] - should we repurpose this JIRA to address the configuration issue? I think it might be better to open another JIRA for it. 

[~rajive] - are you proposing to not support multiple network interfaces? If so, could you please open another JIRA (or a discussion thread on yarn-dev) along with why the current approach is bad, and how the new proposed approach is better?","06/Jan/15 20:09;rajive;bq. Rajiv Chittajallu - are you proposing to not support multiple network interfaces? If so, could you please open another JIRA (or a discussion thread on yarn-dev) along with why the current approach is bad, and how the new proposed approach is better?

Requirements mentioned in HADOOP-8198 can't be addressed by adding support for multiple network interfaces. Its being assumed that, both networks are implemented as Layer 2 and interface routes are sufficient. With current networks, scope of Layer 2 is limited to rack switch. There are much simpler alternatives to the challenges being addressed here.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory sharing across all the Tasks in the Task Tracker to improve the job performance,MAPREDUCE-2647,12512964,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,devaraj,devaraj,devaraj,06/Jul/11 10:59,12/Feb/15 10:18,12/Jan/21 09:52,12/Feb/15 10:18,,,,,,,,,tasktracker,,,,,,0,,,,,"	If all the tasks (maps/reduces) are using (working with) the same additional data to execute the map/reduce task, each task should load the data into memory individually and read the data. It is the additional effort for all the tasks to do the same job. Instead of loading the data by each task, data can be loaded into main memory and it can be used to execute all the tasks.


h5.Proposed Solution:
1. Provide a mechanism to load the data into shared memory and to read that data from main memory.
2. We can provide a java API, which internally uses the native implementation to read the data from the memory. All the maps/reducers can this API for reading the data from the main memory. 


h5.Example: 
	Suppose in a map task, ip address is a key and it needs to get location of the ip address from a local file. In this case each map task should load the file into main memory and read from it and close it. It takes some time to open, read from the file and process every time. Instead of this, we can load the file in the task tracker memory and each task can read from the memory directly.
",,aah,atm,cdouglas,eli,gates,lianhuiwang,srivas,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,69022,,,,,Thu Feb 12 10:18:51 UTC 2015,,,,,,,"0|i0e79j:",80939,,,,,,,,,,,,,,,,,,,,,"12/Feb/15 10:18;devaraj;Closing it as Won't fix as there is no active feature development happening in mrv1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add RunningJob.getJobStatus(),MAPREDUCE-4355,12595177,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,kasha,kasha,kkambatl,19/Jun/12 22:53,03/Nov/14 18:34,12/Jan/21 09:52,05/Jul/12 16:34,1.0.3,2.0.0-alpha,,,,1.2.0,2.0.2-alpha,,mrv1,mrv2,,,,,0,,,,,"Usecase: Read the start/end-time of a particular job.

Currently, one has to iterate through JobClient.getAllJobStatuses() and iterate through them. JobClient.getJob(JobID) returns RunningJob, which doesn't hold the job's start time.

Adding RunningJob.getJobStatus() solves the issue.",,acmurthy,ahmed.radwan,devaraj,hudson,jlowe,kasha,kkambatl,qwertymaniac,revans2,sseth,tgraves,tucu00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/12 22:48;kkambatl;MR-4355_mr1.patch;https://issues.apache.org/jira/secure/attachment/12533723/MR-4355_mr1.patch","29/Jun/12 01:21;kkambatl;MR-4355_mr2.patch;https://issues.apache.org/jira/secure/attachment/12533921/MR-4355_mr2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2012-06-20 18:48:40.209,,,false,,,,,,,,,,,,,,,,,,241060,Reviewed,,,,Tue Jul 10 20:26:42 UTC 2012,,,,,,,"0|i0189b:",5107,,,,,,,,,,,,,,,,,,,,,"20/Jun/12 18:48;kkambatl;Uploading a patch for MR2: MR-4355_mr2.patch.

The patch
- implements JobClient.getJobStatus(JobID)
- renames TestJobClientGetJob to TestJobClient
- adds a test for getJobStatus to the renamed test file","20/Jun/12 18:52;kkambatl;Submitting to make sure Hudson doesn't complain","20/Jun/12 19:16;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12532743/MR-4355_mr2.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 javadoc.  The javadoc tool appears to have generated 13 warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2485//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2485//console

This message is automatically generated.","20/Jun/12 22:14;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12532781/MR-4355_mr1.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2489//console

This message is automatically generated.","20/Jun/12 22:24;kkambatl;W.r.t MR2 patch, Eclipse shows the same number of warnings with and without the patch. Any pointers as to where I should look for these extra warnings Hudson reports?

Otherwise, I believe the patches are good to go.","20/Jun/12 23:39;jlowe;The 13 javadoc warnings were caused by MAPREDUCE-3868, see HDFS-3550.","21/Jun/12 06:42;devaraj;It is good to have this API. Thanks for providing patch. 

There are few minor comments about the patch

1. Can we get the JobStatus directly from Cluster instance instead of getting the Job instance and getting the JobStatus from Job instance? In Cluster.java, getJob(JobID jobId) API is getting the JobStatus and creating the Job instance using the JobStatus, again you are getting the JobStatus from Job instance.

2. I don't see any use of having the jobid as final.

{code:xml}
+  public JobStatus getJobStatus(final JobID jobid) throws IOException {
{code}

3. Can you rename the jobid variable name to jobId, anyway it is minor you can take the final decision.","21/Jun/12 18:59;kkambatl;Devaraj, 

Thanks for your comments. I uploaded another patch with the suggested changes, and updated getJob(JobID) also accordingly.
","21/Jun/12 19:16;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12532926/MR-4355_mr2.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 javadoc.  The javadoc tool appears to have generated 15 warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2493//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2493//console

This message is automatically generated.","25/Jun/12 21:54;tucu00;+1","25/Jun/12 22:01;tucu00;Thanks Karthik. Committed to trunk, branch-1 & branch-2.","25/Jun/12 22:22;hudson;Integrated in Hadoop-Hdfs-trunk-Commit #2456 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2456/])
    MAPREDUCE-4355. Add JobStatus getJobStatus(JobID) to JobClient. (kkambatl via tucu) (Revision 1353757)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1353757
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapred/TestJobClient.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapred/TestJobClientGetJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java
","25/Jun/12 22:43;hudson;Integrated in Hadoop-Common-trunk-Commit #2387 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2387/])
    MAPREDUCE-4355. Add JobStatus getJobStatus(JobID) to JobClient. (kkambatl via tucu) (Revision 1353757)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1353757
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapred/TestJobClient.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapred/TestJobClientGetJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java
","26/Jun/12 00:06;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #2405 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2405/])
    MAPREDUCE-4355. Add JobStatus getJobStatus(JobID) to JobClient. (kkambatl via tucu) (Revision 1353757)

     Result = FAILURE
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1353757
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapred/TestJobClient.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapred/TestJobClientGetJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java
","26/Jun/12 03:31;acmurthy;I'm sorry, but we *cannot* make an incompatible change to JobClient which is a public API, at least in hadoop-1.x

-1 on this change.

This will break a number of existing apis.

It seems we cud just add start-time to RunningJob if necessary.

Alejandro - do you mind reverting this change since it breaks compatibility? Thanks.","26/Jun/12 03:34;acmurthy;My bad, I read the patch wrong as removing getJob. Apologies for the noise.","26/Jun/12 03:35;acmurthy;IAC, we could avoid the new API by adding startTime to RunningJob if that is the current drawback?","26/Jun/12 11:46;hudson;Integrated in Hadoop-Hdfs-trunk #1088 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1088/])
    MAPREDUCE-4355. Add JobStatus getJobStatus(JobID) to JobClient. (kkambatl via tucu) (Revision 1353757)

     Result = FAILURE
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1353757
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapred/TestJobClient.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapred/TestJobClientGetJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java
","26/Jun/12 14:04;revans2;It looks like this added in two new javadoc warnings  

MAPREDUCE-4373","26/Jun/12 14:22;hudson;Integrated in Hadoop-Mapreduce-trunk #1121 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1121/])
    MAPREDUCE-4355. Add JobStatus getJobStatus(JobID) to JobClient. (kkambatl via tucu) (Revision 1353757)

     Result = FAILURE
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1353757
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapred/TestJobClient.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapred/TestJobClientGetJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java
","26/Jun/12 16:34;kkambatl;Arun,

Adding startTime to RunningJob does definitely help. By returning JobStatus, we would be able to serve user requests for start time on old, non-running jobs as well.

If you think we should go the RunningJob route, I could redo it that way. However, it might be a little ambiguous for users.

Thanks.","26/Jun/12 16:35;kkambatl;Robert, sorry about the warnings - I ll upload a patch to MAPREDUCE-4373 at the earliest.

Thanks","26/Jun/12 17:04;acmurthy;bq. Adding startTime to RunningJob does definitely help. By returning JobStatus, we would be able to serve user requests for start time on old, non-running jobs as well.

Not really, the old jobs are 'retired' and won't be served after a while. JobHistory is the option there.

So, please, do change this to add startTime to RunningJob. Thanks.","26/Jun/12 17:43;kkambatl;Alejandro, sorry for the trouble. We need to revert the earlier commit.

Thanks.","26/Jun/12 19:09;kkambatl;Arun, it might be cleaner to add RunningJob.getJobStatus() instead of adding startTime, endTime fields to RunningJob and redundantly maintaining them.

Thanks.","27/Jun/12 03:38;tucu00;I'll revert the current commit WED morning. 

I think adding a getJobStatus() method to RunningJob would be more comprehensive that just adding startTime(). Arun, would you agree with that?","27/Jun/12 18:40;tucu00;reverted from trunk, branch-2 and branch-1.","27/Jun/12 18:52;acmurthy;bq. Arun, it might be cleaner to add RunningJob.getJobStatus() instead of adding startTime, endTime fields to RunningJob and redundantly maintaining them.

+1, good point!","27/Jun/12 22:12;kkambatl;Submitting the MR1 and MR2 patches.

- No tests for MR2 - just added a wrapper call to Job.getStatus()","27/Jun/12 22:27;tucu00;The mr1 patch has a few false changes in the test class, please revert those.

Please add a simple testcase for the mr2 case.

Also, in the mr1 patch you are using 'updateStatus()' to update the jobstatus before returning the object. the method above uses 'ensureFreshStatus()', why the difference?","27/Jun/12 22:28;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12533712/MR-4355_mr2.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2523//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2523//console

This message is automatically generated.","27/Jun/12 22:48;kkambatl;Updated patch for MR1.

ensureFreshStatus() calls updateStatus() only after a particular amount of time has passed since previous updateStatus().

For getJobStatus(), to get the latest status, we need to call updateStatus(). Do you suggest calling ensureFreshStatus() instead for consistency?","27/Jun/12 22:55;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12533723/MR-4355_mr1.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2524//console

This message is automatically generated.","28/Jun/12 00:04;tucu00;regarding changing updateStatus() to ensureFreshStatus(), no I think updateStatus() is more appropriate.
","28/Jun/12 18:22;kkambatl;Updated patch for MR2.
- RunningJob.getJobStatus()
- TestNetworkedJob uses MiniMRClientCluster","28/Jun/12 20:04;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12533864/MR-4355_mr2.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2527//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2527//console

This message is automatically generated.","29/Jun/12 01:21;kkambatl;Patch with updated test:
- TestNetworkedJob defines its own MR job, independent of other tests","29/Jun/12 02:21;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12533921/MR-4355_mr2.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.mapreduce.lib.jobcontrol.TestMapReduceJobControl
                  org.apache.hadoop.mapred.jobcontrol.TestLocalJobControl

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2530//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2530//console

This message is automatically generated.","29/Jun/12 18:10;kkambatl;Re-submitting the patch to make sure Jenkins is fine with it.

The tests it complained earlier about were removed from trunk (and have nothing to do with this patch).","05/Jul/12 16:21;tucu00;+1","05/Jul/12 16:34;tucu00;Thanks Karthik. Committed to trunk, branch-2 and branch-1.","05/Jul/12 16:38;hudson;Integrated in Hadoop-Common-trunk-Commit #2424 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2424/])
    MAPREDUCE-4355. Add RunningJob.getJobStatus() (kkambatl via tucu) (Revision 1357723)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1357723
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/RunningJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestNetworkedJob.java
","05/Jul/12 16:39;hudson;Integrated in Hadoop-Hdfs-trunk-Commit #2492 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2492/])
    MAPREDUCE-4355. Add RunningJob.getJobStatus() (kkambatl via tucu) (Revision 1357723)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1357723
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/RunningJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestNetworkedJob.java
","05/Jul/12 17:27;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #2441 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2441/])
    MAPREDUCE-4355. Add RunningJob.getJobStatus() (kkambatl via tucu) (Revision 1357723)

     Result = FAILURE
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1357723
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/RunningJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestNetworkedJob.java
","06/Jul/12 20:32;hudson;Integrated in Hadoop-Hdfs-trunk #1095 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1095/])
    MAPREDUCE-4355. Add RunningJob.getJobStatus() (kkambatl via tucu) (Revision 1357723)

     Result = FAILURE
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1357723
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/RunningJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestNetworkedJob.java
","06/Jul/12 21:25;hudson;Integrated in Hadoop-Mapreduce-trunk #1128 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1128/])
    MAPREDUCE-4355. Add RunningJob.getJobStatus() (kkambatl via tucu) (Revision 1357723)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1357723
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/RunningJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestNetworkedJob.java
","10/Jul/12 20:17;hudson;Integrated in Hadoop-Hdfs-trunk-Commit #2508 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2508/])
    MAPREDUCE-4355. Add RunningJob.getJobStatus() (kkambatl via tucu) (Revision 1357723)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1357723
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/RunningJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestNetworkedJob.java
","10/Jul/12 20:26;hudson;Integrated in Hadoop-Common-trunk-Commit #2441 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2441/])
    MAPREDUCE-4355. Add RunningJob.getJobStatus() (kkambatl via tucu) (Revision 1357723)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1357723
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/RunningJob.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestNetworkedJob.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support fair-sharing option within a MR2 Capacity Scheduler queue,MAPREDUCE-4257,12555238,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Invalid,kasha,tomwhite,tomwhite,14/May/12 22:16,03/Nov/14 18:05,12/Jan/21 09:52,12/Jul/12 01:33,,,,,,,,,capacity-sched,mrv2,,,,,0,,,,,"The fair scheduler can run jobs in a single pool (queue) in FIFO or fair share mode. In FIFO mode one job runs at a time, in priority order, while in fair share mode multiple jobs can run at the same time, and they share the capacity of the pool. This JIRA is to add the latter feature to Capacity Scheduler as an option - the default would remain FIFO.

",,acmurthy,adferguson,junping_du,kasha,kkambatl,lianhuiwang,phunt,qwertymaniac,robw,sseth,tgraves,tomwhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-06-19 21:38:56.773,,,false,,,,,,,,,,,,,,,,,,247460,,,,,Thu Jul 12 01:33:48 UTC 2012,,,,,,,"0|i08asn:",46348,,,,,,,,,,,,,2.0.0-alpha,,,,,,,,"19/Jun/12 21:38;kkambatl;MR-4327 attempts DRF (fair-sharing) across multiple resources.

That said, it might still be worthwhile to have MR-4257 for fairness based on a single resource (memory).

Should we treat DRF as a super-set of 4327 or treat them as separate JIRAs?","07/Jul/12 04:18;acmurthy;The CS already supports user-sharing. Should we close this?","12/Jul/12 01:33;tomwhite;Yes, I don't think we need this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ability to refresh retention settings on history server,MAPREDUCE-5266,12648919,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,ashwinshankar77,jlowe,jlowe,22/May/13 16:13,03/Sep/14 23:35,12/Jan/21 09:52,29/Jul/13 22:49,2.1.0-beta,,,,,2.3.0,,,jobhistoryserver,,,,,,0,,,,,"It would be very useful if the job and log retention settings of the history server could be refreshed without restarting the history server.  This would include such things as:

* how many to jobs to keep for browsing
* how many jobs to cache
* how long to retain jobs
* how long to retain logs
* how often to check for retention",,devaraj,jlowe,junping_du,kasha,kkambatl,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-5265,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,329248,,,,,Mon Jul 29 22:49:53 UTC 2013,,,,,,,"0|i1kszz:",329587,,,,,,,,,,,,,,,,,,,,,"29/Jul/13 22:49;jlowe;Resolving umbrella JIRA as all subtasks are now resolved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
History server admin service to refresh user and superuser group mappings,MAPREDUCE-5265,12648918,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,ashwinshankar77,jlowe,jlowe,22/May/13 16:10,03/Sep/14 23:35,12/Jan/21 09:52,18/Jul/13 20:48,2.1.0-beta,,,,,2.3.0,,,jobhistoryserver,,,,,,0,,,,,"The history server needs an admin interface with the ability to
1. refresh the super user groups configurations,
2. refresh user to group mappings,
3. refresh its admin acls,
4. get groups given a username 
without requiring a restart of the history server.  This is analogous to the  -refreshSuperUserGroupsConfiguration capabilities provided by hdfs dfsadmin and yarn rmadmin. ",,ashwinshankar77,hudson,jlowe,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-5404,,,,,,,,,,,,,,,,,,,,,"12/Jul/13 17:53;ashwinshankar77;JHS_REFRESH-10.txt;https://issues.apache.org/jira/secure/attachment/12592037/JHS_REFRESH-10.txt","17/Jul/13 23:23;ashwinshankar77;JHS_REFRESH-12.txt;https://issues.apache.org/jira/secure/attachment/12592888/JHS_REFRESH-12.txt","18/Jul/13 19:48;ashwinshankar77;JHS_REFRESH-13.txt;https://issues.apache.org/jira/secure/attachment/12593038/JHS_REFRESH-13.txt","04/Jun/13 22:23;ashwinshankar77;JHS_REFRESH-2.txt;https://issues.apache.org/jira/secure/attachment/12586211/JHS_REFRESH-2.txt","05/Jun/13 16:33;ashwinshankar77;JHS_REFRESH-4.txt;https://issues.apache.org/jira/secure/attachment/12586347/JHS_REFRESH-4.txt","06/Jun/13 15:14;ashwinshankar77;JHS_REFRESH-6.txt;https://issues.apache.org/jira/secure/attachment/12586514/JHS_REFRESH-6.txt","24/Jun/13 20:35;ashwinshankar77;JHS_REFRESH-8.txt;https://issues.apache.org/jira/secure/attachment/12589477/JHS_REFRESH-8.txt","26/Jun/13 20:05;ashwinshankar77;JHS_REFRESH-9.txt;https://issues.apache.org/jira/secure/attachment/12589790/JHS_REFRESH-9.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,2013-06-04 21:59:03.663,,,false,,,,,,,,,,,,,,,,,,329247,Reviewed,,,,Thu Jul 18 20:48:07 UTC 2013,,,,,,,"0|i1kszr:",329586,,,,,,,,,,,,,0.23.10,2.1.0-beta,,,,,,,"04/Jun/13 21:59;ashwinshankar77;I've changed the summary and description to reflect all the features which this patch is going to contain.","04/Jun/13 22:23;ashwinshankar77;This patch(JHS_REFRESH-2.txt) contains all the features mentioned in the description. On a high level I've changed 'mapred' shell script to incorporate the new 'hsadmin' commands. I've created a client(which is instantiated by shell script),the client rpc stub,protocol buffer rpc messages,server rpc stub,admin server which executes the command.NOTE : I've not written unit tests yet. I wanted to get comments first so that I don't have to rewrite tests in case there are major comments .  ","04/Jun/13 22:35;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12586211/JHS_REFRESH-2.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3730//console

This message is automatically generated.","05/Jun/13 16:33;ashwinshankar77;Patch updated.My bad,I was testing the patch with older version of trunk.","05/Jun/13 16:45;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12586347/JHS_REFRESH-4.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3735//console

This message is automatically generated.","06/Jun/13 15:14;ashwinshankar77;Patch updated.I was reusing a class 'ProtoBase' which unfortunately got moved to different package in the last few hours :) causing my previous patch to fail.","06/Jun/13 16:00;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12586514/JHS_REFRESH-6.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3744//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3744//console

This message is automatically generated.","18/Jun/13 15:32;jlowe;The patch has gone stale and needs to be refreshed.  Other comments:

General:
* protocol files are in package org.apache.hadoop.yarn but org.apache.hadoop.mapreduce is more appropriate.  The history server is specific to MapReduce and not a generic YARN history server.  In the future YARN may provide its own, separate history server
* Formatting for 80 columns

log4j.properties:
* Using {{yarn.hs}} instead of {{mapreduce.hs}}

I'm torn on whether the history server should be using YARN RPC or just go straight to Hadoop Common RPC like what was done for the equivalent functionality in the namenode (see NameNodeRpcServer, RefreshUserMappingsProtocolServerSideTranslatorPB, etc.).  As the MRAppMaster is already using YARN RPC for its stuff I could see it going either way.  Do others have an opinion on this?","18/Jun/13 17:11;ashwinshankar77;{quote}
protocol files are in package org.apache.hadoop.yarn but org.apache.hadoop.mapreduce is more appropriate. The history server is specific to MapReduce and not a generic YARN history server. In the future YARN may provide its own, separate history server
{quote}
YARN rpc has hardcoded the protocol package name to ""org.apache.hadoop.yarn""(RpcServerFactoryPBImpl.java). So things wont work if I change history server's protocol package name to *mapreduce.","18/Jun/13 17:58;jlowe;Ah yes, sorry I missed that point.  That makes me lean even more towards implementing this via Hadoop Common RPC and straight protobufs (like what was done for the namenode equivalent to this feature) than the YARN route.  If for some reason we do stay with the YARN RPC, we'll need to put some MapReduce-specific prefix to the classes (e.g.: HistoryServerProtos becomes MRHistoryServerProtos) to avoid collisions with protocol messages for the upcoming, generic YARN history server.","24/Jun/13 20:40;ashwinshankar77;I have changed my rpc layer implementation to use classes from hadoop-common rather than Yarn rpc. I've also incorporated the comments suggested. This patch includes units tests. However please note that this patch doesn't apply to branch-23.","24/Jun/13 21:19;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12589477/JHS_REFRESH-8.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 9 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3793//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3793//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-hs.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3793//console

This message is automatically generated.","24/Jun/13 22:37;ashwinshankar77;All the findbugs warnings are due to sources generated by ProtocolBuffers. How do we handle this ?","26/Jun/13 20:49;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12589790/JHS_REFRESH-9.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 9 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3799//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3799//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-hs.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3799//console

This message is automatically generated.","11/Jul/13 19:36;jlowe;Thanks for the updates, Ashwin.  I think this is a cleaner approach with less boilerplate code for the protocol support.

bq. All the findbugs warnings are due to sources generated by ProtocolBuffers. How do we handle this ?

Normally this is handled by telling findbugs to avoid analyzing the generated protocol buffer source files.  This can be accomplished by adding the following to hadoop-mapreduce-project/dev-support/findbugs-exclude.xml:

{code}
     <Match>
       <Package name=""org.apache.hadoop.mapreduce.v2.hs.proto"" />
     </Match>
{code}

More comments on the patch:

* The new history server configs should be added to mapred-default.xml with their default values for documentation purposes.  This was in previous patches but was dropped.
* RefreshUserMappingsProtocol.proto, GetUserMappingsPrototcol.proto, and the supporting glue code is duplicated from HDFS -- is there a way we can simply use the HDFS version?  I see the PB glue in HDFS is already marked LimitedPrivate for HDFS and MapReduce, so it seems like we should just be using that rather than duplicating it if possible.
* It's a bit odd to have RefreshAdminAclsProtocol in hadoop-common but the protocol buffer support for it is only in the history server.  Arguably either the PB code should be in hadoop-common or we should move RefreshAdminAclsProtocol to hadoop-mapreduce-client-hs.  If we do the latter and eventually it is commonized then we can derive from the common version for backwards compatibility.
* HSAdminServer should be a service (it already has start/stop methods like a service), and that would let us simply bundle it under the JobHistoryServer composite service like the other services in the history server.  The conf can arrive via serviceInit and the RPC setup can be done there.
* Nit: ""HSAdminServer"" string should be a static final to avoid typos and make it easier to change (e.g.: to HSAdminService)
* Nit: The mapred script has a tab in the usage statement which should be converted to spaces to be consistent with the other formatting
* Nit: TestHSAdminServer has some unused code (PrintStream import and outContent field)","12/Jul/13 18:07;ashwinshankar77;Thanks, I've incorporated all your comments except one mentioned below.
bq. RefreshUserMappingsProtocol.proto, GetUserMappingsPrototcol.proto, and the supporting glue code is duplicated from HDFS – is there a way we can simply use the HDFS version? I see the PB glue in HDFS is already marked LimitedPrivate for HDFS and MapReduce, so it seems like we should just be using that rather than duplicating it if possible.

I actually tried this when I started working on this JIRA,however I found that I was not able to use HDFS classes in History server since the former is not added as a dependency to the latter,which makes sense. Ideally we would want move these protocol classes to common and then derive it in HDFS and History server.
I'm planning to file a follow-up JIRA to do that. But for now,I felt that its best to duplicate this part of the code.","12/Jul/13 18:42;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12592037/JHS_REFRESH-10.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3846//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3846//console

This message is automatically generated.","15/Jul/13 18:15;jlowe;bq. I found that I was not able to use HDFS classes in History server since the former is not added as a dependency to the latter,which makes sense. Ideally we would want move these protocol classes to common and then derive it in HDFS and History server.

Agreed, rather than have hadoop-mapreduce-client-hs depend upon the HDFS project just for this, let's move the HDFS protocols to common so they are more readily reusable.  I'll file a separate JIRA.

If we are going to duplicate this code in the short-term, we need to mark them package private or otherwise annotate them so it's clear these should not be used outside of the hs package (because they'd be going away soon).","15/Jul/13 18:30;jlowe;Filed HADOOP-9734 to track commonizing RefreshUserMappingsProtocol and GetUserMappingsProtocol code.","17/Jul/13 23:28;ashwinshankar77;Thanks. I've modified the patch to use the protobuf code which got moved to hadoop-common for GetUserMappings and RefreshUserMappings.
","18/Jul/13 00:58;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12592888/JHS_REFRESH-12.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3863//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3863//console

This message is automatically generated.","18/Jul/13 19:25;jlowe;Thanks, Ashwin.  I think it looks great except for one last thing: we need to mark the PB classes as {{@Private}}.  The {{@LimitedPrivate}} was dropped from HSAdminRefreshProtocolPB but {{@Private}} wasn't put in its place, and we should mark the other *PB classes as private so its clear that although they are public classes they are not intended for use outside of hadoop-mapreduce-client.","18/Jul/13 19:48;ashwinshankar77;Marked new class as @Private.","18/Jul/13 20:12;jlowe;+1 pending Jenkins.","18/Jul/13 20:40;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12593038/JHS_REFRESH-13.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3866//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3866//console

This message is automatically generated.","18/Jul/13 20:46;hudson;SUCCESS: Integrated in Hadoop-trunk-Commit #4111 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4111/])
MAPREDUCE-5265. History server admin service to refresh user and superuser group mappings. Contributed by Ashwin Shankar (jlowe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1504645)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/conf/log4j.properties
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/bin/mapred
* /hadoop/common/trunk/hadoop-mapreduce-project/dev-support/findbugs-exclude.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/jobhistory/JHAdminConfig.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/pom.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HSAuditLogger.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HSProxies.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistoryServer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/client
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/client/HSAdmin.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/protocol
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/protocol/HSAdminProtocol.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/protocol/HSAdminRefreshProtocol.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/protocolPB
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/protocolPB/HSAdminRefreshProtocolClientSideTranslatorPB.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/protocolPB/HSAdminRefreshProtocolPB.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/protocolPB/HSAdminRefreshProtocolServerSideTranslatorPB.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/server
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/server/HSAdminServer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/proto
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/proto/HSAdminRefreshProtocol.proto
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryServer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/server
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/server/TestHSAdminServer.java
","18/Jul/13 20:48;jlowe;Thanks, Ashwin!  I committed this to trunk and branch-2.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support token-preserving restart of history server,MAPREDUCE-5332,12653698,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,jlowe,jlowe,jlowe,19/Jun/13 13:15,03/Sep/14 23:35,12/Jan/21 09:52,27/Sep/13 18:26,,,,,,2.3.0,,,jobhistoryserver,,,,,,0,,,,,"To better support rolling upgrades through a cluster, the history server needs the ability to restart without losing track of delegation tokens.",,acmurthy,daryn,devaraj,hudson,jianhe,jlowe,nroberts,raviprak,sandyr,sseth,tomwhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/13 20:34;jlowe;MAPREDUCE-5332-2.patch;https://issues.apache.org/jira/secure/attachment/12598289/MAPREDUCE-5332-2.patch","21/Aug/13 19:00;jlowe;MAPREDUCE-5332-3.patch;https://issues.apache.org/jira/secure/attachment/12599249/MAPREDUCE-5332-3.patch","11/Sep/13 18:30;jlowe;MAPREDUCE-5332-4.patch;https://issues.apache.org/jira/secure/attachment/12602618/MAPREDUCE-5332-4.patch","12/Sep/13 20:04;jlowe;MAPREDUCE-5332-5.patch;https://issues.apache.org/jira/secure/attachment/12602867/MAPREDUCE-5332-5.patch","12/Sep/13 15:37;jlowe;MAPREDUCE-5332-5.patch;https://issues.apache.org/jira/secure/attachment/12602813/MAPREDUCE-5332-5.patch","13/Sep/13 20:54;jlowe;MAPREDUCE-5332-6.patch;https://issues.apache.org/jira/secure/attachment/12603101/MAPREDUCE-5332-6.patch","23/Sep/13 22:29;jlowe;MAPREDUCE-5332-7.patch;https://issues.apache.org/jira/secure/attachment/12604682/MAPREDUCE-5332-7.patch","15/Aug/13 18:31;jlowe;MAPREDUCE-5332.patch;https://issues.apache.org/jira/secure/attachment/12598265/MAPREDUCE-5332.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,2013-08-15 20:17:39.764,,,false,,,,,,,,,,,,,,,,,,333975,Reviewed,,,,Sat Sep 28 13:46:35 UTC 2013,,,,,,,"0|i1lm1j:",334301,,,,,,,,,,,,,,,,,,,,,"15/Aug/13 18:31;jlowe;Patch that adds token persistence in a similar manner to how it is done for the RM.  One major difference is that an error in the token persistence layer is not fatal as it is for the RM.  My thinking is it would be better for the history server to stay up than just fall over for any filesystem hiccup.  It's easy to change this if people think that the history server should crash when this occurs.","15/Aug/13 20:17;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12598265/MAPREDUCE-5332.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.

    {color:red}-1 core tests{color}.  The following test timeouts occurred in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

org.apache.hadoop.mapreduce.v2.TestUberAM

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3945//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3945//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3945//console

This message is automatically generated.","15/Aug/13 20:34;jlowe;Fixing release audit warning.  The TestUberAM timeout has been occurring on trunk for quite a while and is unrelated to this change.","15/Aug/13 22:30;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12598289/MAPREDUCE-5332-2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The following test timeouts occurred in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

org.apache.hadoop.mapreduce.v2.TestUberAM
org.apache.hadoop.mapred.TestMultiFileInputFormat

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3946//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3946//console

This message is automatically generated.","21/Aug/13 19:00;jlowe;Updated patch based on similar changes in YARN-1082:

* HistoryServerStateStorage is now a service
* Moved filesystem startup to the startStorage method","25/Aug/13 09:48;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12599249/MAPREDUCE-5332-3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 5 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.mapreduce.security.TestJHSSecurity

                                      The following test timeouts occurred in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

org.apache.hadoop.mapreduce.v2.TestUberAM

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3958//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3958//console

This message is automatically generated.","26/Aug/13 14:43;jlowe;The TestJHSSecurity issue is a known problem unrelated to this patch, see MAPREDUCE-5480.  The TestUberAM timeout is also unrelated, see MAPREDUCE-5481.","11/Sep/13 15:07;daryn;+HistoryServerFileSystemStateStore+
# Suggest: It may be clearer to rename the {{TOKEN_FOO_PREFIX}} constants to be {{TOKEN_FOO_DIR_PREFIX}} or {{TOKEN_*_FILE_PREFIX}}.
# Suggest: I'd consider not having the hardcoded {{ROOT_STATE_DIR_NAME}} added to the user's path configured by {{MR_HS_FS_STATE_STORE_URI}}.  Is there an advantage to not using exactly what the user specified?  Up to you.
# Question: In {{startStateStorage}}, why 2 mkdirs instead of 1?  A mkdir via {{createDir}} is only going to check the permissions of the leaf dir which seems dubious.  If any of the parent dirs are owned by another user with open permissions, the directory created by the JHS can be deleted and recreated with open permissions.  Point is I'm not sure the extra checks add value, but I suppose they don't hurt.  Up to you. 
# Bug: Unlike {{HistoryServerMemStateStore}}, there appear to be no checks for things being added twice - although arguably those checks all belong in ADTSM.  Token check is there, but not a secret check.  I think the state stores should behave consistently.
# Bug: In {{getBucketPath}}, I think you want to mod (%) the seq number instead of dividing?  Otherwise it creates a janitorial job for someone to clean up empty directories.

+HistoryServerStateStore+
# Suggest: For clarify, perhaps rename to {{HistoryServerStateStore*Service*}}.  I kept getting it confused with {{HistoryServerState}}.
# Suggest: I'd consider removing the dtsm's recover method.  Perhaps {{loadState}} can take the dtsm as an argument and directly populate it instead of populating an intermediary {{HistoryServerState}} object before populating the dtsm.

+JobHistoryServer+
# Bug: Is the {{stateStore}} going to be started twice?  Once in {{startService}} if {{recoveryEnabled}}, again by {{super#startService}} when it iterates the composite services?
# Bug: Should the state store service be started after being recovered?  Not before?
# Suggest: Perhaps the {{stateStore}} should be conditionally created & registered in {{serviceInit}}, then having the state loading all in {{stateStore#start}} invoked by the composite service.  Then there's no need for additional logic in {{JHS#serviceStart}}.  Just an idea.

+JobHistoryStateStoreFactory+
# Bug? Seems a bit odd if recovery is enabled but there's no class defined, a {{HistoryServerNullStateStore}} is created.  It appears {{JHS#serviceStart}} will fail when it calls {{loadState}} and an {{UnsupportedOperationException}} is thrown.  The null store seems to have no real value other than deferring an error from {{JHS#serviceInit}} to {{JHS#serviceStart}}?
# Suggest: It feels like {{JobHistoryServer}} should only create & register a state store if required - which ties in with the prior comment in JHS.  {{serviceInit}} only asks the factory for a state store if recovery is enabled.  The factory throws if no class is defined.","11/Sep/13 18:28;jlowe;Thanks for the review, Daryn!

bq. Suggest: It may be clearer to rename the TOKEN_FOO_PREFIX constants to be TOKEN_FOO_DIR_PREFIX or TOKEN_*_FILE_PREFIX.

Will do.

bq. Suggest: I'd consider not having the hardcoded ROOT_STATE_DIR_NAME added to the user's path configured by MR_HS_FS_STATE_STORE_URI.

I followed the precedent set by FileSystemRMStateStore.  It seems safer to add a possibly redundant directory level in case the user configured the URI to a public directory intended for other users (e.g.: something like /tmp).

bq. Question: In startStateStorage, why 2 mkdirs instead of 1?

This is primarly because mkdirs isn't guaranteed to set the permissions of any created parent directories properly.  I believe HDFS does this, but RawLocalFileSystem does not.

bq. Bug: Unlike HistoryServerMemStateStore, there appear to be no checks for things being added twice - although arguably those checks all belong in ADTSM. Token check is there, but not a secret check. I think the state stores should behave consistently.

createFile() should fail if the file is already present, so the file system store should correctly fail if a token or key is added redundantly.

bq. Bug: In getBucketPath, I think you want to mod (%) the seq number instead of dividing? Otherwise it creates a janitorial job for someone to clean up empty directories.

Good catch!

bq. Suggest: For clarify, perhaps rename to HistoryServerStateStore*Service*. I kept getting it confused with HistoryServerState.

Will do.

bq. Suggest: I'd consider removing the dtsm's recover method. Perhaps loadState can take the dtsm as an argument and directly populate it instead of populating an intermediary HistoryServerState object before populating the dtsm.

I'm following RMStateStore precedent here as well.  I believe the intent is for the state object to shield the state stores from knowledge of the secret manager internals and vice-versa.

{quote}
Bug: Is the stateStore going to be started twice? Once in startService if recoveryEnabled, again by super#startService when it iterates the composite services?
Bug: Should the state store service be started after being recovered? Not before?
{quote}

The stateStore needs to be started before recovery can occur, and starting a service twice is a no-op.  So it should work OK as written.  However I'll clean up the out-of-band state store start with a simple recovery service that is added to the composite service right after the state store.  The recovery service's start method can check if recovery is enabled and perform the recovery on the (now started) state store before the other services are started.

bq. Bug? Seems a bit odd if recovery is enabled but there's no class defined, a HistoryServerNullStateStore is created. It appears JHS#serviceStart will fail when it calls loadState and an UnsupportedOperationException is thrown. The null store seems to have no real value other than deferring an error from JHS#serviceInit to JHS#serviceStart?

The null store is necessary to avoid is-recovery-enabled checks throughout the secret manager as it updates state.  It fails on recovery to catch the scenario where the user enabled recovery but forgot to configure a state store.","11/Sep/13 18:30;jlowe;Updated patch to address Daryn's comments.  Summary of changes:

* Prefixes prefixed
* getBucketPath div-to-mod fix
* state stores renamed to state store services","11/Sep/13 20:11;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12602618/MAPREDUCE-5332-4.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 5 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.mapreduce.TestMRJobClient

                                      The following test timeouts occurred in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

org.apache.hadoop.mapreduce.v2.TestUberAM

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3993//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3993//console

This message is automatically generated.","11/Sep/13 21:09;jlowe;The test failures are unrelated.  TestUberAM still hasn't been fixed, see MAPREDUCE-5481.  I can reproduce the TestMRJobClient failure on trunk, filed MAPREDUCE-5503.","12/Sep/13 15:37;jlowe;Updating the patch to use temporary files when creating key and token files.  This prevents the recovery from seeing a partially-written file if we crash in the middle of a write.

Also extended the unit tests to check for correct behavior on redundant key and token stores.","12/Sep/13 17:26;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12602813/MAPREDUCE-5332-5.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 5 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.mapred.TestSpecialCharactersInOutputPath
                  org.apache.hadoop.mapred.TestMiniMRClasspath
                  org.apache.hadoop.mapred.TestLazyOutput
                  org.apache.hadoop.mapred.TestMiniMRChildTask
                  org.apache.hadoop.mapred.TestJobSysDirWithDFS
                  org.apache.hadoop.mapreduce.TestMRJobClient
                  org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers

                                      The following test timeouts occurred in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

org.apache.hadoop.mapreduce.v2.TestUberAM
org.apache.hadoop.mapred.TestClusterMapReduceTestCase
org.apache.hadoop.mapred.TestMerge
org.apache.hadoop.mapred.TestMiniMRClientCluster
org.apache.hadoop.mapred.TestReduceFetchFromPartialMem
org.apache.hadoop.mapred.TestReduceFetch
org.apache.hadoop.mapred.TestJobName

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3995//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3995//console

This message is automatically generated.","12/Sep/13 20:04;jlowe;Wow, that's a lot of test breakage.

None of the test failures appear to be related to this change.  Many of them are failing with OOM errors due to too many threads, suspect this is caused by lingering AMs like what was reported in MAPREDUCE-5501 and YARN-1183.  Also, I'm able to reproduce many of the failures on trunk without this patch.

Uploading the same patch again to see if we can get a clean(er) run this time.","12/Sep/13 21:51;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12602867/MAPREDUCE-5332-5.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 5 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.mapreduce.TestMRJobClient

                                      The following test timeouts occurred in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

org.apache.hadoop.mapreduce.v2.TestUberAM

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3998//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3998//console

This message is automatically generated.","13/Sep/13 20:54;jlowe;Minor tweak to patch to set the permissions on the file during the create which should reduce the number of RPC calls when using HDFS as the filesystem.","20/Sep/13 17:28;daryn;+JHSDelegationTokenSecretManager+
# I'm not sure {{updateStoredToken}} should remove and then ""recreate"" the token.  The impact of removing the token will cause tokens to be lost if the store can't save another.  In the case of the FS, a renewal will delete the persisted token file, and may then fail to write the token with the updated renewal time.  Since you're creating a tmp file and then renaming it, the delete can be implicitly handled by the rename.  This also reduces an rpc call.

+HistoryServerFileSystemStateStoreService+
# Closing the fs isn't advised since it is using the global fs cache instance...
# NUM_TOKENS_PER_BUCKET isn't ""per"" anymore
# Closing streams in a finally block should be via {{IOUtils.closeStream}}
# {{loadTokenState}} calls {{loadTokensFromBucket}} without checking if the bucket is really a directory.  {{loadTokensFromBucket}} calls {{listStatus}} which returns the file if it's a file, so it will just warn about the path.  Later when {{storeToken}} is invoked, it calls {{createDir}} which is happy that the path exists even though it's a file, then when {{storeToken}} tries to write the new token file it will blow up with a very confusing error and probably take down the server when the first job submission wants a token.
# You tried to reduce the RPC overhead when writing/renaming tokens, but {{storeToken}} is going to stat the bucket directory every time.  Perhaps initialization should create any missing bucket dirs.
# {{createFile}} doesn't remove the tmp file if the write or rename fails.

+JobHistoryServer+
# Ideally, but just a suggestion, {{createJHSSecretManager}} should take the {{stateStore}} as an arg instead of implicitly relying on it already being defined.
# The DTSM has the {{stateStore}} so its recovery method could load the state - instead of the caller loading the state from the {{stateStore}} and passing it in.  The code may become a bit easier to follow, but just a suggestion.

+HistoryServerMemStateStoreService+
# Should {{startStorage}} instantiate the {{state}} instance rather than {{initStorage}}?  Otherwise if you close it, the state gets nulled, starting it again does nothing, then the code will later NPE.  The semantics are a bit unclear.

","23/Sep/13 22:29;jlowe;Thanks for the thorough review, Daryn!  Updated the patch to address all but one of the concerns.  High-level changes include:

* Added an updateToken method to the state store interface, and filesystem store uses rename to try to make this atomic.
* Token buckets are created up front

bq. The DTSM has the stateStore so its recovery method could load the state - instead of the caller loading the state from the stateStore and passing it in. The code may become a bit easier to follow, but just a suggestion.

I kept this as-is.  It makes more sense if the history server were to persist more items in the future than just these tokens, as you'd want to load the state once then dole out the bits of state to the various entities that need to recover using that state.  Either that or the state stores should just be separate and per-service, then I agree that the recovery would be handled by each service.
","24/Sep/13 01:12;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12604682/MAPREDUCE-5332-7.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 5 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.mapreduce.TestMRJobClient

                                      The following test timeouts occurred in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

org.apache.hadoop.mapreduce.v2.TestUberAM

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4028//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4028//console

This message is automatically generated.","24/Sep/13 14:34;jlowe;The TestMRJobClient and TestUberAM failures are still unrelated and are tracked by MAPREDUCE-5503 and MAPREDUCE-5481, respectively.","26/Sep/13 18:06;daryn;+1","26/Sep/13 22:09;jlowe;Thanks for the reviews, Daryn!  Will commit this tomorrow unless there are any objections.","27/Sep/13 18:25;hudson;SUCCESS: Integrated in Hadoop-trunk-Commit #4483 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4483/])
MAPREDUCE-5332. Support token-preserving restart of history server. Contributed by Jason Lowe (jlowe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1527015)
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/jobhistory/JHAdminConfig.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerFileSystemStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerNullStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerStateStoreServiceFactory.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JHSDelegationTokenSecretManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistoryServer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerMemStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestHistoryServerFileSystemStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJHSDelegationTokenSecretManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryServer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/security/TestJHSSecurity.java
","27/Sep/13 18:26;jlowe;I committed this to trunk and branch-2.","28/Sep/13 10:58;hudson;FAILURE: Integrated in Hadoop-Yarn-trunk #346 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/346/])
MAPREDUCE-5332. Support token-preserving restart of history server. Contributed by Jason Lowe (jlowe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1527015)
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/jobhistory/JHAdminConfig.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerFileSystemStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerNullStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerStateStoreServiceFactory.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JHSDelegationTokenSecretManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistoryServer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerMemStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestHistoryServerFileSystemStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJHSDelegationTokenSecretManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryServer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/security/TestJHSSecurity.java
","28/Sep/13 13:29;hudson;SUCCESS: Integrated in Hadoop-Hdfs-trunk #1536 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1536/])
MAPREDUCE-5332. Support token-preserving restart of history server. Contributed by Jason Lowe (jlowe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1527015)
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/jobhistory/JHAdminConfig.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerFileSystemStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerNullStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerStateStoreServiceFactory.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JHSDelegationTokenSecretManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistoryServer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerMemStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestHistoryServerFileSystemStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJHSDelegationTokenSecretManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryServer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/security/TestJHSSecurity.java
","28/Sep/13 13:46;hudson;FAILURE: Integrated in Hadoop-Mapreduce-trunk #1562 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1562/])
MAPREDUCE-5332. Support token-preserving restart of history server. Contributed by Jason Lowe (jlowe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1527015)
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/jobhistory/JHAdminConfig.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerFileSystemStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerNullStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerStateStoreServiceFactory.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JHSDelegationTokenSecretManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistoryServer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerMemStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestHistoryServerFileSystemStateStoreService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJHSDelegationTokenSecretManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryServer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/security/TestJHSSecurity.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Running hi Ram jobs when TTs are blacklisted,MAPREDUCE-1964,12469985,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,,balajirg,balajirg,23/Jul/10 07:51,30/Jul/14 23:09,12/Jan/21 09:52,30/Jul/14 23:09,,,,,,,,,,,,,,,0,,,,,"More slots are getting reserved for HiRAM job tasks then required 

Blacklist more than 25% TTs across the job.  Run high ram job.  No java.lang.RuntimeException should be displayed. ",,balajirg,iyappans,vinaythota,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/10 07:52;balajirg;hiRam_bList_y20.patch;https://issues.apache.org/jira/secure/attachment/12450278/hiRam_bList_y20.patch","27/Jul/10 13:17;balajirg;hiRam_bList_y20_1.patch;https://issues.apache.org/jira/secure/attachment/12450596/hiRam_bList_y20_1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2010-07-23 08:33:16.707,,,false,,,,,,,,,,,,,,,,,,149891,,,,,Wed Jul 28 11:29:57 UTC 2010,,,,,,,"0|i0e8bb:",81109,,,,,,,,,,,,,,,,,,,,,"23/Jul/10 07:52;balajirg;First patch for review","23/Jul/10 08:33;vinaythota;1. Please add some brief description about class.

2. Add java doc information for each public methods.
3.
{noformat}
 +    Assert.assertEquals(""Job has not been succeeded"", 
+          jInfo.getStatus().getRunState(), JobStatus.SUCCEEDED);
{noformat}
don't use the above statement in helper method and it left up to test.

4.
{noformat} 
+  private int runTool(Configuration job, Tool tool, 
+      String[] jobArgs) throws Exception {
+      int returnStatus = ToolRunner.run(job, tool, jobArgs);
+      return returnStatus;
+  }
{noformat}
Instead of writing the separate method use ToolRunner statement directly.

{noformat}
+    JobID jobId = helper.runHighRamJob(conf,jobClient,remoteJTClient);
{noformat}

final HighRamJobHelper helper = new HighRamJobHelper();
JobID jobId = helper.runHighRamJob(conf,jobClient,remoteJTClient);
 Make it final and use it locally instead of defining globally.Because its using only one place in the class.


","27/Jul/10 13:17;balajirg;implemented vinay's comments","28/Jul/10 11:29;vinaythota;Overall patch looks good but there is one small indentation issue.
bq. +    for( int i =0; i < bListedTT.size() ; ++i) {",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
maxConcurrentMapTask & maxConcurrentReduceTask per job,MAPREDUCE-1859,12466730,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,,oae,oae,11/Jun/10 11:10,30/Jul/14 20:14,12/Jan/21 09:52,30/Jul/14 20:14,0.20.2,,,,,,,,job submission,,,,,,0,,,,,"It would be valuable if one could specify the max number of map/reduce slots which should be used for a given job. An example would be an map-reduce job importing from a database where you don't want 50 map tasks querying one db at a time but also you don't want to shrink the overall map task count.
Also this is probably already possible through Fair/Capacity-Scheduler or an own Extension i think it would be a good addition for the default TaskScheduler since this seems to be more then a rare used feature.
This would have the benefit in situations where you don't have control/ownership over the cluster as well. 
And its more job-centric whereas the existing scheduler extensions seems to be more job-type-centric.

Implementing this feature should be relatively straightforward. Adding something like jobConf.setMaxConcurrentMapTask(int) and respecting this configuration in JobQueueTaskScheduler.

Not sure if this feature would be harmonical with the existing Fair/Capacity-Schedulers.


",,aah,acmurthy,aw,oae,omalley,ravidotg,schen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-06-11 17:17:05.818,,,false,,,,,,,,,,,,,,,,,,149813,,,,,Wed Jul 30 20:14:49 UTC 2014,,,,,,,"0|i0e8kn:",81151,,,,,,,,,,,,,,,,,,,,,"11/Jun/10 17:17;omalley;Putting in per-job limits on number of tasks is relatively fraught with errors and only works if you have exactly one job that *ever* pulls from the database. In the Capacity scheduler, you can set a max size for a given queue. That is at least closer to the right semantics so that you can say that any job using the database needs to use the DB queue.","28/Aug/10 21:36;oae;The Capacity scheduler solution does not seem to be flexible engough for cases where you have different kind of input source and different configurations of input source and all these kinds and configurations are not known at cluster startup. 
If you have a system where a user can setup an import from a database the limits they might want to put on that import can be very different cause one imports something from a mysql-db, one from oracle, one from a clustered db, one from a db wich is in other use as well, etc.... ","30/Jul/14 20:14;aw;Closing as won't fix.  With YARN, you don't need to do these types of operations in MR anymore.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Distributed cache should provide an option to fail the job or not, if cache file gets modified on the fly.",MAPREDUCE-1729,12463066,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,aajisaka,amareshwari,amareshwari,27/Apr/10 04:16,30/Jul/14 18:22,12/Jan/21 09:52,30/Jul/14 18:22,,,,,,,,,distributed-cache,,,,,,0,,,,,"Currently, distributed cache fails the job if the cache file gets modified on the fly. But there should be an option to fail a job or not.
See discussions in MAPREDUCE-1288.
",,aajisaka,aw,schen,sinchii,yhemanth,yvesbastos,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-07-15 18:45:11.494,,,false,,,,,,,,,,,,,,,,,,149715,,,,,Wed Jul 30 18:22:07 UTC 2014,,,,,,,"0|i0e8tr:",81192,,,,,,,,,,,,,,,,,,,,,"15/Jul/13 18:45;aajisaka;Hi, I need this option.

A long-term job failed in our production environment because the file which was used as distributed cache was modified at fixed intervals. As the output of the job would be better if distributed cache is newer, we don't want to fail the job if cache file gets modified on the fly. Our workaround is, copy original file to tmpfile and use tmpfile as distributed cache. If the option exists, we don't need to copy original file before the job begin.","26/Aug/13 23:11;sinchii;Hi,

When a target file is updated during MapReduce Job, the result will change.
Because it may cause inconsistencies in the result of MapReduce Job, the architecture of the current DistributedCache is valid.
But it may set the following options,

1. Addition of a property the update of the target file in MapReduce Job
2. It copies a target file before MapReduce Job start in another directory and use that file

By any policy, this know-how should document it as an important usage of DistributedCache.","27/Aug/13 21:06;aajisaka;[~yamashitasni], thanks for your comment.
I'll try to implement your 2nd option.","30/Jul/14 18:22;aw;Since this JIRA started based upon my request, I have no issues closing it as Won't Fix.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some new features for CapacityTaskScheduler,MAPREDUCE-1674,12461204,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Incomplete,,xiaokang,xiaokang,06/Apr/10 02:30,30/Jul/14 17:27,12/Jan/21 09:52,30/Jul/14 17:27,0.20.2,,,,,,,,capacity-sched,,,,,,0,,,,,Some new features for CapacityTaskScheduler developed at Baidu.,,aah,acmurthy,aw,hong.tang,mahadev,ravidotg,sreekanth,srivas,tlipcon,yhemanth,zhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-04-06 03:49:11.176,,,false,,,,,,,,,,,,,,,,,,149681,,,,,Wed Jul 30 17:27:05 UTC 2014,,,,,,,"0|i0e8xj:",81209,,,,,,,,,,,,,,,,,,,,,"06/Apr/10 03:49;acmurthy;More details please? *smile*","06/Apr/10 04:03;xiaokang;Details as follows:

# support queue priority : higher priority queue get capacity first

# support queue refresh : reload queue configuration at run time, including change queue properties and adding/removing queues

# queue over capacity controlled : add a queue property to indicate whether this queue can use more slot than its configured capacity

# seperate web page for capacity-scheduler : just as fair-scheduler, a new web page dadicated for capacity-scheduler is added

# job capacity : allowing user to specify per job map/reduce capacity both in cluster/tasktracker level

# job initialization : do not initialize a job if it will not get a chance to run if initialized
","06/Apr/10 04:07;xiaokang;Sorry, wrong wiki syntext used, correct here:

# *support queue priority* : higher priority queue get capacity first
# *support queue refresh* : reload queue configuration at run time, including change queue properties and adding/removing queues
# *queue over capacity controlled* : add a queue property to indicate whether this queue can use more slot than its configured capacity
# *seperate web page for capacity-scheduler* : just as fair-scheduler, a new web page dadicated for capacity-scheduler is added
# *job capacity* : allowing user to specify per job map/reduce capacity both in cluster/tasktracker level
# *job initialization* : do not initialize a job if it will not get a chance to run if initialized
","06/Apr/10 04:10;xiaokang;Creating a sub task for each feature may be more convient for discuss.","06/Apr/10 05:40;yhemanth;Since Hadoop 0.21, there has been a good deal of changes in the capacity scheduler, primarily to support hierarchical queues. Since the 'Affects version' was set to an earlier version, I just thought of calling this out so you are aware.","07/Apr/10 11:30;xiaokang;Thank you for remind.

I have read about the interesting features in hadoop-0.21. We will follow the new features. Among the six features I listed above, 2 & 3 have been implemeted in hadoop-0.21.

Since great changes have been made to capacity-scheduler, QueueManager and TaskScheduler, it's difficult to migrate our improvents made against hadoop-0.20.2 to the latest version. If they are still valuable for the earlier version, sub task will be created for each.","30/Jul/14 17:27;aw;Closing as stale.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
High Availability for JobTracker,MAPREDUCE-2648,12513001,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,,devaraj,devaraj,06/Jul/11 14:42,30/Jul/14 17:05,12/Jan/21 09:52,30/Jul/14 17:05,,,,,,,,,,,,,,,1,,,,,"In Hadoop cluster, JobTracker is responsible for managing the life cycle of MapReduce jobs. If JobTracker fails, then MapReduce service will not be available until JobTracker is restarted. We propose an automatic failover solution for JobTracker to address such single point of failure. It is based on Leader Election Framework suggested in ZOOKEEPER-1080

Please refer to attached document.",,aajisaka,abhijit.shingate,ahmed.radwan,atm,aw,cdouglas,devaraj,eli,eric14,fengshen,gemini5201314,harivishnu,ivanmi,junping_du,kawamon,larsgeorge,liangly,lianhuiwang,longmi,mahadev,nemon,nidaley,ozawa,qwertymaniac,rajesh.balamohan,raviteja,sho.shimauchi,srivas,tdunning,tlipcon,tucu00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1209600,1209600,,0%,1209600,1209600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jul/11 15:17;devaraj;High Availability for JobTracker.pdf;https://issues.apache.org/jira/secure/attachment/12485434/High+Availability+for+JobTracker.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2011-07-06 16:22:54.873,,,false,,,,,,,,,,,,,,,,,,68766,,,,,Wed Jul 30 17:05:55 UTC 2014,,,,,,,"0|i01487:",4454,,,,,,,,,,,"MapReduce, HA, JobTracker, High Availability",,,,,,,,,,"06/Jul/11 16:22;mahadev;devaraj,
 Am not sure, if you already know about the MR-279 branch (the next version of MR framework). We've been trying to integrate ZK into the framework from the beginning. As for now, we are just doing restart with ZK but soon we should have a HA soln with ZK.","07/Jul/11 11:49;devaraj;Hi Mahadev,

Sorry for the delay in response.

Yes. I am aware of MapRed NextGen.

From my understanding, it might take some time for MapRed NextGen to stabilize and become production ready.

So I was considering following points.

ZOOKEEPER-1080 provides very simple, generic solution to support HA scenario.

This solution tries to incorporate it for JobTracker.


Thanks & Regards,
Abhijit","07/Jul/11 13:57;devaraj;Small mistake in above comment!!
It should be

Thanks & Regards,
Devaraj & Abhijit 






","07/Jul/11 14:03;abhijit.shingate;To add,

We tested this solution on a 100 node cluster and 1000 Jobs, it was measured that STANDBY JobTracker can detect the failure of ACTIVE JobTracker and becomes ACTIVE and starts serving requests in less than 1 minute. This includes failure detection time also.

It will be useful for the organizations which are already using hadoop in production environment.","04/Oct/12 00:09;tucu00;Devaraj, Abhijit,

It has been more than a year since your last comments and a patch was never uploaded. What is the status of this on your your end? ","29/Jul/13 06:47;ozawa;What's going on about this JIRA? We have clusters with Hadoop 0.20 or 1.0 series, so we need HA for JT. If no one is implementing this, I'd like to tackle this ticket based on Devaraj's design note.","30/Jul/14 17:05;aw;Closing this as Won't Fix.

HARM (YARN-149) is essentially a replacement for all/most of the stuff happening here.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JT Availability,MAPREDUCE-2288,12497026,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,,eli,eli,28/Jan/11 07:48,30/Jul/14 17:03,12/Jan/21 09:52,30/Jul/14 17:03,,,,,,,,,jobtracker,,,,,,1,,,,,"This is an umbrella jira, like HDFS-1064, for discussing and providing references to jobtracker availability jiras (eg from JT restart on a host or to cross host fail-over).",,abacus,acmurthy,ahmed.radwan,amolk,atm,aw,cdouglas,cutting,ekoontz,eli,fangy,gates,gemini5201314,hammer,hong.tang,kksclt,larsgeorge,liangly,lianhuiwang,longmi,mattyb,octo47,ophchu,ozawa,philip,qwertymaniac,rvadali,sho.shimauchi,srivas,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-225,MAPREDUCE-65,MAPREDUCE-737,YARN-149,MAPREDUCE-2648,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-08-06 10:51:38.433,,,false,,,,,,,,,,,,,,,,,,150114,,,,,Wed Jul 30 17:03:16 UTC 2014,,,,,,,"0|i0e7pj:",81011,,,,,,,,,,,,,,,,,,,,,"06/Aug/12 10:51;amolk;Can you tell  me what actually it means?","06/Sep/12 06:48;kksclt;Do we have any timeline by when this feature will be incorporated ","20/Sep/13 03:13;ozawa;Does this feature mean MRAppMaster's HA?","30/Jul/14 17:03;aw;Closing this as Won't Fix.

HARM (YARN-149) is essentially a replacement for all/most of the stuff happening here.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop should provide a simplified way of fetching user-logs,MAPREDUCE-1776,12459738,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Incomplete,,aloknsingh,aloknsingh,20/Mar/10 04:08,30/Jul/14 16:41,12/Jan/21 09:52,30/Jul/14 16:41,0.20.1,0.20.2,,,,,,,tasktracker,,,,,,1,,,,,"Hi,

   Currently , in both streaming and normal mode hadoop mapred program, user can't download the 
logs programatically.

jobclient.output.filter=ALL, allows user to print output in the stdout/stderr


We need to have the ability to fetch the user logs from the framework.

in 0.18, under the hod , we used to store the logs in the hdfs.
For 0.20, as long as the jobhistory resides user can view the logs but then it's gone.

So in short , I am proposing that hadoop 
1) provide the options to the JobClient (i.e jobconf var jobclient.output.logs.location), so that it will download the output in the hdfs location instead of printing to stdout.

Thanks
Alok




Alok
",,ashutoshc,cutting,macyang,ravidotg,vinodkv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,149748,,,,,Sat Mar 20 04:11:16 UTC 2010,,,,,,,"0|i0e8pz:",81175,,,,,,,,,,,"LOGS, HADOOP, FETCHING",,,,,,,,,,"20/Mar/10 04:11;aloknsingh;With the 0.20S comming , there would be some impacts the way user will download the 
logs. Probably will use the standard kerboros mechanisms.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ability to grab the number of spills,MAPREDUCE-1257,12442065,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,,sriranjan,sriranjan,01/Dec/09 09:34,29/Jul/14 20:56,12/Jan/21 09:52,29/Jul/14 20:56,0.22.0,,,,,,,,,,,,,,0,,,,,The counters should have information about the number of spills in addition to the number of spill records.,,cdouglas,gates,hammer,hong.tang,lianhuiwang,ravidotg,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/09 05:09;tlipcon;mapreduce-1257.txt;https://issues.apache.org/jira/secure/attachment/12426868/mapreduce-1257.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2009-12-04 05:09:50.421,,,false,,,,,,,,,,,,,,,,,,149386,,,,,Wed Dec 09 08:30:16 UTC 2009,,,,,,,"0|i0e9n3:",81324,,,,,,,,,,,,,,,,,,,,,"04/Dec/09 05:09;tlipcon;Patch adds the following counters:

|MAP_SPILLS|Map spills performed|
|MAP_MERGE_PASSES.name|Map output merge passes|
|REDUCE_MERGE_PASSES.name|Reduce input merge passes|

There were some tricky decisions about what to count as a merge pass. I decided to count the merge that feeds directly into the reducer as a merge pass, but didn't count intermediate in-memory merges, for example. Would appreciate review of whether the decisions of which to count make sense, or if we should add more counters to separate out the different cases for in-memory, on-disk, etc.","04/Dec/09 07:53;cdouglas;I disagree with the premise of this issue. What information does the number of spills provide? What's important for performance is how many records are hitting local disk as intermediate data. Consider applications running with a combiner; only a small fraction of data spilled may hit disk. The number of spills is a crude approximation of what the existing metrics provide: guidance on how to set io.sort.record.percent, which will be removed in 0.22.","04/Dec/09 08:48;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426868/mapreduce-1257.txt
  against trunk revision 887061.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/288/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/288/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/288/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/288/console

This message is automatically generated.","04/Dec/09 22:46;tlipcon;Chris: I don't feel strongly about this. I like it for the exact reason you mentioned - makes it easier to tune io.sort.record.percent (or at least see at a glance whether such tuning could help). My plan was to backport it into our distribution for 20, where a backport of MAPREDUCE-64 is pretty unlikely since that change is much riskier.

If no one else wants this, happy to resolve as wontfix. Would be interested to hear from the original reporter, though, before doing so.","09/Dec/09 08:30;cdouglas;bq. My plan was to backport it into our distribution for 20, where a backport of MAPREDUCE-64 is pretty unlikely since that change is much riskier.

*nod* Makes sense. I'm going to cancel the patch for now, pending a current use case.

Do you count the in-memory merge into the reduce?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Send out-of-band heartbeat to avoid fake lost tasktracker,MAPREDUCE-1247,12441923,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,buptzhugy,buptzhugy,buptzhugy,30/Nov/09 07:14,29/Jul/14 20:54,12/Jan/21 09:52,29/Jul/14 20:54,,,,,,,,,,,,,,,0,,,,,"Currently the TaskTracker report task status to jobtracker through heartbeat, sometimes if the tasktracker  lock the tasktracker to do some cleanup  job, like remove task temp data on disk, the heartbeat thread would hang for a long time while waiting for the lock, so the jobtracker just thought it had lost and would reschedule all its finished maps or un finished reduce on other tasktrackers, we call it ""fake lost tasktracker"", some times it doesn't acceptable especially when we run some large jobs.  So We introduce a out-of-band heartbeat mechanism to send an out-of-band heartbeat in that case.",,acmurthy,buptzhugy,hammer,hanfoo_001,hong.tang,philip,rksingh,schen,tlipcon,zhong,zshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-1662,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-11-30 07:54:39.255,,,false,,,,,,,,,,,,,,,,,,149380,,,,,Wed Sep 08 06:30:47 UTC 2010,,,,,,,"0|i0e9nj:",81326,,,,,,,,,,,,,,,,,,,,,"30/Nov/09 07:54;tlipcon;-1 on this approach. Any long blocking tasks like removing files should be deferred to a separate thread rather than worked around like this. Having to deal with multiple threads heartbeating adds a lot of confusion and complexity.","30/Nov/09 08:10;buptzhugy;We print the java jstack when it became fake lost tasktracker on hadoop version 0.19,  and found:

7 times the heartbeat thread waiting the TaskTracker lock ( 5 times because of taskCleanup thread hold for a long time, 2 times because of reduce sub jvm call TaskTracker.getMapCompletionEvents())


4 times the heartbeat thread waiting for the TaskTracker.TaskInProgress lock ( 3 times because of taskCleanup thread hold for a long time, 1 time because of TaskLauncher hold for a long time)

2 times the heartbeat thread waiting for the AllocatorPerContext lock 


The heartbeat thread should only answer for the live or death status of tasktracker, but current implentition it has too many others things to do, we should let the heartbeat thread only do what it has to do.","30/Nov/09 08:15;buptzhugy;The out-of-band heartbeat thread (or we could call it the true heartbeat thread) only send tasktracker's name to jobtracker, and the jobtracker just update it's last seen time, we could add a new interface to InterTrackerProtocol, so it doesn't add  a lot of confusion or complexity. 

","30/Nov/09 08:16;tlipcon;Any chance you have job jars that contain many thousands of classes? MAPREDUCE-967 may help with the cleanup taking a long time. Nevertheless I agree that some of those things should be deferred to other threads so the brunt of the work (eg IO bound things) don't hold critical locks.","30/Nov/09 08:24;tlipcon;My worry about a heartbeat thread that's entirely disconnected from the operation of the TT is that there are certain cases where the TT is ""as good as dead"" but not actually dead. For example, if the TT's in a deadlocked state, your ""true heartbeat"" would continue to function, whereas the TT is not healthy and should be considered dead.

I agree that the optimal system would separate these things, and provide some kind of health check interface to ensure that the service is actually getting work done. For a more achievable short term goal, I think deferring these slow operations to other threads is the safer route. Admittedly I don't work on the guts of this part of the system much, so will defer now to those that do.","30/Nov/09 08:35;buptzhugy;The taskCleanup thread lock the TaskTracker when it call MapOutputFile.removeAll() through TaskTracker.purgeTask() to cleanup a task or TaskTracker.purgeJob() to cleanup a job, if the midoutput file larger than 50GB, and there some other io operations on this disk, it would hold the tasktracker lock for a long time enough to let the jobtracker treat this tasktracker as dead.

I think the current heartbeat thread has to handle too many things which doesn't its duty.  the deadlock in tasktracker currently may still happen and may not be found in current implentition. And I don't think it is the hearbeat's duty to found the deadlock in tasktracker.","30/Nov/09 10:59;buptzhugy;We could make the out-of-band heartbeat thread in tasktracker as optionally(default not start the thread through a configurable parameter),  small cluster (running small jobs) are not needed. The additional thread is very usefull for the cluster running large jobs. Our Product hadoop cluster became more Robustness and never fake-lost-tasktracker any more,  I would attach the patch if someone interested.","30/Nov/09 22:31;acmurthy;I agree with Todd, we should *never* do any i/o operation holding locks... we've added the taskcleanup thread long ago for precisely the same reason. It is quite possible that we've since violated that - we should fix the primary cause rather than hide it with out-of-band heartbeats.","01/Dec/09 02:36;buptzhugy;I agree, seperate the overtime lock method from heartbeat thread and never do i/o operations holding locks is the best solution. We had tried, but found it's not very easy to achieved and would not resolve recently, I propose a tempary solution. ","30/Aug/10 06:47;liangly;Hi Guanyin, our product cluster met the same problem. Would you please attach your patch file? tks.","08/Sep/10 06:30;liangly;I think the  ""fake lost tasktracker"" problem can be fixed by MAPREDUCE-1662.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MAPREDUCE framework should issue warning with too many locations for a split,MAPREDUCE-801,12431340,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,,hong.tang,hong.tang,24/Jul/09 08:20,23/Jul/14 20:23,12/Jan/21 09:52,23/Jul/14 20:23,,,,,,,,,,,,,,,0,,,,,"Customized input-format may be buggy and report misleading locations through input-split, an example of which is PIG-878. When an input split returns too many locations, it would not only artificially inflate the percentage of data local or rack local maps, but also force scheduler to use more memory and work harder to conduct task assignment.",,acmurthy,aw,cutting,ddas,dhruba,ravidotg,tomwhite,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-07-27 18:06:48.315,,,false,,,,,,,,,,,,,,,,,,87671,,,,,Wed Jul 23 20:23:55 UTC 2014,,,,,,,"0|i0jemv:",111301,,,,,,,,,,,,,,,,,,,,,"24/Jul/09 08:21;hong.tang;I suggest we discard location information completely when the number of locations reported by an input split is greater than a threshold (e.g. 20).","27/Jul/09 18:06;cutting;> discard location information completely when the number of locations reported by an input split is greater than a threshold (e.g. 20).

This seems rather arbitrary to me, since one might reasonably increase the replication for an input file to 20 or more, to, e.g., ensure local availability on every rack or node.
","27/Jul/09 19:07;hong.tang;@doug, this is a fair concern. We will probably need to expose this as a conf parameter and allow advanced users to override. Also, we need to issue a warning in the log so that a user can still see what might go wrong.","27/Jul/09 19:42;cutting;I am not yet convinced this is a common enough error that the framework need guard against it.  It might be more reasonable to have a limit on the total number of locations per job rather than locations per split.","28/Jul/09 07:40;hong.tang;@doug, the motivation of this jira is PIG-878. The fact that such a bug has slipped through our eyes for a long time is worrisome. I am open to alternative proposals. But limiting the total number of locations for a very job may not be useful because a large job is likely to touch some blocks on every host.","28/Jul/09 09:53;vinodkv;Hong, I am trying to understand the effect of this issue. Sorry, I couldn't quite follow PIG-878.

You mentioned two:
 (1) artificial inflation of locality
 (2) more memory usage by scheduler(actually JobTracker), and more work in heartbeats.

(1) affects the user's job itself, for e.g., the job might take longer to complete. So, I think the onus is on the user to provide a correct a proper input-split.
(2) is a burden on the framework. May be the framework can handle this by `memorizing' only a fraction of the locations returned, may be a percentage of cluster capacity (instead of disregarding all of them as you've proposed earlier).

May be, if you throw some light on what impact this might have, we can arrive at an appropriate solution.","28/Jul/09 11:01;hong.tang;@vinod, PIG has its own input handling system (Slicer ~= InputFormat, Slice ~= Input Split), when PIG uses MapReduce as the backend, the default Slicer (PigSlicer) creates slices for each DFS block. However, there is a bug in the code that instead of returning the hosts for that particular block, it returns the aggregation of all hosts for all blocks of a file (ignoring the offset and length of the slice). It probably would help you understand the problem by simply looking at the patch attached with PIG-878.

I can imagine similar problems may happen for non-expert users trying to write his/her input formats. You may argue that (1) only affects the user (directly), however, we are sharing the same cluster with many users, and poor locality could thrash the whole cluster and thus affecting all users' jobs (indirectly). The proposal does not really solve the problem, it merely makes sure the problem does not go silently without being noticed.

For (2), yes, we may choose to use a fraction of the locations, but do we need to worry that the scheduler may try to schedule tasks on those subset of hosts and thus could make the actual job running much slower (than not specifying locations at all)?","28/Jul/09 21:08;cutting;In PIG-878 Arun made a sensible suggestion, that the number of locations of a split should not be greater than the replication level of the file.  This could be checked by FileInputFormat.

Another approach might be to add counters for rack-local and local task placements and i/o.  If the tasks are placed locally but the i/o is not done locally, that's a bad sign.","29/Jul/09 03:12;hong.tang;bq. In PIG-878 Arun made a sensible suggestion, that the number of locations of a split should not be greater than the replication level of the file. This could be checked by FileInputFormat. 
We probably want to have infrastructure to check this, and the precise reason why PIG-878 happens is because PigInputFormat is not derived from FileInputFormat (which would have implemented the split location correctly).

bq. Another approach might be to add counters for rack-local and local task placements and i/o. If the tasks are placed locally but the i/o is not done locally, that's a bad sign.
+1. ","29/Jul/09 06:17;eric14;Hi Doug,

I think we are making the perfect the enemy of the good here.  A real bug existed that cost us performance.  Having 20 options on placement is not going to improve scheduling noticeably.  Having hundreds can bring down the centralize resources of the system and even 20 would cause lots of completely unneeded work in the JT for little gain. 

I'd like to see us discard anything beyond the first 5 options in the JT just to keep bugs from DOSing the central server.  I am not aware of any use case where this would hinder performance.  Having a warning and truncating this list would have saved use a lot of resource and time.  

The system is full of numbers.  Sometime it is simpler to harden the system then ID general principles.  There are many places in the system where I think this would be the wrong approach, but huge huge split lists are much more likely to be the result of bugs or ignorance than need.

If we inject a warning and anyone hits the case, we can then do more work to enhance this. 

E14
","29/Jul/09 16:37;cutting;> I'd like to see us discard anything beyond the first 5 options in the JT [ ... ]

Truncating is probably fine.  The original proposal was to ""discard location information completely"".
","30/Jul/09 21:13;acmurthy;bq. Truncating is probably fine. The original proposal was to ""discard location information completely"".

Hmm... truncating is probably fine, but a couple of points to ponder:

# The #locations per split to keep should probably be a cluster-wide config limit?
# Should we pick first n locations or pick randomly? I'd lean towards randomly picked splits in light of features such as HADOOP-548
# We should do truncation on both the JobClient _and_ JobTracker to be wary of DOS if a malicious client submits too many locations per split...

Thoughts?","30/Jul/09 21:30;cutting;> The #locations per split to keep should probably be a cluster-wide config limit?

Sounds reasonable.

> Should we pick first n locations or pick randomly?

That depends on whether locations are ordered.  For example, one might list locations which have 90% of the data in a split ahead of locations that only have 20%.  (Think map-side-join, where a split might contain segments of multiple files.)  If that scenario sounds plausible, then we should pick the N first, no?

> We should do truncation on both the JobClient and JobTracker to be wary of DOS if a malicious client submits too many locations per split...

This all still all feels like overkill to me.  It reminds me of TSA policies about shoes and liquids.  There are not that many InputFormat implementations.  We should seek to make it easy to debug them generally rather than guard against a particular bug seen once.  To prevent DOS, we could put an overall limit on the number of locations per job, or even the size of the splits file, so that the JT doesn't run out of memory trying to process a job.  We should make it easier to notice when job locality is poor.  But there are so many ways folks can write poorly-performing applications and frameworks that spending a lot of time guarding against this particular one seems a poor investment.

Also, truncation does nothing to, e.g., prevent an application that simply lists the wrong locations.  Truncation would not help locality in the case of the PIG bug, since those locations were mostly wrong.  The only thing truncation does is protect against a job using too many resources in the JT, and there are simpler ways to protect against that.

So, sure, if we're checking it once, why not twice!","30/Jul/09 23:09;cutting;I just chatted with Arun and he asked me to make a more specific proposal, so here goes:
 - change the limit on tasks per job to be total locations per job
 - if the jobtracker buffers locations from multiple jobs
 -- record the total number of locations in the job file header
 -- stop buffering jobs when the location limit is exceeded
 - list the total number of locations in the web ui
 - list the percentage of non-local i/o in the web ui

A limit on the number of locations per split would disallow reasonable applications, e.g., those which might have small, very highly replicated input and that should hence be easy to schedule on a busy cluster.  For example, distcp sets the replication of its input file to sqrt(clustersize) so that no single datanode is hammered when all of the tasks read the same file.  With a 4k node cluster, that's a replication of 63, e.g., one split, 63 locations.
","03/Aug/09 06:10;ddas;I like the idea of truncating the number of locations to some fixed number like 5, and ignoring the others. It's a simple fix in the framework to limit the number of locations to read per split. If the split generation code is buggy w.r.t generating the locations for the splits, then we can't do much anyway. The location information is only used for creating the cache in the JobTracker for doing optimal task assignments.

The other thing is the split bytes (the raw bytes corresponding to the serialized split object). If the split data is too large and there are many splits, then the JT again becomes vulnerable. The JT reads the split bytes, and stores it in memory, so that it can be sent as part of the task object to the tasktracker chosen to run the task. There are multiple approaches to solve the problem:

1) Limit the size of the split file 

2) Back the splits on disk. The idea here is to create an index file while the JobTracker is reading the split file. The splits are read one by one and their offsets in the file are stored in the index file. The split data is discarded; the location information is retained (after truncating maybe) and the location info is used to create the cache as is already done. The index file is kept in memory. When a map task is to be handed out to a TT, the JobTracker reads the split data by looking up the index and seeking into the split file (similar to the way we handle map outputs during shuffle).

We could have a cap on the max split size per split (instead of a cap on the total split size) so that we don't use up too much RPC bandwidth while transferring the split data to the tasktracker. The alternative here would be to have the JT just pass the index information to the TT, and have the TT read the split data from the hdfs directly while localizing the task before the launch.. 

Thoughts?","12/Aug/09 16:46;hong.tang;Yet another solution, which I think is more general, is proposed and a jira MAPREDUCE-841 is created for track it.","23/Jul/14 20:23;aw;Definitely fixed, given how many times I was asked what the warning meant.... (Making the message something normal humans can digest would be an obvious follow on)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document Hadoop Map-Reduce Architecture,MAPREDUCE-791,12431188,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,,acmurthy,acmurthy,22/Jul/09 22:05,23/Jul/14 20:05,12/Jan/21 09:52,23/Jul/14 20:05,,,,,,,,,documentation,,,,,,0,,,,,It would be nice to document the Map-Reduce architecture similar to the HDFS design document: http://hadoop.apache.org/common/docs/current/hdfs_design.html.,,aw,enis,hammer,hong.tang,jpatterson,jrideout,omalley,roelofs,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-07-23 20:05:30.955,,,false,,,,,,,,,,,,,,,,,,149046,,,,,Wed Jul 23 20:05:30 UTC 2014,,,,,,,"0|i0ebh3:",81621,,,,,,,,,,,,,,,,,,,,,"23/Jul/14 20:05;aw;Closing this as stale, given YARN.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
High Availability support for Hadoop,MAPREDUCE-737,12429180,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,,qiujie,qiujie,30/Jun/09 10:05,23/Jul/14 18:31,12/Jan/21 09:52,23/Jul/14 18:31,,,,,,,,,,,,,,,1,,,,,"Currently, We look at the HA of Hadoop cluster. We need to consider the NameNode HA as well as Jobtracker HA. For NameNode, we want to build primary/standy or master-slaves pattern to provide NameNode HA. Therefore, we need to consider how to ship log between primary/standby/slaves and how commit ""write"" operation to NameNode after the agreement among primary/standby/slaves on log. Whether will we use Linux HA package or NameNode-built-in HA package without the help of outter Linux HA package. 
After NameNode become high availability, is it necessary to provide HA for Jobtracker? Can Jobtracker  persist the states of Jobs and tasks into HA NameNode? Or Jobtracker also needs the same approach from NameNode for HA support.",,acmurthy,ahmed.radwan,aw,carlos.valiente,chingshen,cutting,ddas,dhruba,dongtalk@gmail.com,ehf,fern,fwang,gasolwu,guyan,hammer,jochenfrey,kevinweil,knoguchi,kzhang,lianhuiwang,mahadev,nidaley,omalley,philip,qiujie,qwertymaniac,romainr,sanjay.radia,scott_carey,sharadag,shv,sureshms,svenkat,tomwhite,withoutyou,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HDFS-243,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-07-01 13:53:52.165,,,false,,,,,,,,,,,,,,,,,,149008,,,,,Wed Jul 23 18:31:14 UTC 2014,,,,,,,"0|i0ixr3:",108562,,,,,,,,,,,"HA, High Availability",,,,,,,,,,"01/Jul/09 13:53;steve_l;The JobTracker can persist its state, but for HA you'd need to do failover in the clients- currently to achieve that you need to switch the replacement to the same hostname/IP address of the original.

The issue of HA for HDFS is raised in HDFS-243","01/Jul/09 18:05;scott_carey;If you really want to go very robust with the JobTracker, you could implement it by embedding ZooKeeper and using it for all the RPC coordination and state tracking.  This is a more natural fit than ZK for the NN, IMO.

A job could be a node in ZK, TaskTrackers would be able to see what work they should do by looking in specific nodes  -- and they can register their status and health as well.  Something like a Reduce job getting the map outputs it needs would simply be listing children from a reduce node for a job. 
Fault tolerance would come for free, and this form of RPC would likely be more efficient than the current one.

Furthermore, with watches firing off, there would be no ping-response sleep delays which would improve performance, and significantly improve latency for latency sensitive jobs.","02/Jul/09 06:03;qiujie;Good suggestion on ZK with Jobtracker. 

Do you know who is implementing ZK for the NameNode? How doese it works?  Currently, we employed the database-like replication approach to ship log from primary to standby(multiple standbys).  We also plan to support different level synchnization, i.e. tight, loose. 

For tight mode, we guarantee primary disk write and standby disk write. For loose mode, we guarantee primary disk write and standby receive log.","02/Jul/09 07:42;qiujie;Currently, in my experiment, my approach follows:

1. Hook Hadoop for Memory write and Disk write.  
     Memory write means changes to the BTree, the corresponding log operations (OP_ADD, OP_RENAME, OP_DELETE,....)
     Disk write means write the log to disks

2. Replication log from primary node to slaves(one standy at this stage)

3. Replay log in slaves

4. Standby heartbeat primary and take over if it feels primary down

I would like to know other approaches for Hadoop HA on NameNode. What's additional requirements on Hadoop HA?
","02/Jul/09 17:09;steve_l;whatever you come up with, can it work on virtualised clusters where clocks are jittery? 

I encounter lots of interesting timeouts with VMs, depending on the underlying infrastructure and its load, and its easy for anything that assumes that multicast works or that clocks move forward at the same rate to get confused. At this point Lamport will probably post a comment implying that timeouts are essential, and I agree -things just need to work well in a world where time is jerky, and the virtual HDDs may not be flushed when calling sync(). ","02/Jul/09 17:36;scott_carey;{quote}Do you know who is implementing ZK for the NameNode?{quote}

I don't even know if such a thing is happening, but given that Google's MapReduce paper and BigTable paper indicate that Chubby is leveraged for HA on various hadoop equivalents I figured that was part of the discussion.

I am in the process of using ZK for an internal project that has some similarities with the JT.  Previously it was using ping-response for both state communication and triggering (distributed) events.  Switching to ZK proved to be a bit outside the bounds of its typical use case but is very fast, reliable, and light-weight.  State management and persistence is much easier, and communication with watches very reliable and fast.

Anyhow, its an option.  The reliability, availability, and time sensitivity issues it addresses.  Other special aspects of the JT use case would probably drive some ZK features -- like an api to return only 'new' children of a node after a specific transaction id, easier out-of-the-box embedding, and some higher level APIs for read-only/watch-only use cases.","02/Jul/09 17:51;mahadev;bq. whatever you come up with, can it work on virtualised clusters where clocks are jittery? I encounter lots of interesting timeouts with VMs, depending on the underlying infrastructure and its load, and its easy for anything that assumes that multicast works or that clocks move forward at the same rate to get confused. At this point Lamport will probably post a comment implying that timeouts are essential, and I agree -things just need to work well in a world where time is jerky, and the virtual HDDs may not be flushed when calling sync().

steve, 
   We have seen ZooKeeper being used on EC2 in virtualized environment. As far I can say, it works for those folks.  ","03/Jul/09 01:27;aaa;
bq. Do you know who is implementing ZK for the NameNode?

see bookkeeper:

https://issues.apache.org/jira/browse/ZOOKEEPER-276
","03/Jul/09 08:26;withoutyou;scott,
We find that the JobTracker only persist the state of competed and dead jobs in DFS, not including the state of in-process jobs. 
However, we can use the existing approach by which JT persists the competed and dead jobs to persist the in-process jobs in JT. 
I feel it is much simpler than ZK-based approach, since we already make NN high availability.","03/Jul/09 08:55;withoutyou;To Amr:
We found that Bookkeeper said Hadoop used Write-Ahead logging approach. 
However, based on my understanding, in 0.19, Hadoop modifies the contents in memory first, and then persists the log. And there is no transaction relationship between modifying memory and persisting log.
For example, in the Create operation (Create a new file entry in the namespace)
[FSNamesystem.java 998] startFileInternal(src, permissions, holder, clientMachine, overwrite, false, replication, blockSize); 
// Add a node child to the namespace, and write the log to a buffer.
[FSNamesystem.java 1000] getEditLog().logSync(); 
// persist the log to the disk.
","03/Jul/09 09:26;yhemanth;bq. We find that the JobTracker only persist the state of competed and dead jobs in DFS, not including the state of in-process jobs.

Umm. This is not true. JobTracker persists the state of running jobs as well in the job history files and can recover from this state upon a restart. This was done as part of HADOOP-3245.","07/Jul/09 00:31;shv;Jie Qiu> we need to consider how to ship log between primary/standby/slaves 

Are you aware of BackupNode introduced in HADOOP-4539?
I thought the log shipping part has been solved by that. Or did you mean anything else?

Jie Qiu> Memory write means changes to the BTree,

What B-tree did you mean?

Jie Qiu> Standby heartbeat primary and take over if it feels primary down

How exactly the standby will decide (""feel"" in your terms) that the primary is down?

Also it would be really good to have a document, which gives a general overview and details of your design. You can refer to other jiras for examples. We used to have design documents for large changes like the one you are attempting here.","08/Feb/13 17:06;qwertymaniac;Hey Jie,

Are you still working on any JT HA bits? If not, can we close this out in favor of the other related JIRAs (as the talk is duplicated)? HDFS-1623 covers the NN HA part btw.","23/Jul/14 18:31;aw;Closing this as a dupe at this point.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Security features for Map/Reduce,MAPREDUCE-563,12428534,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,,omalley,omalley,22/Jun/09 05:27,22/Jul/14 22:02,12/Jan/21 09:52,22/Jul/14 22:02,,,,,,,,,,,,,,,0,,,,,"This is a top-level tracking JIRA for security work we are doing in Map/reduce. Please add reference to this when opening new security related JIRAs.
 
Logically a subpiece of HADOOP-4487.",,aw,kzhang,mahadev,vinodkv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-720,,,,,,,,,,,,,,,,,HADOOP-4487,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-07-22 22:02:46.751,,,false,,,,,,,,,,,,,,,,,,37126,,,,,Tue Jul 22 22:02:46 UTC 2014,,,,,,,"0|i02r7z:",14011,,,,,,,,,,,,,,,,,,,,,"22/Jul/14 22:02;aw;All your jobs are belong to someone who has a Kerberos principal.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support task preemption in Capacity Scheduler,MAPREDUCE-533,12428432,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,,szetszwo,szetszwo,20/Jun/09 00:23,22/Jul/14 22:01,12/Jan/21 09:52,22/Jul/14 22:01,,,,,,,,,capacity-sched,,,,,,0,,,,,"Without preemption, it is not possible to guarantee capacity since long running jobs may occupy task slots for an arbitrarily long time.",,aah,aw,matei,rksingh,robw,tianming.mao,vinodkv,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-06-26 10:20:32.701,,,false,,,,,,,,,,,,,,,,,,148869,,,,,Thu Jun 26 10:20:32 UTC 2014,,,,,,,"0|i0je3r:",111215,,,,,,,,,,,,,,,,,,,,,"20/Jun/09 00:23;szetszwo;See also [this example|https://issues.apache.org/jira/browse/HADOOP-5701?focusedCommentId=12700284&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12700284].","26/Jun/14 10:20;tianming.mao;Any updates on this?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
distcp can support bandwidth limiting,MAPREDUCE-653,12428315,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,ravidotg,ravidotg,ravidotg,19/Jun/09 05:00,22/Jul/14 21:58,12/Jan/21 09:52,22/Jul/14 21:58,,,,,,,,,distcp,,,,,,1,,,,,distcp should support an option for user to specify the bandwidth limit for the distcp job.,,acmurthy,alexlod,aw,cutting,eric14,hammer,kimballa,ravidotg,szetszwo,tomwhite,vicaya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/09 05:27;ravidotg;d_bw.patch;https://issues.apache.org/jira/secure/attachment/12411188/d_bw.patch","08/Sep/09 15:24;ravidotg;d_bw.v1.patch;https://issues.apache.org/jira/secure/attachment/12418930/d_bw.v1.patch","17/Sep/09 10:59;ravidotg;d_bw.v2.patch;https://issues.apache.org/jira/secure/attachment/12419871/d_bw.v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2009-06-19 17:40:27.498,,,false,,,,,,,,,,,,,,,,,,87585,,,,,Tue Jul 22 21:58:22 UTC 2014,,,,,,,"0|i0ixpb:",108554,,,,,,,,,,,,,,,,,,,,,"19/Jun/09 05:27;ravidotg;Attaching patch that supports the option  -bwlimit <bandwidthInBytesPerSec>.

A separate thread in each map monitors the bandwidth by perodically calculating the available bandwidth for this map and sets sleeptime for the copy-thread. copy-thread sleeps for that amount of time and starts copying again.
distcp.bandwidth.limit  is the config property equivalent to -bwlimit option.
distcp.bandwidth.monitor.interval(with default value of 60000millisec) is the period for the bandwidth monitor thread to execute periodically and set the sleeptime.

Please review and provide your comments.","19/Jun/09 17:40;cutting;Some comments:
 - 'sleeptime' should be 'getSleeptime()' to be thread safe, no?  or maybe use int as a sleep time, since updates to an int are atomic.
 - getNumRunningMaps() is expensive to call from each node at each interval, since reports for all tasks must be retrieved from the JT.  better would be to just fetch the job's counters each time, since they're constant-sized, not proportional to the number of tasks.  You'd need to add a maps_completed counter, then use the difference between that and TOTAL_LAUNCHED_MAPS to calculate the number running.
 - the interval to contact the JT might be randomized a bit, so that not all tasks hit it at the same time, e.g., by adding a random value that's 10% of the specified value.
 - when InterruptedException is caught a thread should generally exit, not simply log a warning.  if things will no longer work correctly without the thread, then it should somehow cause other threads dependent threads to fail too.
 - getNumRunningMaps() should either return a correct value or throw an exception.  if it cannot contact the JT or if the task does not know its Id it should fail, no?
","22/Jun/09 17:44;ravidotg;>>getNumRunningMaps() is expensive to call from each node at each interval, since reports for all tasks must be retrieved from the JT. better would be to just fetch the job's counters each time, since they're constant-sized, not proportional to the number of tasks. You'd need to add a maps_completed counter, then use the difference between that and TOTAL_LAUNCHED_MAPS to calculate the number running.

Created JIRA MAPREDUCE-564 for adding new counters. Uploaded patch for the same.

>> the interval to contact the JT might be randomized a bit, so that not all tasks hit it at the same time, e.g., by adding a random value that's 10% of the specified value.

Do we really need this ? All maps may not start at the same time and also may not come to the scheduling point (where scheduling of bandwidthMonitorThread is done) at the same time.","08/Sep/09 15:24;ravidotg;Attaching patch that has the suggested changes except the randomization of interval.
This patch applies after applying  MAPREDUCE-649, MAPREDUCE-654, MAPREDUCE-645, MAPREDUCE-664, MAPREDUCE-661, MAPREDUCE-650, MAPREDUCE-648 and MAPREDUCE-564.

Because of making getNumRunningMaps() throw an Exception, testcase in earlier patch doesn't work. So removed it in this patch. Couldn't think of an easy way of adding a testcase for this.","17/Sep/09 10:59;ravidotg;Attaching new patch as the dependent patch of MAPREDUCE-564 is changed and that provdes api to access the number of running maps/reduces and not new counters.","05/Mar/10 09:35;ravidotg;We need to experiment and check if this way of asking JobTracker for counters from each map task will be a considerable overhead on JobTracker(and would this make JobTracker heavily loaded serving these many requests, with lot more new connections to JobTracker).","06/Mar/10 07:34;eric14;So how will this behave if distcp gets 100 slots and is limited to 100mb?  Will you use 100 slots badly?

Typically we've handled the need for throttling by controlling the bandwidth / client and the number of clients.  This lets you reason about the utilization of the clients & bandwidth.  This would imply a fixed number of maps that pull files to copy off a queue maintained by the distcp client.  You'd need to handle recovery carefully, but that could probably be done at the distcp client (the slave signals done, the client does a final move and updates the queue).","08/Mar/10 05:41;ravidotg;Current proposal does not impose restriction on the number of slots(number of maps) used by distcp job based on the bandwidth limit provided by user.
 Each map task tries to adjust the bandwidth usage of the whole job by adjusting its own bandwidth usage by sleeping for calculated amount of time(this sleep time is calculated based on the current bandwidth usage by this map task and the bandwidth limit specified by user and the number of currently running map tasks in this distcp job).
","08/Mar/10 06:45;eric14;Right.  My point is that this would seem likely to lead to the bad outcome of many slots used lightly.  Sleeps are not a good use of cluster resources.  To keep things in check, one would need to use other means to limit the number of maps that a distcp job can run.  This would seem to be a bad thing to me.  Users could easily use distcp to gum up clusters with many tasks that are just sleeping.","08/Mar/10 09:44;ravidotg;Yes. But we need to make sure that a particular distcp-Job's task on a node is not hogging the whole bandwidth available for the node(just to be fair to the other job's tasks running on this node).
 Also at the time of launching the distcp job, we will not have enough information on the number of tasks of this job that will run paralelly on the cluster at different points of time(because (a) at some point few tasks of this distcp job could be running --- allowing tasks of this distcp job to use high bandwidth per task and (b) at some other time, different number of tasks of this distcp job running --- allowing tasks of this distcp job to use low bandwidth per task).
So if sleeping by map task of distcp job looks bad, then distcp needs to find out a way of identifying the number of map tasks to be run(actually, number of map tasks that can run paralelly on the cluster at any point of time ?) so that it can set the bandwidth usage per task properly with lower sleep time.","22/Jul/14 21:58;aw;distcpv2 does this now. closing as won't fix.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Elegant decommission of lighty loaded tasktrackers from a map-reduce cluster,MAPREDUCE-459,12428306,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Incomplete,namit,dhruba,dhruba,19/Jun/09 00:37,22/Jul/14 21:48,12/Jan/21 09:52,22/Jul/14 21:48,,,,,,,,,,,,,,,1,,,,,There is a need to elegantly move some machines from one map-reduce cluster to another. This JIRA is to discuss how to find lightly loaded tasktrackers that are candidates for decommissioning and then to elegantly decommission them by waiting for existing tasks to finish.,,acmurthy,aw,cutting,hammer,he yongqiang,namit,rschmidt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-06-19 01:33:22.738,,,false,,,,,,,,,,,,,,,,,,148814,,,,,Tue Jul 22 21:48:37 UTC 2014,,,,,,,"0|i0ixo7:",108549,,,,,,,,,,,,,,,,,,,,,"19/Jun/09 01:33;namit;The JobTracker needs a new API:

something like: 

decommissionTaskTrackers(int numberOfTT);

The TaskTracker may need to expose a new API to return its current load.

The JobTracker will get the current load of each TaskTracker and then decide to decommission the
most lightly loaded 'n' takstrackers.

When a TaskTracker is being decommissioned, it will stop accepting new jobs, and will
die when all the current jobs are finished. This may lead to wastage of resources in the cluster.

The jobtracker can optionally pass a timeout after which the tasktracker will definitely die.
At that time, it might be a good idea to increase the number of retires for the tasks being
executed.

The UI may neeed to be changed to show the new status of the task tracker as well.","19/Jun/09 06:07;dhruba;Another option is to expose an APi from the JobTracker that  retrieves the status of all known slave nodes and the load (#running maps, #running reduces) on that slave node. (This will be equivalent to bin/hadoop dfsadmin -report command for HDFS).

","19/Jun/09 06:23;dhruba;Also, we might not need any changes to the decommissioning feature because when a node is decommissioned, currently running tasks are ""killed"" and not ""failed"", so they do increase the probability of a job failure.","19/Jun/09 19:50;rschmidt;I think we should make the jobtracker as light as possible. Dhruba's option sounds a bit better in that sense, since the burden of choosing the tasktrackers to be decommissioned is pushed to some external tool.","29/Jun/09 06:22;amar_kamat;bq. Also, we might not need any changes to the decommissioning feature because when a node is decommissioned, currently running tasks are ""killed"" and not ""failed"", so they do increase the probability of a job failure.
Do you mean ""so they dont increase ..""?

I think its a nice to have feature but it might make the whole process complicated. There is no guarantee as to when the tracker will be free. Also we plan to move the decommissioning/recommissioning strategy/code to a common place (see HADOOP-5772). I personally think we should have a two step process
# *mark* daemons that are to be recommissioned (jobtracker/scheduler might stop scheduling things on this tracker and/or might kill all the tasks on this node while namenode might kickoff some rebalancing thingy)
# *decommission* the marked nodes which actually removes them from the system.

What nodes to decommission and their priority should be external to the system IMHO. Also when to actually start the actual decommissioning should be externally triggered. Thoughts?","29/Jun/09 06:29;dhruba;Hi Amar,

Yes I meant ""currently running tasks are ""killed"" and not ""failed"", so they do not increase the probability of a job failure.""

I think we should close this JIRA. ","22/Jul/14 21:48;aw;Me too. Closing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ability to re-configure hadoop daemons online,MAPREDUCE-442,12428217,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,,amar_kamat,amar_kamat,18/Jun/09 08:19,22/Jul/14 21:47,12/Jan/21 09:52,22/Jul/14 21:47,,,,,,,,,,,,,,,0,,,,,"Example : 
Like we have _bin hadoop mradmin -refreshNodes_ we should also have _bin hadoop mradmin -reconfigure_ which re-configures mr while the cluster is online. Few parameters like job-expiry-interval etc can be changed in this way without having to restart the whole cluster. 

Master, once reconfigured, can ask the slaves to reconfigure (reload its config) from a well defined location on hdfs or via heartbeat. 

We can have some whitelisted configs that have _reloadable_ property. ",,aah,aw,hammer,vinodkv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-06-18 19:04:24.046,,,false,,,,,,,,,,,,,,,,,,148798,,,,,Tue Jul 22 21:47:09 UTC 2014,,,,,,,"0|i0ixnr:",108547,,,,,,,,,,,,,,,,,,,,,"18/Jun/09 19:04;tlipcon;I like this idea. Here are a couple thoughts:
- In addition to having a whitelist of reloadable properties, we should generate loud warnings about any config properties that are modified but are *not* whitelisted. Something like:

ERROR: configuration property fs.default.name modified on disk but not is not reloadable. Retaining old value.

These errors would ideally spit out on the console of the operator, not just in the log4j output on the server side.

- We should also probably have diagnostic output for any changes that did take effect. For example:

INFO: Configuration property foo.bar changed from ""blah"" to ""blahblah""

- Does this introduce the need for additional synchronization inside Configuration?","19/Jun/09 13:14;steve_l;* You could do much of this just by taking down the specific nodes and bringing them up again.
* Hot reconfig is tricky, for as todd points out, some values are cached everywhere.
* Yet I'd like my TTs to be able to rebind to a JT that has just been brought up on a different address.

At the very least, all the loops where workers spin waiting for their masters should reread their configuration values every iteration. That way anything that has subclassed Configuration to give live data could provide updated locations

FWIW, although I've subclassed  JobConf for my configuration, I stopped trying to do live updates, as the objects end up being serialized and reread. I think it would be really hard to do live reconfiguration with the current design. Node restart is cleaner. ","22/Jul/14 21:47;aw;I'm going to dupe this to HADOOP-7001, since it's closer to reality.  Other jiras tend to point to it as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add new options to mapred job -list-attempt-ids to dump counters and diagnostic messages,MAPREDUCE-197,12424532,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,omalley,omalley,omalley,04/May/09 16:13,22/Jul/14 19:21,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,newbie,,,,"It would be very nice when tracking down tasks that have strange values for their counters, if there was a command line tool to print out the task attempts and their counters and diagnostic messages. I propose adding switches to -list-attempt-ids to accomplish that:

{quote}
mapred job -list-attempt-ids [-counters] [-diagnostics] <job> <type> <state>
{quote}",,acmurthy,jlowe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-05-04 17:53:44.213,,,false,,,,,,,,,,,,,,,,,,148587,,,,,Mon May 04 17:53:44 UTC 2009,,,,,,,"0|i0g5p3:",92353,,,,,,,,,,,,,,,,,,,,,"04/May/09 17:53;acmurthy;+1 

We should also expand the supported 'types' to include KILLED and FAILED (currently it's only running and completed).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding Multiple Reducers implementations.,MAPREDUCE-2811,12422144,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,sid,sid,06/Apr/09 18:00,22/Jul/14 18:28,12/Jan/21 09:52,,,,,,,,,,,,,,,,1,,,,,"Like HADOOP-372, we have a multi format Reducer too. Someone suggested that if we need different reducers and map implementations(like what i need) I was better of by writing 2 jobs. I dont quite agree. I am calculating 2 big matrices that must be calculated in the map step, summed in the reducers multiplied and then written to a file. The First mapper sums a matrix  based on the i,j th index(key) into the file and the second mapper adds the N*1  dimension vector that uses a new line as key. These keys must be passed as such to the reduce process.",,acmurthy,ferrerabertran,hammer,lianhuiwang,omalley,sid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,65417,,,,,2009-04-06 18:00:01.0,,,,,,,"0|i0e75j:",80921,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide an admin page displaying events in the cluster along with cluster status/health,MAPREDUCE-208,12417161,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,,amar_kamat,amar_kamat,18/Mar/09 11:29,21/Jul/14 22:04,12/Jan/21 09:52,21/Jul/14 22:04,,,,,,,,,,,,,,,0,,,,,"Here are few things that will help admins understand whats happening in the cluster
# Events updates
  ## recently added tracker
  ## lost trackers
  ## recently submitted jobs
  ## user updates
  ## killed/failed attempts/tasks 
  ## killed jobs and the reason
  ## recent exceptions like oom etc
  ## expired tasks
  ## recovery manager updates
  ## memory/cpu usage
  ## black listing of tracker 
  ## killing of maps based on fetch failures
  ## info about why some jobs was rejected(acls, max tasks)/failed(failures)/killed (user)
  ## etc
# Status :
  ## tracker health and status
  ## User status
    ### num jobs submitted
    ### total time the cluster was used
    ### success/failed/killed history
  ## job status
     ### task completion events
     ### recently scheduled tasks
     ### progress
     ### killed/failed/success history
  ## space on the box where the jt is running
  ## etc
# Config :
  ## slot info
  ## acl info
  ## etc

----
Graphical views and auto updation would be cool. Raising alarms upon certain events would be super cool.",,gmporter,jghoman,ravidotg,rksingh,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-1027,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,148597,,,,,2009-03-18 11:29:44.0,,,,,,,"0|i0iwiv:",108363,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide a way to query job tracker about its daemon thread's status,MAPREDUCE-213,12417146,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Incomplete,amareshwari,amareshwari,amareshwari,18/Mar/09 08:57,21/Jul/14 22:04,12/Jan/21 09:52,21/Jul/14 22:04,,,,,,,,,jobtracker,,,,,,0,,,,,Admin needs to know the status of all threads in JobTracker (whether they are alive or not) at any point of time. This helps alot in debugging crashes.,,amirhyoussefi,aw,cdouglas,rksingh,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/09 06:44;amareshwari;patch-5525.txt;https://issues.apache.org/jira/secure/attachment/12403588/patch-5525.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2009-03-20 09:06:27.548,,,false,,,,,,,,,,,,,,,,,,148601,,,,,Mon Jul 21 22:04:09 UTC 2014,,,,,,,"0|i07ixz:",41836,,,,,,,,,,,,,,,,,,,,,"20/Mar/09 08:52;amareshwari;ThreadStatus could contain thread name, and its state: WAITING, TIMED_WAITING, RUNNABLE, BLOCKED, TERMINATED. 
getClusterStatus() RPC can include required JT thread statuses. 
We can provide an option bin/hadoop job -threads to display the thread statuses.

Thoughts?","20/Mar/09 09:06;yhemanth;If a thread throws an uncaught exception and dies, will it be in the state TERMINATED ? The main use case is to find out what threads are not running. It would be very nice if an uncaught exception's stack trace is printed as a diagnostic. Since that information would be available from the log or out file, that may not be a mandatory thing to resolve this issue, only a nice to have feature.","20/Mar/09 09:53;amareshwari;bq. If a thread throws an uncaught exception and dies, will it be in the state TERMINATED ?
yes","25/Mar/09 06:44;amareshwari;Attaching patch for review.","26/Mar/09 07:44;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12403588/patch-5525.txt
  against trunk revision 758495.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/140/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/140/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/140/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/140/console

This message is automatically generated.","26/Mar/09 10:54;amareshwari;test failures are not related to the patch. All tests passed on my machine.","27/Mar/09 06:48;cdouglas;This seems like it should be part of the {{ping}} functionality in HADOOP-3628 instead of an addition to a status summary. It's more about JT health than the state of the cluster...

More to the point, if critical JT threads are dying, adding an admin tool to ask after their state seems like the wrong response. What is this in support of?","07/Apr/09 04:41;amareshwari;bq. This seems like it should be part of the ping functionality in HADOOP-3628 instead of an addition to a status summary. It's more about JT health than the state of the cluster.
Agreed. I also feel this should be part of ping in JobTracker, after HADOOP-3628.
","21/Jul/14 22:04;aw;Closing this as stale.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add atomic move option,MAPREDUCE-650,12416471,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,ravidotg,theiger,theiger,09/Mar/09 22:23,21/Jul/14 21:48,12/Jan/21 09:52,21/Jul/14 21:48,,,,,,,,,distcp,,,,,,0,,,,,"Provide support for update to move directories/files atomically by copying the src directory to a tmp directory (with random/unique name) then move the directory to its target destination name after all subdirs/files are copied and verified.

example option ideas
  hadoop ... distcp -update -move src dst
or
  hadoop ... distcp -update -atomic src dst

to assure file correctness at the destination, before distcp performs the  'move' at the end of the copy process, it should first perform a strong signature/cksum (e.g. MD4) on the files.

The issue/need for this is that applications may attempt to start processing data (because files are present), prior to completion of a whole directory copy -- resulting in work against an incomplete data set.",,aw,cutting,hammer,hong.tang,jghoman,szetszwo,tucu00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/09 05:51;ravidotg;d_retries_atomic.patch;https://issues.apache.org/jira/secure/attachment/12410903/d_retries_atomic.patch","08/Sep/09 15:11;ravidotg;d_retries_atomic650_651.patch;https://issues.apache.org/jira/secure/attachment/12418922/d_retries_atomic650_651.patch","16/Sep/09 06:00;ravidotg;d_retries_atomic650_651.v1.patch;https://issues.apache.org/jira/secure/attachment/12419732/d_retries_atomic650_651.v1.patch","29/Jun/09 11:02;ravidotg;d_retries_atomic_v1.patch;https://issues.apache.org/jira/secure/attachment/12412059/d_retries_atomic_v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2009-06-17 05:51:57.552,,,false,,,,,,,,,,,,,,,,,,87654,,,,,Mon Jul 21 21:48:10 UTC 2014,,,,,,,"0|i0iwdr:",108340,,,,,,,,,,,,,,,,,,,,,"17/Jun/09 05:51;ravidotg;Here is a patch that supports atomic copies and atomic updates.

distcp -atomic <stagedir> src* dst

Instead of ending up in quota issues(if we consider our own stage dir some where) or access permissions(if we consider the stage dir as a sibling to the dest dir), stagedir is taken as argument with -atomic option.

Mapreduce job would copy to stagedir in case of atomic copy and finally the contents of stagedir are moved to dest dir by distcp. In case of atomic update(-update and -atomic <stagedir>), final move would happen file by file as some of the files/dirs could already be there in dest dir.

This patch also includes code changes of -retries <num_tries> option (HADOOP-6060), as there are dependent code changes.
With -retries <num_tries>, distcp would launch at most num_tries jobs in case of transient failures. Retries are done with -update option enabled.

Patch also contains a testcase to test atomic copy with job retries.

Please review and provide your comments.","17/Jun/09 06:39;hong.tang;Would the program litter partially copied directories if the command fails or interrupted by the client? ","29/Jun/09 09:32;ravidotg;Files are copied to target dir only when the mapreduce job finishes successfully(by then, everything would be copied to stagedir).
If mapreduce job fails after copying few files, then job retry(because of -retries; default value of 3 tries) is done with -update added to existing options. And remaining files are copied to stagedir in the job retries. If all retries of distcp failed, then user can see the files copied in stagedir and actual targetDir is not touched at all.

As per the patch attached, with -update, first try would not compare files of source dir with stagedir(file sizes and checksums). Compares source dir with target dir only.

With -update, It seems better to compare source dir with both stagedir & target dir for filesizes and checksums in first try also.","29/Jun/09 10:45;ravidotg;>> With -update, It seems better to compare source dir with both stagedir & target dir for filesizes and checksums in first try also.

This is basically when distcp fails after copying some of the files to stagedir and then when user wants to copy the remaining files using -update, it is simple for the user to give the same stagedir if distcp could avoid recopying the files existing in stagedir already.","29/Jun/09 11:02;ravidotg;Attaching new patch that makes distcp to compare file sizes and checksums of sourceDir with stageDir also in addition to comparing with targetDir when -update is given in the first try also.

Please review and provide your comments.","08/Jul/09 03:48;ravidotg;Since with -update, after copying files to stagedir, moving from stagedir to actual destinationDir cannot be atomic. So am planning to change the patch to support -atomic <stageDir> option only without -update. If both options are used together, distcp would give error message and exits.","08/Sep/09 15:11;ravidotg;Attaching patch that supports -retries option and -atomic option. -update and -atomic can not specified for a single distcp job. This patch applies after MAPREDUCE-649, MAPREDUCE--654, MAPREDUCE-645, MAPREDUCE-664 and MAPREDUCE-661.

Please review and provide your comments.","16/Sep/09 06:00;ravidotg;Attaching new patch as MAPREDUCE-648 got committed.","21/Jul/14 21:44;aw;With distcpv2, is this issue close-able?","21/Jul/14 21:48;aw;Just checked, yes, atomic is supported in distcpv2. Resolving.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logging turned on by default while using distcp,MAPREDUCE-652,12416490,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,ravidotg,sjaiswal,sjaiswal,10/Mar/09 04:07,21/Jul/14 21:47,12/Jan/21 09:52,21/Jul/14 21:47,,,,,,,,,distcp,,,,,,0,,,,,"Distcp should have an option to disable logging during a distcp

eg: hadoop distcp --nolog <source dir> <destination dir>

By default logging is enabled or turned on while using distcp.This generates logs in DFS, which need to be cleaned up periodically. During this time, critical applications sometimes fail, when they see distcp logs in the DFS.","Linux RHEL54, Linux RHEL5",aw,szetszwo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/09 09:58;ravidotg;d_log_5447.patch;https://issues.apache.org/jira/secure/attachment/12410776/d_log_5447.patch","29/Jun/09 08:25;ravidotg;d_log_5447_v1.patch;https://issues.apache.org/jira/secure/attachment/12412046/d_log_5447_v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2009-03-21 01:08:02.433,,,false,,,,,,,,,,,,,,,,,,87594,,,,,Mon Jul 21 21:47:43 UTC 2014,,,,,,,"0|i0iwe7:",108342,,,,,,,,,,,,,,,,,,,,,"21/Mar/09 01:08;szetszwo;For disable logging, you may change the values set in log4j.properties.","17/Apr/09 20:33;szetszwo;> For disable logging, you may change the values set in log4j.properties.

Oops, I mis-understood the problem.

In such case, you may use -log <logdir> to specify a logging directory, say /tmp/foo, which is outside the critical applications' directories.  Of course, it is still good to have the suggested new feature.","16/Jun/09 09:58;ravidotg;Without -log option and with relative path specified   to -log, the logs dir distcp_logs_randomID was created in copy destination dir.

Attaching patch that makes distcp to create distcp_logs_randomID dir in mapred.system.dir and not in the copy destination dir. Also displays a message giving the path of this log dir for users to look into its contents, if needed. Since the log dir name contains randomID as part of it, multiple distcp jobs shouldn't get the same log dir name, thus avoiding clashing.

Please review and provide your comments.","29/Jun/09 08:25;ravidotg;Attaching new patch with few verbose descriptive messages added so that user would know what is happening(which phase of distcp is going on).","21/Jul/14 21:47;aw;discpv1 is dead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JT should remember blacklisted TT after restart,MAPREDUCE-219,12416362,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,,rramya,rramya,07/Mar/09 07:29,21/Jul/14 21:36,12/Jan/21 09:52,21/Jul/14 21:36,,,,,,,,,,,,,,,0,,,,,"Currently, when JT is restarted , it does not remember any TT(s) which was blacklisted across the cluster before the restart. It would be useful if a new feature could be added for JT to remember blacklisted TT(s) even after restart. This would avoid JT from assigning tasks to faulty TT(s) each time after restart.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,148607,,,,,2009-03-07 07:29:41.0,,,,,,,"0|i0iwdb:",108338,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need a servlet in JobTracker to stream contents of the job history file,MAPREDUCE-1941,12469257,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,sriksun,sriksun,sriksun,14/Jul/10 13:56,21/Jul/14 18:14,12/Jan/21 09:52,21/Jul/14 18:14,0.22.0,,,,,,,,jobtracker,,,,,,0,,,,,"There is no convenient mechanism to retrieve the contents of the job history file. Need a way to retrieve the job history file contents from Job Tracker. 

This can perhaps be implemented as a servlet on the Job tracker.

* Create a jsp/servlet that accepts job id as a request parameter
* Stream the contents of the history file corresponding to the job id, if user has permissions to view the job details.",,amareshwari,cdouglas,cutting,philip,rvadali,sharadag,sreekanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-291,MAPREDUCE-323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-07-15 04:44:28.877,,,false,,,,,,,,,,,,,,,,,,149877,,,,,Thu Jul 15 05:28:23 UTC 2010,,,,,,,"0|i0e8ef:",81123,,,,,,,,,,,,,,,,,,,,,"14/Jul/10 13:58;sriksun;The new servlet to provide the raw history contents should perhaps be hosted on an independent history server to avoid burdening the job tracker.","15/Jul/10 04:44;amareshwari;This can be done in Job client itself, no? History url is already available in JobStatus. ","15/Jul/10 05:28;sriksun;{quote}
This can be done in Job client itself, no? History url is already available in JobStatus. 
{quote} 

While the history file name may be available through JobStatus, the history file is owned by user who runs the job tracker. However access to history file should be governed by JobACL.VIEW_JOB. Hence the request to have a separate servlet to provide job history file contents.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fault tolerant Hadoop Job Tracker,MAPREDUCE-225,12407770,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Incomplete,fsalbaroli,fsalbaroli,fsalbaroli,04/Nov/08 11:23,18/Jul/14 22:27,12/Jan/21 09:52,18/Jul/14 22:27,,,,,,,,,,,,,,,3,,,,,"The Hadoop framework has been designed, in an eort to enhance perfor-
mances, with a single JobTracker (master node). It's responsibilities varies
from managing job submission process, compute the input splits, schedule
the tasks to the slave nodes (TaskTrackers) and monitor their health.
In some environments, like the IBM and Google's Internet-scale com-
puting initiative, there is the need for high-availability, and performances
becomes a secondary issue. In this environments, having a system with
a Single Point of Failure (such as Hadoop's single JobTracker) is a major
concern.
My proposal is to provide a redundant version of Hadoop by adding
support for multiple replicated JobTrackers. This design can be approached
in many dierent ways. 

In the document at: http://sites.google.com/site/hadoopthesis/Home/FaultTolerantHadoop.pdf?attredirects=0

I wrote an overview of the problem and some approaches to solve it.

I post this to the community to gather feedback on the best way to proceed in my work.

Thank you!",High availability enterprise system,aah,acmurthy,ahmed.radwan,alexlod,amolk,anthonyr,atm,bshi,chl501,cutting,cwensel,devaraj,dhruba,ehf,eli,enis,fern,hagleitn,hammer,harivishnu,henryr,jenvor,jimhuang,kimballa,kmacinni,lant,lohit,mahadev,matei,mry.maillist,otis,qiujie,qwertymaniac,romainr,ruikubo,sharadag,shixing,sureshms,tlipcon,tomwhite,tucu00,vinithra,vivekr,yhemanth,zhong,zwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-65,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-3245,HADOOP-1876,,,"18/Dec/08 11:16;fsalbaroli;Enhancing the Hadoop MapReduce framework by adding fault.ppt;https://issues.apache.org/jira/secure/attachment/12396381/Enhancing+the+Hadoop+MapReduce+framework+by+adding+fault.ppt","04/Nov/08 11:26;fsalbaroli;FaultTolerantHadoop.pdf;https://issues.apache.org/jira/secure/attachment/12393292/FaultTolerantHadoop.pdf","18/Dec/08 10:47;fsalbaroli;HADOOP-4586-0.1.patch;https://issues.apache.org/jira/secure/attachment/12396379/HADOOP-4586-0.1.patch","14/Jan/09 16:22;fsalbaroli;HADOOP-4586v0.3.patch;https://issues.apache.org/jira/secure/attachment/12397891/HADOOP-4586v0.3.patch","18/Dec/08 10:47;fsalbaroli;jgroups-all.jar;https://issues.apache.org/jira/secure/attachment/12396380/jgroups-all.jar",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,2008-11-04 11:46:35.876,,,false,,,,,,,,,,,,,,,,,,71761,,,,,Mon Aug 06 08:28:53 UTC 2012,,,,,,,"0|i0ivan:",108164,,,,,,,,,,,,,,,,,,,,,"04/Nov/08 11:26;fsalbaroli;Copy of the document with proposals","04/Nov/08 11:46;amar_kamat;Francesco, HADOOP-3245 allows jobtracker to (re)start. If the history files are maintained on dfs then the jobtracker can start on any other machine. The only issue that prevents us from achieving complete fault tolerance is HADOOP-4016. Once that gets fixed, we can _port_ the jobtracker to any machine and _continue_ from there. Hence the jobtracker is fault tolerant but the switch is not seamless and porting requires manual intervention.","04/Nov/08 11:58;fsalbaroli;Thanks for your quick response Amar.Just a quick question, is it possible now to have multiple instances of JobTracker running simultaneously on different machines with transparent failover to a redundant copy?

It is a problem reported by the IBM HiPODS team working on the academic initiative","05/Nov/08 14:55;steve_l;This is an interesting project which will provide much thesis work, especially from the testing and proof of correctness perspectives.

-There are some implicit assumptions about the ability of the infrastructure to provision hardware, namely that Cold Standby is inappropriate. If a virtual machine can be provisioned and brought up live within a minute, Cold Standby is surprisingly viable, and, on pay-as-you-go infrastructure, cost-effective.

-The statement that forwarding all state changes to all slaves -Hot Standby- is best needs to be qualified with estimated load values and the impact of the events on the network. Is there a cluster size or map/reduce job lifetime in which the state traffic will become an issue, or is it just load on the nodes.  

-How do you intend to implement failover without notifying the task trackers? DNS update?  

-I would like to see some coverage of the election protocol, in particular, how to coordinate such an election over an infrastructure which denies multicast IP (e.g. Amazon EC2).

-Determining the liveness of the JobTracker is going to be hard. Using Lamport's definitions, it is only live if it is capable of performing work within bounded time, so the true way to determine health is to submit work to the system. Early failures: IPC deadlock, host outage etc, may be detectable early, but some failure modes may be hard to detect. Some of the ongoing work in HADOOP-3628 can act as a starting point, but it is inadequate if you really want ""HA"". 

-Ignoring HDFS availability, What is going to happen when the farm partitions and both partitions have slaves and a set of task trackers? Who will be in charge?

-I would have expected some citations for an MSc project; presumably this is an early draft.

-Take a look at Anubis; this is how we implement partition awareness/HA, though it currently uses Multicast to bootstrap, so will not work on EC2 without adding a new discovery mechanism (simpleDB?)
 http://wiki.smartfrog.org/wiki/display/sf/Anubis
","06/Nov/08 11:32;fsalbaroli;Thank you for the comment Steve.

1) You're right, but I don't have exact details about the infrastructure on which the Hadoop cluster will run (probably here at IBM will be possible to run Hadoop on top of IBM BlueCloud cloud computing infrastructure). So in an environment with fault-detection and fast, automatic VM provisioning, cold standby can be an option. This point need further investigation and I hope to receive feedback again on this.

2) In Hot-Standby I supposed a very small number of replicas of the JobTracker (between 2 and 4), so the high network traffic shouldn't be a major concern. But this can be verified only after extensive test sessions.

3) Yes, DNS update seems to be the best option.

4) I wasn't aware of the limitation on multicast of Amazon EC2. Two possible solutions: define statically the JobTracker nodes or maintain a list of nodes on DFS or shared cache

5) I thought about a heartbeat mechanism.

6) Network partitioning is an issue. Ignoring HDFS, using an election protocol will produce separate smaller fully functional cluster (that is an unwanted feature) but, it should be able to detect multiple running masters and re-run the election to reach another stable state.

7)This will be part of my M.Sc. but I produced this document only to propose my ideas to the community and gather feedback. And, yes, this is an early draft.

8) I will take a look at it

So, thank you again for the interest you show in my work and I hope to hear from you and other community members soon.

Francesco

","19/Nov/08 15:03;fsalbaroli;What the community think about using the JGroups reliable multicast system for communicating and status monitoring between master and slaves?

It has 2 major benefits:
1) Implements reliable multicast communications
2) Abstracts from the protocol used (can exploit benefits of Multicast UDP, where available, or using TCP where Multicast is forbidden i.e. Amazon EC2) 

Regards,
Francesco","11/Dec/08 11:09;fsalbaroli;To obtain transparent fail-over to a different JobTracker, a dynamic JT resolution mechanism must be provided.","11/Dec/08 11:30;fsalbaroli;I will release a preliminary test version of Fault tolerant Hadoop before 17th Dec.

Features will include:
-JGroups 2.6.7 toolkit for reliable multicast communication that is based on a highly configurable protocol stack to adapt to different environments (I will post documentation about it).
-Completely wraps around the Hadoop sourcecode to minimize modifications in the source tree.
-Dynamic JobTracker address resolution using HDFS as a support.

Enhancement in future versions:
-Higher level of abstraction
-Better exception handling

I'll post the sourcecode at the beginning of the next week (hopefully).

Can I be added to the developers?

Best regards,
Francesco
","11/Dec/08 15:24;brainlounge;Francesco,

about how to contribute, please see 
  
  http://wiki.apache.org/hadoop/HowToContribute

Have fun,

Bernd

","18/Dec/08 10:47;fsalbaroli;This is a very preliminary (and tested only locally) release of Fault tolerant Hadoop.

The hadoop source tree is only slightly modified in the org.apache.hadoop.mapred.TaskTracker class.

The package containing the fault tolerance wrapper is org.apache.hadoop.mapred.faulttolerant.

The JGroups library (jgroups-all.jar) must be copied in the lib/ folder.

To run the FT version of Hadoop:
1) Configure hadoop-site.xml to match the environment
2) Format the HDFS filesystem ($HADOOP_HOME/bin/namenode -format)
3) Run the HDFS daemons (to run locally $HADOOP_HOME/bin/start-dfs.sh)
4) Run one or more instances of FTJobTracker ($HADOOP_HOME/bin/hadoop org.apache.hadoop.mapred.faulttolerant.FTJobTracker)
5) Run one or more instance of FTTaskTracker ($HADOOP_HOME/bin/hadoop org.apache.hadoop.mapred.faulttolerant.FTTaskTracker)

Regards,
	Francesco Salbaroli

","18/Dec/08 11:16;fsalbaroli;This are the charts from a very introductory presentation I have held at the IBM Innovation Centre to give an update about progress of my work.


","24/Dec/08 12:14;steve_l;openoffice says the PPT file is password encrypted. Is that right? Could you upload a PDF?","24/Dec/08 19:51;otis;Steve, just open it read-only, it works that way.
","05/Jan/09 22:35;nidaley;Francesco, given this is a new feature, it can be integrated into the next major release (which is 0.21).  It can't be integrated into a sustaining release.","08/Jan/09 17:31;bshi;> In my opinion, it is better to avoid using active copies due to the high
> complexity of the coordination protocol and, instead, using a master-slave
> model with soft-state shared between copies through a distributed cache
> mechanism or saved on HDFS.

Please forgive me if I'm being naive here (I see that I'm a bit late to the show), but wouldn't using Zookeeper to persist jobtracker state effectively mask this complexity?

Has anyone explored refactoring the job tracker to use Zookeeper instead of engineering a new master/slave replication system?","08/Jan/09 17:51;sureshms;Sorry for the late comments:
For a master/slave HA solution, two main problems are:
1. Mechanism that determines a master in a cluster during startup and failover. Handling loss of quorum, split-brain and fencing in case of split-brain. It also requires comprehensive management tools for configuring, managing and monitoring cluster.
2. Sharing state information between master and slave, so that a slave node can take over as master.

Currenly the proposed solution addresses mainly the second problem. I have not seen much information on how the first problem is addressed. While the sharing of information between master and slave can be done in many ways, managing the master/slave cluster is a more complicated problem. Could you please add more information on how the design handles these issues and some notes on how administrator uses this functionality to manage the cluster.

Also analysis of the impact of job tracker performance due to the introduction of this feature needs to be done.

> Has anyone explored refactoring the job tracker to use Zookeeper instead of engineering a new master/slave replication system?
Storing the jobtracker state on Zookeeper may not be a viable option, given that ZooKeeper is intended for storing small amount of data (KB) and JobTracker has lot more data than that to persist.","08/Jan/09 18:25;dhruba;Zookeeper might not work well for maintaining JobTracker state (or for that matter, Namenode persistent state) because these processes have lots of metadata to store. ","09/Jan/09 10:30;fsalbaroli;>Sorry for the late comments:
>For a master/slave HA solution, two main problems are:
>1. Mechanism that determines a master in a cluster during startup and failover.

The JGroups library (whose manual can be found here: [http://www.jgroups.org/javagroupsnew/docs/manual/pdf/manual.pdf] ) handles automatically the election of a group coordinator. The node elected group coordinator is also the master of the cluster. In case of a failure a new group coordinator (and, consequentially, a new cluster master) will be elected.

>Handling loss of quorum, 

The shared state resides entirely on HDFS (see issues HADOOP-1876 and HADOOP-3245) so, until now, there is no shared soft-state between nodes. However the facilities for managing a shared state are present and can be used in a future update.

>split-brain and fencing in case of split-brain.

The JGroups library tries to automatically handle network partitions and merging, but given that:

* There is no shared soft-state
* There is only one access point in the whole Hadoop cluster to the HDFS (the NameNode)

the network partition problem should not be an issue (only one partition at a time can access the HDFS). In future versions a more elegant way of dealing with network partitions should be added.

> It also >requires comprehensive management tools for configuring, managing and monitoring cluster.

I am now adding JMX support. After the initial testing phase I will post result and an updated version.

>2. Sharing state information between master and slave, so that a slave node can take over as master.
>Currenly the proposed solution addresses mainly the second problem. I have not seen much information on how the first problem is addressed. While the sharing >of information between master and slave can be done in many ways, managing the master/slave cluster is a more complicated problem. Could you please add >more information on how the design handles these issues and some notes on how administrator uses this functionality to manage the cluster.

I hope I have given an answer to your question. If you need more, feel free to contact me.

>Also analysis of the impact of job tracker performance due to the introduction of this feature needs to be done.

I am about to begin the testing phase, results will follow

Regards,
Francesco","09/Jan/09 16:10;dhruba;>the network partition problem should not be an issue (only one partition at a time can access the HDFS). 

I think the problem still exists. Suppose a network partition occurs between a master and the remainder of the nodes. A new master is elected allright, and the new master will now own the metadata state. The new master will start to update JobTracker metadata strored in HDFS beucae he thinks he is the sole owner of this metadata. At this time, how can you be guaranteed that the old master is not continuing to update the same metadata?
","09/Jan/09 18:19;cutting;> Zookeeper might not work well for maintaining JobTracker state (or for that matter, Namenode persistent state) because these processes have lots of metadata to store.

That's the key concern.  Zookeeper's in-memory datastructures would probably take much more space than those in the namenode and/or jobtracker do today.  Other than that, Zookeeper seems ideally suited to these tasks.  Perhaps if Zookeeper were to support namespace partitioning and rebalancing (hard problems) then it could be used to store such data.  It would certainly vastly simplify many things.","14/Jan/09 16:22;fsalbaroli;Some bugfixes in release 0.3 to work in a distributed environment.

This version has been tested in a distributed VMware Infrastructure 3 environment (using RHEL 5.2 as a guest OS).
","16/Jun/09 08:37;sharadag;If I understand correctly, the current patch doesn't share the state between master and slaves. It relies on HADOOP-3245 for keeping the state. I assume this to work the state has to be kept on HDFS instead of local filesystem. In case a new master is elected, the jobtracker is started using the state from HDFS, right?
Also, reading the master info from HDFS at frequent interval from each node may not scale well. I think Zookeeper would be better suited in the case where we are just doing master election and keeping watch on the master changes.","16/Jun/09 08:41;bshi;Hi,

I am in China until June 22nd and will only have intermittent access
to email until then.

Thanks,
Bo

-- 
Bo Shi
207-469-8264
","16/Jun/09 08:41;bshi;Hi,

I am in China until June 22nd and will only have intermittent access
to email until then.

Thanks,
Bo

-- 
Bo Shi
207-469-8264
","19/Jan/11 10:14;harivishnu;Hi,

In my team, we also have been analysing on how to provide HA for Job Tracker. Our approach is also quite similar to Francesco's approach. 

The complete HA solution can be divided to three aspects

1. Sharing of job related state between Master and Slave job trackers

	This can be achieved with issues HADOOP-1876 and HADOOP-3245. 

2. Failure Detection and Master Election
	
	We are preferring Zookeeper for this. We had quite bad experience with JGroups in some of our previous projects which include Deadlocks, network traffic overhead etc (May be latest version of JGroups is stable). We were forced to replace jgroups. Zookeeper is the best solution available for leader election. We have seen that Zookeeper is very well used in similar situations in ""Katta"" project and also some of our internal projects.

3. How to Notify JobClients and Task Trackers about the new Master, on failure. 
	One option would be DNS as mentioned. 
	Another option is providing a list of job tracker ips to JobClients and Task trackers. They can silently retry on all available ips in case of failure. At the server side, slave job trackers will not accept any service request. This way we can avoid split brain and network partition scenarios. Zookeeper cluster inherently avoids the split brain issues in leader election.

We have not yet started our work. Please provide your valuable opinions. 

thanks
Hari
","23/Feb/11 11:59;ltguo;We also have a solution to JobTracker HA based on LVS:

(1) Start a JobTracker attatched with a virtual IP. All TaskTrackers and JobClients connect with JT via virtual IP;
(2) 2 LVS servers (for hot standby) monitor the state of JobTracker;
(3) When JobTracker down, LVS will trigger a script to start another Jobtracker on another server. Jobs information will be achived with issues HADOOP-1876 and HADOOP-3245. 

This solution need not any changes on JobTracker, but a little complicated deployment. ","23/Feb/11 12:05;ltguo;@Hari, your solution is just the same with HBase master HA, I think it works!

I think I can contribute to this issue. Should I start a new JIRA about ZK+JT or reassign this issue to me ?
","28/Feb/11 18:43;acmurthy;Leitao and Hari - apologies for coming in late. I've missed this so far.

HADOOP-1876 and HADOOP-3245 have had too many issues in the past and we have since moved away from this model - in fact we never deployed either at an reasonable scale due to issues we have seen with them. Also, we have actually have removed a lot of this code in future versions of Hadoop since they didn't work well at all and complicated the JobTracker to a very large extent.

OTOH, we have been working on a completely revamped architecture for Hadoop Map-Reduce via MAPREDUCE-279. You guys might we interested... also we would *love* your feedback based on your experiences there. Thanks!","01/Mar/11 06:40;ltguo;Thanks for your response, Arun. 

Although HADOOP-1876 and HADOOP-3245 do not work well, I think the failover for JobTracker is still considerable.

If JobTracker on one server down, we need to restart JobTracker or migrate JobTracker to another server. In our scenario, we may not care about whether the job will continue just from the same progress before JobTracker failed, but the automatic failover is needed.  Integrating zookeeper with JobTracker is a workable solution for failover I think.","22/Apr/11 16:19;wangxin0072000;I want to know how about this issue,I want to get it HA future in 0.21v,but I want to know how to integration with ZK.someone can tell me how about Jobtrackers with Jgroup or others.","08/Jun/11 09:09;harivishnu;hi,

Sorry for a very late response. 
@Arun: Yes MAPREDUCE-225 is a completely new architecture. May be still need to wait for longer time to get it done. For those who uses 0.20 version and need a simple ""availability solution"", a much simpler approach would be helpful
@Leitao: Yes, its similar to HMaster HA. It works. I have finished the development of ZK based framework and integrated with JT. I am in the process of contributing it back. As a first step, i have opened a Jira in Zookeeper for a generic LeaderElectionService (ZOOKEEPER-1080). I will upload the patch soon.

ZK+JT may not be a full fledged HA solution. But what it tries to address is 
1. Avoid manual intervention during a Jobtracker failure.
2. Recover and Continue the jobs ( even re-submitting the jobs) without notifying to clients who submitted the job. 

Solution remains very simple as no need to synchronize the ""state of the jobs"". 

Cons
-------
Job may take longer time to finish during failover due to re-submission of jobs

Please provide suggestions

-Hari

","06/Aug/12 08:28;amolk;whether this issue is resolved or not I want to work on this...Pls help me",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
map-reduce doctor (Mr Doctor),MAPREDUCE-2823,12402417,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,,amirhyoussefi,amirhyoussefi,14/Aug/08 23:36,18/Jul/14 20:57,12/Jan/21 09:52,18/Jul/14 20:57,,,,,,,,,,,,,,,0,,,,,"Problem Description: 

 Users typically submit jobs with sub-optimal parameters resulting in under-utilization, black-listed task-trackers, time-outs, re-tries etc.

 Issue can be mitigated by submitting job with custom Hadoop parameters.",,acmurthy,amirhyoussefi,aw,cutting,dhruba,jaideep,johanoskarsson,lianhuiwang,nemon,tanjiaqi,viraj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-4179,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2008-08-15 02:08:34.837,,,false,,,,,,,,,,,,,,,,,,65385,,,,,Fri Jul 18 20:57:20 UTC 2014,,,,,,,"0|i0e73z:",80914,,,,,,,,,,,,,,,,,,,,,"15/Aug/08 00:37;amirhyoussefi;Proposed Solutions:

Hopefully in future, Hadoop can develop dynamic configuration capabilities. Given complexity of the issue it may take a long time to get there. 

Meanwhile, we can attack this problem from different angles or levels: 

 1) Having metrics: Providing understandable metrics on Web UI to raise user awareness. We can expand counters web page (or another page) to have more understandable and actionable metrics (e.g. a cluster utilization number) and more flow diagrams.
 2) Detecting issues: Have an agent to interpret logs then highlight issue or trigger a process. Example: A rule-based agent loads a set of exentsible rules and follows Hadoop logs. Applicable rule creates a message/highlight in UI or triggers a separate process.
 3) Notification: User gets notification (e.g. email) from a process triggered by rule-based agent above. This way, user doesn't need to be pinned to his monitor looking at web UI all the time.

Focus of this JIRA is development of rule-based agent of item 2 above which we call Mr Doctor (map-reduce doctor aka Hadoop Doctor). It simply processes Hadoop Logs and will be part of contrib. Mr Doctor will provide recommendations/prescriptions while following a live log of running process or postmortem logs. ","15/Aug/08 02:08;vgogate;1. I have an agent written in perl, which I call it as  ""Hadoop Performance Adviser"". It provides an extensible framework for evaluating the performance of a map/reduce job. It generates a report indicating potential problems affecting the job performance and the advice (if any) to take any corrective actions to rectify the problem.  

2. Framework is extensible in the sense, 
     -- it allows adding new entries to a pre-defined list of performance and cluster utilization hints  and framework evaluates them against job execution counters and configuration parameters parsed through log files.  
     -- Also the hint subroutines are written in such a fashion that more complex hints can be built using a boolean expression around existing set of hints.


I agree that there is a lot of potential in such tools that can help user (as well as a grid service provider) to get more targeted advice on the job efficiency.  ","15/Aug/08 02:24;amirhyoussefi;Sample issues we detect from task/job logs: 

 -  Shuffle

 -  Map Spill

 -  Lagging single reducer (un-even distribution from time or row count point of view)

and more... 

Runping brought up a good point on availability of data while process is running. In some installations, logs are gathered by HOD after job is finished. User needs to wait for a job to finish to see all logs. We can change logging process to some extent and improve availability of items in progressing Live Log. Task counters are available when each task is finished.

BTW, diagram in item 1 above refers to a progress diagram developed by Owen O'Malley. ","15/Aug/08 02:33;amirhyoussefi;More: 

 - Task initialization problems. As we speak I can see some tasks get stuck in initialization stage for 2 minutes or so. User can detect it by watching UI for a few hours but Mr. Doctor can improve on this for live/postmortem logs.

","15/Aug/08 04:06;amirhyoussefi;Also there is another path to gathering logs as the job runs, that is to use Chukwa. 

This way Chukwa can give us other metrics e.g. CPU utilization to be fed to Mr. Doctor.","18/Jul/14 20:57;aw;See also the work done by Duke on Starfish, Apache MRUnit, etc etc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobClient should work with -1/+1 version of JobTracker,MAPREDUCE-223,12402199,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Incomplete,,tucu00,tucu00,12/Aug/08 11:22,18/Jul/14 20:53,12/Jan/21 09:52,18/Jul/14 20:53,,,,,,,,,,,,,,,1,,,,,"Currently there is version check on the RPC calls that enforces the same Hadoop version on the client and the server.

To enable phased upgrades of systems using Hadoop and Hadoop itself the {{JobClient}} should be able to interact with a {{JobTracker}} of the previous and the next version of Hadoop (or with a range).",all,aw,cutting,omalley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2008-08-12 16:32:07.029,,,false,,,,,,,,,,,,,,,,,,148610,,,,,Fri Jul 18 20:53:44 UTC 2014,,,,,,,"0|i0iua7:",108000,,,,,,,,,,,,,,,,,,,,,"12/Aug/08 11:24;tucu00;This could be done by either allowing a range version of clients on the {{JobTracker}}, or having an HTTP transport with potentially reduced functionality (like the HDFS over HTTP).

","12/Aug/08 16:32;omalley;This requires more version tolerant rpc. The current version of rpc can't do it and so this is a big project.","12/Aug/08 16:37;tucu00;And doing it as my second option? over HTTP to isolate rpc versions as HDFS over HTTP","11/Aug/10 19:41;ramach;it has been sitting for over 2 years and I do not believe anything has changed. hdfs I believe provide read only interface for listing/retrieving data over http. 

Is this still critical  - to  have similar interface  to embedded JT http server on top of what the web interface already provides (for accessing task or job logs?)
","12/Aug/10 03:16;tucu00;Wasn't the idea that Avro would help fixing this?

Yes, doing things over HTTP (assuming you take care of not breaking things a payload level) works. 

Still Hadoop does not support HTTP natively for client side calls, so this is not option without add-on protocol adapter systems fronting JT and NN/DNs. In other words, a JT proxy and a HDFS proxy. 

FYI, Oozie is planning to provide JT proxy capabilities.
","18/Jul/14 20:53;aw;I need a state between stale and fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide accounting functionality for Hadoop resource manager,MAPREDUCE-534,12399748,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,yhemanth,yhemanth,yhemanth,07/Jul/08 17:52,18/Jul/14 17:46,12/Jan/21 09:52,18/Jul/14 17:46,,,,,,,,,,,,,,,0,,,,,"HADOOP-3421 describes requirements for a new resource manager in Hadoop to schedule Map/Reduce jobs. In production systems, it would be useful to produce accounting information related to the scheduling - such as job start and run times, resources used, etc. This information can be consumed by other systems to build accounting for shared resources. This JIRA is for tracking the requirements, approach and implementation for producing accounting information.",,amirhyoussefi,aw,dhruba,eyang,gurmukhd,hong.tang,lianhuiwang,macyang,mahadev,matei,matei@eecs.berkeley.edu,sameerp,vivekr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/08 20:50;matei@eecs.berkeley.edu;history-scripts-0.16.tgz;https://issues.apache.org/jira/secure/attachment/12392894/history-scripts-0.16.tgz","27/Oct/08 21:48;matei@eecs.berkeley.edu;history-scripts-0.18.tgz;https://issues.apache.org/jira/secure/attachment/12392896/history-scripts-0.18.tgz","22/Oct/08 22:47;matei@eecs.berkeley.edu;history-scripts.patch;https://issues.apache.org/jira/secure/attachment/12392682/history-scripts.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2008-10-17 19:17:26.008,,,false,,,,,,,,,,,,,,,,,,148870,,,,,Fri Jul 18 17:46:04 UTC 2014,,,,,,,"0|i0itxb:",107942,,,,,,,,,,,,,,,,,,,,,"17/Sep/08 03:38;yhemanth;This won't make 0.19. Moving out.","17/Oct/08 19:17;matei@eecs.berkeley.edu;A lot of this data is available in the job history logs written by the job tracker. At Facebook, we have a set of scripts that parse these logs and put the data in a MySQL database, where it can be queried. Would you be interested in something like that? Or do you want something more real-time, integrated into the Hadoop web UI?","20/Oct/08 03:38;yhemanth;Matei, the system we are currently using with HOD does something similar to what you're doing at Facebook. So, yes, we would definitely be interested to see what you have. Can you please provide this as a patch or something ?","22/Oct/08 22:47;matei@eecs.berkeley.edu;I've attached a patch with the scripts I built at Facebook. There is also a readme with an explanation of what they do. Basically there are two scripts: One that just parses the history logs and jobconfs and puts the exact same data into MySQL, creating tables of jobs, jobconf XML key-value pairs, tasks and task attempts, and then a second script that performs some joins on these tables to create a set of job summary reports with ~1 KB of data per job that can be used to run queries quickly for the purposes of visualization. You can build a visualization based on this database using your favorite tool. Unfortunately I can't open source the one used at Facebook because it depends on a lot of internal web libraries.","27/Oct/08 20:50;matei@eecs.berkeley.edu;Here is a set of scripts for Hadoop 0.16, for those interested. I am also currently investigating what needs to be done to get the scripts working on 0.18.","27/Oct/08 21:48;matei@eecs.berkeley.edu;Here are some modified scripts for 0.18 too. It looks like some naming in the history log changed slightly, unfortunately (e.g. task attempts being called task_someting instead of tip_something, or counters being written as key:value instead of key=value). Are you guys planning to finalize the history log format sometime? It would make it easier to do accounting after the fact using tools like this.","18/Jul/14 17:46;aw;The basics of this is in place and other projects such as LinkedIn's White Elephant do more.  Closing this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
want InputFormat for task logs,MAPREDUCE-212,12366497,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,,cutting,cutting,03/Apr/07 18:00,18/Jul/14 05:06,12/Jan/21 09:52,18/Jul/14 05:06,,,,,,,,,,,,,,,0,,,,,"We should provide an InputFormat implementation that includes all the task logs from a job. Folks should be able to do something like:

job = new JobConf();
job.setInputFormatClass(TaskLogInputFormat.class);
TaskLogInputFormat.setJobId(jobId);
...

Tasks should ideally be localized to the node that each log is on.

Examining logs should be as lightweight as possible, to facilitate debugging. It should not require a copy to HDFS. A faster debug loop is like a faster search engine: it makes people more productive. The sooner one can find that, e.g., most tasks failed with a NullPointerException on line 723, the better. ",,aw,bien,cutting,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/07 23:44;stack;hadoop1199-v2.patch;https://issues.apache.org/jira/secure/attachment/12355290/hadoop1199-v2.patch","06/Apr/07 18:05;stack;hadoop1199.patch;https://issues.apache.org/jira/secure/attachment/12355094/hadoop1199.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2007-04-06 18:05:17.563,,,false,,,,,,,,,,,,,,,,,,148600,,,,,Fri Jul 18 05:06:42 UTC 2014,,,,,,,"0|i0irvb:",107609,,,,,,,,,,,,,,,,,,,,,"06/Apr/07 18:05;stack;Here's a first cut at a TaskLogInputFormat.

+ The TaskLogInputFormat class when it runs will read all local userlogs associated with the configured jobid.  The number of splits is equal to the number of configured maptasks (Do folks have better ideas regards how to do the split? See the next item for problems with this scheme).
+ If there are no associated logs on the local host for the configured jobid, the task fails and will be scheduled, usually, elsewhere.  The 'elsewhere' may have already had a log analysis task run against it so logs can be double-counted.  This makes is so TaskLogInputFormat, as currently written, is inappropriate for precision reporting based off log output.
+ As currently written, it will read the content of the tasks' stdout, stderr or syslog subdirectory.  In other words, you must run a job per subdirectory.
+ It gives JobTracker#idFormat package rather than private access to avoid duplicating number formating.
+ It includes the patch attached to hadoop-1181 (I've resolved this issue as ""won't fix""). This patch opens access to TaskLog so it can be used outside of the mapred package.  It also makes it so TaskLog$Reader can take URLs to userlog subdirectories.  Folks need to be able to get to their logs.","10/Apr/07 23:44;stack;Version 2.  Keys, rather than LongWritable line numbers, are now a compound of host, taskid, and line number: e.g. debord.archive.org:task_0023_m_000000_0:11223.

","11/Apr/07 18:39;cutting;> The number of splits is equal to the number of configured maptasks (Do folks have better ideas regards how to do the split?

I was thinking that the InputFormat would read a config parameter to get a jobId, then use JobClient to query the jobtracker and get the URL for the task log of each task in that job, and package these URLs into the splits.  The 'getLocations()' implementation for these splits would return the hostname of the URL, so that attempts would be made to run the task on the host where the log resides.  Does that make sense?","18/Jul/14 05:06;aw;I'm going to close this as won't fix.  In some cases, Hadoop has slowly been moving log files over to avro.  In other cases, the logs are 3rd party/ecosystem controlled and could be in the format for that project.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Should be able to re-run jobs, collecting only missing output",MAPREDUCE-460,12343126,New Feature,Reopened,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,bpendleton,bpendleton,17/May/06 01:10,18/Jul/14 04:56,12/Jan/21 09:52,,,,,,,,,,mrv2,,,,,,0,,,,,"For jobs with no side effects (roughly == jobs with speculative execution enabled), if partial output has been generated, it should be possible to re-run the job, and fill in the missing pieces. I have now run the same job twice, once finishing 42 of 44 reduce tasks, another time finishing only 17. Each time, many nodes have failed, causing many many tasks to fail ( in one case, 5k failures from 15k map tasks, 23 failures from 44 reduces), but some valid output was generated. Since the output is only dependent on the input, and both jobs used the same input, I will now be able to combine these two failed task outputs to get a completed job's output. This should be something that can be more automatic.

In particular, it should be possible to resubmit a job, with a list of partitions that should be ignored. A special Combiner, or pre-Combiner, would throw out any map output for partitions that have already been successfully completed, thus reducing the amount of data that needs to be reduced to complete the job. It would, of course, be nice to support ""filling in"" existing outputs, rather than having to do a move operation on completed outputs.",,acmurthy,aw,devaraj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2006-05-17 04:32:55.0,,,false,,,,,,,,,,,,,,,,,,148815,,,,,Fri Jul 18 04:56:12 UTC 2014,,,,,,,"0|i0ir9b:",107510,,,,,,,,,,,,,,,,,,,,,"17/May/06 04:32;eric14;This is motivated by the same problem as HADOOP-91.  The suggested sollution is different.","01/Sep/06 18:13;bpendleton;This issue still plagues me. I have a large enough dataset, that processing through it and storing temporary outputs often causes many failures. I still have my local fix for re-running (very cumbersome). I'd love to see a general solution supported.

What I currently do is to inject a special combiner, that only produces output for partitions that didn't complete in a previous run. Thus, if 20% of my job finishes (completes writing reduce output to DFS), I need 20% less working space to run the subsequent job attempt. Usually, this is enough progress.

However, that suffers from a lot of limits:
1) There's no auto-magic storage of what partitions were generated.
2) This loses the ability to have a Combiner doing something else
3) It also requires a new output directory, and hand-reassembly of the completed tasks.

It seems like it should be solvable more generally by:
1) Storing the partitions that are generated when a job is initially run.
2) Keeping track of what partitions have completed.
3) Allowing a job to be re-submitted, using the previous partitions (and not failing on the output existance check)

Anyone else run into these issues? I'd be happy to write up how I actually work around the problem now, but it seems like a more general solution would be helpful to a wider audience.","16/Jan/12 09:13;qwertymaniac;This has gone stale, closing out. We can discuss how best to solve this in a new ticket now that MR2 is out.","17/Jan/12 03:53;acmurthy;This is easy to support in MR2, I think we should take a crack at this - sounds like a very useful feature.","18/Jul/14 04:56;aw;So is there actually another JIRA that is covering this and it just isn't linked to this one?  Is there really any reason to keep this and MAPREDUCE-443 open?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Random read benchmark for DFS,MAPREDUCE-2593,12387426,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,davet,rangadi,rangadi,30/Jan/08 01:05,17/Jul/14 20:18,12/Jan/21 09:52,,,,,,,,,,mrv2,,,,,,1,,,,,"We should have at least one  random read benchmark that can be run with rest of Hadoop benchmarks regularly.

Please provide benchmark  ideas or requirements.",,aah,apurtell,aw,carlos.valiente,cutting,davet,dhruba,eli,hammer,hong.tang,jbooth,jly,lianhuiwang,luoli,mholderba,nroberts,pfxuan,sanjay.radia,stack,tlipcon,tomwhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HDFS-641,,,,,,,,,,,,,,,,,,,,,"27/Jun/09 01:51;rangadi;HDFS-236.patch;https://issues.apache.org/jira/secure/attachment/12411975/HDFS-236.patch","10/Jun/11 15:30;davet;RndRead-TestDFSIO-061011.patch;https://issues.apache.org/jira/secure/attachment/12482070/RndRead-TestDFSIO-061011.patch","12/Dec/11 21:43;davet;RndRead-TestDFSIO-MR2593-trunk121211.patch;https://issues.apache.org/jira/secure/attachment/12507064/RndRead-TestDFSIO-MR2593-trunk121211.patch","23/May/11 19:37;davet;RndRead-TestDFSIO.patch;https://issues.apache.org/jira/secure/attachment/12480146/RndRead-TestDFSIO.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2008-01-30 19:49:21.856,,,false,,,,,,,,,,,,,,,,,,74,Reviewed,,,,Thu Jul 17 20:18:00 UTC 2014,,,,,,,"0|i0e7cf:",80952,Enhancement to TestDFSIO to do random read test.,,,,,,,,,,,,,,,,,,,,"30/Jan/08 19:49;cutting;TestSetFile tests random access.  The unit test creates a SetFile containing 10,000 RandomDatum instances, then seeks to sqrt(10,000)=100 random entries in that file and checks that the expected entry is found.

We could also run it on a MiniDFS cluster, specifying a small block size (~32kb?) to test random access on HDFS.

We might also specify block compression for a run, to better test seeks within a block-compressed SequenceFile.
","27/Jun/09 01:51;rangadi;
This patch adds random read test toTestDFSIO. The test provides quite a few options and as a result is larger than other mappers in in TestDFSIO. I moved the main implementation of this mapper into its own file. 

The patch currently includes a native java app that performs equivalent random reads on same set of physical files (more on this later). This gives a base line to asses HDFS overhead and helps us experiment different policies (e.g. does it help if we don't close the file after each read or not a start a new thread?).

A few changes in datanode are temporary. An option to skip the CRC file while serving data is added. This should ideally be a client option.

Instructions on how to run these tests is provided at the bottom of this comment.  See JavaDoc for RandomReadMapperImpl for various configuration parameters for random read test.

Concerns :
=========

     * How bad is HDFS random access?
        ** Random access in HDFS always seemed to have bad PR though hardly anyone used the interface. Claims/rumours range from ""transfers a lot of excess data"" (not true)  to ""we noticed it is 10 times slower than our non-hdfs app"" (hard to see how if the app is I/O bound and/or is doing at least semi random reads).
        ** It was good see HBase successfully used the interface for its speed up. It can not achieve competitive performance with out reasonable random access performance in HDFS (for HFile).

    * How important is connection caching for pread (position read)
        ** clearly it saves 1-2ms latency (more closer to 1ms) . Should not have effect on throughput or scalability with multiple readers.
         ** For many loads, this latency could be important. 10% latency reduction is 10% throughput increase with a 10ms seek. 

    * Do checksum files add noticeable overhead
         ** Each 64MB block has 0.5MB checksum file. Each read read a few bytes at the front and a few bytes at a random offset in the file.
         ** This could cost a seek or two doubling the seek load.
         ** The preliminary tests with 10GB data set show this is not the case.

     * Other HDFS specific issues :
         ** Each read opens and closes data and checksum files. Does it help to cache them?
         ** same with new thread for each read.

     * How well does random access scale?
         ** I have not done any tests on larger clusters. but plan to.
         ** I don't see any reason why it would not scale.

The results depend on hardware more than I thought. The numbers presented here are for a single node cluster with one spindle.

Environment : 
===========
        * Single node Hadoop cluster
        * Random access over 10 1GB files.
        * CPU : Dual core Opteron 2GHz, Memory : 4GB
        * Harddisk : 400 GB WD (WD4000YR-01P)
        * Kernel : 2.6.9-22.12 64bit (Based on RedHat kernel I think)
        * Kernel I/O scheduler : cfq

Preliminary results :
===============

All the tests are done with single map at a time. It is very important to set ""mapred.tasktracker.map.tasks.maximum"" to '1'. Single mapper is used to simplify interpretation of the results. The actual commands run are given below the results. Each of the 10 maps performs 500 random reads over one of the 10 files (there is an option to read over all the the 10 files). The results vary a bit over runs, usually the first run or first few access would be costlier since OS is still caching file block indexes. All the reads are for *4KB* of data.

|| Description of read || Time for each read in ms||
| 1000 native reads over block files | 09.5 |
| Random Read 10x500 | 10.8 |
| Random Read without CRC | 10.5 |
| Random Read with 'seek() and read()' | 12.5 |
| Read with sequential offsets | 01.7 |
| 1000 native reads without closing files | 07.5 |

Comments :
========== 
   * It was surprising to see with Native reads, not closing the files saves 2ms per read (it increases to 3ms with 5000 reads). So closing the file probably affects kernel caching in important ways. I didn't notice such difference on a similar machine with 4 disk hardware raid (both over 10GB of data made up of 160 64MB block files).

   * Effect of CRC reads is smaller than expected. This might be attributable to not so big range of 10GB. But such range might not be off from being practical. Even if the range increases, we could easily increase 'io.bytes.per.checksum' to something large (4kB or 16kB). Tests show latency does not increase noticeably until 64KB or so.
       ** implies inline CRCs or caching of CRC data in datanode is probably not immediately required.

   * Reads with sequential offsets is a good indicator of all the overhead other than the hard disk seeks.

TODO :
======
      * Tests on larger clusters
      * larger data set
      * We would very much appreciate running the tests on realistic data sets and different environments

Running the tests :
===============
The following commands are used to run the tests. comments are inline.

{noformat}
# TestDFSIO:
# ==========
# first start the cluster :
$ bin/start-dfs.sh
$ bin/start-mapred.sh

# write 10 files 
$ bin/hadoop jar build/hadoop-0.21.0-dev-test.jar TestDFSIO -write -nrFiles 10 -fileSize 1024

# Run random read test :
$ bin/hadoop jar build/hadoop-0.21.0-dev-test.jar TestDFSIO -randomread -nrFiles 10 -fileSize 1024

#examples of options : Perform 1000 reads per map
$ bin/hadoop jar build/hadoop-0.21.0-dev-test.jar TestDFSIO -Dtest.io.randomread.num.reads=1000 -randomread -nrFiles 10 -fileSize 1024 

#Make each map iterate over all the 10 files instead of one per map
$ bin/hadoop jar build/hadoop-0.21.0-dev-test.jar TestDFSIO -Dtest.io.randomread.num.files=10 -randomread -nrFiles 10 -fileSize 1024 

# To test without crc files, set ""dfs.datanode.skip.crc"" to true and restart the datanode.

# Native FS Reader :
#===============

# to create the jar :
$jar -cvf build/randomread.jar -C build/examples build/examples/org/apache/hadoop/examples/NativeFSRandomReader*.class
# run 
$bin/hadoop jar build/randomread.jar org.apache.hadoop.examples.NativeFSRandomReader -i tmp/test-blocks -n 1000

# ""tmp/test-blocks"" contains hard links to all the blocks in datanode directory.

# run without any options for help on command line options.

{noformat}

 



","23/May/11 19:37;davet;I've taken Raghu's patch from 6/27/09 with random read TestDFSIO enhancement, and ported it to the latest (now mapreduce) trunk 5/4/11 svn rev 1099590.   Patch attached RndRead-TestDFSIO.patch.

enjoy,
Dave","09/Jun/11 14:29;kihwal;* Some test.io.randomread.* seem to deserve a spot in command line args. 
* The buffer size can be used as the read size in random reads. I see no reason to separate the two in the random read mode.
* The default behavior is, one random reader operates on just one file out of N files. Since it already has ability to limit the number of files that each reader can access, it might be better to make it work on all N files by default.
","09/Jun/11 18:13;stack;Reread Raghu's comments above.  Its (still) great.
","10/Jun/11 15:22;davet;I agree.   I've taken Raghu's random read tests, ported to trunk (above), and tied in the config params to command line (Kihwal's comments).   Submitting patch now.","10/Jun/11 15:42;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12482070/RndRead-TestDFSIO-061011.patch
  against trunk revision 1134170.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/764//console

This message is automatically generated.","10/Jun/11 16:35;davet;Hmmm... It looks like Jenkins is attempting to apply this patch to the HDFS trunk, a reasonable assumption given this is an HDFS Jira, and was the correct place when Raghu created this bug.   Though TestDFSIO has since been moved into the mapreduce trunk.","10/Jun/11 16:56;stack;How hard to pull it around to hdfs Dave?","10/Jun/11 20:10;davet;Not sure just yet.  TestDFSIO is a mapreduce program.    I assume a few had some opinions on the matter of location when they moved the whole mr test directory out of hdfs and into mapreduce.  Perhaps if it is to be moved back, that could be handled as a separate Jira.","15/Jun/11 00:44;shv;This was done to avoid circular project dependencies. Still valid. I tried to kick in the build. But it is trying to find mapreduce under ""http://svn.apache.org/repos/asf/hadoop/mapreduce/trunk"", which is not there anymore.","15/Jun/11 00:51;tlipcon;Sorry, I must have missed the MR precommit when I updated the hudson jobs. I just fixed it and will kick this build again.","15/Jun/11 03:42;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12482070/RndRead-TestDFSIO-061011.patch
  against trunk revision 1135462.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:
                  org.apache.hadoop.cli.TestMRCLI
                  org.apache.hadoop.fs.TestFileSystem

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/395//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/395//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/395//console

This message is automatically generated.","16/Jun/11 23:36;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12482070/RndRead-TestDFSIO-061011.patch
  against trunk revision 1136261.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:
                  org.apache.hadoop.cli.TestMRCLI
                  org.apache.hadoop.fs.TestFileSystem

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/400//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/400//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/400//console

This message is automatically generated.","01/Jul/11 18:46;davet;Jenkins appears to be triggering a failure because ""delete a file in archive"" and ""rename a file in archive"" unit tests are failing, despite that those test have nothing to do with this patch.

","12/Dec/11 21:43;davet;Updated path names for trunk 0.23 (12/12/11).  Otherwise, it's the same patch. ","12/Dec/11 22:24;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12507064/RndRead-TestDFSIO-MR2593-trunk121211.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in .

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1425//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1425//console

This message is automatically generated.","17/Jul/14 20:04;aw;Ping!","17/Jul/14 20:16;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12507064/RndRead-TestDFSIO-MR2593-trunk121211.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4751//console

This message is automatically generated.","17/Jul/14 20:18;aw;Ha. Cancelling the patch so jenkins isn't freaked out.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
limit running tasks per job,MAPREDUCE-224,12385987,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,,cutting,cutting,10/Jan/08 17:47,17/Jul/14 19:38,12/Jan/21 09:52,17/Jul/14 19:38,,,,,,,,,,,,,,,0,,,,,"It should be possible to specify a limit to the number of tasks per job permitted to run simultaneously.  If, for example, you have a cluster of 50 nodes, with 100 map task slots and 100 reduce task slots, and the configured limit is 25 simultaneous tasks/job, then four or more jobs will be able to run at a time.  This will permit short jobs to pass longer-running jobs.  This also avoids some problems we've seen with HOD, where nodes are underutilized in their tail, and it should permit improved input locality.

",,acmurthy,aw,cwensel,dhruba,hammer,jaideep,l0s,martind,tdunning@veoh.com,viper799,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2008-01-10 19:30:28.829,,,false,,,,,,,,,,,,,,,,,,148611,,,,,Thu Jul 17 19:38:20 UTC 2014,,,,,,,"0|i0iswn:",107777,,,,,,,,,,,,,,,,,,,,,"10/Jan/08 17:49;cutting;This addresses issues raised in HADOOP-2510.","10/Jan/08 19:30;acmurthy;I'd like to throw *job priority* into this festering pool...

At least changing the job-priority (done by the cluster-admin) should  in a change in number of max_slots... thoughts?

","10/Jan/08 19:41;cutting;Some discussion of this issue may be found at:

http://www.nabble.com/question-about-file-glob-in-hadoop-0.15-tt14702242.html#a14741794
","10/Jan/08 19:48;cutting;I think a static limit for all jobs would be useful and best to implement first.  After some experience with this, we would be better able to address its shortcomings.  Possible future extensions might be:
- dynamically altering the limit, e.g., limit=max(min.tasks.per.job, numSlots/numJobsOutstanding)
 -- ramping up the limit slowly, so that a users's sequential jobs don't have all their slots immediately taken when one job completes
 -- ramping down the limit slowly, so that tasks are given an opportunity to finish normally before they are killed.
- incorporating job priority into the limit
","10/Jan/08 19:52;tdunning@veoh.com;(oops... yes, doug anticipated this in his comment and I didn't read very well)

Presumably the limit could be made dynamic.  The limit could be max(static_limit, number of cores in cluster / # active jobs) 


On further reflection, I should note that my big jobs are all limited in pretty much the way that Doug suggests because they are processing a few large files that are unsplittable.  This limits the number of slots these big jobs can eat up.

The result is pretty OK.  My little jobs with lots of maps can slide through the cracks most of the time and everything runs pretty well.

","10/Jan/08 19:55;cutting;> The limit could be max(static_limit, number of cores in cluster / # active jobs)

Jinx!","22/May/08 06:30;un_brice;The fix for bug 3412 also fix this one","03/Jun/09 08:36;tomwhite;I think this is covered by HADOOP-5170. If so, we can close this issue as a duplicate.","17/Jul/14 19:38;aw;I'm going to close this out as a duplicate of MAPREDEUCE-5583.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Improve facilities for job-control, job-queues etc.",MAPREDUCE-215,12385108,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,,acmurthy,acmurthy,21/Dec/07 21:20,17/Jul/14 19:19,12/Jan/21 09:52,17/Jul/14 19:19,,,,,,,,,,,,,,,0,,,,,"Today, Map-Reduce has _some_ support for job-control - basically JobClient provides a facility to monitor jobs, one can setup a job-ending notification and there is {{JobControl}}.

Links:
http://lucene.apache.org/hadoop/docs/r0.15.1/mapred_tutorial.html#Job+Control
http://lucene.apache.org/hadoop/docs/r0.15.1/mapred_tutorial.html#JobControl

Looks like users could do more with better facilities for job-control and maybe more advanced features like job-queues etc.

Lets discuss... ",,aw,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PIG-195,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2008-04-03 11:29:06.056,,,false,,,,,,,,,,,,,,,,,,148603,,,,,Thu Jul 17 19:19:08 UTC 2014,,,,,,,"0|i0istb:",107762,,,,,,,,,,,,,,,,,,,,,"03/Apr/08 11:29;amar_kamat;There should be some way to pass a *DAG* like structure. Look at the DAG node as a configuration. So each node will represent a conf file. Users can generate conf files, arrange then in a DAG and pass it to the {{JobControl}}. User should specify just the input folder(s) and the output folder(s) and JC should generate/manage in-between output folders. The users can provide hints if the intermediate job outputs are desired or not. Also some level of fault-tolerance feature should be provided so that in case of some failures the _meta_ job should complete. ","08/Apr/08 08:08;acmurthy;More features:

 * Ability to submit and monitor more than one job in a parallel-manner (independent sub-graphs in a DAG)
 * Ability to have custom notifications at various states
 * Features to re-submit jobs on failures
 * Ability to 'cron' jobs
 * Allow users to track 'progress' of the DAG
 * Allow users to submit templatized configs (same set of jobs run daily)
 * Heh, fancy UI ;-)","08/Apr/08 13:01;pi_song;A newbie question.
How do you chain up input output files between jobs? By manipulating input/output in JobConf manually?","08/Apr/08 18:38;amar_kamat;bq. How do you chain up input output files between jobs? By manipulating input/output in JobConf manually?
Yes.
","14/Apr/08 18:46;amar_kamat;- Facility to re-execute a subgraph. This will fasten the process of incorporating dynamic config changes since only the related nodes/jobs will be re-run with the changed environment.","25/Apr/08 12:24;shravanmn;How do I query for the progress of a jobcontrol object that has been submitted?","07/Apr/09 04:17;hammer;With HOD, the Capacity Scheduler, and the Fair Scheduler, can we close this ticket now?","17/Jul/14 19:19;aw;I'm going to close as fixed for a variety of reasons:

- some of these features are now native to Hadoop
- some of these features are now part of Oozie, Azkaban, etc
- some of these features are part of Tez

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide a command line option to check if a Hadoop jobtracker is idle,MAPREDUCE-229,12383097,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,,yhemanth,yhemanth,23/Nov/07 05:04,17/Jul/14 18:42,12/Jan/21 09:52,17/Jul/14 18:42,,,,,,,,,,,,,,,0,,,,,"This is an RFE for providing a way to determine from the hadoop command line whether a jobtracker is idle. One possibility is to have something like hadoop jobtracker -idle <time>. Hadoop would return true (maybe via some stdout output) if the jobtracker had no work to do (jobs running / prepared) since <time> seconds, false otherwise.

This would be useful for management / provisioning systems like Hadoop-On-Demand [HADOOP-1301], which can then deallocate the idle, provisioned clusters automatically, and release resources.",,aw,enis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2007-11-25 05:55:01.396,,,false,,,,,,,,,,,,,,,,,,148614,,,,,Thu Jul 17 18:42:08 UTC 2014,,,,,,,"0|i0isn3:",107734,,,,,,,,,,,,,,,,,,,,,"25/Nov/07 05:55;omalley;Another approach to this would be to have an admin command for the job tracker that would have the job tracker stop accepting new jobs and shut itself down when the last job completed. That way the external process wouldn't need to continually poll the job tracker to see if it was done yet. Thoughts?","26/Nov/07 08:19;enis;bq. Another approach to this would be to have an admin command for the job tracker that would have the job tracker stop accepting new jobs and shut itself down when the last job completed.
I remember this was discussed before but never implemented. I think this and a command to gracefully stop the jt would be great. ","27/Nov/07 18:08;bien;It's possible to do this through the JobClient API currently.","28/Nov/07 03:57;yhemanth;bq. Another approach to this would be to have an admin command for the job tracker that would have the job tracker stop accepting new jobs and shut itself down when the last job completed. That way the external process wouldn't need to continually poll the job tracker to see if it was done yet. Thoughts?

The requirement we have in Hadoop On Demand is to shutdown a job tracker when it has not run any jobs for a period of time. This is to help in releasing unused resources, and reclaim them. The approach suggested above actually makes sense if we want to run the job tracker for a period of time and then shutdown. This seems to be different from what we need, right ? 

The problem of continous polling does exist. But I feel it may not be too intensive. Any alternative that does not have this problem, but solves the use-case would also be great.","28/Nov/07 17:50;acmurthy;The other other issue is see with the JT shutting itself down is that the TTs (and HoD) don't know why that happened. If we just implement the *isidle <for_this_long>*, we could let HoD make the decision to shut down the entire cluster via the usual scripts... thoughts?","28/Nov/07 17:56;acmurthy;Let me clarify, I agree 'graceful shutdown' initiated by the JT is the _right_ solution. 

However, the command, as described here, is a good interim solution IMHO.","17/Jul/14 18:42;aw;I'm going to close this as fixed.

Clients can ask the JT if they are currently busy.  By doing this periodically, they can build an idle time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job Tracker needs to collect more job/task execution stats and save them to DFS file,MAPREDUCE-2833,12379125,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,,runping,runping,26/Sep/07 18:28,17/Jul/14 18:01,12/Jan/21 09:52,17/Jul/14 18:01,,,,,,,,,,,,,,,0,newbie,,,,"
In order to facilitate offline analysis on the dynamic behaviors and performance characterics of map/reduce jobs, 
we need the job tracker to collect some data about jobs and save them to DFS files. Some data are  in time series form, 
and some are not.
Below is a preliminary list of desired data. Some of them are already available in the current job trackers. Some are new.

For each map/reduce job, we need the following non time series data:
   1. jobid, jobname,  number of mappers, number of reducers, start time, end time, end of mapper phase
   2. Average (median, min, max) of successful mapper execution time, input/output records/bytes
   3. Average (median, min, max) of uncessful mapper execution time, input/output records/bytes
   4.Total mapper retries,  max, average number of re-tries per mapper
   5. The reasons for mapper task fails.

   6. Average (median, min, max) of successful reducer execution time, input/output reocrds/bytes
           Execution time is the difference between the sort end time and the task end time
   7. Average (median, min, max) of successful copy time (from the mapper phase end time  to the sort start time).
   8. Average (median, min, max) of successful sorting time for successful reducers

   9. Average (median, min, max) of unsuccessful reducer execution time (from the end of mapper phase or the start of the task, 
       whichever later, to the end of task)
   10. Total reducer retries,  max, average number of per reducer retries
   11. The reasons for reducer task fails (user code error, lost tracker, failed to write to DFS, etc.)

For each map/reduce job, we collect the following  time series data (with one minute interval):

    1. Numbers of pending mappers, reducers
    2. Number of running mappers, reducers

For the job tracker, we need the following data:

    1. Number of trackers 
    2. Start time 
    3. End time 
    4. The list of map reduce jobs (their ids, starttime/endtime)
    
The following time series data (with one minute interval):
    1. The number of running jobs
    2. The numbers of running mappers/reducers
    3. The number pending mappers/reducers 


The data collection should be optional. That is, a job tracker can turn off such data collection, and 
in that case, it should not pay the cost.

The job tracker should organize the in memory version of the collected data in such a way that:
1. it does not consume excessive amount of memory
2. the data may be suitable for presenting through the Web status pages.

The data saved on DFS files should be in hadoop record format.


",,aw,raviteja,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-07-17 18:01:26.99,,,false,,,,,,,,,,,,,,,,,,65341,,,,,Thu Jul 17 18:01:26 UTC 2014,,,,,,,"0|i0e72f:",80907,,,,,,,,,,,,,,,,,,,,,"27/Sep/07 21:35;runping;Just realized that hadoop job tracker already creates one history file per map/reduce job.
Most of the data this Jira requested can be re-generated from there.
","17/Jul/14 18:01;aw;I'm closing this as fixed since the history files pretty much over this request.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for metrics aggregation module in JobTracker,MAPREDUCE-209,12378598,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,,senthil,senthil,18/Sep/07 20:58,17/Jul/14 17:58,12/Jan/21 09:52,17/Jul/14 17:58,,,,,,,,,,,,,,,0,,,,,"JobTracker should support starting up and shutting down a generic metrics aggregation module. We are currently thinking about plugging in a module that gets time series data from the task trackers, aggregates it and log this data into a global DFS so that it is analysed later (even after the map reduce cluster is shutdown). Some of this data can also be plotted on the JobTracker UI in realtime. This is particulary useful for analyzing data from dynamic mapreduce cluster like the ones deployed using HOD.",,aw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-07-17 17:58:33.043,,,false,,,,,,,,,,,,,,,,,,148598,,,,,Thu Jul 17 17:58:33 UTC 2014,,,,,,,"0|i0isc7:",107685,,,,,,,,,,,,,,,,,,,,,"17/Jul/14 17:58;aw;Closing this as stale.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Making reduce tasks locality-aware,MAPREDUCE-2038,12472748,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,hong.tang,hong.tang,27/Aug/10 21:36,17/Jul/14 17:37,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"Currently Hadoop MapReduce framework does not take into consideration of data locality when it decides to launch reduce tasks. There are several cases where it could become sub-optimal.
- The map output data for a particular reduce task are not distributed evenly across different racks. This could happen when the job does not have many maps, or when there is heavy skew in map output data.
- A reduce task may need to access some side file (e.g. Pig fragmented join, or incremental merge of unsorted smaller dataset with an already sorted large dataset). It'd be useful to place reduce tasks based on the location of the side files they need to access.

This jira is created for the purpose of soliciting ideas on how we can make it better.",,aah,ashutoshc,atm,aw,cdouglas,ddas,dms,gates,hammer,jdonofrio,junping_du,liangly,lianhuiwang,liyuanera,Naganarasimha,nemon,omalley,philip,romainr,sarutak,sayali_dehedkar,srivas,tlipcon,yanz,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-259,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-08-27 21:43:02.684,,,false,,,,,,,,,,,,,,,,,,149950,,,,,Fri Jul 19 19:08:29 UTC 2013,,,,,,,"0|i0e85z:",81085,,,,,,,,,,,,,,,,,,,,,"27/Aug/10 21:43;tlipcon;This is a very interesting direction. We have another use case for HBase bulk loads, where we know that a given reducer partition is going to end up on a particular region server (often colocated with a TT). Scheduling the reducer to run on the same node or rack will ensure a local replica of the HFile when it comes time to serve it.

Another interesting use case is for aggregation queries where we can make use of something like a ""rack combiner"". We can simply implement a Partitioner that returns the rack index of the mapper, and then schedule that reduce task on the same rack. Thus we end up with a result set per rack, and can do a second small job to recombine those. This is not unlike the multilevel query execution trees used in Dremel - I imagine Hive and Pig's query planners could make use of plenty of techniques like this.","27/Aug/10 21:59;hong.tang;bq. This is a very interesting direction. We have another use case for HBase bulk loads, where we know that a given reducer partition is going to end up on a particular region server (often colocated with a TT). Scheduling the reducer to run on the same node or rack will ensure a local replica of the HFile when it comes time to serve it.

I believe this is similar to the second usage case I described.

bq. Another interesting use case is for aggregation queries where we can make use of something like a ""rack combiner"". We can simply implement a Partitioner that returns the rack index of the mapper, and then schedule that reduce task on the same rack. Thus we end up with a result set per rack, and can do a second small job to recombine those. This is not unlike the multilevel query execution trees used in Dremel - I imagine Hive and Pig's query planners could make use of plenty of techniques like this.

Do you mean that for aggregation operations that would reduce data-volume along the way, so you want to do a hierarchical approach? (So the real hierarchy would be: inside-the-map-combiner, same host-multiple-map-combining, inside-the-rack reduction, and cross-rack reduction).","27/Aug/10 22:20;tlipcon;bq. Do you mean that for aggregation operations that would reduce data-volume along the way, so you want to do a hierarchical approach

Yep, that's the basic idea. Implementing rack-combiners as a first class concept would be neat, but the point above is that we can ""fake"" it if we have locality for reducers, with a lot less work. I don't know if it would have a huge performance improvement, but we could experiment with it easily given this feature.","27/Aug/10 22:25;hong.tang;bq. Yep, that's the basic idea. Implementing rack-combiners as a first class concept would be neat, but the point above is that we can ""fake"" it if we have locality for reducers, with a lot less work. I don't know if it would have a huge performance improvement, but we could experiment with it easily given this feature.

Makes sense to me.","12/Sep/12 06:00;sayali_dehedkar;There is one paper in Cloud Computing Technology and Science (CloudCom), 2011. Link is http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6133196&contentType=Conference+Publications&queryText%3DHadoop-0.20.2. Can we incorporate this idea?
","19/Jul/13 19:08;yanz;For the HBase bulk load case, it actually also applies in a more generic way to many, if not all, distributed standing services like Tez, which are becoming popular in the current trend of low latency or interactive activities. Another factor is the coming HDFS caching. Having the locality-aware reducer scheduling is expected to have big impacts on performances of those ""single-shot"" map/reduce jobs. In general DAG executions will benefit but the executor scheduling and output replication mechanism, if any, is expected to be more complex.

The 2011 paper seems to maximize locality likelihood in reducer scheduling based upon cluster physical topology, which might be well targeting the above ""rack combiner"" case. But for a DAG execution, some ""external"" info related to that execution has to be provided for scheduling.

In the cases of a HBase bulk loader, or a downstream standing executor, a hint in the Partitioner could be heeded for scheduling reducers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
adjustable task priority,MAPREDUCE-4058,12547726,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,mwagner,aw,aw,23/Mar/12 01:07,17/Jul/14 15:05,12/Jan/21 09:52,17/Jul/14 15:05,1.0.0,,,,,,,,task-controller,,,,,,0,,,,,"For those of us that completely destroy our CPUs, it is beneficial to be able to run user tasks at a different priority than the tasktracker. This would allow for TTs (and by extension, DNs) to get more CPU clock cycles so that things like heartbeats don't disappear.",,anthonyr,atm,aw,cdouglas,eli2,jayf,jghoman,jlowe,kasha,kkambatl,knoguchi,qwertymaniac,revans2,stevel@apache.org,tgraves,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,YARN-443,,,,,,,,,,,,,,,,,,,,,"23/Mar/12 01:09;aw;MAPREDUCE-4058-branch-1.0.patch;https://issues.apache.org/jira/secure/attachment/12519567/MAPREDUCE-4058-branch-1.0.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-03-26 08:57:59.424,,,false,,,,,,,,,,,,,,,,,,232823,,,,,Thu Jul 17 15:05:38 UTC 2014,,,,,,,"0|i0e5vr:",80715,,,,,,,,,,,,,,,,,,,,,"23/Mar/12 01:09;aw;This patch adds a variable to the task-controller.cfg called 'task.niceness.adjustment' that acts similarly to renice. Note that setpriority() is called while still root, allowing for the tasks to be set to a HIGHER priority than the TT, if someone so desires to.","26/Mar/12 08:57;stevel@apache.org;seems a reasonable patch as there's no obvious cost to running the TT at a higher level than tasks. Running the DN at a higher level could give it priority over the tasks, which may hurt their throughput when the DN was under heavy external load -but does that matter?

# patch is --git not --no-prefix so jenkins can't apply
# the jenkins runs won't test this anyway; would visual review suffice, or could the tests be added to the bigtop test suite?","17/Jul/14 15:05;aw;Y! was able to get a different patch into 0.23 and 2.x that provides similar (if limited) capability.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kill task attempts longer than a configured queue max time,MAPREDUCE-4085,12548770,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,,aw,aw,30/Mar/12 01:25,17/Jul/14 15:04,12/Jan/21 09:52,17/Jul/14 15:04,,,,,,,,,task,,,,,,0,,,,,"For some environments, it is desirable to have certain queues have an SLA with regards to task turnover.  (i.e., a slot will be free in X minutes and scheduled to the appropriate job)  Queues should have a 'task time limit' that would cause task attempts over this time to be killed. This leaves open the possibility that if the task was on a bad node, it could still be rescheduled up to max.task.attempt times.",,aah,acmurthy,aw,cdouglas,devaraj,jghoman,rajesh.balamohan,revans2,tgraves,wind5shy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/May/13 00:52;aw;MAPREDUCE-4085-branch-1.0.4.txt;https://issues.apache.org/jira/secure/attachment/12583595/MAPREDUCE-4085-branch-1.0.4.txt","01/May/12 01:10;aw;MAPREDUCE-4085-branch-1.0.txt;https://issues.apache.org/jira/secure/attachment/12525147/MAPREDUCE-4085-branch-1.0.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2012-03-30 14:44:46.528,,,false,,,,,,,,,,,,,,,,,,233867,,,,,Fri May 17 00:52:28 UTC 2013,,,,,,,"0|i0e5un:",80710,,,,,,,,,,,,,,,,,,,,,"30/Mar/12 02:37;aw;I've talked with a few folks over this idea and even have a somewhat hacky patch that ""works"".... but it is not in a sharable state.  

The way I've written it is that the TaskTracker has code similar to the progress checker that compares task attempt run time vs. mapred.queue.(queuename).task-time-limit.  If it is over this time, it kills the attempt, logs an error and moves on.

The problem comes in the form of how to get the queue time limit to the TaskTracker in a scalable, secure way. The TaskTracker appears to read the uploaded jobconf directly without it going through any modification.  The JobTracker appears to do most of the job vetting prior to scheduling the tasks.  Since this is a queue variable, the JT should ideally be the one that 'owns' the variable.  This also allows for easy mradmin refresh functionality.

Some of the ideas that have been bounced around:

* JT rewrites the JobConf file prior to scheduling
* TT opens a connection to a jsp on the JT, fetches the info, stores it into a local TT cache w/TTL
* TT uses a InterTrackerProtocol to ask the JT, stores it into a local TT cache w/TTL
* JT passes the info along with the heartbeat response

I'd like to have some discussion on some of these ideas to see what folks think is the most viable.","30/Mar/12 14:44;revans2;I can see the need for something like this, to ensure that new jobs can run and meet their SLAs, but I think it would be better to have it be part of a preemption like mechanism, where we let the tasks run until there is some other Task/Container(for MRv2) that is requested.  Once there is a need for those resources iff the current task/container has gone over the configured limit the JT/RM, on the next heartbeat, can inform the TT/NM to kill the task/container.  The fair scheduler already supports preemption and perhaps this could be added there.

MAPREDUCE-3938 was filed to add preemption to the Capacity Scheduler for 2.0 and it might be good to add this in as part of the design there.

I don't really like the idea of having a hard limit on the runtime.  What is more if there is a hard limit on how long a task can run for I see very little benefit in having it rescheduled more then once.  If it was a slow node, then OK we can pick another node and it might finish in time, but unless the cluster is very heterogeneous the task is just going to run to the maximum time limit 4 times and then the Job will be failed.  ","30/Mar/12 15:21;aw;It seems very wrong to me to include this functionality in a specific scheduler.

The other thing to keep in mind is that at least in our use case, we want to punish jobs in that are essentially in the wrong queue.  Pre-emption works out to be the incorrect action in this case; we ultimately do want to the job to be failed as negative feedback to the user that they did something wrong, especially for primarily ad hoc usage (or, as you said, very heterogeneous).","01/May/12 01:10;aw;Here's a code dump of what I'm currently working with on my test setup.  I have a slightly different version running in front of users that is working well, but lacks the RPC support this one provides.  Many thanks to those of you who have given some guidance on this...","17/May/13 00:52;aw;Here's an updated version for anyone who wants it.  This one also includes the ability for users to set a smaller task time limit (mapred.job.{map|reduce}.task-wallclock-limit) in case they want something faster. i.e., ""I know my task should finish in 5 minutes, so kill it if it doesn't"".  Of course, the queue time out will still kick in if the user provided time is longer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job Counters - ability to perform calculations other than sum,MAPREDUCE-5394,12657916,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,bunty sharma,sheetal_dolas,sheetal_dolas,16/Jul/13 01:55,02/Jun/14 05:32,12/Jan/21 09:52,,1.2.0,2.1.0-beta,,,,,,,,,,,,,0,,,,,"Many a times the applications need to report values as more than sums across tasks. Such as min/max/avg of certain metric or metric grouped by nodes/racks.

Having this kind of ability in counters will greatly enhance the utility of counters.",,bunty sharma,ozawa,sarutak,sheetal_dolas,yvesbastos,zhangmuhua,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-07-21 15:38:56.614,,,false,,,,,,,,,,,,,,,,,,338110,,,,,Sun Jul 21 15:38:56 UTC 2013,,,,,,,"0|i1mbgv:",338431,,,,,,,,,,,,,,,,,,,,,"21/Jul/13 15:38;zhangmuhua;this is a good idea.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop 2.4 Java execution issue: remotely submission jobs fail ,MAPREDUCE-5901,12715838,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,,MicCar,MicCar,21/May/14 19:31,21/May/14 22:39,12/Jan/21 09:52,21/May/14 22:39,,,,,,,,,,,,,,,0,,,,,"I have installed Hadoop 2.4 on remote machine in Single-Mode setting. From another machine (client) I run a Java application that submit a job to a remote Hadoop machine (cluster), I have used the attached code. The problem is that the real execution of the map process is run on my local machine (client) not on the cluster machine.
JobConf job = new JobConf(SOF.class);
job.setJobName(""SIM-""+sim_id);
System.setProperty(""HADOOP_USER_NAME"", ""hadoop"");
FileInputFormat.addInputPath(job,new Path(""hdfs://cluster_ip:port""+USERS_HOME+user+""/SIM-""+sim_id+""/""+INPUT_FOLDER_HOME+""/input.tmp"")/*new_inputs_path*/);
FileOutputFormat.setOutputPath(job, new Path(""hdfs://cluster_ip:port""+USERS_HOME+user+""/SIM-""+sim_id+""/""+OUTPUT_FOLDER_HOME));
job.set(""jar.work.directory"", ""hdfs://cluster_ip:port""+SOF.USERS_HOME+user+""/SIM-""+sim_id+""/flockers.jar"");
job.setMapperClass(Mapper.class);
job.setReducerClass(Reducer.class);
job.setOutputKeyClass(org.apache.hadoop.io.Text.class);
job.setOutputValueClass(org.apache.hadoop.io.Text.class);

job.set(""mapred.job.tracker"", ""cluster_ip:port"");
job.set(""fs.default.name"", ""hdfs://cluster_ip:port"");
 job.set(""hadoop.job.ugi"", ""hadoop,hadoop"");
job.set(""user"", ""hadoop"");       
        try {
            JobClient jobc=new JobClient(job);
            System.out.println(jobc+"" ""+job);
            RunningJob runjob;
            runjob = jobc.submitJob(job);
            System.out.println(runjob);
            System.out.println(""VM ""+Inet4Address.getLocalHost());
            while(runjob.getJobStatus().equals(JobStatus.SUCCEEDED)){}
        } catch (Exception e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        } 
    }
I have tried to set up correctly hadoop using the following mapred-site.xml:
<configuration>
     <property>
         <name>mapred.job.tracker</name>
         <value>cluster_ip:port</value>
     </property>
 <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>","java, hadoop v. 2.4 ",MicCar,vinodkv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-05-21 22:39:57.978,,,false,,,,,,,,,,,,,,,,,,394123,,,,,Wed May 21 22:39:57 UTC 2014,,,,,,,"0|i1vv07:",394261,,,,,,,,,,,hadoop2.4 mapreduce java,,,,,,,,,,"21/May/14 22:39;vinodkv;Folks have reported working of multiple node clusters.

In any case, please use the user mailing lists for debugging such issues. JIRA is for tracking bugs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backport DistCpV2 and the related JIRAs to branch-1,MAPREDUCE-5081,12637719,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,szetszwo,szetszwo,szetszwo,19/Mar/13 08:18,13/May/14 15:17,12/Jan/21 09:52,28/Mar/13 05:03,,,,,,1.2.0,,,distcp,,,,,,0,,,,,"Here is a list of DistCpV2 JIRAs:
- MAPREDUCE-2765: DistCpV2 main jira
- HADOOP-8703: turn CRC checking off for 0 byte size 
- HDFS-3054: distcp -skipcrccheck has no effect.
- HADOOP-8431: Running distcp without args throws IllegalArgumentException
- HADOOP-8775: non-positive value to -bandwidth
- MAPREDUCE-4654: TestDistCp is ignored
- HADOOP-9022: distcp fails to copy file if -m 0 specified
- HADOOP-9025: TestCopyListing failing
- MAPREDUCE-5075: DistCp leaks input file handles
- distcp part of HADOOP-8341: Fix findbugs issues in hadoop-tools
- MAPREDUCE-5014: custom CopyListing
",,cnauroth,jingzhao,mahadev,mattf,qwertymaniac,sanjay.radia,sureshms,svenkat,szetszwo,tychang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-2765,,,,,,,,,,,,,,,,,,,,,"28/Mar/13 01:32;szetszwo;DistCp.java.diff;https://issues.apache.org/jira/secure/attachment/12575816/DistCp.java.diff","28/Mar/13 02:21;szetszwo;m5081_20130328.patch;https://issues.apache.org/jira/secure/attachment/12575819/m5081_20130328.patch","28/Mar/13 04:46;szetszwo;m5081_20130328b.patch;https://issues.apache.org/jira/secure/attachment/12575828/m5081_20130328b.patch","21/Mar/13 08:19;szetszwo;m5981_20130321.patch;https://issues.apache.org/jira/secure/attachment/12574748/m5981_20130321.patch","21/Mar/13 11:29;szetszwo;m5981_20130321b.patch;https://issues.apache.org/jira/secure/attachment/12574777/m5981_20130321b.patch","23/Mar/13 03:49;szetszwo;m5981_20130323.patch;https://issues.apache.org/jira/secure/attachment/12575151/m5981_20130323.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,2013-03-19 15:23:38.984,,,false,,,,,,,,,,,,,,,,,,318199,Reviewed,,,,Tue May 13 15:17:39 UTC 2014,,,,,,,"0|i1iwl3:",318540,,,,,,,,,,,,,,,,,,,,,"19/Mar/13 15:23;cnauroth;Hi, Nicholas.  You may also want to include MAPREDUCE-5075 in this list, which fixes a file handle leak in DistCp on trunk.  I've uploaded a patch on that jira, which is waiting for review and commit.","20/Mar/13 10:51;szetszwo;Sure, MAPREDUCE-5075 is a useful bug fix.","21/Mar/13 08:19;szetszwo;m5981_20130321.patch: backports all JIRAs except MAPREDUCE-5014.","21/Mar/13 10:06;szetszwo;{noformat}
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 41 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     -1 findbugs.  The patch appears to introduce 19 new Findbugs (version 1.3.9) warnings.
{noformat}
","21/Mar/13 11:29;szetszwo;Two of the findbugs warnings are related.

m5981_20130321b.patch: backports distcp part of HADOOP-8341.","21/Mar/13 11:32;szetszwo;{noformat}
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 41 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     -1 findbugs.  The patch appears to introduce 17 new Findbugs (version 1.3.9) warnings.
{noformat}
The remaining findbugs warnings are not related to this.","21/Mar/13 18:09;szetszwo;All tests passed with the patch in my machine.","23/Mar/13 03:48;szetszwo;MAPREDUCE-5014 was committed recently; revised description.","23/Mar/13 03:49;szetszwo;m5981_20130323.patch: includes MAPREDUCE-5014.","23/Mar/13 03:58;szetszwo;For m5981_20130323.patch:
{noformat}
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 41 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     -1 findbugs.  The patch appears to introduce 17 new Findbugs (version 1.3.9) warnings.
{noformat}","26/Mar/13 17:23;sureshms;This is a large patch. All the new code that is added in this patch, I compared with the trunk version of the files and found very little changes. These changes are mainly related method changes and going back to old ways of doing things.

Comments:
# sslConfig.xml, distcp-default.xml is missing Apache license header.
# There is a lot difference in DistCp.java in trunk and DistCp.java in this patch?
# I have concerns about replacing existing distcp with new distcp2. Would this have some behavior that could make the tool behave in a way that could be deemed incompatible by existing users. Why not leave the old distcp as is and add a new command for distcp2?
","26/Mar/13 20:35;sanjay.radia;bq. Why not leave the old distcp as is and add a new command for distcp2?
Agree this is safer.
Only challenge is whether we will carry distcp1 onto Hadoop 2 line for compatibility? We could take it forward and mark it deprecated and remove in a later release.","28/Mar/13 01:30;szetszwo;> 1. sslConfig.xml, distcp-default.xml is missing Apache license header.

Will do.

> 2. There is a lot difference in DistCp.java in trunk and DistCp.java in this patch?

Are you sure that you diff the correct file since there two DistCp.java files?  Let me post the diff.

> 3. ... Why not leave the old distcp as is and add a new command for distcp2?

Sure, I can add a new command for distcp2.","28/Mar/13 01:32;szetszwo;DistCp.java.diff: diff between branch-1 tools/distcp2/DistCp.java (not tools/DistCp.java) and trunk tools/DistCp.java

diff b-1/src/tools/org/apache/hadoop/tools/distcp2/DistCp.java t3/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java  > DistCp.java.diff
","28/Mar/13 02:02;szetszwo;{quote}
> 1. sslConfig.xml, distcp-default.xml is missing Apache license header.

Will do.
{quote}
In branch-1, other xml files like core-default.xml and hdfs-default.xml also do not have license header.  So let's don't fix the new xml here and fix them with other xml files together later.
","28/Mar/13 02:21;szetszwo;m5081_20130328.patch:
- keeps distcp (version 1) unchanged and adds a new distcp2 command;
- also updates the doc.","28/Mar/13 03:58;sureshms;bq. Are you sure that you diff the correct file since there two DistCp.java files? Let me post the diff.
Sorry, I must have compared the wrong files.

bq. In branch-1, other xml files like core-default.xml and hdfs-default.xml also do not have license header. So let's don't fix the new xml here and fix them with other xml files together later.
Sounds good.

+1 for the patch.","28/Mar/13 04:46;szetszwo;Oops, I accidentally added a new vaidya entry to site.xml.
{code}
     <distcp         label=""DistCp""  href=""distcp.html"" />
+    <distcp2        label=""DistCp Version 2""  href=""distcp2.html"" />
+    <vaidya         label=""Vaidya""  href=""vaidya.html""/>
     <vaidya         label=""Vaidya""  href=""vaidya.html""/>
{code}

m5081_20130328b.patch: removes the new vaidya entry.

","28/Mar/13 05:03;szetszwo;Thanks Suresh for reviewing the patch.

I have committed this.","15/May/13 05:15;mattf;Closed upon release of Hadoop 1.2.0.","12/May/14 20:15;tychang;[~szetszwo] is this distcpV2 available if I am using hdfs2 with Mapreduce1? It seems it has a API break change on this API:  org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir(Lorg/apache/hadoop/mapreduce/Cluster;Lorg/apache/hadoop/conf/Configuration;)    BTW, we are using Hadoop 2.0.0-cdh4.2.0 

","13/May/14 15:17;szetszwo;By hdfs2, do you mean hdfs in branch-2?  The DistCpV2 hers is for branch-1.  You may try it in your setup.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DistCp Rewrite,MAPREDUCE-2765,12516911,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,mithun,mithun,mithun,02/Aug/11 12:04,12/May/14 03:02,12/Jan/21 09:52,26/Jan/12 06:50,0.20.203.0,0.23.0,,,,0.23.1,,,distcp,mrv2,,,,,0,,,,,"This is a slightly modified version of the DistCp rewrite that Yahoo uses in production today. The rewrite was ground-up, with specific focus on:
1. improved startup time (postponing as much work as possible to the MR job)
2. support for multiple copy-strategies
3. new features (e.g. -atomic, -async, -bandwidth.)
4. improved programmatic use
Some effort has gone into refactoring what used to be achieved by a single large (1.7 KLOC) source file, into a design that (hopefully) reads better too.

The proposed DistCpV2 preserves command-line-compatibility with the old version, and should be a drop-in replacement.

New to v2:

1. Copy-strategies and the DynamicInputFormat:
	A copy-strategy determines the policy by which source-file-paths are distributed between map-tasks. (These boil down to the choice of the input-format.) 
	If no strategy is explicitly specified on the command-line, the policy chosen is ""uniform size"", where v2 behaves identically to old-DistCp. (The number of bytes transferred by each map-task is roughly equal, at a per-file granularity.) 
	Alternatively, v2 ships with a ""dynamic"" copy-strategy (in the DynamicInputFormat). This policy acknowledges that 
		(a)  dividing files based only on file-size might not be an even distribution (E.g. if some datanodes are slower than others, or if some files are skipped.)
		(b) a ""static"" association of a source-path to a map increases the likelihood of long-tails during copy.
	The ""dynamic"" strategy divides the list-of-source-paths into a number (> nMaps) of smaller parts. When each map completes its current list of paths, it picks up a new list to process, if available. So if a map-task is stuck on a slow (and not necessarily large) file, other maps can pick up the slack. The thinner the file-list is sliced, the greater the parallelism (and the lower the chances of long-tails). Within reason, of course: the number of these short-lived list-files is capped at an overridable maximum.
	Internal benchmarks against source/target clusters with some slow(ish) datanodes have indicated significant performance gains when using the dynamic-strategy. Gains are most pronounced when nFiles greatly exceeds nMaps.
	Please note that the DynamicInputFormat might prove useful outside of DistCp. It is hence available as a mapred/lib, unfettered to DistCpV2. Also note that the copy-strategies have no bearing on the CopyMapper.map() implementation.
	
2. Improved startup-time and programmatic use:
	When the old-DistCp runs with -update, and creates the list-of-source-paths, it attempts to filter out files that might be skipped (by comparing file-sizes, checksums, etc.) This significantly increases the startup time (or the time spent in serial processing till the MR job is launched), blocking the calling-thread. This becomes pronounced as nFiles increases. (Internal benchmarks have seen situations where more time is spent setting up the job than on the actual transfer.)
	DistCpV2 postpones as much work as possible to the MR job. The file-listing isn't filtered until the map-task runs (at which time, identical files are skipped). DistCpV2 can now be run ""asynchronously"". The program quits at job-launch, logging the job-id for tracking. Programmatically, the DistCp.execute() returns a Job instance for progress-tracking.
	
3. New features:
	(a)   -async: As described in #2.
	(b)   -atomic: Data is copied to a (user-specifiable) tmp-location, and then moved atomically to destination.
	(c)   -bandwidth: Enforces a limit on the bandwidth consumed per map.
	(d)   -strategy: As above.    
	
A more comprehensive description the newer features, how the dynamic-strategy works, etc. is available in src/site/xdoc/, and in the pdf that's generated therefrom, during the build.

High on the list of things to do is support to parallelize copies on a per-block level. (i.e. Incorporation of HDFS-222.)

I look forward to comments, suggestions and discussion that will hopefully ensue. I have this running against Hadoop 0.20.203.0. I also have a port to 0.23.0 (complete with unit-tests).

P.S.
A tip of the hat to Srikanth (Sundarrajan) and Venkatesh (Seetharamaiah), for ideas, code, reviews and guidance. Although much of the code is mine, the idea to use the DFS to implement ""dynamic"" input-splits wasn't.
	
",,aah,amareshwari,atm,aw,coderplay,cutting,ddas,devaraj,eli,esteban,johnvijoe,lianhuiwang,mahadev,mithun,nroberts,omalley,raviprak,sharadag,sho.shimauchi,shv,sriksun,sseth,stevenmz,svenkat,szetszwo,thiruvel,tomwhite,tshiran,tychang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-6448,OOZIE-611,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-4654,,,,,,,,,,,,,,,,,,,,,"19/Jan/12 02:54;mithun;2765_hadoop-branch-0.23.patch;https://issues.apache.org/jira/secure/attachment/12511080/2765_hadoop-branch-0.23.patch","19/Jan/12 02:54;mithun;2765_trunk.patch;https://issues.apache.org/jira/secure/attachment/12511079/2765_trunk.patch","02/Aug/11 12:06;mithun;distcpv2.20.203.patch;https://issues.apache.org/jira/secure/attachment/12488877/distcpv2.20.203.patch","30/Nov/11 08:23;mithun;distcpv2_hadoop-0.23.1.patch;https://issues.apache.org/jira/secure/attachment/12505590/distcpv2_hadoop-0.23.1.patch","30/Nov/11 07:43;mithun;distcpv2_hadoop-0.23.1.patch;https://issues.apache.org/jira/secure/attachment/12505584/distcpv2_hadoop-0.23.1.patch","30/Nov/11 08:23;mithun;distcpv2_hadoop-trunk.patch;https://issues.apache.org/jira/secure/attachment/12505591/distcpv2_hadoop-trunk.patch","02/Dec/11 22:26;mithun;distcpv2_patch_0.23.1-SNAPSHOT_tucu_reviewed.patch;https://issues.apache.org/jira/secure/attachment/12505936/distcpv2_patch_0.23.1-SNAPSHOT_tucu_reviewed.patch","02/Dec/11 22:26;mithun;distcpv2_patch_hadoop-trunk_tucu_reviewed.patch;https://issues.apache.org/jira/secure/attachment/12505937/distcpv2_patch_hadoop-trunk_tucu_reviewed.patch","12/Aug/11 13:10;mithun;distcpv2_trunk.patch;https://issues.apache.org/jira/secure/attachment/12490238/distcpv2_trunk.patch","24/Aug/11 14:25;mithun;distcpv2_trunk_post_review_1.patch;https://issues.apache.org/jira/secure/attachment/12491487/distcpv2_trunk_post_review_1.patch",,,,,,,,,,,,,,,,,,,,,,,,,10.0,,,,,,,,,,,,,,,,,,,,2011-08-08 12:05:54.246,,,false,,,,,,,,,,,,,,,,,,61995,Reviewed,,,,Mon May 12 03:02:01 UTC 2014,,,,,,,"0|i0987b:",51761,DistCpV2 added to hadoop-tools.,,,,,,,,,,distcp distcpv2 DynamicInputFormat,,,,,,,,,,"02/Aug/11 13:45;mithun;Code's up on Github as well, if that works better for some. 
(https://github.com/mithunr/DistCpV2-0.20.203)","08/Aug/11 12:05;amareshwari;First of all, the code needs go into a contrib project. So, you need to regenerate the patch putting the code in contrib.
Also, build environment needs changes. Will this be blocked on mavenization of MapReduce?

Overall, design looks fine. Here are some comments on the code:
* CopyMapper:
  ** 
{noformat}
    if (targetFS.exists(targetFinalPath) && targetFS.isFile(targetFinalPath)) {
      overWrite = true; // When target is an existing file, overwrite it.
    }
{noformat}
Target file is overwritten irrespective of overwrite configuration? why?

* Dynamic\*
  ** DynamicInputChunk is not public?
  ** DynamicInputFormat creates FileSplits with zero length. Instead should it be created with the size of chunk as the size of the split.
  ** DynamicRecordReader has commented code. Should remove it.

* CopyCommitter:
  ** Atomic commit should not delete the final directory. Should throw out an error if it exists even before starting the job.
  ** deleteMissing() counts the files which do not exists at both source and target paths as deleted entries.
  ** Preserving status for the root folder does not happen at all? Can you check?
  ** If I’m not wrong, preserveFileAttributes() does preserve only for directories. Can we rename the method accordingly?
  ** The methods deleteMissing(), preserveFileAttributes() etc need more doc.
  ** Deleting attempt temp files happens in each attempt. Why are we doing delete again in Committer? Committer should just delete the work path.

General comment:
All public classes and public methods need javadoc

Haven't looked at testcases.","10/Aug/11 17:49;mithun;A couple of notes on the comments:

0. In its current state, DistCpV2 is maven-ized. I'm considering not switching to ant and then switching back.

1. CopyMapper behaviour: The quoted check is for overwriting a file when the source-path and the target-path are both complete paths to pre-existing files.
""distcp /source/file.txt /target/file.txt"" doesn't skip file if the target-file exists. This behaviour is consistent with old-distcp (which in turn adopts the behaviour of Unix cp.)

2. DynamicInputChunk isn't public since it's specific to the implementation of DynamicInputFormat, and is as such not meant to be consumed except in conjunction with the DynamicInputFormat. I will change the FileSplit-size as you suggest, and remove the commented code.

3. CopyCommitter:
* You're right. Atomic-commit removing existing directories is not intuitive. I've changed that. Testing.
* preserveFileAttributes() should have been named preserveFileAttributesOnDirectory(). I'll change that.
* Will fix up the javadoc.

I'll post a revised patch shortly.","11/Aug/11 01:43;mithun;Would the reviewers/watchers kindly comment on whether it's alright to deprecate the ""-filelimit"" and ""-sizelimit"" options, in DistCpV2?

Old DistCp would use this to limit the number of files/bytes moved, but the results are a tad random, particularly in the -update case. (The first ""n"" files are added to the copy-list.) It sounds like a way to avoid a mis-configuration. I'm inclined to think it's not used often.

Since DistCpV2 determines whether to skip files only in the Map-job, these would be pretty hard to implement.

","11/Aug/11 09:06;vinodkv;bq. Would the reviewers/watchers kindly comment on whether it's alright to deprecate the ""-filelimit"" and ""-sizelimit"" options, in DistCpV2?

Haven't looked at the code yet, but can clearly understand how useless these knobs are. +1 for not supporting them at all, we should probably continue to parse these options but just print a warning saying they are not supportable.","12/Aug/11 13:10;mithun;Patch for hadoop-trunk.","12/Aug/11 13:23;mithun;I've now attached a patch that applies to hadoop-trunk. This takes care of nearly all the review comments so far, save the following:
1. Preserve-status on the root-directory: I've left it out, because this can't really be handled. (For instance, if there are more than one source-path, it is ambiguous to determine the target-root's permissions.)
2. The count in deleteMissing(): It turns out that this is accurate.

I've also modified -atomic not to blow away the target-path, if it exists. This will need removing before an atomic commit can be made. As per Amareshwari's advice, this is checked before the copy is initiated. (There's also a check just before the commit is done, as a corner case. The copied data is retained in the temporary workspace.) Tests have been altered accordingly.

(We briefly toyed with the idea of using -overwrite along with -atomic, to forcibly overwrite the target, but this mixes disjoint concerns, while only offering the dubious convenience of deleting pre-existing targets. We're leaving -atomic and -overwrite mutually exclusive.)

I'm leaving it mavenized, so that it may be built from mapreduce/src/contrib/distcpv2/ by ""mvn package"". The jar may then be used with ""bin/hadoop jar hadoop-distcp.jar [OPTIONS]"". The usage is described at greater length in the pdf generated in the build.

Thank you, reviewers.","12/Aug/11 16:03;knoguchi;bq. Would the reviewers/watchers kindly comment on whether it's alright to deprecate the ""-filelimit"" and ""-sizelimit"" options, in DistCpV2?
bq.
+1.  I think we (Yahoo) requested but ended up not using it at all.  

Just to be clear
bq. The file-listing isn't filtered until the map-task runs 
bq.
This used to be the case in old old distcp.  We changed that when we added this -filelimit feature (that we never used).
 ","12/Aug/11 21:56;vicaya;Does the trunk patch incorporate MAPREDUCE-2257 (parallel block copy)? ","12/Aug/11 23:05;mithun;No, Luke. Not at this time. What we have now is a port of the code that works on 0.20.203.

Parallel block copy is at the top of the TODO list, though.","22/Aug/11 07:39;amareshwari;One thing I like about this patch is the documentation in site/. Everything is very well described. Good work Mithun!

Please generate the patch relative to hadoop-mapreduce(which was mapreduce before). The files would start with src/
Your pom.xml contains a url to Yahoo! wiki. Please remove. Also remove all the references to Yahoo! In the patch. Searching for yahoo in the patch would give more details.
The pom.xml is hardcoded to use 0.23.0-SNAPSHOT. Since this is part the hadoop project now, it should be changed to the current hadoop version, no?

I'm going through the code again. Will update if I have any comments.
","23/Aug/11 05:24;amareshwari;Patch is very close. Some comments on code:
CopyMapper:
bq.     String workDir = conf.get(""mapred.local.dir"") + ""/work"";
Please use the String constant defined for this config. That would ease any change in the config name, deprecation and etc.

Do you want to add ssl.client* config names to DistCp constants?

DistCpConstants:
bq.   public static final String CONF_LABEL_NUM_MAPS = ""mapred.map.tasks"";
Please use the string constant defined for this config. This is a deprecated config and soon be removed.

bq.  public static final String CONF_LABEL_TOTAL_BYTES_TO_BE_COPIED = ""mapred.total.bytes.expected"";
bq.  public static final String CONF_LABEL_TOTAL_NUMBER_OF_RECORDS = ""mapred.number.of.records"";
Shall we change these names to start with distcp.?

bq. All public classes and public methods need javadoc
Can you check this once again? For ex. DistCp.java does not have javadoc

DistCp:
bq.    String userChosenName = getConf().get(""mapred.job.name"");
bq.     job.getConfiguration().set(""mapred.map.tasks.speculative.execution"", ""false"");
Please use the String constant defined for this config. That would ease any change in the config name, deprecation and etc.
bq.   A hack to get the DistCp job id. New Job API doesn't return the job id
This is no more required. MAPREDUCE-118 is resolved now.

CopyCommitter:
bq.        //Skip the root folder, preserve the status after atomic commit is complete
Do you want to change this comment to reflect why we are skipping root folder?

bq. Deleting attempt temp files happens in each attempt. Why are we doing delete again in Committer? Committer should just delete the work path.
bq. The methods deleteMissing(), preserveFileAttributes() etc need more doc.
bq. DynamicInputFormat creates FileSplits with zero length. Instead should it be created with the size of chunk as the size of the split.
I don't see any change for the above from the previous patch. Are you planning to do them?

Also, please run findbugs, javadoc and releaseaudit and make sure there are no warnings.","24/Aug/11 14:03;mithun;Thanks for the thorough review, Amareshwari. I've incorporated a majority of your suggestions, save a couple.

1. Constants for mapred.local.dir, mapred.job.name, etc.: I've switched to using the constants from JobContext, as you've suggested.
2. ssl.client.* conf-labels: I've moved those to DistCpConstants.
3. CONF_LABEL_TOTAL_*: The reason why the values of these constants don't start with ""distcp"" is that CONF_LABEL_TOTAL_NUMBER_OF_RECORDS is used from within DynamicInputFormat (which will eventually be independent of distcp). I've kept the other one for uniformity.
4. Javadocs: You're right. I had missed the DistCp driver class. Thank you for pointing that out. I've also resolved all the warnings from the generation of Javadocs for the project.
5. Job-id hack: I have removed the hack. I'm using Job.getJobID().toString() instead.
6. CopyCommitter documentation: I've corrected the documentation for preserveFileAttributesForDirectories() pertaining to the target-root-path (which was previously misleading). I've also elaborated on the workings of deleteMissing(), etc.
7. FileSplits with zero-length: For the moment, I've set the FileSplit to be set up with a fixed non-zero size. I'd intended to initialize it with the number of paths in the FileSplit, but that would be intrusive. If required, I'll change that in a future patch.
8. Deleting attempt tmp-files: This is a *very* good suggestion. Incorporating it would be slightly invasive at this time. I'd like very much to include this at a later date. The cleanup would be much quicker.

Findbugs seems to indicate 2 ""bugs"" that are false positives:
1. One points at a possible race condition between the cleanup-handler shutdown-hook (from DistCp.java). There's no problem here, given that the cleanup-state is protected via synch. I've verified this with Amareshwari.
2. The other is a silly one, indicating that some exceptions are caught and ignored. (Dead store to local variable).

I'd like to suppress these 2 warnings, if that's alright.

I'll post a newer patch (for hadoop-mapreduce) very shortly.","24/Aug/11 14:25;mithun;Patch for latest trunk. Incorporates Amareshwari's review comments.","26/Aug/11 07:12;aw;IIRC, -filelimit and -sizelimit were the original attempts to try and control bandwidth used.  They did get used for a while until it became evident that when doing an update they broke in horrid ways (as pointed out above).  

The one thing that sticks in mind is if they would still be useful in any use cases involving quotas.  But I suppose that ultimately the lack of these options could be worked around by manipulating the command line arguments given to distcp, right?

","26/Aug/11 19:34;tucu00;Comments on the POM:

* 1 It should use hadoop-project POM as parent
* 2 Version should be hardcoded, not property
* 3 It should not define repositories (they are defined in hadoop-project POM)
* 4 All dependencies should not have versions, thus taking the version from the hadoop-project POM dependenciesManagement section (if there is a new dependency introduced by distcp, it should be added to the hadoop-project POM dependenciesManagement section and in this POM without version as the rest, this ensure across all Hadoop modules the same versions of all JARs are used at compile/test/dist time)
* 5 Why aspect JARs have compile scope (no scope means compile scope)?
* 6 Similarly Plugin versions should be defined in the pluginManagement section in the hadoop-project POM
* 7 For checkstyle/findbugs/clover see to reuse the settings of hadoop-project POM
* 8 clover license is already defined in hadoop-project POM (and the one here seems an internal Y! location)
* 9 The surefire plugin <systemProperties> section, you should use the syntax <PROP_NAME>PROP_VALUE</PROP_NAME> (see hadoop-project POM)
* 10 It seems the dependency plugin is being used to create the tool dist in the target/ dir. This is not correct, an assembly should be used. Ideally the existing hadoop-dist assembly should be reused.

#10 brings up to the table how do we want to run tools? How their classpath/librarypath is created, which JARs are used bundled with the tool, etc.

Finally, based on the outcome of the discussion of where tools should be (trunk/hadoop-tools-project/ or at /trunk/hadoop-*-project/hadoop-*-tools) the patch should be updated to it.","22/Nov/11 10:11;tucu00;Any update on this JIRA? 

The module hadoop-tools is now available.","23/Nov/11 22:21;mithun;Not yet, Alejandro. I'll fix the patch up early next week. (Sorry, been occupied.)","29/Nov/11 09:23;tucu00;Hi Mithun, Any update on this? Thanks.","29/Nov/11 19:07;mithun;Working on (only) this right now. Some of the tests are broken again. Should have an updated patch shortly.","30/Nov/11 07:43;mithun;Patch that applies against 0.23.1-SNAPSHOT.","30/Nov/11 07:48;mahadev;@Mithun,
 Can you please re upload the patch and also make sure you grant license to ASF for inclusion? 

Thanks","30/Nov/11 08:17;mithun;Yes, Mahadev. Apologies, I'll do that in just a moment. I'll also upload the patch that applies against trunk/.

","30/Nov/11 08:32;mithun;The updated patches are up. 

Thanks.","01/Dec/11 17:49;mithun;Sorry I missed this before, but it looks like this bug has been made a blocker for 6448. The intention was for the new DistCp to replace the original one, but not in the immediate time-frame. (It'll take time to port this not to use FileSystem.) Which is also why old-distcp was left untouched in this patch.

I'd appreciate comment/advice on whether this patch fits the bill, otherwise.","01/Dec/11 19:23;tucu00;Feedback on the build part:



* dir name should be hadoop-distcp instead of distpcv2

pom.xml:

* line 127+: why do you need to do this? This should be taken care in the same way as for hadoop-streaming MAPREDUCE-3435 (maybe this JIRA should be generalized to tools)

* line 150+: the resources plugin version should be defined in the dependency management section of the hadoop-project/pom.xml

* line 164: why do we need to generate the test JAR?

* line 177+: not needed, this comes for parent pom.

* line 214+: plugin management should be done in the hadoop-project/pom.xml

documentation:

* In different parts we are starting to use APT (worth considering)

formatting:

* in many files there are trailing spaces 

","02/Dec/11 22:26;mithun;New patches uploaded, incorporating all of Tucu's comments, except for using an assembly. I guess that'll have to wait until we have a hadoop-tools-dist. (I'm also guessing that'll take time to get right.)

(P.S. Thanks, Tucu. :])","04/Jan/12 00:28;mithun;Amareshwari, would it be possible for this to be committed?","18/Jan/12 21:37;mahadev;@Mithun,
 Are you going to make this jira patch available? I think we should commit this patch soon!","19/Jan/12 02:54;mithun;Updated patches (yet again) for changes in trunk and branch/.","19/Jan/12 02:59;mithun;@Mahadev: I'm with you on that. I've updated the DistCpV2 patches and hit submit. (2765*patch). Thanks for reviewing.","19/Jan/12 07:53;mahadev;@Mithun,

  I am getting the following error when running: 

{code}
mvn clean package -DskipTests -Pdist -Dtar -Dmaven.javadoc.skip=true
{code}

{code}

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-pdf-plugin:1.1:pdf (pdf) on project hadoop-distcp: Error during document generation: Error parsing /Users/mahadev/workspace/hadoop-workspace/hadoop-git-0.23/hadoop-tools/hadoop-distcp/target/pdf/site.tmp/fml/faq.fml: Error validating the model: Fatal error:
[ERROR] Public ID: null
[ERROR] System ID: http://maven.apache.org/xsd/fml-1.0.1.xsd
[ERROR] Line number: 2771
[ERROR] Column number: 38
[ERROR] Message: XML document structures must start and end within the same entity.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hadoop-distcp

{code}

Can you please take a look at it?","19/Jan/12 09:22;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12511080/2765_hadoop-branch-0.23.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 66 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1634//console

This message is automatically generated.","19/Jan/12 18:12;mithun;How odd. I checked that the patch applied to branch-0.23 (http://svn.apache.org/repos/asf/hadoop/common/branches/branch-0.23/) yesterday. I'll check again.","19/Jan/12 19:00;johnvijoe;@Mahadev - I had the same error, but the build succeeded once I cleaned my ivy and m2 caches.","19/Jan/12 19:05;mahadev;@John,
 Thats an issue. There is definitely something wrong if we need to clean our m2 caches for this.","19/Jan/12 19:20;mithun;@John: Thanks for trying that out.

@Mahadev: I'm not sure what was in the .m2/ cache before, but I've not had to clean the cache to build this. :] The quoted error seems to point at http://maven.apache.org/xsd/fml-1.0.1.xsd::2771. My guess would be that the pdf-plugin had issues with the fml-xsd, that have since been resolved.","20/Jan/12 16:46;mithun;To clarify, since one doesn't need to clear .m2/ for every build (i.e. since this is one-off), can this be submitted now? I'm loath to delay this longer.","20/Jan/12 16:54;mahadev;@Mithun,
 I am not sure what you mean by: 

bq. My guess would be that the pdf-plugin had issues with the fml-xsd, that have since been resolved.

I am still seeing the issue. Something I need to do to make it go away?","20/Jan/12 21:33;mithun;Clearing(/moving) the .m2/repository directory (once) should do the trick, from what I gather from John. As far as I can tell, this is all that should need doing.","23/Jan/12 18:46;mahadev;@Mithun,
 Any reason the cleaning up of m2 repo fixes it? No other way around it? This will cause all builds to break if we check this is as it is.","26/Jan/12 06:30;mahadev;Looks like when I reran the command, it started working all fine. I will go ahead and commit this.","26/Jan/12 06:47;hudson;Integrated in Hadoop-Common-trunk-Commit #1588 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/1588/])
    MAPREDUCE-2765. DistCp Rewrite. (Mithun Radhakrishnan via mahadev)

mahadev : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1236045
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-project/pom.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/README
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/pom.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpConstants.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptionSwitch.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/FileBasedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/GlobbedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/SimpleCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyOutputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableDirectoryCreateCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/UniformSizeInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputChunk.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicRecordReader.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/RetriableCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/ThrottledInputStream.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/resources
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/resources/distcp-default.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/fml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/fml/faq.fml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/pdf.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/appendix.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/architecture.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/cli.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/index.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/usage.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/StubContext.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCp.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestFileBasedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestGlobbedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestIntegration.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestOptionsParser.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyCommitter.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyMapper.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyOutputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestUniformSizeInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib/TestDynamicInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestDistCpUtils.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestRetriableCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestThrottledInputStream.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/resources
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/resources/sslConfig.xml
* /hadoop/common/trunk/hadoop-tools/pom.xml
","26/Jan/12 06:50;mahadev;I just committed this. Thanks Mithun!","26/Jan/12 06:52;hudson;Integrated in Hadoop-Hdfs-trunk-Commit #1661 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/1661/])
    MAPREDUCE-2765. DistCp Rewrite. (Mithun Radhakrishnan via mahadev) - wrong version in poms
MAPREDUCE-2765. DistCp Rewrite. (Mithun Radhakrishnan via mahadev)

mahadev : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1236048
Files : 
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/pom.xml

mahadev : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1236045
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-project/pom.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/README
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/pom.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpConstants.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptionSwitch.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/FileBasedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/GlobbedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/SimpleCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyOutputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableDirectoryCreateCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/UniformSizeInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputChunk.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicRecordReader.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/RetriableCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/ThrottledInputStream.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/resources
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/resources/distcp-default.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/fml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/fml/faq.fml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/pdf.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/appendix.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/architecture.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/cli.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/index.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/usage.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/StubContext.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCp.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestFileBasedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestGlobbedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestIntegration.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestOptionsParser.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyCommitter.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyMapper.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyOutputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestUniformSizeInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib/TestDynamicInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestDistCpUtils.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestRetriableCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestThrottledInputStream.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/resources
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/resources/sslConfig.xml
* /hadoop/common/trunk/hadoop-tools/pom.xml
","26/Jan/12 06:57;hudson;Integrated in Hadoop-Common-0.23-Commit #422 (See [https://builds.apache.org/job/Hadoop-Common-0.23-Commit/422/])
    MAPREDUCE-2765. DistCp Rewrite. (Mithun Radhakrishnan via mahadev)

mahadev : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1236049
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-project/pom.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/README
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/pom.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpConstants.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptionSwitch.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/FileBasedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/GlobbedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/SimpleCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyOutputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableDirectoryCreateCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/UniformSizeInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputChunk.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicRecordReader.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/RetriableCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/ThrottledInputStream.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/resources
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/resources/distcp-default.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/fml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/fml/faq.fml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/pdf.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/appendix.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/architecture.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/cli.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/index.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/usage.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/StubContext.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCp.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestFileBasedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestGlobbedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestIntegration.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestOptionsParser.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyCommitter.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyMapper.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyOutputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestUniformSizeInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib/TestDynamicInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestDistCpUtils.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestRetriableCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestThrottledInputStream.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/resources
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/resources/sslConfig.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/pom.xml
","26/Jan/12 06:58;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #1605 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/1605/])
    MAPREDUCE-2765. DistCp Rewrite. (Mithun Radhakrishnan via mahadev)

mahadev : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1236045
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-project/pom.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/README
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/pom.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpConstants.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptionSwitch.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/FileBasedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/GlobbedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/SimpleCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyOutputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableDirectoryCreateCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/UniformSizeInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputChunk.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicRecordReader.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/RetriableCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/ThrottledInputStream.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/resources
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/resources/distcp-default.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/fml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/fml/faq.fml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/pdf.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/appendix.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/architecture.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/cli.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/index.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/usage.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/StubContext.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCp.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestFileBasedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestGlobbedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestIntegration.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestOptionsParser.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyCommitter.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyMapper.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyOutputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestUniformSizeInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib/TestDynamicInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestDistCpUtils.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestRetriableCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestThrottledInputStream.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/resources
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/resources/sslConfig.xml
* /hadoop/common/trunk/hadoop-tools/pom.xml
","26/Jan/12 07:01;hudson;Integrated in Hadoop-Hdfs-0.23-Commit #413 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Commit/413/])
    MAPREDUCE-2765. DistCp Rewrite. (Mithun Radhakrishnan via mahadev)

mahadev : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1236049
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-project/pom.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/README
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/pom.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpConstants.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptionSwitch.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/FileBasedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/GlobbedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/SimpleCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyOutputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableDirectoryCreateCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/UniformSizeInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputChunk.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicRecordReader.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/RetriableCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/ThrottledInputStream.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/resources
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/resources/distcp-default.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/fml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/fml/faq.fml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/pdf.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/appendix.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/architecture.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/cli.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/index.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/usage.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/StubContext.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCp.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestFileBasedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestGlobbedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestIntegration.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestOptionsParser.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyCommitter.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyMapper.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyOutputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestUniformSizeInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib/TestDynamicInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestDistCpUtils.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestRetriableCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestThrottledInputStream.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/resources
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/resources/sslConfig.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/pom.xml
","26/Jan/12 07:07;hudson;Integrated in Hadoop-Common-trunk-Commit #1589 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/1589/])
    MAPREDUCE-2765. DistCp Rewrite. (Mithun Radhakrishnan via mahadev) - wrong version in poms

mahadev : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1236048
Files : 
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/pom.xml
","26/Jan/12 07:09;hudson;Integrated in Hadoop-Mapreduce-0.23-Commit #438 (See [https://builds.apache.org/job/Hadoop-Mapreduce-0.23-Commit/438/])
    MAPREDUCE-2765. DistCp Rewrite. (Mithun Radhakrishnan via mahadev)

mahadev : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1236049
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-project/pom.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/README
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/pom.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpConstants.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptionSwitch.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/FileBasedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/GlobbedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/SimpleCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyOutputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableDirectoryCreateCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/UniformSizeInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputChunk.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicRecordReader.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/RetriableCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/ThrottledInputStream.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/resources
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/resources/distcp-default.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/fml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/fml/faq.fml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/pdf.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/appendix.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/architecture.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/cli.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/index.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/usage.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/StubContext.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCp.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestFileBasedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestGlobbedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestIntegration.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestOptionsParser.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyCommitter.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyMapper.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyOutputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestUniformSizeInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib/TestDynamicInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestDistCpUtils.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestRetriableCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestThrottledInputStream.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/resources
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/resources/sslConfig.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/pom.xml
","26/Jan/12 07:16;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #1606 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/1606/])
    MAPREDUCE-2765. DistCp Rewrite. (Mithun Radhakrishnan via mahadev) - wrong version in poms

mahadev : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1236048
Files : 
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/pom.xml
","26/Jan/12 12:42;hudson;Integrated in Hadoop-Hdfs-trunk #937 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/937/])
    MAPREDUCE-2765. DistCp Rewrite. (Mithun Radhakrishnan via mahadev) - wrong version in poms
MAPREDUCE-2765. DistCp Rewrite. (Mithun Radhakrishnan via mahadev)

mahadev : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1236048
Files : 
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/pom.xml

mahadev : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1236045
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-project/pom.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/README
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/pom.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpConstants.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptionSwitch.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/FileBasedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/GlobbedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/SimpleCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyOutputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableDirectoryCreateCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/UniformSizeInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputChunk.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicRecordReader.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/RetriableCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/ThrottledInputStream.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/resources
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/resources/distcp-default.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/fml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/fml/faq.fml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/pdf.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/appendix.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/architecture.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/cli.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/index.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/usage.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/StubContext.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCp.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestFileBasedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestGlobbedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestIntegration.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestOptionsParser.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyCommitter.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyMapper.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyOutputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestUniformSizeInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib/TestDynamicInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestDistCpUtils.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestRetriableCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestThrottledInputStream.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/resources
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/resources/sslConfig.xml
* /hadoop/common/trunk/hadoop-tools/pom.xml
","26/Jan/12 12:48;hudson;Integrated in Hadoop-Hdfs-0.23-Build #150 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/150/])
    MAPREDUCE-2765. DistCp Rewrite. (Mithun Radhakrishnan via mahadev)

mahadev : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1236049
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-project/pom.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/README
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/pom.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpConstants.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptionSwitch.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/FileBasedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/GlobbedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/SimpleCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyOutputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableDirectoryCreateCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/UniformSizeInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputChunk.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicRecordReader.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/RetriableCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/ThrottledInputStream.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/resources
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/resources/distcp-default.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/fml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/fml/faq.fml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/pdf.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/appendix.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/architecture.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/cli.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/index.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/usage.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/StubContext.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCp.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestFileBasedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestGlobbedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestIntegration.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestOptionsParser.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyCommitter.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyMapper.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyOutputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestUniformSizeInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib/TestDynamicInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestDistCpUtils.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestRetriableCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestThrottledInputStream.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/resources
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/resources/sslConfig.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/pom.xml
","26/Jan/12 13:20;hudson;Integrated in Hadoop-Mapreduce-0.23-Build #172 (See [https://builds.apache.org/job/Hadoop-Mapreduce-0.23-Build/172/])
    MAPREDUCE-2765. DistCp Rewrite. (Mithun Radhakrishnan via mahadev)

mahadev : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1236049
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-project/pom.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/README
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/pom.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpConstants.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptionSwitch.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/FileBasedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/GlobbedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/SimpleCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyOutputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableDirectoryCreateCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/UniformSizeInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputChunk.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicRecordReader.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/RetriableCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/ThrottledInputStream.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/resources
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/resources/distcp-default.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/fml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/fml/faq.fml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/pdf.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/appendix.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/architecture.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/cli.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/index.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/site/xdoc/usage.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/StubContext.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCp.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestFileBasedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestGlobbedCopyListing.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestIntegration.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestOptionsParser.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyCommitter.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyMapper.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyOutputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestUniformSizeInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib/TestDynamicInputFormat.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestDistCpUtils.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestRetriableCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestThrottledInputStream.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/resources
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/resources/sslConfig.xml
* /hadoop/common/branches/branch-0.23/hadoop-tools/pom.xml
","26/Jan/12 13:23;hudson;Integrated in Hadoop-Mapreduce-trunk #970 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/970/])
    MAPREDUCE-2765. DistCp Rewrite. (Mithun Radhakrishnan via mahadev) - wrong version in poms
MAPREDUCE-2765. DistCp Rewrite. (Mithun Radhakrishnan via mahadev)

mahadev : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1236048
Files : 
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/pom.xml

mahadev : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1236045
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-project/pom.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/README
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/pom.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpConstants.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptionSwitch.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/FileBasedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/GlobbedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/SimpleCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyOutputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableDirectoryCreateCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/UniformSizeInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputChunk.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicRecordReader.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/RetriableCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/ThrottledInputStream.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/resources
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/resources/distcp-default.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/fml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/fml/faq.fml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/pdf.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/appendix.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/architecture.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/cli.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/index.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/site/xdoc/usage.xml
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/StubContext.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCp.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestFileBasedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestGlobbedCopyListing.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestIntegration.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestOptionsParser.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyCommitter.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyMapper.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyOutputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestUniformSizeInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/lib/TestDynamicInputFormat.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestDistCpUtils.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestRetriableCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestThrottledInputStream.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/resources
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/resources/sslConfig.xml
* /hadoop/common/trunk/hadoop-tools/pom.xml
","05/Sep/12 16:49;eli;Why did this change add a test class that doesn't run? Did TestDistCp ever pass with this change?

{code}
+@Ignore
+public class TestDistCp {
{code}

Filed HADOOP-8768 for this, doesn't seem like distcp has any test coverage. ","07/Feb/14 16:19;stevenmz;Why, in the org.apache.hadoop.tools.DistCp class, is the createJob method private instead of protected? In the programmatic way of using the DistCp class, I can see use cases where derived classes would like to modify the job settings (add parameters to the configuration for use in the mappers, extend distcp by adding reducers, etc.). 

Is it because there is a different way of accomplishing these extensions in derived classes without having to overload the createJob method? 

I see in the [DistCpV2|https://github.com/mithunr/DistCpV2-0.20.203/blob/master/src/main/java/org/apache/hadoop/tools/DistCp.java] project that the createJob method is in fact protected, but that does not seem to have made it into the official Hadoop DistCp tool as part of the rewrite.","07/Feb/14 18:39;mithun;Hello, Steven.

It's been a while, but I think the idea was that most settings could be put in the Configuration used to create DistCp.

But yes, I definitely see value in changing createJob() to a protected method. I'm curious to see in what direction you're considering extending DistCp. ","08/May/14 04:07;tychang;[~mithun] Is it possible to use your distcp v2 in Mapreduce 1? It seems has some compatibility issue for getStagingDir(). Is there a way to make it working without have to install YARN.?","12/May/14 03:02;svenkat;[~tychang], MAPREDUCE-5081 backported this to hadoop-1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PUMA Benchmark Suite,MAPREDUCE-5116,12639689,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Later,farazahmad,farazahmad,farazahmad,28/Mar/13 21:50,25/Apr/14 14:30,12/Jan/21 09:52,25/Apr/14 14:30,,,,,,,,,benchmarks,,,,,29/Mar/13 00:00,0,,,,,"A benchmark suite which represents a broad range of ""real-world"" MapReduce applications exhibiting application characteristics with high/low computation and high/low shuffle volumes. These benchmarks have been published as part of MaRCO (http://dx.doi.org/10.1016/j.jpdc.2012.12.012) project in JPDC '12.",,acmurthy,apurtell,cdouglas,farazahmad,jeagles,kasha,kkambatl,ozawa,sandyr,sseth,tgraves,vicaya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,";28/Mar/13 22:04;farazahmad;172800",,,172800,0,172800,100%,172800,0,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-03-28 22:04:15.762,,,false,,,,,,,,,,,,,,,,,,320158,,,,,Tue Apr 22 16:13:32 UTC 2014,,,,,,,"0|i1j8of:",320499,Introducing PUMA benchmark suite,,,,,,,,,,benchmarks,,0.20.205.0,0.23.5,1.1.2,2.0.3-alpha,,,,,"28/Mar/13 22:04;kkambatl;Removing the fix versions; usually, those are set by the committer at commit time.","29/Mar/13 00:07;apurtell;Reading the link provided in the description requires a payment of US$39.95.","29/Mar/13 02:29;farazahmad;Here is a link to the document for free download (http://web.ics.purdue.edu/~fahmad/papers/marco.pdf).
The benchmarks are available to view separately as http://web.ics.purdue.edu/~fahmad/benchmarks.htm
Here is the link to download the code and input data sets (http://web.ics.purdue.edu/~fahmad/benchmarks/datasets.htm).
Once committed, they should be available through the release.","29/Mar/13 02:58;kkambatl;Hey Faraz, thanks for sharing the links. Looks like there are quite a few new examples you implemented. It might be easier to create one subtask per example for easier review and commit.","29/Mar/13 20:41;farazahmad;Thanks Karthik, I have uploaded the patch for the first example. Will it be better if I create the patch for all benchmarks and upload them or wait for this one to pass through review and commit and then proceed with the other ones? ","22/Apr/14 16:13;jeagles;There hasn't been any traffic on this jira in some time. [~algol], are you still interested in getting this benchmark suite into Hadoop? Please comment on you interest in finishing up this work and I'll be happy to keep this ticket open. Otherwise, I'll close this ticket at the end of the week.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
implement the support for using the shared cache for the job jar and libjars,MAPREDUCE-5662,12682208,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Invalid,sjlee0,sjlee0,sjlee0,02/Dec/13 19:35,06/Mar/14 19:21,12/Jan/21 09:52,06/Mar/14 19:21,,,,,,,,,,,,,,,0,,,,,,,cdouglas,ctrezzo,jira.shegalov,jrottinghuis,lohit,maysamyabandeh,rohithsharma,sjlee0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,YARN-1465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,361465,,,,,Thu Mar 06 19:21:10 UTC 2014,,,,,,,"0|i1qb5r:",361764,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 19:21;sjlee0;I'll close out these JIRAs for YARN-1492, as the design has changed from the time these JIRAs were filed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FixedLengthInputFormat and FixedLengthRecordReader,MAPREDUCE-1176,12439628,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,masokan,bitsofinfo,bitsofinfo,01/Nov/09 18:38,24/Feb/14 20:57,12/Jan/21 09:52,12/Nov/13 03:11,2.2.0,,,,,2.3.0,,,,,,,,,4,,,,,"Hello,
I would like to contribute the following two classes for incorporation into the mapreduce.lib.input package. These two classes can be used when you need to read data from files containing fixed length (fixed width) records. Such files have no CR/LF (or any combination thereof), no delimiters etc, but each record is a fixed length, and extra data is padded with spaces. The data is one gigantic line within a file.

Provided are two classes first is the FixedLengthInputFormat and its corresponding FixedLengthRecordReader. When creating a job that specifies this input format, the job must have the ""mapreduce.input.fixedlengthinputformat.record.length"" property set as follows

myJobConf.setInt(""mapreduce.input.fixedlengthinputformat.record.length"",[myFixedRecordLength]);

OR

myJobConf.setInt(FixedLengthInputFormat.FIXED_RECORD_LENGTH, [myFixedRecordLength]);

This input format overrides computeSplitSize() in order to ensure that InputSplits do not contain any partial records since with fixed records there is no way to determine where a record begins if that were to occur. Each InputSplit passed to the FixedLengthRecordReader will start at the beginning of a record, and the last byte in the InputSplit will be the last byte of a record. The override of computeSplitSize() delegates to FileInputFormat's compute method, and then adjusts the returned split size by doing the following: (Math.floor(fileInputFormatsComputedSplitSize / fixedRecordLength) * fixedRecordLength)

This suite of fixed length input format classes, does not support compressed files. ",Any,bitsofinfo,cdouglas,cutting,de.saha@gmail.com,dhruba,hudson,jhclark,jrideout,masokan,MichaelHubber,sandyr,sarutak,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/09 00:28;bitsofinfo;MAPREDUCE-1176-v1.patch;https://issues.apache.org/jira/secure/attachment/12424922/MAPREDUCE-1176-v1.patch","14/Nov/09 03:18;bitsofinfo;MAPREDUCE-1176-v2.patch;https://issues.apache.org/jira/secure/attachment/12424931/MAPREDUCE-1176-v2.patch","02/Dec/09 04:00;bitsofinfo;MAPREDUCE-1176-v3.patch;https://issues.apache.org/jira/secure/attachment/12426619/MAPREDUCE-1176-v3.patch","02/Feb/10 01:41;bitsofinfo;MAPREDUCE-1176-v4.patch;https://issues.apache.org/jira/secure/attachment/12434480/MAPREDUCE-1176-v4.patch","03/Sep/13 14:27;masokan;mapreduce-1176_v1.patch;https://issues.apache.org/jira/secure/attachment/12601168/mapreduce-1176_v1.patch","03/Sep/13 18:22;masokan;mapreduce-1176_v2.patch;https://issues.apache.org/jira/secure/attachment/12601195/mapreduce-1176_v2.patch","11/Nov/13 16:00;masokan;mapreduce-1176_v3.patch;https://issues.apache.org/jira/secure/attachment/12613169/mapreduce-1176_v3.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,2009-11-02 01:04:33.555,,,false,,,,,,,,,,,,,,,,,,149330,Reviewed,,,,Wed Nov 13 13:47:57 UTC 2013,,,,,,,"0|i00erj:",320,"Addition of FixedLengthInputFormat and FixedLengthRecordReader in the org.apache.hadoop.mapreduce.lib.input package. These two classes can be used when you need to read data from files containing fixed length (fixed width) records. Such files have no CR/LF (or any combination thereof), no delimiters etc, but each record is a fixed length, and extra data is padded with spaces. The data is one gigantic line within a file. When creating a job that specifies this input format, the job must have the ""mapreduce.input.fixedlengthinputformat.record.length"" property set as follows myJobConf.setInt(""mapreduce.input.fixedlengthinputformat.record.length"",[myFixedRecordLength]); 

Please see javadoc for more details.",,,,,,,,,,"fixed length, fixed width, recordreader, inputformat",,,,,,,,,,"01/Nov/09 18:40;bitsofinfo;Attached source files","02/Nov/09 01:04;tlipcon;Hi,

Could you please post this as a patch file against the hadoop-mapreduce trunk? This will allow Hudson to automatically test the change.

Also, a couple notes:
- please include the Apache license header at the top of these files.
- @author tags are discouraged in Apache projects
- Please include unit tests for this new code.

Thanks for the contribution - look forward to seeing this in trunk!","14/Nov/09 00:28;bitsofinfo;Attached is a patch file which adds FixedLengthInputFormat, FixedLengthRecordReader and a unit test for the new input format. This patch was made against the trunk as of 11/13.","14/Nov/09 00:31;bitsofinfo;Re-attached FixedLengthInputFormat source file, to contain same version as in patch file","14/Nov/09 00:33;bitsofinfo;Re-attached source to match the same as in patch file","14/Nov/09 00:34;bitsofinfo;Attached is a patch file which adds FixedLengthInputFormat, FixedLengthRecordReader and a unit test for the new input format. This patch was made against the trunk as of 11/13.","14/Nov/09 01:34;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12424924/FixedLengthRecordReader.java
  against trunk revision 836063.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/243/console

This message is automatically generated.","14/Nov/09 01:43;tlipcon;Hi,

- Please *only* upload the patch to Hudson. Otherwise the QA bot gets confused and tries to apply your .java files as a patch.
- Also, the coding style guidelines for Hadoop are have an indentation level of 2 spaces. It looks like your patch is full of tabs. There are a few other style violations. The coding style is http://java.sun.com/docs/codeconv/ with the change of 2 spaces instead of 4. It's probably easier to look through other parts of the Hadoop codebase and simply follow their example.
- There's a comment referring to the 0.20.1 code. Since this patch is slated for trunk, not 0.20.1, please remove that.
- There are some other bits of commented-out code. These are a no-no - either the code works and is important, in which case it should be there, or it's not important (or broken) and it shouldn't.

Thanks again for contributing to Hadoop! The review process can take a while but it's important to maintain style consistency across the codebase.

","14/Nov/09 02:01;bitsofinfo;I followed the instructions listed @ http://wiki.apache.org/hadoop/HowToContribute  

""Finally, patches should be attached to an issue report in Jira via the Attach File link on the issue's Jira. When you believe that your patch is ready to be committed, select the Submit Patch link on the issue's Jira. ""

So are you saying to delete the 2 *.java files and only upload the .patch?

The *.patch file does contain a unit test in it so I am not sure why the comment above reported no tests were included. I ran this patch file against a clean trunk copy locally on my test machine and also verified it was ok through the ""test-patch"" task on the contribute how-to page.

I'll remove the 4 space indents, when looking through other sources I found the 2 spaces and tons of wrapping unreadable.","14/Nov/09 03:18;bitsofinfo;Updated version of patch

- Moved from 4 spaces to 2 spaces
- Fixed long lines
- Getting rid of java file attachments
- removed 0.20.1 comment and other old comments","14/Nov/09 03:21;tlipcon;toggling patch status (this kicks the qa bot to rerun the checks)","14/Nov/09 07:06;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12424931/MAPREDUCE-1176-v2.patch
  against trunk revision 836063.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/244/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/244/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/244/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/244/console

This message is automatically generated.","19/Nov/09 14:17;bitsofinfo;What is the next step for this issue? Does anything else need to be submitted?","19/Nov/09 19:36;tlipcon;Hi,

Just had a chance to look at your patch in more detail now that the initial formatting issues have been straightened out:

- Please provide a static setter for the mapreduce.input.fixedlengthinputformat.record.length configuration property, rather than having users have to specify this property manually. An example of this is in o.a.h.mapreduce.lib.input.NLineInputFormat.setNumLinesPerSplit. Also update the javadoc and tests to refer to this new method

- The following code makes me nervous:
{code}
+       long splitSize = 
+         ((long)(Math.floor((double)defaultSize / 
+                 (double)recordLength))) * recordLength;
{code}
Why can't you just keep defaultSize and recordLength as longs? Division of longs will give you floor-like behavior and you won't have to worry about floating point inaccuracies.

- In isSplitable, you catch the exception generated by getRecordLength and turn off splitting. If there is no record length specified doesn't that mean the input format won't work at all?
- FixedLengthRecordReader: ""This record reader does not support compressed files."" Is this true? Or does it just not support *splitting* compressed files? I see that you've explicitly disallowed it, but I don't understand this decision.
- Throughout, you've still got 4-space indentation in the method bodies. Indentation should be by 2.
- In FixedLengthRecordReader, you hard code a 64KB buffer. Why's this? You should let the filesystem use its default.
- In your read loop, you're not accounting for the case of read returning 0 or -1, which I believe can happen at EOF, right? Consider using o.a.h.io.IOUtils.readFully() to replace this loop.

As a general note, I'm not sure I agree with the design here. Rather than forcing the split to lie on record boundaries, I think it would be simpler to simply let FileInputFormat compute its own splits, and then when you first open the record reader, skip forward to the next record boundary and begin reading from there. Then for the last record of the file, ""over read"" your split into the beginning of the next one. This is the strategy that other input formats take, and should be compatible with the splittable compression codecs (see TextInputFormat for example).

I don't want to harp too much on the compression thing, in my experience the sorts of datasets that have these fixed-length records are very highly compressible - lots and lots of numeric fields/UPCs/zipcodes/etc.","20/Nov/09 01:43;bitsofinfo;>>>Why can't you just keep defaultSize and recordLength as longs?

Because the findbugs threw warnings if they were not cast, secondly the code works as expected. Please just shoot over how you want that calculation re-written and I can certainly change it. 

>>>- In isSplitable, you catch the exception generated by getRecordLength and turn off splitting.
>>> If there is no record length specified doesn't that mean the input format won't work at all?

Nope, it would still work, as I have yet to see an original raw data file containing records of a fixed width, that for some reason does not contain complete records. But that's fine, we can just exit out here to let the user know they need to configure that property. If there is a better place to check for the existence of that property please let me know.

>>>- FixedLengthRecordReader: ""This record reader does not support compressed files."" Is this true?

Correct, as stated in the docs. Reason being is that in my case, when I wrote this I was not dealing with compressed files. Secondly, if a input file were compressed, I was not sure the procedure to properly compute the splits against a file that is compressed and the byte lengths of the records would be different in a compressed form, vs. once passed to the RecordReader. 

>>>- Throughout, you've still got 4-space indentation in the method bodies. Indentation should be by 2.

Does anyone know of a automated tool that will fix this? Driving me nut going line by line and hitting delete 2x...... When I look at this in eclipse I am not seeing 4 spaces.

>>>- In FixedLengthRecordReader, you hard code a 64KB buffer. Why's this? You should let the filesystem use its default.

Sure, I can get rid of that

>>>- In your read loop, you're not accounting for the case of read returning 0 or -1, which I believe
>>> can happen at EOF, right? Consider using o.a.h.io.IOUtils.readFully() to replace this loop.

Ditto, I can change to that.

>>>As a general note, I'm not sure I agree with the design here. Rather than forcing the split to lie on record boundaries,

Ok, thats fine, I just wanted to contribute what I wrote that is working for my case. 

>>> open the record reader, skip forward to the next record boundary 

Hmm, ok, do you have suggestion on how I detect where one record begins and one record ends when records are not identifiable by any sort of consistent ""start"" character or ""end"" character ""boundary"" but just flow together?  I could see the RecordReader detecting that it only read < RECORD LENGTH bytes and hitting the end of the split and discarding it. But I am not sure how it would detect the start of a record, with a split that has partial data at the start of it. Especially if there is no consistent boundary/char marker that identifies the start of a record.




","02/Dec/09 04:00;bitsofinfo;- Got rid of tabs, cleaned up formatting
- Added static setters for config
- throw exception if record length property is not set
- handle EOF -1 in loop","09/Dec/09 03:24;bitsofinfo;How can I trigger the latest patch v3 to be processed?","09/Dec/09 04:16;tlipcon;To retrigger hudson, hit ""Cancel patch"" and then ""Submit patch"" again","09/Dec/09 08:22;bitsofinfo;triggering hudson","09/Dec/09 08:22;bitsofinfo;triggering hudson","11/Dec/09 04:33;bitsofinfo;trying to trigger hudson to check v3","11/Dec/09 04:33;bitsofinfo;trying to trigger hudson to check v3","11/Dec/09 07:41;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426619/MAPREDUCE-1176-v3.patch
  against trunk revision 889496.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/187/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/187/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/187/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/187/console

This message is automatically generated.","19/Dec/09 00:51;cdouglas;Generally:
* Please clean up unused/overly general imports.
* Remove member fields that are not used after initialization.
* This should offer fixed key/value bytes, not just value bytes
* The double arithmetic should be replaced by modular arithmetic. It is trivially known at split generation whether the length of the file is evenly divisible into fixed-size records. There should be no surprises in the RecordReader, barring IO errors.
* Each split boundary is a multiple of the record size; a delimiter char is not required.
* {{isSplittable}} need only verify that the file is not compressed, not that recordLength is sane.
* The value type should be BytesWritable, not Text
* Reuse the key/value types- reading directly into them- rather than allocating a new byte array for each record","23/Dec/09 15:47;bitsofinfo;Chris, thanks for the comments, to address these:


>>This should offer fixed key/value bytes, not just value bytes. 
>>The value type should be BytesWritable, not Text.

To clarify, so I'll modify this to change the KEY and VAL to be <BytesWritable, BytesWritable>, then propose to add a config property to the FixedLengthInputFormat to allow someone to configure the ""prefix"" number of value (record) bytes to use as the KEY, such as ""mapreduce.input.fixedlengthinputformat.keyprefixcount"" or ""keyprefixbytes"" etc. Please send over any suggestions or other ideas.



>> The double arithmetic should be replaced by modular arithmetic.

Will change to:
{code}
    // determine the split size, it should be as close as possible to the 
    // default size, but should NOT split within a record... each split
    // should contain a complete set of records with the first record
    // starting at the first byte in the split and the last record ending
    // with the last byte in the split.
    long splitSize =  (defaultSize / recordLength) * recordLength;
{code}



>> isSplittable need only verify that the file is not compressed, not that recordLength is sane.

Moving the record length config property validation to ""getSplits()"" instead


>> Reuse the key/value types- reading directly into them- rather than allocating a new byte array for each record

Will do


>>Please clean up unused/overly general imports.

Will do


>> Remove member fields that are not used after initialization.

I identified two of these in FixedLengthRecordReader, will remove


","23/Dec/09 18:08;bitsofinfo;On second thought, I am thinking it might be better for this new FixedLengthInputFormat config property to work as follows which gives greater control over what can be used as the ByteWritable KEY value.

Permit the user to specify the start and end positions within a record which define they key such as

FixedLengthInputFormat.defineKeyBoundaries(long start, long end)","02/Feb/10 01:41;bitsofinfo;latest patch file, containing changes mentioned above","02/Feb/10 01:42;bitsofinfo;triggering hudson for v4 patch file","03/Feb/10 14:34;bitsofinfo;triggering hudson","03/Feb/10 14:35;bitsofinfo;triggering hudson","03/Feb/10 17:50;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12434480/MAPREDUCE-1176-v4.patch
  against trunk revision 905875.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/431/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/431/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/431/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/431/console

This message is automatically generated.","05/Mar/10 03:42;bitsofinfo;regtriggering hudson on latest patch, could not see output from last run
","05/Mar/10 03:43;bitsofinfo;regtriggering hudson on latest patch, could not see output from last run
","05/Mar/10 06:56;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12434480/MAPREDUCE-1176-v4.patch
  against trunk revision 919277.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/22/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/22/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/22/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/22/console

This message is automatically generated.","23/Mar/10 02:37;cdouglas;* The comments so dense that they make the code hard to read. Getting the right mix is a balance, but this sort of annotation is just noise:
{noformat}
+    // fetch configuration
+	Configuration conf = job.getConfiguration();
[snip]
+    // if the currentPosition is less than the split end..
+    if (currentPosition < splitEnd) {
[snip]
+  // reference to the input stream
+  private FSDataInputStream fileInputStream;
{noformat}
Please limit comments to only those sections (and user-visible javadoc) that require non-local context.
* The utility {{toBytes}} can be replaced with use of {{DataOutputBuffer::writeLong}}
* The {{BytesWritable}} key/value instances can be initialized in the {{initialize}} method, rather than lazily in {{nextKeyValue}}
* While splitting compressed files is not meaningful to this reader in general, as long as each split is one file (and the fixed record size is enforced by the reader), it need not be illegal to use it on compressed files
* {{recordKeyEndAt}} seems mostly unused. Isn't the key start and length sufficient?
* Is there any advantage to setting the fixed attributes separately? Would a single method- performing the relevant boundary checks- setting all the record attributes be sufficient?
* The unit test includes {{!}} to confirm it has found a record boundary, but it is also included in the random charset (possible, albeit unlikely, false positives)
* Instead of random data, validating that each record is composed of deterministic key/record data would be a more complete test (e.g. {{^VVVVKKKKVVVVV$}} for all records. If you want to include a random test, varying the key start and length would work.","04/Mar/13 01:29;jhclark;This patch appeared to be within inches of being incorporated 3 years ago and this feature is still sorely missing from Hadoop. Where do things stand now?","23/Mar/13 22:00;bitsofinfo;I stopped working on it due to lack of time and mainly because of the tedium involved with the back and forth over some minor points noted above. The code was functional, passed unit tests and worked for my user case that triggered the work. 

I'd be happy to work with anyone who wants to take it over and get it moved forward into a release.

","25/Jul/13 22:59;sankerkr;I agree with Jonathan, this feature is very critical to do lot of files I deal with. Chris and others, Can you please allow this feature to be included?","25/Jul/13 23:00;sankerkr;Can you allow this patch to be included?","06/Aug/13 18:11;masokan;I was looking for an implementation of this record format as well.  I agree with the following comment by Todd:
{quote}
As a general note, I'm not sure I agree with the design here. Rather than forcing the split to lie on record boundaries, I think it would be simpler to simply let FileInputFormat compute its own splits, and then when you first open the record reader, skip forward to the next record boundary and begin reading from there. Then for the last record of the file, ""over read"" your split into the beginning of the next one. This is the strategy that other input formats take, and should be compatible with the splittable compression codecs (see TextInputFormat for example).
{quote}
I think we should support fixed length records spanning across HDFS blocks.

BitsOfInfo, do you mind if I pick up your patch, enhance it to take care of the above case, and post a patch for the trunk?

I would appreciate if a committer can come forward to review the patch and commit it to the trunk.

Thanks.

-- Asokan","06/Aug/13 18:41;de.saha@gmail.com;The reason other input format takes that approach is they don't have any
other way to figure out exact boundary. With fixed format you can exactly
know the boundary and in my opinion you should take advantage of it.






-- 
- Deba
--~<O>~--
","06/Aug/13 18:48;masokan;Hi Debashis,
  You are correct.  It is easy to identify records spanning across HDFS blocks.

-- Asokan","06/Aug/13 19:29;bitsofinfo;Asokan: Sure go ahead make whatever changes are necessary; as I have no time to work on this anymore; yet would like to see this put into the project as I had a use for it when I created it and I'm sure others do as well.

BTW: Never had my original question answered from a few years ago in regards to the ""design"", maybe I'm was missing something.

bq. ""Hmm, ok, do you have suggestion on how I detect where one record begins and one record ends when records are not identifiable by any sort of consistent ""start"" character or ""end"" character ""boundary"" but just flow together? I could see the RecordReader detecting that it only read < RECORD LENGTH bytes and hitting the end of the split and discarding it. But I am not sure how it would detect the start of a record, with a split that has partial data at the start of it. Especially if there is no consistent boundary/char marker that identifies the start of a record.""

","06/Aug/13 20:39;masokan;BitsOfInfo,
   For each split, you need to compute how many bytes to skip(to account partial record that spans across previous and current splits.)  Let us say we are processing split N(where N is a 0-based number) in the record reader, Z is the cumulative total of split sizes for splits from 0 thru N-1, L is the record length, and S is the number of bytes to skip at the beginning of split N.  When N = 0, S = 0 and for all other N, S = L - (Z mod L)

The record reader should account the last record in a split by reading additional bytes from next split if necessary.

Hope I clarified the logic.

-- Asokan","08/Aug/13 13:59;bitsofinfo;Thanks, anyways, the implementation I originally threw together worked great for our needs. If this is an optimization for it in just computing the split size, then go ahead.","12/Aug/13 15:35;masokan;I went over the code for TextInputFormat.  Here are my conclusions:
* I think we can make FixedLengthInputFormat look very similar to TextInputFormat.  Specifically the key can be LongWritable(which indicates the position of the record in the file) and value can be BytesWritable(since the records can contain arbitrary binary data.)  Also the implementation can be simpler and similar to TextInputFormat.  There is no need for custom key and value settings.
* Splittable compressed input can be supported.
* Since the start location of each split is available(in a FileSplit object) it is easy to compute the number of bytes to skip at the beginning of each split.

I will proceed with the implementation and post a patch.  Also, I raised MAPREDUCE-5455 to support the complement namely FixedLengthOutputFormat.  With these, I think we can cut down some CPU time in TeraSort benchmark since the records are 100 bytes long and have fixed lengths.  There is no need for byte-by-byte scanning to identify records.","03/Sep/13 14:25;masokan;I am posting a patch for the implementation of both old and new APIs for FixedLengthInputFormat.  Compressed input is supported.  I thought of supporting splittable compressed input as multiple splits.  However, the uncompressed size of a split is not available for splittable compressed input.  So any compressed input is treated as one split.","03/Sep/13 16:14;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12601168/mapreduce-1176_v1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 1150 javac compiler warnings (more than the trunk's current 1148 warnings).

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The following test timeouts occurred in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

org.apache.hadoop.mapreduce.v2.TestUberAM

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3977//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3977//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3977//console

This message is automatically generated.","03/Sep/13 18:21;masokan;Got rid of javac warnings and uploaded a new patch.","03/Sep/13 20:06;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12601195/mapreduce-1176_v2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The following test timeouts occurred in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

org.apache.hadoop.mapreduce.v2.TestUberAM

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3978//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3978//console

This message is automatically generated.","09/Sep/13 16:34;masokan;TestUberAM timeouts due to a different reason not related to this patch.  See MAPREDUCE-5481.

-- Asokan","09/Nov/13 00:04;sandyr;Thanks for picking this up [~masokan].

A few minor stylistic nits:
{code}
+  private static final Log LOG 
+                             = LogFactory.getLog(FixedLengthRecordReader.class);
{code}
{code}
+      CompressionInputStream cIn
+                               = codec.createInputStream(fileIn, decompressor);
{code}
{code}
+  public static final String FIXED_RECORD_LENGTH =
+                                        ""fixedlengthinputformat.record.length""; 
{code)}
The second line should only be indented four spaces past where the text on the first line starts.  There might be a couple more of these to fix.

{code}
+      while(numBytesToRead > 0) {
{code}
Need a space after ""while""

{code}
+        if (numBytesRead == -1) // EOF
+          break;
{code}
Curly braces should be used even in one line if blocks.  This applies to a couple other places in the patch as well.

{code}
+      if (! isCompressedInput) {
{code}
No space needed between the exclamation point and the variable name.

{code}
+    return(null == codec);
{code}
Add a space after ""return"".

Can we rename numRecordsInSplit to numRecordsRemainingInSplit to emphasize that it gets decremented as we read?

If we're going to include random tests, we should log the seed loudly so that failures can be reproduced.

If possible, the static block at the beginning of the test class should be moved to a static method with the JUnit BeforeClass annotation.

Other than these, the patch looks good to me.","11/Nov/13 15:59;masokan;Hi Sandy,
   Thanks for reviewing the patch.  I have followed all your suggestions and uploaded a new patch.  Please review it.  By the way, the seed for the random number generator is already logged.  Am I missing something?  Please let me know.

-- Asokan","11/Nov/13 17:43;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12613169/mapreduce-1176_v3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.mapred.TestJobCleanup

                                      The following test timeouts occurred in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

org.apache.hadoop.mapreduce.v2.TestUberAM

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4188//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4188//console

This message is automatically generated.","11/Nov/13 19:58;sandyr;The test failures are unrelated - we're seeing them on other JIRAs as well.

+1.  Will commit this later today or tomorrow unless anybody has additional concerns.","12/Nov/13 03:11;sandyr;I just committed this.  Thanks [~masokan] and [~bitsofinfo]!","12/Nov/13 03:45;hudson;SUCCESS: Integrated in Hadoop-trunk-Commit #4717 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4717/])
MAPREDUCE-1176. FixedLengthInputFormat and FixedLengthRecordReader (Mariappan Asokan and BitsOfInfo via Sandy Ryza) (sandy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1540931)
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FixedLengthInputFormat.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FixedLengthRecordReader.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FixedLengthInputFormat.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FixedLengthRecordReader.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestFixedLengthInputFormat.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/input/TestFixedLengthInputFormat.java
","12/Nov/13 19:36;masokan;Thanks Sandy!

-- Asokan","12/Nov/13 21:14;bitsofinfo;Glad to see this finally making it in. Thanks to those who picked it up and helped push it through.","13/Nov/13 11:12;hudson;SUCCESS: Integrated in Hadoop-Yarn-trunk #390 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/390/])
MAPREDUCE-1176. FixedLengthInputFormat and FixedLengthRecordReader (Mariappan Asokan and BitsOfInfo via Sandy Ryza) (sandy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1540931)
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FixedLengthInputFormat.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FixedLengthRecordReader.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FixedLengthInputFormat.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FixedLengthRecordReader.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestFixedLengthInputFormat.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/input/TestFixedLengthInputFormat.java
","13/Nov/13 13:28;hudson;FAILURE: Integrated in Hadoop-Mapreduce-trunk #1607 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1607/])
MAPREDUCE-1176. FixedLengthInputFormat and FixedLengthRecordReader (Mariappan Asokan and BitsOfInfo via Sandy Ryza) (sandy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1540931)
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FixedLengthInputFormat.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FixedLengthRecordReader.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FixedLengthInputFormat.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FixedLengthRecordReader.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestFixedLengthInputFormat.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/input/TestFixedLengthInputFormat.java
","13/Nov/13 13:47;hudson;SUCCESS: Integrated in Hadoop-Hdfs-trunk #1581 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1581/])
MAPREDUCE-1176. FixedLengthInputFormat and FixedLengthRecordReader (Mariappan Asokan and BitsOfInfo via Sandy Ryza) (sandy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1540931)
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FixedLengthInputFormat.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FixedLengthRecordReader.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FixedLengthInputFormat.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FixedLengthRecordReader.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestFixedLengthInputFormat.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/input/TestFixedLengthInputFormat.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job tracker is not able to recover job in case of crash and after that no user can submit job.,MAPREDUCE-3837,12541714,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,mayank_bansal,mayank_bansal,mayank_bansal,07/Feb/12 23:39,10/Feb/14 07:04,12/Jan/21 09:52,11/Jul/12 16:05,0.22.0,1.1.1,,,,0.22.1,1.1.0,,,,,,,,0,,,,,"If job tracker is crashed while running , and there were some jobs are running , so if job tracker's property mapreduce.jobtracker.restart.recover is true then it should recover the job.

However the current behavior is as follows
jobtracker try to restore the jobs but it can not . And after that jobtracker closes its handle to hdfs and nobody else can submit job. 

Thanks,
Mayank",,acmurthy,devaraj,eli2,hammer,jimhuang,kasha,kkambatl,manish_malhotra,mattf,mayank_bansal,qwertymaniac,shv,tlipcon,tomwhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/12 12:51;acmurthy;MAPREDUCE-3837_addendum.patch;https://issues.apache.org/jira/secure/attachment/12536022/MAPREDUCE-3837_addendum.patch","02/Mar/12 20:36;mayank_bansal;PATCH-HADOOP-1-MAPREDUCE-3837-1.patch;https://issues.apache.org/jira/secure/attachment/12516883/PATCH-HADOOP-1-MAPREDUCE-3837-1.patch","05/Mar/12 22:26;mayank_bansal;PATCH-HADOOP-1-MAPREDUCE-3837-2.patch;https://issues.apache.org/jira/secure/attachment/12517140/PATCH-HADOOP-1-MAPREDUCE-3837-2.patch","26/Jun/12 22:11;mayank_bansal;PATCH-HADOOP-1-MAPREDUCE-3837-3.patch;https://issues.apache.org/jira/secure/attachment/12533547/PATCH-HADOOP-1-MAPREDUCE-3837-3.patch","29/Jun/12 21:50;mayank_bansal;PATCH-HADOOP-1-MAPREDUCE-3837-4.patch;https://issues.apache.org/jira/secure/attachment/12534051/PATCH-HADOOP-1-MAPREDUCE-3837-4.patch","27/Feb/12 19:47;mayank_bansal;PATCH-HADOOP-1-MAPREDUCE-3837.patch;https://issues.apache.org/jira/secure/attachment/12516202/PATCH-HADOOP-1-MAPREDUCE-3837.patch","09/Feb/12 22:04;mayank_bansal;PATCH-MAPREDUCE-3837.patch;https://issues.apache.org/jira/secure/attachment/12514015/PATCH-MAPREDUCE-3837.patch","09/Feb/12 22:47;mayank_bansal;PATCH-TRUNK-MAPREDUCE-3837.patch;https://issues.apache.org/jira/secure/attachment/12514029/PATCH-TRUNK-MAPREDUCE-3837.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,2012-02-09 23:07:58.293,,,false,,,,,,,,,,,,,,,,,,227001,Reviewed,,,,Tue Jan 22 23:29:51 UTC 2013,,,,,,,"0|i02ytj:",15242,,,,,,,,,,,,,,,,,,,,,"09/Feb/12 22:11;mayank_bansal;PATCH-MAPREDUCE-3837.patch

this one is for 22 branch. Please review that. Shortly I will be putting the same for trunk as well.

Thanks,
Mayank
","09/Feb/12 23:07;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12514029/PATCH-TRUNK-MAPREDUCE-3837.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests:
                  org.apache.hadoop.yarn.util.TestLinuxResourceCalculatorPlugin

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1832//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1832//console

This message is automatically generated.","13/Feb/12 20:59;shv;+1 The patch looks good. It enables an important feature of automatic job recovery on JT startup.","13/Feb/12 21:22;shv;I just committed this. Thank you Mayank.","13/Feb/12 21:25;hudson;Integrated in Hadoop-Hdfs-trunk-Commit #1797 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/1797/])
    MAPREDUCE-3837. Job tracker is not able to recover jobs after crash. Contributed by Mayank Bansal. (Revision 1243695)

     Result = SUCCESS
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1243695
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTracker.java
","13/Feb/12 21:25;hudson;Integrated in Hadoop-Common-0.23-Commit #546 (See [https://builds.apache.org/job/Hadoop-Common-0.23-Commit/546/])
    MAPREDUCE-3837. Job tracker is not able to recover jobs after crash. Contributed by Mayank Bansal. (Revision 1243698)

     Result = SUCCESS
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1243698
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTracker.java
","13/Feb/12 21:27;hudson;Integrated in Hadoop-Hdfs-0.23-Commit #534 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Commit/534/])
    MAPREDUCE-3837. Job tracker is not able to recover jobs after crash. Contributed by Mayank Bansal. (Revision 1243698)

     Result = SUCCESS
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1243698
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTracker.java
","13/Feb/12 21:29;mahadev;@Mayank,
 You should Grant license to Apache when uploading patches.","13/Feb/12 21:30;hudson;Integrated in Hadoop-Common-trunk-Commit #1723 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/1723/])
    MAPREDUCE-3837. Job tracker is not able to recover jobs after crash. Contributed by Mayank Bansal. (Revision 1243695)

     Result = SUCCESS
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1243695
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTracker.java
","13/Feb/12 22:06;hudson;Integrated in Hadoop-Mapreduce-0.23-Commit #550 (See [https://builds.apache.org/job/Hadoop-Mapreduce-0.23-Commit/550/])
    MAPREDUCE-3837. Job tracker is not able to recover jobs after crash. Contributed by Mayank Bansal. (Revision 1243698)

     Result = ABORTED
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1243698
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTracker.java
","13/Feb/12 22:08;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #1734 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/1734/])
    MAPREDUCE-3837. Job tracker is not able to recover jobs after crash. Contributed by Mayank Bansal. (Revision 1243695)

     Result = ABORTED
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1243695
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTracker.java
","13/Feb/12 22:12;hudson;Integrated in Hadoop-Mapreduce-0.23-Build #195 (See [https://builds.apache.org/job/Hadoop-Mapreduce-0.23-Build/195/])
    MAPREDUCE-3837. Job tracker is not able to recover jobs after crash. Contributed by Mayank Bansal. (Revision 1243698)

     Result = FAILURE
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1243698
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTracker.java
","14/Feb/12 01:14;hudson;Integrated in Hadoop-Mapreduce-22-branch #100 (See [https://builds.apache.org/job/Hadoop-Mapreduce-22-branch/100/])
    MAPREDUCE-3837. Job tracker is not able to recover jobs after crash. Contributed by Mayank Bansal. (Revision 1243700)

     Result = SUCCESS
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1243700
Files : 
* /hadoop/common/branches/branch-0.22/mapreduce/CHANGES.txt
* /hadoop/common/branches/branch-0.22/mapreduce/src/java/org/apache/hadoop/mapred/JobTracker.java
","14/Feb/12 12:34;hudson;Integrated in Hadoop-Hdfs-trunk #955 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/955/])
    MAPREDUCE-3837. Job tracker is not able to recover jobs after crash. Contributed by Mayank Bansal. (Revision 1243695)

     Result = FAILURE
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1243695
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTracker.java
","14/Feb/12 12:36;hudson;Integrated in Hadoop-Hdfs-0.23-Build #168 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/168/])
    MAPREDUCE-3837. Job tracker is not able to recover jobs after crash. Contributed by Mayank Bansal. (Revision 1243698)

     Result = FAILURE
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1243698
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTracker.java
","14/Feb/12 13:56;hudson;Integrated in Hadoop-Mapreduce-trunk #990 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/990/])
    MAPREDUCE-3837. Job tracker is not able to recover jobs after crash. Contributed by Mayank Bansal. (Revision 1243695)

     Result = SUCCESS
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1243695
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTracker.java
","27/Feb/12 19:46;mayank_bansal;For Haddop-1 Patch","27/Feb/12 19:48;mayank_bansal;Attached the patch for Hadoop -1, please review that.

Thanks,
Mayank","02/Mar/12 17:01;tucu00;Mayank,

* Built branch-1 with your patch
* Configured the cluster, run a job test is OK
* Configured the mapred-site.xml with 'mapred.jobtracker.restart.recover=true'
* Restarted the JT
* Created a IN data file in my HDFS home dir
* Submitted 5 wordcount jobs

{code}
bin/hadoop jar hadoop-*examples*jar wordcount IN OUT0 &
bin/hadoop jar hadoop-*examples*jar wordcount IN OUT1 &
bin/hadoop jar hadoop-*examples*jar wordcount IN OUT2 &
bin/hadoop jar hadoop-*examples*jar wordcount IN OUT3 &
bin/hadoop jar hadoop-*examples*jar wordcount IN OUT4 &
{code}

* Waited till they are all running
* Killed the JT
* Restarted the JT

The jobs are not recovered, and what I see in the logs is:

{code}
2012-03-02 08:55:22,164 INFO org.apache.hadoop.mapred.JobTracker: Found an incomplete job directory job_201203020852_0001. Deleting it!!
2012-03-02 08:55:22,194 INFO org.apache.hadoop.mapred.JobTracker: Found an incomplete job directory job_201203020852_0002. Deleting it!!
2012-03-02 08:55:22,204 INFO org.apache.hadoop.mapred.JobTracker: Found an incomplete job directory job_201203020852_0003. Deleting it!!
2012-03-02 08:55:22,224 INFO org.apache.hadoop.mapred.JobTracker: Found an incomplete job directory job_201203020852_0004. Deleting it!!
2012-03-02 08:55:22,236 INFO org.apache.hadoop.mapred.JobTracker: Found an incomplete job directory job_201203020852_0005. Deleting it!!
{code}

Am I missing some additional configuration?
","02/Mar/12 18:49;acmurthy;-1 on committing to branch-1. We've had innumerable issues with this before, not a good idea for a stable branch.","02/Mar/12 20:42;mayank_bansal;Hi Alejandro

Thanks for your help testing this patch, I am really sorry about confusion as I missed one function in the patch.  I have attached the new patch , tested it and it is working fine in my local environment. I am not sure how I missed that before.

Please let me know if you find any more issues with that.

Arun,

I believe the issues were in terms of recovering the jobs from the point they crashed. Here what I am doing is very simplistic approach. I am reading the job token file and resubmitting the jobs in case of crash and recover. I am not trying to recover from the point it left from the last run.

In this scenario it is a new run of the job and works well. The downside is the whole job will re run however the upside is Users don't need to resubmit the jobs.

Please let me know your thoughts.

Thanks,
Mayank ","02/Mar/12 21:16;tucu00;I've tested the last patch and works as expected. I'd agree with Mayank that this approach (rerun the full job) seems much less risky than the previous approach (rerun from where it was left).  Thus I'm good with the patch as it is much better than what currently is in. 

Arun, would you reconsider based on the explanation of what Mayank's patch does?
","02/Mar/12 23:17;shv;I've been reviewing this patch, and have a couple of cosmetic comments below.
I agree with Alejandro. This is not introducing new feature, it is just enabling already existing feature. There is low risk, since the feature is enabled in a restricted context, that is restarting failed jobs from scratch rather than trying to continue from the point they were terminated.
The patch seems to be larger than it actually is, because it is removing the [troubled] logic responsible for resurrecting the job from its history. Besides that it is simple. Take a look, Arun.

Cosmetic comments
- Several lines are too long
- See several tabs - should be spaces
- indentation is wrong in couple of places
          recoveryManager.addJobForRecovery(JobID.forName(fileName));
          shouldRecover = true; // enable actual recovery if num-files > 1
- Add spaces after commas in method calls and parameters
Otherwise it looks good. ","05/Mar/12 22:26;mayank_bansal;Incorporating review comments","12/Mar/12 22:46;acmurthy;Apologies for the late response, I missed this.

Thanks for the clarification Mayank, Tucu & Konst. I agree it's much more palatable without all the complexities of trying to recover jobs from point-of-crash.

Couple of questions:
a) How does it work in a secure setting?
b) We should at least add some docs on this feature.

Makes sense?","13/Mar/12 18:35;mayank_bansal;Thanks Arun for your reply.

a) It reads the user id from the job token stored into the system directory and submits the job as that user, so the actual job runs as that user.
b) Yeah you are right, I will add the documentation and append it to the patch.

Thanks,
Mayank","20/Jun/12 21:20;tomwhite;TestRecoveryManager and TestJobTrackerRestartWithLostTracker failed for me with this patch. Mayank - can you update them for this JIRA please?","21/Jun/12 02:58;mayank_bansal;When I put this patch it did not have this issue,Let me update the patch.
Thanks for finding this out.

Thanks,
Mayank","23/Jun/12 19:42;tlipcon;Arun: I noticed this is listed as one of the patches in HDP. Does that imply that you're removing your -1? Or do you have a new patch that you're shipping in your product that you haven't open-sourced yet?","23/Jun/12 19:53;mayank_bansal;Hi Todd,

Arun gave -1 because he was in impression that I m trying to restore the state however when I explained it is not restore it is resubmit then he was OK.

What Arun told me more or less the patch is the same in HDP but one bug fix which he did.

I will update the patch based on Tom's comment.

Arun can you also put the bug fix which you did ?

Thanks,
Mayank
","25/Jun/12 19:27;mayank_bansal;Hi Tom,

I just took the latest 1.1 code base and ran the two testcases which you mentioned abobe, without my patch and they are still failing.

Thanks,
Mayank","25/Jun/12 21:02;tomwhite;Mayank - thanks for pointing that out. I just tried and they fail for me on the latest branch-1 code too. We do need tests for job tracker recovery though, so they should be fixed to ensure that the code in this patch is tested and doesn't regress, don't you think?","25/Jun/12 21:07;mayank_bansal;Agree, working on it will update soon.

Thanks,
Mayank","25/Jun/12 21:22;acmurthy;Mayank, as we briefly discussed you'll need to fix the re-submit to read jobtokens from HDFS and pass them along (i.e. Credentials object) to the submitJob api. Sorry, I've been traveling a lot and missed commenting here, my bad.

Other nits:

# You've removed the call to JobClient.isJobDirValid which is dangerous. Since the contents have changed in hadoop-1 post security, please add a private isJobDirValid method to the JT and use it. This method should check for jobInfo file on HDFS (JobTracker.JOB_INFO_FILE) and the jobTokens file (TokenCache.JOB_TOKEN_HDFS_FILE).
# Also, since we only care about jobIds now for JT recovery, it's better to add a Set<JobId> jobIdsToRecover rather than rely on Set<JobInfo> jobsToRecover. This way we can avoid all the unnecessary translations b/w o.a.h.mapred.JobId and o.a.h.mapreduce.JobId.","26/Jun/12 22:18;mayank_bansal;Hi Arun,

As suggested by you

1) I added the credentials to resubmit api.
2) I added the isJobdirvalid api as well.
3) my patch already uses jobid instead of jobinfo so no change required.


Hi Tom,

I added the new test case and fixed the recoverymanager test case well in the latest patch.

I fixed one more issue in terms of recovery which i found here in production.

Please review the patch.

Thanks,
Mayank","27/Jun/12 19:25;tomwhite;Mayank - thanks for the changes. Here's my feedback:

* If there is no need for restart count anymore - since jobs are re-run from the beginning each time - then would it be cleaner to remove it entirely?
* In JobTracker you changed ""shouldRecover = false;"" to ""shouldRecover = true;"" without updating the comment on the line before. (This might be related to the previous point about not having restart counts.)
* Remove the @Ignore annotation from TestRecoveryManager and the comment about MAPREDUCE-873.
* The new test testJobresubmission (should be testJobResubmission) should test that the job succeeded after the restart. Also, there's no reason to run it as a high-priority job.
* There's a comment saying it is a ""faulty job"" - which it isn't.
* Have setUp and tearDown methods to start and stop the cluster. At the moment there is code duplication, and clusters won't be shut down cleanly on failure.
* testJobTracker would be better named testJobTrackerRestartsWithMissingJobFile
* testRecoveryManager would be better named testJobTrackerRestartWithBadJobs
* There are multiple typos and formatting errors (including indentation, which should be 2 spaces) in the new code. See Konstantin's comment above.
* TestJobTrackerRestartWithLostTracker still fails, as does TestJobTrackerSafeMode. These should be fixed as a part of this work.
","29/Jun/12 21:49;mayank_bansal;Thanks Tom for your comments. I incorporated everything except below point 
bq. If there is no need for restart count anymore - since jobs are re-run from the beginning each time - then would it be cleaner to remove it entirely?
Yeah you are right and we should cleanup the restart count, However it looks to me it needs to be looked at more closely and more testing required. Do you mind If I open a separate JIRA and work on that separately then this JIRA?

Rest of the comments are incorporated in my latest patch.

Thanks,
Mayank
","29/Jun/12 21:50;mayank_bansal;Attaching latest patch after incorporating Tom's comments.

Thanks,
Mayank","02/Jul/12 16:12;tomwhite;+1 to the latest patch - thanks for addressing my feedback Mayank. Can you run test-patch and the unit test if you haven't already please.

Cleaning up the restart count code in a separate JIRA is fine by me.","02/Jul/12 20:43;mayank_bansal;Test Patch Results are as follows:

 [exec] BUILD SUCCESSFUL
     [exec] Total time: 4 minutes 7 seconds
     [exec] 
     [exec] 
     [exec] 
     [exec] 
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 9 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.
     [exec] 
     [exec] 
     [exec] 
     [exec] 
     [exec] ======================================================================
     [exec] ======================================================================
     [exec]     Finished build.
     [exec] ======================================================================
     [exec] ======================================================================
","02/Jul/12 21:33;mayank_bansal;I just now completed commit-tests successfully.
I ran all unit test previously before attaching the patch those as well completed successfully.

Thanks,
Mayank","03/Jul/12 20:09;tomwhite;I just committed this to branch-1. Thanks Mayank!","11/Jul/12 12:47;acmurthy;Looks like this needs a minor update to get it to work on Mac OSX...","11/Jul/12 12:51;acmurthy;I see this on a single node cluster.

Without this patch, tasks which are re-run fail with:

{noformat}

2012-07-11 05:43:18,299 INFO org.apache.hadoop.mapred.TaskInProgress: Error from attempt_201207110542_0001_m_000000_0: java.lang.Throwable: Child Error
	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)
Caused by: java.io.IOException: Creation of /tmp/hadoop-acmurthy/mapred/local/userlogs/job_201207110542_0001/attempt_201207110542_0001_m_000000_0 failed.
	at org.apache.hadoop.mapred.TaskLog.createTaskAttemptLogDir(TaskLog.java:104)
	at org.apache.hadoop.mapred.DefaultTaskController.createLogDir(DefaultTaskController.java:71)
	at org.apache.hadoop.mapred.TaskRunner.prepareLogFiles(TaskRunner.java:316)
	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:228)
{noformat}



The problem is that mkdirs (at least on mac-osx) returns false if the directory exists and wasn't created during the call. 

Straight-fwd patch to check for existence fixes it.","11/Jul/12 13:54;acmurthy;bq. Looks like this needs a minor update to get it to work on Mac OSX...

Could be any single-node cluster too...","11/Jul/12 15:17;tomwhite;+1 to the fix. FWIW I didn't see this when testing on a single-node cluster (on Mac OS X).","11/Jul/12 15:22;mayank_bansal;Even I did not see this when testing to my single node cluster on MAC OSX, however fiz looks good to me.

+1 Thanks Arun.

Thanks,
Mayank","11/Jul/12 16:05;acmurthy;Thanks for the reviews Tom & Mayank. I've just committed the small patch.","25/Sep/12 17:24;acmurthy;I just merged this to branch-1.1 after Matt's go ahead.","17/Oct/12 18:27;mattf;Closed upon release of Hadoop-1.1.0.","07/Dec/12 02:48;mattf;It seems that the merge to branch-1.1 on 25/Sep/12, which went into 1.1.0, only included the base fix.
The addendum from Arun was merged to branch-1.1 on 06/Dec/12 and will be part of release 1.1.2.
","22/Jan/13 23:29;manish_malhotra;Hi,

Thanks for this patch, its very important, in future I think with ZK would be great for JT failover stuff. 
I need some help for patching this. I'm using hadoop 1.0.2 and want to apply this patch. 
I believe 1.0.2 is descendent of 0.20.2. 

So, please let me know does any of this patch will work for 1.0.2 or not.  

Regards,
Manish",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic configuration for task slots on TT,MAPREDUCE-4900,12625136,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,decster,junping_du,junping_du,23/Dec/12 08:25,10/Oct/13 23:33,12/Jan/21 09:52,,1.1.1,,,,,,,,resourcemanager,tasktracker,,,,,0,,,,,"The current Hadoop MRV1 resource management logic assumes per node slot number is static during the lifetime of the TT process. Allowing run-time configuration on per node slot will give us finer granularity of resource elasticity. This allows Hadoop workloads to coexist with other workloads on the same hardware efficiently, whether or not the environment is virtualized.
For more background or design details of this effort, please refer proposal in HADOOP-9165.",,decster,eli,gujilangzi,hasonhai,hongyu.bi,jlowe,junping_du,kkambatl,mattf,qwertymaniac,revans2,szetszwo,vicaya,yvesbastos,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-5381,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-9165,,,,,"27/Dec/12 09:15;decster;MAPREDUCE-4900-demo-with-JMX.patch;https://issues.apache.org/jira/secure/attachment/12562451/MAPREDUCE-4900-demo-with-JMX.patch","27/Dec/12 09:28;decster;MAPREDUCE-4900-demo-with-JMX.v2.patch;https://issues.apache.org/jira/secure/attachment/12562452/MAPREDUCE-4900-demo-with-JMX.v2.patch","27/Dec/12 02:43;decster;MAPREDUCE-4900-demo.patch;https://issues.apache.org/jira/secure/attachment/12562431/MAPREDUCE-4900-demo.patch","26/Sep/13 03:28;decster;MAPREDUCE-4944-4900.jmx.v3.patch;https://issues.apache.org/jira/secure/attachment/12605177/MAPREDUCE-4944-4900.jmx.v3.patch","10/Oct/13 08:17;decster;MAPREDUCE-4944-4900.jmx.v4.patch;https://issues.apache.org/jira/secure/attachment/12607766/MAPREDUCE-4944-4900.jmx.v4.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,2012-12-27 02:43:22.384,,,false,,,,,,,,,,,,,,,,,,301672,,,,,Thu Oct 10 23:33:47 UTC 2013,,,,,,,"0|i16vfj:",248224,,,,,,,,,,,,,1.3.0,,,,,,,,"27/Dec/12 02:43;decster;A demo patch to demonstrate how this can be done, not the final solution.","27/Dec/12 09:15;decster;Add the same interface to JobTrackerMXBean to demonstrate JMX solution. ","27/Dec/12 09:28;decster;fix bug, better handling the case that setting slot number is large than max slot number.","16/Jan/13 07:46;junping_du;Binglin, Thanks for the work. I see your code also include list slots of each node. Can we separate this out from this jira and replaced with backport YARN-40 (including: listClusterNodes and printNodeStatus)?","16/Jan/13 08:09;decster;Yes, create MAPREDUCE-4944 for this.
","14/May/13 05:14;mattf;Changed Target Version to 1.3.0 upon release of 1.2.0. Please change to 1.2.1 if you intend to submit a fix for branch-1.2.","26/Sep/13 03:28;decster;Attach patch for MAPREDUCE-4900 and MAPREDUCE-4944, since it is closely related. Changes:
1. Add new admin protocol DynamicResourceProtocol, which include APIs to list node, get node information and set node slot by host.
2. intercept TaskTracker's heartbeat to JobTracker, change slot number.
3. Also add related API to JMX.
4. Add a script(bin/update-slots.sh) to batch update slots.
","09/Oct/13 22:14;vicaya;Thanks for the patch Binglin! Adding a new admin protocol is good idea vs. modifying {{AdminOperationsProtocol}}, which would break backward compatibility.

Reflecting comments on YARN-291 related JIRAs:
# {{DynamicResourceProtocol}}#setNodeSlot set the number of slots one node and one type at a time, which is inefficient for batch operations. Suggest a more flexible API: {{setDynamicSlots(String requestJson)}}, where the request json is a list of resource info objects, which could be easily to extended later. e.g. {code}[{""node"": ""tt1"", ""mapslots"": 8}, {""node"": ""tt2""....}]{code}
# Save the original (statically configured) slots info, such that when an admin manually changes slots in the mapred-site.xml and bounce a TT, the value (if different than the original) can overrides the dynamic slots. This behavior would be more backward compatible and less surprising as the dynamic resource protocol is new and mostly used by resource pool schedulers.","10/Oct/13 08:17;decster;Thanks for the review Luke. Attach new patch addressing your comments.","10/Oct/13 23:33;junping_du;Thanks Binglin for the patch and effort! Can you hold on a bit on this? I will meet [~vinodkv] next week and finalize YARN-291, then we can see how to keep consistent between them. Does that make sense?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement delay scheduling in capacity scheduler for improving data locality,MAPREDUCE-4305,12559019,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,mayank_bansal,mayank_bansal,mayank_bansal,01/Jun/12 22:14,08/Oct/13 03:00,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"Capacity Scheduler data local tasks are about 40%-50% which is not good.
While my test with 70 node cluster i consistently get data locality around 40-50% on a free cluster.

I think we need to implement something like delay scheduling in the capacity scheduler for improving the data locality.
http://radlab.cs.berkeley.edu/publication/308

After implementing the delay scheduling on Hadoop 22 I am getting 100 % data locality in free cluster and around 90% data locality in busy cluster.

Thanks,
Mayank",,acmurthy,amar_kamat,anthonyr,atm,cdouglas,devaraj,eli2,jlowe,kkambatl,lianhuiwang,mayank_bansal,nroberts,revans2,robw,sandyr,shv,sseth,tgraves,tlipcon,tomwhite,tucu00,zhoukaibo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,YARN-80,,,,,,,,,,,,,,,,,,,,,"05/Jun/12 00:33;mayank_bansal;MAPREDUCE-4305;https://issues.apache.org/jira/secure/attachment/12530881/MAPREDUCE-4305","05/Jun/12 22:04;mayank_bansal;MAPREDUCE-4305-1.patch;https://issues.apache.org/jira/secure/attachment/12531025/MAPREDUCE-4305-1.patch","10/Jan/13 00:42;mayank_bansal;PATCH-MAPREDUCE-4305-MR1-1.patch;https://issues.apache.org/jira/secure/attachment/12564066/PATCH-MAPREDUCE-4305-MR1-1.patch","14/Feb/13 00:23;mayank_bansal;PATCH-MAPREDUCE-4305-MR1-2.patch;https://issues.apache.org/jira/secure/attachment/12569290/PATCH-MAPREDUCE-4305-MR1-2.patch","15/Feb/13 21:20;mayank_bansal;PATCH-MAPREDUCE-4305-MR1-3.patch;https://issues.apache.org/jira/secure/attachment/12569603/PATCH-MAPREDUCE-4305-MR1-3.patch","06/May/13 21:49;mayank_bansal;PATCH-MAPREDUCE-4305-MR1-6.patch;https://issues.apache.org/jira/secure/attachment/12581964/PATCH-MAPREDUCE-4305-MR1-6.patch","07/May/13 00:00;mayank_bansal;PATCH-MAPREDUCE-4305-MR1-7.patch;https://issues.apache.org/jira/secure/attachment/12582002/PATCH-MAPREDUCE-4305-MR1-7.patch","09/Jan/13 01:30;mayank_bansal;PATCH-MAPREDUCE-4305-MR1.patch;https://issues.apache.org/jira/secure/attachment/12563866/PATCH-MAPREDUCE-4305-MR1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,2012-06-04 03:53:26.27,,,false,,,,,,,,,,,,,,,,,,253856,,,,,Tue Oct 08 03:00:17 UTC 2013,,,,,,,"0|i0e5of:",80682,,,,,,,,,,,,,,,,,,,,,"04/Jun/12 03:53;amar_kamat;Mayank,
I assume that you are using Hadoop 0.22. The numbers that we are seeing (on 0.20.x) is different from what you have reported. IIRC, Hadoop 22 code is still the old Hadoop codebase (compared to 0.23/trunk) and should be similar to Hadoop 0.20. Can you re-run your experiments on 0.20.x (i.e branch 1.x) and share your finding?","04/Jun/12 13:39;tgraves;I'm curious. Is improving the data locality improving overall job performance/decreasing runtime?  What is the other 50-60% - is it rack local or off rack?","05/Jun/12 00:26;mayank_bansal;@Amar 

Can you please please share the numbers for data locality which you are seeing? I am running Tera sort for my benchmarking.

@Thomas

2-5% tasks are off rack and rest are rack local.","05/Jun/12 00:33;mayank_bansal;Initial patch for 22","05/Jun/12 22:04;mayank_bansal;Uploading the Updated patch","08/Jun/12 00:20;shv;Task locality is important. Interesting that it is only necessary to hook Capacity Scheduler up to the logic that already existed in JobInProgress etc. I went over the general logic of the patch. It looks good. But I have several formatting and code organization comments.
# Append _PROPERTY to new config key constants, e.g. NODE_LOCALITY_DELAY_PROPERTY. Looks like other constants in CapacitySchedulerConf are like that.
# Bend longs lines.
# In CapacitySchedulerConf convert comments describing variables to a JavaDoc.
# In initializeDefaults() you should use {{capacity-scheduler}} not {{fairscheduler}} config variables. Also since you introduced constants for the keys, use them rather than the raw keys.
# JobInfo is confusing because there is already a class with that name. Call it something like JobLocality. I'd rather move it into JobQueuesManager, because the latter maintains the map of those
# Correct indentations in CapacityTaskScheduler, particularly eliminate all tabs, should be spaces only.
# Add spaces between arguments, operators, and in some LOG messages.
# Add empty lines between new methods.
# updateLocalityWaitTimes() and updateLastMapLocalityLevel() should belong to JobQueuesManager, imo.
# JobQueuesManager.infos is a map keyed with JobInProgress. It'd be better to use JobID as a key?
# In TaskSchedulingMgr you need only one version of obtainNewTask to be abstract, the one with cachelevel parameter. The other one should not be abstract and just call the abstract obtainNewTask() with cachelevel set to any.
","24/Sep/12 18:27;mayank_bansal;Hi,

Thanks Konst for your comments.

I am working on MR-1 patch, will put it shortly.

Thanks,
Mayank","09/Jan/13 01:30;mayank_bansal;Initial Patch for MR1

Thanks,
Mayank","09/Jan/13 01:31;mayank_bansal;I am still working on adding more tests will update the latest soon.

Thanks,
Mayank","09/Jan/13 05:27;kkambatl;Thanks Mayank. The overall approach in the patch seems correct. I have a few code-specific comments, but it might be better to review the final patch.","09/Jan/13 07:12;mayank_bansal;Thanks Karthik for looking at the patch. If you have some comments please provide I will try to incorporate those asap.

Thanks,
Mayank","09/Jan/13 16:08;acmurthy;Karthik & Mayank - CS already has delay scheduling built-in, one area for improvement is to backport something like YARN-80 to branch-1.","09/Jan/13 19:27;mayank_bansal;Hi Arun,

In Hadoop-1 we have something called scheduling opportunities counter for a job which gets incremented every time is get the opportunity to get scheduled.
In Yarn-80 we are using the same counter for delaying the schedule for Node.

In this patch I used the scheduling opportunity counter as well as time outs for staging the jobs for different scheduling categories.

Initially jobs will be eligible to schedule only on node local and after the (scheduling opportunity are greater then the number of nodes in cluster or timeout for node which ever comes first) will be graduated for Next Level which is Rack.

Again Job will be waiting for (scheduling opportunity are greater then the number of nodes in cluster or timeout for node+ rack which ever comes first) And will be graduated for the next level which is off rack.

And once the job is off-rack and it will scheduled immediately based on prior logic.

Actually this approach is the combination of Yarn-80 and Fair scheduler delay scheduling algo which gives us the flexibility of staging the jobs between different levels and the same time using the scheduling opportunity counter which was already there.

Please review the approach and let me know if this needs some improvement.

Thanks,
Mayank
","09/Jan/13 20:50;kkambatl;Thanks Arun. My understanding is that Mayank's patch is mostly just backporting YARN-80 to MR1, along with other MR1 specific changes.","09/Jan/13 21:03;mayank_bansal;HI Karthik,

There is more to it then YARN-80, Please followup with my previous comments.

Its combination of YARN-80 and part of fair scheduler delay scheduling with timeouts.

Thanks,
Mayank","09/Jan/13 21:11;kkambatl;Didn't mean to undermine the patch's scope, completely agree it is a combination of YARN-80 and delay scheduling with timeouts from FS. Personally, I like this approach better, may be we can augment the one in YARN where applicable.","10/Jan/13 00:42;mayank_bansal;Fixing small bug

Thanks,
Mayank","10/Jan/13 21:31;acmurthy;Mayank - the time-based configs are a bad idea (I've said the same about FairScheduler long ago) - it doesn't consider cluster sizes, job length, job current progress etc.

I promise you that porting YARN-80 is sufficient and will get you required locality improvements. Please, let us not add more configs if possible. Thanks.","14/Feb/13 00:18;mayank_bansal;Hi,

Thanks Arun for your comments.
I had a offline discussion with Arun.

The reason behind the timeouts which I have added was due to save jobs from starving if we have priority however we need more work in that case.
So I am refactoring my patch in to two patches.
This patch is mostly Yarn-80 for Hadoop-1 with test framework for testing those scenarios like node local , rack local etc.
I will file the JIRA with priority and timeouts and update the patch there.

Thanks,
Mayank
","15/Feb/13 21:20;mayank_bansal;Updating the patch with default value.

Thanks,
Mayank
","26/Apr/13 19:24;kkambatl;Thanks Mayank. From a logic point of view, the code looks good.

Have a few nits, mostly formatting:
# The patch has a few unrelated diffs - mostly whitespace and formatting changes[
# Definition of {{SKIP_SCHEDULING_TIMES}} seems way over 80 chars - can we wrap it around?
# Formatting seems off at
{code}long numActiveNodesinCluster = scheduler.taskTrackerManager
      .getClusterStatus().getTaskTrackers()
      - scheduler.taskTrackerManager.getClusterStatus()
      .getGraylistedTrackers();
{code}
# Switch statement in {{getAllowedLocalityLevel()}} has a few statements over 80 chars
# LocalityStage javadoc refers to FairScheduler
# In TestCapacityScheduler, formatting in method parameters of {{obtain*MapTask}}: no space after comma","06/May/13 21:49;mayank_bansal;Thanks Arun and Karthik for your valuable comments.

I am updating the patch with all your comments.

Please take a look.

Thanks,
Mayank","06/May/13 22:35;kkambatl;Thanks Mayank. +1 on the code part.

Sorry for missing these readability nits in my last review. Choose to ignore some/all of them.
# Should we call NumberBasedDelayScheduling to SkipsBasedDelayScheduling, makes it easier to understand? (Might have to change other method names accordingly)
# Rename calcNumberBasedDelayScheduling to resetNumberBasedDelayScheduling
# Within calcNumberBasedDelayScheduling, use ternary operator instead of if-else
","07/May/13 00:00;mayank_bansal;Thanks Karthik for the review.

Updated your comments.

Thanks,
Mayank","07/May/13 00:04;kkambatl;Thanks Mayank. +1.","15/Jul/13 18:44;kkambatl;Mayank's latest patch looks good. [~acmurthy], do you have any further comments on this? ","08/Oct/13 03:00;kkambatl;[~acmurthy], can you take a look at this when you get a chance. It would be a nice to have addition.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
distcp should infer optimal number of mappers,MAPREDUCE-5555,12672079,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,robw,robw,03/Oct/13 02:44,03/Oct/13 02:44,12/Jan/21 09:52,,,,,,,,,,distcp,,,,,,0,,,,,"Rather than requiring the user to calculate and provide an optimal number of mappers with the -m option, distcp should (if the option is not provided) be able to estimate a reasonable number.
",,cutting,cwimmer,robw,yvesbastos,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,351705,,,,,2013-10-03 02:44:25.0,,,,,,,"0|i1on0v:",351993,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move MapReduce services to YARN-117 stricter lifecycle,MAPREDUCE-5298,12650737,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,stevel@apache.org,stevel@apache.org,stevel@apache.org,03/Jun/13 21:00,27/Aug/13 22:22,12/Jan/21 09:52,13/Jun/13 16:01,2.0.4-alpha,,,,,2.1.0-beta,,,applicationmaster,,,,,,0,,,,,The MR services need to be in sync with the YARN-117 lifecycle enhancements,,cdouglas,hudson,stevel@apache.org,vinodkv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,YARN-117,,,,,,,,,,,,,,,,,YARN-530,,,,"03/Jun/13 22:38;stevel@apache.org;MAPREDUCE-5298-016.patch;https://issues.apache.org/jira/secure/attachment/12585975/MAPREDUCE-5298-016.patch","04/Jun/13 14:13;stevel@apache.org;MAPREDUCE-5298-018.patch;https://issues.apache.org/jira/secure/attachment/12586106/MAPREDUCE-5298-018.patch","04/Jun/13 23:09;stevel@apache.org;MAPREDUCE-5298-019.patch;https://issues.apache.org/jira/secure/attachment/12586219/MAPREDUCE-5298-019.patch","05/Jun/13 23:51;stevel@apache.org;MAPREDUCE-5298-020.patch;https://issues.apache.org/jira/secure/attachment/12586424/MAPREDUCE-5298-020.patch","06/Jun/13 13:26;stevel@apache.org;MAPREDUCE-5298-021.patch;https://issues.apache.org/jira/secure/attachment/12586497/MAPREDUCE-5298-021.patch","11/Jun/13 08:39;stevel@apache.org;MAPREDUCE-5298-022.patch;https://issues.apache.org/jira/secure/attachment/12587225/MAPREDUCE-5298-022.patch","13/Jun/13 04:25;vinodkv;MAPREDUCE-5298-023.patch;https://issues.apache.org/jira/secure/attachment/12587558/MAPREDUCE-5298-023.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,2013-06-03 22:52:22.096,,,false,,,,,,,,,,,,,,,,,,331064,Reviewed,,,,Fri Jun 14 14:15:36 UTC 2013,,,,,,,"0|i1l45b:",331397,,,,,,,,,,,,,2.1.0-beta,,,,,,,,"03/Jun/13 22:38;stevel@apache.org;Patch in sync w/ YARN-530-016.patch","03/Jun/13 22:40;stevel@apache.org;This patch is in sync with the YARN-530 lifecycle changes
# moves them all to the new serviceStop/serviceStart methods
# incorporates MAPREDUCE-3502: Review all Service.stop() operations and make sure that they work before a service is started
# adds more service stopping in test runs

JobHistoryServer is the most troublesome here -though I can't see any obvious cause","03/Jun/13 22:52;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12585975/MAPREDUCE-5298-016.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 12 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3720//console

This message is automatically generated.","04/Jun/13 14:13;stevel@apache.org;Patch in sync w/ YARN-117-018 and YARN-530-018; rebased for the YARN-635 exception rename changes.

Patch only applies after YARN-530 is in, and is failing on tests where JobHistoryServer isn't shutting down as expected -though there's no obvious reason","04/Jun/13 14:25;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12586106/MAPREDUCE-5298-018.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 12 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3725//console

This message is automatically generated.","04/Jun/13 23:09;stevel@apache.org;in sync with YARN-530-019.

Changes since last patch
#src/test reverted some of the appmaster.stop in finally operations to that of trunk; reduces patch size.
# src/main: {{JobHistoryEventHandler}} now gives its event handling thread a name for aid in reading the logs, ""eventHandlingThread""","05/Jun/13 00:54;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12586219/MAPREDUCE-5298-019.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 39 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 2 warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 6 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy:

                  org.apache.hadoop.conf.TestConfiguration
                  org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryParsing
                  org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryServer
                  org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryEvents
                  org.apache.hadoop.yarn.client.TestNMClientAsync

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3731//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3731//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-yarn-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3731//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-app.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3731//console

This message is automatically generated.","05/Jun/13 23:51;stevel@apache.org;Added lots of logging to {{JobHistoryServer}} and {{HistoryFileManager}}. It looks like shutdown of the {{JobHistoryServer}} is happening in such a way it stops the _tmp file being renamed -but from the logs its not obvious what is happening","06/Jun/13 00:05;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12586424/MAPREDUCE-5298-020.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 12 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3740//console

This message is automatically generated.","06/Jun/13 13:26;stevel@apache.org;patch to move to new lifeycle *and* make service stop operations robust.","06/Jun/13 13:35;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12586497/MAPREDUCE-5298-021.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 13 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3743//console

This message is automatically generated.","11/Jun/13 08:39;stevel@apache.org;Patch in sync w/ YARN-117-022: rebased to trunk of June 10; all tests passing.","11/Jun/13 08:45;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12587225/MAPREDUCE-5298-022.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 13 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3763//console

This message is automatically generated.","13/Jun/13 00:53;vinodkv;Looked at the latest patch. Looks good. +1. Ready to commit with other patches.","13/Jun/13 04:25;vinodkv;Patch suppressing findBugs warnings reported at https://issues.apache.org/jira/browse/YARN-117?focusedCommentId=13681875&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13681875.","13/Jun/13 16:01;vinodkv;Committed this to trunk, branch-2 and branch-2.1 along with YARN-117/YARN-530. Closing this.","13/Jun/13 16:06;hudson;Integrated in Hadoop-trunk-Commit #3911 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3911/])
    YARN-530. Defined Service model strictly, implemented AbstractService for robust subclassing and migrated yarn-common services. Contributed by Steve Loughran.
YARN-117. Migrated rest of YARN to the new service model. Contributed by Steve Louhran.
MAPREDUCE-5298. Moved MapReduce services to YARN-530 stricter lifecycle. Contributed by Steve Loughran. (Revision 1492718)

     Result = SUCCESS
vinodkv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1492718
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/dev-support/findbugs-exclude.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryCopyService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryEventHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/TaskHeartbeatHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/client/MRClientService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/commit/CommitterEventHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/launcher/ContainerLauncherImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/local/LocalContainerAllocator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMCommunicator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/speculate/DefaultSpeculator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/jobhistory/TestJobHistoryEventHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MRApp.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MRAppBenchmark.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestFail.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestMRAppMaster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestStagingCleanup.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CachedHistoryStorage.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryClientService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistory.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistoryServer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryEvents.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryServer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestClientRedirect.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestResourceMgrDelegate.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestYarnClientProtocolProvider.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/MiniMRYarnCluster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/dev-support/findbugs-exclude.xml
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDistributedShell.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher/src/test/java/org/apache/hadoop/yarn/applications/unmanagedamlauncher/TestUnmanagedAMLauncher.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher/src/test/resources/log4j.properties
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/AMRMClientAsync.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/AMRMClientImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/NMClientAsync.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/NMClientImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/YarnClientImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/TestNMClient.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/TestNMClientAsync.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/AggregatedLogDeletionService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/AbstractService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/CompositeService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/FilterService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/LifecycleEvent.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/LoggingStateChangeListener.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/Service.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/ServiceOperations.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/ServiceStateException.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/ServiceStateModel.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/AbstractLivelinessMonitor.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/BreakableService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/BreakableStateChangeListener.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/TestGlobalStateChangeListener.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/TestServiceLifecycle.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/TestServiceOperations.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestCompositeService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DeletionService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LocalDirsHandlerService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeHealthCheckerService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeHealthScriptRunner.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/AuxServices.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainersLauncher.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/monitor/ContainersMonitorImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/WebServer.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestDeletionService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLocalDirsHandlerService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeManagerShutdown.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/BaseContainerManagerTest.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/TestAuxServices.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/webapp/TestNMWebServer.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/AdminService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ApplicationMasterService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NMLivelinessMonitor.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NodesListManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/amlauncher/ApplicationMasterLauncher.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/AMLivelinessMonitor.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/ContainerAllocationExpirer.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMTokens.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/resourcetracker/TestNMExpiry.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/security/TestClientTokens.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/MiniYARNCluster.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxy.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServer.java
","14/Jun/13 10:53;hudson;Integrated in Hadoop-Yarn-trunk #240 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/240/])
    YARN-530. Defined Service model strictly, implemented AbstractService for robust subclassing and migrated yarn-common services. Contributed by Steve Loughran.
YARN-117. Migrated rest of YARN to the new service model. Contributed by Steve Louhran.
MAPREDUCE-5298. Moved MapReduce services to YARN-530 stricter lifecycle. Contributed by Steve Loughran. (Revision 1492718)

     Result = SUCCESS
vinodkv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1492718
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/dev-support/findbugs-exclude.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryCopyService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryEventHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/TaskHeartbeatHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/client/MRClientService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/commit/CommitterEventHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/launcher/ContainerLauncherImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/local/LocalContainerAllocator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMCommunicator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/speculate/DefaultSpeculator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/jobhistory/TestJobHistoryEventHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MRApp.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MRAppBenchmark.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestFail.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestMRAppMaster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestStagingCleanup.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CachedHistoryStorage.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryClientService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistory.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistoryServer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryEvents.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryServer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestClientRedirect.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestResourceMgrDelegate.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestYarnClientProtocolProvider.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/MiniMRYarnCluster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/dev-support/findbugs-exclude.xml
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDistributedShell.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher/src/test/java/org/apache/hadoop/yarn/applications/unmanagedamlauncher/TestUnmanagedAMLauncher.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher/src/test/resources/log4j.properties
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/AMRMClientAsync.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/AMRMClientImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/NMClientAsync.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/NMClientImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/YarnClientImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/TestNMClient.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/TestNMClientAsync.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/AggregatedLogDeletionService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/AbstractService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/CompositeService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/FilterService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/LifecycleEvent.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/LoggingStateChangeListener.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/Service.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/ServiceOperations.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/ServiceStateException.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/ServiceStateModel.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/AbstractLivelinessMonitor.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/BreakableService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/BreakableStateChangeListener.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/TestGlobalStateChangeListener.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/TestServiceLifecycle.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/TestServiceOperations.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestCompositeService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DeletionService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LocalDirsHandlerService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeHealthCheckerService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeHealthScriptRunner.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/AuxServices.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainersLauncher.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/monitor/ContainersMonitorImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/WebServer.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestDeletionService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLocalDirsHandlerService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeManagerShutdown.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/BaseContainerManagerTest.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/TestAuxServices.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/webapp/TestNMWebServer.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/AdminService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ApplicationMasterService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NMLivelinessMonitor.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NodesListManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/amlauncher/ApplicationMasterLauncher.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/AMLivelinessMonitor.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/ContainerAllocationExpirer.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMTokens.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/resourcetracker/TestNMExpiry.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/security/TestClientTokens.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/MiniYARNCluster.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxy.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServer.java
","14/Jun/13 14:08;hudson;Integrated in Hadoop-Mapreduce-trunk #1457 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1457/])
    YARN-530. Defined Service model strictly, implemented AbstractService for robust subclassing and migrated yarn-common services. Contributed by Steve Loughran.
YARN-117. Migrated rest of YARN to the new service model. Contributed by Steve Louhran.
MAPREDUCE-5298. Moved MapReduce services to YARN-530 stricter lifecycle. Contributed by Steve Loughran. (Revision 1492718)

     Result = FAILURE
vinodkv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1492718
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/dev-support/findbugs-exclude.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryCopyService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryEventHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/TaskHeartbeatHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/client/MRClientService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/commit/CommitterEventHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/launcher/ContainerLauncherImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/local/LocalContainerAllocator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMCommunicator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/speculate/DefaultSpeculator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/jobhistory/TestJobHistoryEventHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MRApp.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MRAppBenchmark.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestFail.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestMRAppMaster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestStagingCleanup.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CachedHistoryStorage.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryClientService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistory.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistoryServer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryEvents.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryServer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestClientRedirect.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestResourceMgrDelegate.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestYarnClientProtocolProvider.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/MiniMRYarnCluster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/dev-support/findbugs-exclude.xml
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDistributedShell.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher/src/test/java/org/apache/hadoop/yarn/applications/unmanagedamlauncher/TestUnmanagedAMLauncher.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher/src/test/resources/log4j.properties
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/AMRMClientAsync.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/AMRMClientImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/NMClientAsync.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/NMClientImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/YarnClientImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/TestNMClient.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/TestNMClientAsync.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/AggregatedLogDeletionService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/AbstractService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/CompositeService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/FilterService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/LifecycleEvent.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/LoggingStateChangeListener.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/Service.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/ServiceOperations.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/ServiceStateException.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/ServiceStateModel.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/AbstractLivelinessMonitor.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/BreakableService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/BreakableStateChangeListener.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/TestGlobalStateChangeListener.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/TestServiceLifecycle.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/TestServiceOperations.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestCompositeService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DeletionService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LocalDirsHandlerService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeHealthCheckerService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeHealthScriptRunner.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/AuxServices.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainersLauncher.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/monitor/ContainersMonitorImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/WebServer.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestDeletionService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLocalDirsHandlerService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeManagerShutdown.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/BaseContainerManagerTest.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/TestAuxServices.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/webapp/TestNMWebServer.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/AdminService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ApplicationMasterService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NMLivelinessMonitor.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NodesListManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/amlauncher/ApplicationMasterLauncher.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/AMLivelinessMonitor.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/ContainerAllocationExpirer.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMTokens.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/resourcetracker/TestNMExpiry.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/security/TestClientTokens.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/MiniYARNCluster.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxy.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServer.java
","14/Jun/13 14:15;hudson;Integrated in Hadoop-Hdfs-trunk #1430 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1430/])
    YARN-530. Defined Service model strictly, implemented AbstractService for robust subclassing and migrated yarn-common services. Contributed by Steve Loughran.
YARN-117. Migrated rest of YARN to the new service model. Contributed by Steve Louhran.
MAPREDUCE-5298. Moved MapReduce services to YARN-530 stricter lifecycle. Contributed by Steve Loughran. (Revision 1492718)

     Result = FAILURE
vinodkv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1492718
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/dev-support/findbugs-exclude.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryCopyService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryEventHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/TaskHeartbeatHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/client/MRClientService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/commit/CommitterEventHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/launcher/ContainerLauncherImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/local/LocalContainerAllocator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMCommunicator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/speculate/DefaultSpeculator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/jobhistory/TestJobHistoryEventHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MRApp.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MRAppBenchmark.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestFail.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestMRAppMaster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestStagingCleanup.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CachedHistoryStorage.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryClientService.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistory.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistoryServer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryEvents.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryServer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestClientRedirect.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestResourceMgrDelegate.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestYarnClientProtocolProvider.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/MiniMRYarnCluster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/dev-support/findbugs-exclude.xml
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDistributedShell.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher/src/test/java/org/apache/hadoop/yarn/applications/unmanagedamlauncher/TestUnmanagedAMLauncher.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher/src/test/resources/log4j.properties
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/AMRMClientAsync.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/AMRMClientImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/NMClientAsync.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/NMClientImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/YarnClientImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/TestNMClient.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/TestNMClientAsync.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/AggregatedLogDeletionService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/AbstractService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/CompositeService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/FilterService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/LifecycleEvent.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/LoggingStateChangeListener.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/Service.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/ServiceOperations.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/ServiceStateException.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/ServiceStateModel.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/AbstractLivelinessMonitor.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/BreakableService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/BreakableStateChangeListener.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/TestGlobalStateChangeListener.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/TestServiceLifecycle.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/service/TestServiceOperations.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestCompositeService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DeletionService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LocalDirsHandlerService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeHealthCheckerService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeHealthScriptRunner.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/AuxServices.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainersLauncher.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/monitor/ContainersMonitorImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/WebServer.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestDeletionService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLocalDirsHandlerService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeManagerShutdown.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/BaseContainerManagerTest.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/TestAuxServices.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/webapp/TestNMWebServer.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/AdminService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ApplicationMasterService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NMLivelinessMonitor.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NodesListManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/amlauncher/ApplicationMasterLauncher.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/AMLivelinessMonitor.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/ContainerAllocationExpirer.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMTokens.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/resourcetracker/TestNMExpiry.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/security/TestClientTokens.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/MiniYARNCluster.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxy.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServer.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for running combiners without reducers,MAPREDUCE-5153,12642753,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,hsn,hsn,16/Apr/13 15:41,26/Jul/13 12:35,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"scenario: Workflow mapper -> sort -> combiner -> hdfs

No api change is need, if user set combiner class and reducers = 0 then run combiner and sent output to HDFS.

Popular libraries such as scalding and cascading are offering this functionality, but they use caching entire mapper output in memory.",,aajisaka,cutting,hsn,jira.shegalov,kkambatl,ozawa,sarutak,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-06-28 18:25:08.34,,,false,,,,,,,,,,,,,,,,,,323167,,,,,Fri Jul 26 12:35:29 UTC 2013,,,,,,,"0|i1jr9b:",323512,,,,,,,,,,,,,,,,,,,,,"28/Jun/13 18:25;sarutak;Radim, what kind of workload do you want to use combiners without reducers?
We consider whether the feature is really needed or not .","28/Jun/13 23:02;hsn;Result aggregation. It does not needs to be perfect, because its going to be stored in HIVE.","26/Jul/13 05:42;ozawa;This discussion is ""in-mapper combining vs disk-based combining"" essentially. If user program including scalding and cascading does in-mapper combining and emits their values based on memory usage,  the similar effect can be gotten, although it's partially. In most case, this partial approach is enough to get more performance. What do you think?","26/Jul/13 12:35;hsn;its very simple to implement. 

If you want to push things forward then do it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Umbrella: Preemption and restart of MapReduce tasks,MAPREDUCE-4584,12604941,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,cdouglas,sriramsrao,sriramsrao,25/Aug/12 00:14,05/Jun/13 04:35,12/Jan/21 09:52,,,,,,,,,,applicationmaster,mrv2,performance,resourcemanager,task,,0,,,,,"This JIRA will track the implementation of improvements to the handling of intermediate data (e.g., map output). Specifically, it tracks changes in support of preempting running tasks, checkpointing completed work, and spawning one or more tasks to complete the original split/partition. These mechanisms allow one to manage skew in intermediate data, respond to resource abundance or scarcity (particularly with preemption), speculatively execute on the remaining work from checkpointed tasks, and automatically tune parameters for performance.

Iterations will build on learnings from previous work, including the following:

Technical reports:
http://research.yahoo.com/files/yl-2012-002.pdf
http://research.yahoo.com/files/yl-2012-003.pdf

Source code:
http://code.google.com/p/sailfish",,acmurthy,ahmed.radwan,benoyantony,bowang,cdouglas,eric14,hongyu.bi,ivanmi,jdonofrio,jianhe,jlowe,kkambatl,lianhuiwang,longmi,mayank_bansal,nemon,ozawa,peng.zhang,qwertymaniac,revans2,rohithsharma,sandyr,sriramsrao,sseth,subru,tlipcon,vicaya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,YARN-569,YARN-568,YARN-567,,MAPREDUCE-4585,MAPREDUCE-4587,MAPREDUCE-4588,MAPREDUCE-4589,MAPREDUCE-4586,MAPREDUCE-4590,MAPREDUCE-4591,MAPREDUCE-4592,YARN-45,MAPREDUCE-5176,MAPREDUCE-5194,MAPREDUCE-5192,MAPREDUCE-5196,MAPREDUCE-5197,YARN-650,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-08-25 00:23:06.652,,,false,,,,,,,,,,,,,,,,,,253814,,,,,Thu May 09 09:25:51 UTC 2013,,,,,,,"0|i0e5f3:",80640,,,,,,,,,,,,,,,,,,,,,"25/Aug/12 00:23;cdouglas;Our strategy is _not_ to port existing code to Hadoop, but rather to iterate on the existing framework and gradually introduce mechanisms that support these goals. The experimental support in the referenced reports should help to justify these improvements, though no single iteration should cause a regression. The subtasks are intended to outline our plan in broad strokes, leaving plenty of space for collaboration and refinement as we learn.","30/Apr/13 17:23;ozawa;I agree with your strategy. I'm working in MAPREDUCE-4502, a related work of yours, however the patch become too large to review. Now I've planed to split the patches, but the change of your work affects my work. Therefore, I'd like to work with your strategy.  Essentialy, your proposal and the node-level map-side aggregation(MAPREDUCE-4502) are complement each other, therefore the impact on performance can get much better if all features are included in MapReduce.

One proposal is: using node-level aggregation as an optimization technique of reducer-side preemption. If a lot of IFiles are needed to fetch and the job is an aggregation type, mapper-side aggregation is more effective to reduce the size of fetching than fetching in parallel by using reducer preemption. Cooperating these features or switching strategy is possible. Any idea?","09/May/13 09:25;cdouglas;[~ozawa]: I've been reading some of the iterations of your patch(es) as you've updated them over the last few months. Our proposals are absolutely complementary. Your approach (IIRC) involved reusing map tasks to aggregate map output on the same host, right? MAPREDUCE-4502 can accomplish more than checkpointing by aggregating across partitions.

We added some metadata to {{IFile}} to track which task attempts a segment contains. I haven't looked at a recent version of your patch, but that's certainly shared functionality.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update task placement policy for NetworkTopology with 'NodeGroup' layer,MAPREDUCE-4660,12607824,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,junping_du,junping_du,junping_du,16/Sep/12 19:14,15/May/13 05:16,12/Jan/21 09:52,21/Dec/12 22:51,,,,,,1.2.0,,,jobtracker,mrv1,scheduler,,,,0,,,,,,,eli,ivanmi,junping_du,mattf,tgraves,vicaya,wind5shy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/12 13:55;junping_du;MAPREDUCE-4660-v2.patch;https://issues.apache.org/jira/secure/attachment/12560086/MAPREDUCE-4660-v2.patch","17/Dec/12 09:20;junping_du;MAPREDUCE-4660-v3.patch;https://issues.apache.org/jira/secure/attachment/12561262/MAPREDUCE-4660-v3.patch","17/Sep/12 07:30;junping_du;MAPREDUCE-4660.patch;https://issues.apache.org/jira/secure/attachment/12545372/MAPREDUCE-4660.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2012-12-19 20:29:59.954,,,false,,,,,,,,,,,,,,,,,,253793,Reviewed,,,,Wed May 15 05:16:04 UTC 2013,,,,,,,"0|i0e5af:",80619,,,,,,,,,,,,,1.2.0,,,,,,,,"09/Dec/12 13:55;junping_du;Update patch to address recent changes on branch-1 in v2 patch.","17/Dec/12 09:20;junping_du;As HDFS-3942 is checked in to branch-1, so remove some overlap code in v3 patch.","17/Dec/12 09:21;junping_du;The test-patch result is as following: the patch doesn't involve new unit test failure and findbugs warnings.
 
+1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 8 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 225 new Findbugs (version 2.0.1) warnings.
","19/Dec/12 20:29;vicaya;v3 patch lgtm. The findbugs warnings didn't increase due to this patch. +1. Will commit within 24 hours if there is no further objection.","21/Dec/12 22:51;vicaya;Committed (after whitespace cleanup) to branch-1. Thanks Junping!","28/Dec/12 09:21;junping_du;Thanks Luke! A very trivial bug related in MAPREDUCE-4904, would you help to review it?","15/May/13 05:16;mattf;Closed upon release of Hadoop 1.2.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide a mechanism for jobs to indicate they should not be recovered on restart,MAPREDUCE-4824,12617812,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,tomwhite,tomwhite,tomwhite,27/Nov/12 15:51,15/May/13 05:16,12/Jan/21 09:52,05/Apr/13 12:25,1.1.0,,,,,1.2.0,,,mrv1,,,,,,0,,,,,"Some jobs (like Sqoop or HBase jobs) are not idempotent, so should not be recovered on jobtracker restart. MAPREDUCE-2702 solves this problem for MR2, however the approach there is not applicable for MR1, since even if we only use the job-level part of the patch and add a isRecoverySupported method to OutputCommitter, there is no way to use that information from the JT (which initiates recovery), since the JT does not instantiate OutputCommitters - and it shouldn't since they are user-level code. (In MR2 it's OK since the MR AM calls the method.)

Instead, we can add a MR configuration property to say that a job is not recoverable, and the JT could safely read this from the job conf.",,acmurthy,bikassaha,eli,ivanmi,mattf,qwertymaniac,sseth,tomwhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/13 12:08;acmurthy;MAPREDUCE-4824.patch;https://issues.apache.org/jira/secure/attachment/12577205/MAPREDUCE-4824.patch","04/Apr/13 13:53;acmurthy;MAPREDUCE-4824.patch;https://issues.apache.org/jira/secure/attachment/12576982/MAPREDUCE-4824.patch","18/Dec/12 14:42;tomwhite;MAPREDUCE-4824.patch;https://issues.apache.org/jira/secure/attachment/12561496/MAPREDUCE-4824.patch","14/Dec/12 15:14;tomwhite;MAPREDUCE-4824.patch;https://issues.apache.org/jira/secure/attachment/12560982/MAPREDUCE-4824.patch","03/Dec/12 16:07;tomwhite;MAPREDUCE-4824.patch;https://issues.apache.org/jira/secure/attachment/12555779/MAPREDUCE-4824.patch","29/Nov/12 11:52;tomwhite;MAPREDUCE-4824.patch;https://issues.apache.org/jira/secure/attachment/12555344/MAPREDUCE-4824.patch","28/Nov/12 15:34;tomwhite;MAPREDUCE-4824.patch;https://issues.apache.org/jira/secure/attachment/12555176/MAPREDUCE-4824.patch","27/Nov/12 15:55;tomwhite;MAPREDUCE-4824.patch;https://issues.apache.org/jira/secure/attachment/12555023/MAPREDUCE-4824.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,2012-11-27 20:04:23.997,,,false,,,,,,,,,,,,,,,,,,292376,,,,,Wed May 15 05:16:01 UTC 2013,,,,,,,"0|i0rxjj:",161078,,,,,,,,,,,,,,,,,,,,,"27/Nov/12 15:55;tomwhite;Here's a patch that implements this idea. Jobs that shouldn't be recovered should set mapred.job.restart.recover to false.","27/Nov/12 20:04;qwertymaniac;Hi,

- The message below in the exception can be improved I feel. I think its better to say ""Job ID was not recovered since it disabled recovery-upon-restart (mapred.job.restart.recover set to false)."". Also, since this case is to be expected (non-default override), I think it ought to be a simple INFO log, but I understand we need to throw an Exception to halt the loading of the JIP.

{code}
+      if (recovered && !conf.getBoolean(""mapred.job.restart.recover"", true)) {
+        throw new IOException(""Job "" + jobId + "" should not be recovered "" +
+            ""since mapred.job.restart.recover is set to false."");
+      }
{code}

- We could also add this property to mapred-default.xml and document it that way.

The test changes look good.","28/Nov/12 05:40;bikassaha;Agree with Harsh.
I assume this config is job specific and cannot be inadvertently set to disable recovery of all jobs?","28/Nov/12 15:34;tomwhite;Thanks for the feedback. Here's an updated patch with the improved message.

I didn't add the property to mapred-default.xml, since it is a job-specific property and these are generally not added there. There's no way to have true job-specific properties, since if someone adds the property to the jobtracker's mapred-site.xml file then it will be picked up. I'm not sure there's an easy way around this. ","28/Nov/12 16:05;qwertymaniac;bq. I didn't add the property to mapred-default.xml, since it is a job-specific property and these are generally not added there.

We do have several job-specific properties with proper defaults listed in that file. Unless someone overrides them manually, how come there is harm in doing this, and must we remove the ones already present?

The file just helps serve as a good doc. behind the config feature, cause otherwise there's no doc reference to this in the patch.","29/Nov/12 11:52;tomwhite;Good point, Harsh. Here's a new patch with the property documented in mapred-default.xml.","30/Nov/12 16:33;qwertymaniac;+1, please commit. Thanks Tom!","30/Nov/12 19:39;acmurthy;Tom, I'm concerned that this might blow up different schedulers in different ways. I need to re-check, but have you tested this with all 3 scehdulers?

Maybe we need to do an 'if' check during recovery and not throw an IOException? ","30/Nov/12 19:40;acmurthy;Also, we might want to optimize this for hadoop-2, where in JobClient should set a field in AppSubmissionContext where-by it informs the RM that 'I do not want retries.'

Thoughts?","03/Dec/12 16:07;tomwhite;> I'm concerned that this might blow up different schedulers in different ways.

I don't think that's a problem since the code change only affects job submission, which kicks in before scheduling code is run.

> Maybe we need to do an 'if' check during recovery and not throw an IOException?

I had another look at this and came up with a new patch. Does it look better?

The Hadoop 2 change sounds like the right approach. At first I thought we didn't need the property in Hadoop 2, due to MAPREDUCE-2702, but actually it would allow users to mark a job as non-recoverable on a per-instance basis. It would build on YARN-128.
","13/Dec/12 14:24;qwertymaniac;+1, took a look again and this alternative approach (no exception throw) looks good as well.","13/Dec/12 18:18;bikassaha;Does it matter if recovered is true if the job conf says dont recover? Please ignore this comment in case I have not understood the logic correclty :) I am just going by the if condition.
{code}
+    if (recovered && 
+        !job.getJobConf().getBoolean(""mapred.job.restart.recover"", true)) {
+      return null;
+    }
{code}

Did not quite get the resolution of the defaults.xml issue Harsh referred to earlier. Dont see any config changes in the last patch.","14/Dec/12 15:14;tomwhite;> Does it matter if recovered is true if the job conf says dont recover?

Yes, because if the job is running for the first time then you don't want to not submit it if mapred.job.restart.recover has been set to false.

> Did not quite get the resolution of the defaults.xml issue Harsh referred to earlier. Dont see any config changes in the last patch.

Oops, I inadvertently dropped them in the last patch. Here's a new patch with the change to mapred-default.xml.","18/Dec/12 14:42;tomwhite;Updated with latest branch.","29/Mar/13 14:15;acmurthy;Tom - sorry, looks like we lost track of this. Is this good to go? Tx","31/Mar/13 18:39;acmurthy;Also, with RM recovery, we'll need a similar mechanism for YARN too.","01/Apr/13 16:06;tomwhite;Yes, this can go in.","04/Apr/13 13:53;acmurthy;Rebased patch.

Tom, some changes for you to review:
# I've renamed the config to be mapreduce.job.recover.on.restart to be more explicit/clear. We should use 'mapreduce' for new configs to avoid deprecations in future.
# I've also introduced a static final MAPREDUCE_RECOVER_JOB variable in JobConf to avoid using the actual config string by hand.","05/Apr/13 05:37;tomwhite;Arun, thanks for updating the patch. I selected the property name to be similar to the existing mapred.jobtracker.restart.recover, but your point about using the ""mapreduce"" prefix is a good one (especially if we support this in MR2), so how about mapreduce.jobtracker.restart.recover? The rest looks good.","05/Apr/13 12:08;acmurthy;Thanks for taking a look at the update Tom. Let's go with mapreduce.job.restart.recover since, essentially, this is your patch - appreciate you taking f/b on naming! :)","05/Apr/13 12:12;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12577205/MAPREDUCE-4824.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3503//console

This message is automatically generated.","05/Apr/13 12:25;acmurthy;I just committed this. Thanks Tom!","15/May/13 05:16;mattf;Closed upon release of Hadoop 1.2.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Port MAPREDUCE-463 (The job setup and cleanup tasks should be optional) to branch-1,MAPREDUCE-4488,12600353,New Feature,Reopened,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,tomwhite,tomwhite,tomwhite,26/Jul/12 18:05,14/May/13 05:14,12/Jan/21 09:52,,1.0.3,,,,,,,,mrv1,performance,,,,,0,,,,,,,acmurthy,ahmed.radwan,kkambatl,mattf,mayank_bansal,sseth,tlipcon,tomwhite,tucu00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-4487,,,,,,,,,,,,,,,,,,,,,"26/Jul/12 18:16;tomwhite;MAPREDUCE-4488.patch;https://issues.apache.org/jira/secure/attachment/12538035/MAPREDUCE-4488.patch","31/Aug/12 01:32;kkambatl;fix-mr-4488.patch;https://issues.apache.org/jira/secure/attachment/12543216/fix-mr-4488.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2012-08-03 01:04:44.001,,,false,,,,,,,,,,,,,,,,,,253830,Reviewed,,,,Tue May 14 05:14:42 UTC 2013,,,,,,,"0|i0e5in:",80656,,,,,,,,,,,,,1.3.0,,,,,,,,"26/Jul/12 18:16;tomwhite;For some jobs the setup and cleanup tasks are not needed, so they can be skipped. With this patch applied, I ran

{noformat}
bin/hadoop jar hadoop-*examples*jar sleep -D mapred.committer.job.setup.cleanup.needed=false -m 1 -r 1 -mt 1 -rt 1
{noformat}

(along with the changes in MAPREDUCE-4487) and the mean time across 10 runs was 6.051s (sd 0.81).","03/Aug/12 01:04;ahmed.radwan;+1 Thanks Tom! On a related note, I think this property need better documentation so (from a user perspective) can be clear when separate setup and cleanup tasks are not needed and it is safe to set it to false.","03/Aug/12 05:07;tucu00;looks good, some minor comments:

* JobInProgress constructors, is there a need to create a JobContext to get the value of the flag? Why just not do a conf.get() ?

* JobInProgress initSetupCleanupTask(), revert the IF condition and do the logic within the IF block, then no need for a return call.

* JobInProgress setupComplete(), do an ELSE instead of return call at the end of the first IF block.

","03/Aug/12 15:30;tomwhite;Alejandro - the code is from MAPREDUCE-463. Can I make the changes you suggest in another JIRA so that branches 1 and 2 are kept the same?","03/Aug/12 15:44;tucu00;+1","13/Aug/12 18:36;tomwhite;I just committed this to branch-1. (Ran unit tests and test-patch successfully.)","30/Aug/12 23:06;kkambatl;It looks like the change, in particular, the implementation of {{JobInProgress#setupComplete()}} seems to have introduced a race leading to the following deadlock as noticed in our clusters:

{noformat}
Thread 42 (IPC Server handler 1 on 8021):
State: BLOCKED
Blocked count: 203661
Waited count: 563040
Blocked on org.apache.hadoop.mapred.JobInProgress@6ab8d396
Blocked by 243 (pool-7-thread-1)
Stack:
org.apache.hadoop.mapred.JobInProgress.runningMaps(JobInProgress.java:884)
org.apache.hadoop.mapred.JobSchedulable.getRunningTasks(JobSchedulable.java:110)
org.apache.hadoop.mapred.PoolSchedulable.getRunningTasks(PoolSchedulable.java:132)
org.apache.hadoop.mapred.FairScheduler.assignTasks(FairScheduler.java:351)
org.apache.hadoop.mapred.JobTracker.heartbeat(JobTracker.java:2935)
sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
java.lang.reflect.Method.invoke(Method.java:597)
org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker.call(WritableRpcEngine.java:474)
org.apache.hadoop.ipc.RPC$Server.call(RPC.java:898)
org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1693)
org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1689)
java.security.AccessController.doPrivileged(Native Method)
javax.security.auth.Subject.doAs(Subject.java:396)
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1332)
org.apache.hadoop.ipc.Server$Handler.run(Server.java:1687)
Thread 243 (pool-7-thread-1):
State: BLOCKED
Blocked count: 435
Waited count: 569
Blocked on org.apache.hadoop.mapred.JobTracker@3cfa54fe
Blocked by 42 (IPC Server handler 1 on 8021)
Stack:
org.apache.hadoop.mapred.JobTracker.getClusterStatus(JobTracker.java:3616)
org.apache.hadoop.mapred.JobInProgress.jobComplete(JobInProgress.java:2713)
org.apache.hadoop.mapred.JobInProgress.setupComplete(JobInProgress.java:837)
org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:790)
org.apache.hadoop.mapred.JobTracker.initJob(JobTracker.java:3750)
org.apache.hadoop.mapred.EagerTaskInitializationListener$InitJob.run(EagerTaskInitializationListener.java:79)
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
java.lang.Thread.run(Thread.java:662)
{noformat}

We should probably revert the commit, and fix it.","30/Aug/12 23:13;kkambatl;As the stack trace shows, the deadlock is because of the following:

- {{JobInProgress#jobComplete()}} (while holding {{JobInProgress}} lock) is blocked on the {{JobTracker}} lock via the call to synchronized method {{JobTracker#getClusterStatus()}}
- {{FairScheduler}} (while holding the {{JobTracker}} lock by calling {{synchronized heartbeat()}}) tries to acquire the {{JobInProgress}} lock via the call to the synchronized method {{JobInProgress#runningMaps()}}","30/Aug/12 23:16;kkambatl;On examining the code, it appears safe to modify {{JobTracker#getClusterStatus()}} to non-synchronized. All the statements in the method are guarded by a {code}synchronized (taskTracker) {} {code}","31/Aug/12 01:32;kkambatl;Uploading a patch to fix the reported deadlock.

The fix is essentially making {{JobTracker#initJob()}} synchronized. I am working on a testcase to test the same.","31/Aug/12 02:45;kkambatl;As we cannot deterministically validate the lack of deadlocks in a piece of code, I was thinking of the following two options:
- Verify lock ordering: in this case, we can write a test to verify that {{JobTracker#initJob()}} acquires the lock on {{JobTracker}} before acquiring the lock on {{JobInProgress}}. This would prevent future changes to the lock-ordering.
- Run two threads with sleep statements to force a deadlock in most cases. However, it remains a best-effort test.

I am very keen on learning alternate ways of testing deadlocks and which option to prefer.

","31/Aug/12 03:26;acmurthy;I'm concerned, let's spend time on this one. JT locking is one of my worst nightmares.","31/Aug/12 03:32;kkambatl;Arun, what do you think of tests verifying lock ordering on all JT methods? We can verify that we hold JT lock before holding any other lock. That way, the JT itself wouldn't be involved in deadlocks?","31/Aug/12 05:23;tlipcon;You can use jcarder to check for lock inversions like this. See http://wiki.apache.org/hadoop/HowToUseJCarder for details. I haven't run it on branch-1 for a while but I'd be really surprised if it didn't catch this deadlock.","31/Aug/12 06:06;kkambatl;Thanks Todd. I ll run JCarder before and after the fix and report back.","31/Aug/12 14:07;acmurthy;Let's revisit the original patch? Tom?","31/Aug/12 14:15;tomwhite;I agree. I'm going to revert this and MAPREDUCE-4567.","31/Aug/12 15:00;tomwhite;Karthik - thanks for investigating. Regarding your fix, it would be better to reduce the scope of the lock on JT to the {{job.initTasks()}} statement. However even this might be excessively wide since initTasks() reads input split files, etc.

There might be a way of reducing the scope of the synchronization on JobInProgress in initTasks() so that it can take a lock on the JT first before making the setupComplete() call. But as Arun rightly points out the locking in JT is very delicate so we have to be conservative here, so at least having a clean jcarder run would be prudent.","31/Aug/12 15:48;acmurthy;The concern I have is that MAPREDUCE-463 is very different from the current JT. Originally, I did this work for the 2009 terasort record and was since ported over to branch-0.21. However, since then the locking in the JT has changed significantly - hence my advise to revisit. Thoughts?","05/Sep/12 09:34;tomwhite;Arun - the patch correctly allows setup and cleanup to be disabled, however the problem is that the locking is incorrect. So that's what we need to fix - or did you have another idea? ","14/May/13 05:14;mattf;Changed Target Version to 1.3.0 upon release of 1.2.0. Please change to 1.2.1 if you intend to submit a fix for branch-1.2.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Port Fair Scheduler to MR2,MAPREDUCE-3451,12532264,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,pwendell@gmail.com,pwendell@gmail.com,pwendell@gmail.com,22/Nov/11 04:03,02/May/13 02:30,12/Jan/21 09:52,13/Jul/12 00:48,,,,,,2.0.2-alpha,,,mrv2,scheduler,,,,,0,,,,,"The Fair Scheduler is in widespread use today in MR1 clusters, but not yet ported to MR2. This is to track the porting of the Fair Scheduler to MR2 and will be updated to include design considerations and progress.",,aching,acmurthy,adferguson,ahmed.radwan,ashutoshc,atm,dapengsun,devaraj,dheeren,dhruba,eli,esteban,gemini5201314,hammer,hudson,jayf,kasha,kkambatl,lianhuiwang,mahadev,matei,mayank_bansal,mholderba,phunt,prashant,pwendell@gmail.com,qwertymaniac,raviprak,raviteja,rvadali,sseth,tgraves,tlipcon,tomwhite,tucu00,vicaya,vkruglikov,wenfengbx,zhihyu@ebaysf.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-4462,MAPREDUCE-4441,,,,,,,,,,,,,,,,,,,,"24/Jan/12 06:37;pwendell@gmail.com;MAPREDUCE-3451.v1.patch.txt;https://issues.apache.org/jira/secure/attachment/12511640/MAPREDUCE-3451.v1.patch.txt","27/Mar/12 06:43;pwendell@gmail.com;MAPREDUCE-3451.v2.patch.txt;https://issues.apache.org/jira/secure/attachment/12520071/MAPREDUCE-3451.v2.patch.txt","17/Apr/12 07:59;pwendell@gmail.com;MAPREDUCE-3451.v3.patch.txt;https://issues.apache.org/jira/secure/attachment/12522923/MAPREDUCE-3451.v3.patch.txt","09/May/12 00:56;pwendell@gmail.com;MAPREDUCE-3451.v4.patch.txt;https://issues.apache.org/jira/secure/attachment/12526086/MAPREDUCE-3451.v4.patch.txt","09/May/12 10:56;qwertymaniac;MAPREDUCE-3451.v5.patch;https://issues.apache.org/jira/secure/attachment/12526140/MAPREDUCE-3451.v5.patch","12/Jul/12 00:09;pwendell@gmail.com;MAPREDUCE-3451.v6.patch;https://issues.apache.org/jira/secure/attachment/12536145/MAPREDUCE-3451.v6.patch","12/Jul/12 00:13;pwendell@gmail.com;MAPREDUCE-3451.v7.patch;https://issues.apache.org/jira/secure/attachment/12536147/MAPREDUCE-3451.v7.patch","12/Jul/12 18:10;pwendell@gmail.com;MAPREDUCE-3451.v8.patch;https://issues.apache.org/jira/secure/attachment/12536248/MAPREDUCE-3451.v8.patch","12/Jul/12 23:51;pwendell@gmail.com;MAPREDUCE-3451.v9.patch;https://issues.apache.org/jira/secure/attachment/12536308/MAPREDUCE-3451.v9.patch",,,,,,,,,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,2011-11-22 18:02:06.832,,,false,,,,,,,,,,,,,,,,,,217997,Reviewed,,,,Sun Mar 17 06:16:23 UTC 2013,,,,,,,"0|i0178f:",4941,,,,,,,,,,,,,,,,,,,,,"22/Nov/11 18:02;acmurthy;Thanks for taking this up Patrick. Let me know if you need help.","29/Nov/11 23:05;pwendell@gmail.com;I have a few questions about the way that schedulers are implemented in MR2.

It's not clear to me whether the Scheduler class used by the Resource Manager is responsible for only inter-application scheduling or both inter- ant intra-application scheduling. It looks to me like only the former is supported, because the resource requests (via allocate()) do not give any meta-data regarding the user initiating the request. So if we wanted to enforce Fair Sharing for jobs requested from a single AM, this doesn't seem possible. When I look at the Capacity Scheduler, for instance, it seems like allocation requests are assigned to queues based only on the application from which they are made, not the actual user launching the job.

Is the idea that, in a shared cluster, each administrative unit has its own MR Application Master running? 

I have some follow-up but first want to see if this is actually a correct interpretation of the design spec.
","30/Nov/11 05:51;mahadev;Patrick,
  I'd say it could do both. If you look at the Capacity Scheduler, it  does inter application scheduling (deciding on which application gets a resource) and then also decides which priority request within the application should be satisfied first (in CS its the highest first).

Take a look at 

{code}
LeafQueue.assignContainers()
ParentQueue.assignContainers()
{code}

As for the user information, its is available in the SchedulerApp.getUser(). 

Also, a single AM amounts to a single job currently, I am not sure what you mean by ""jobs requested for a single AM"".

Hope that helps.
","30/Nov/11 06:02;pwendell@gmail.com;Ah, perhaps my issue is a broader misunderstanding of the MR2 architecture.

I thought the Application Master was a long lived service for a particular application (given its name). I.e. for MapReduce you would expect to have a single AM running and coordinating submissions for several jobs. Your last sentence seems to suggest this is incorrect. Is a new AM instantiated for *each* MR job?

Also, you mentioned priorities. I can't find any details on the scheduling semantic associated with priorities. I'm assuming those are supposed to convey the scheduler ordering for requests from a particular AM? I.e. all priority 1 containers should be scheduled before the first priority 2 container? This made no sense to me before, but it makes more sense if there is a new AM for each job.","30/Nov/11 06:02;pwendell@gmail.com;Ah, perhaps my issue is a broader misunderstanding of the MR2 architecture.

I thought the Application Master was a long lived service for a particular application (given its name). I.e. for MapReduce you would expect to have a single AM running and coordinating submissions for several jobs. Your last sentence seems to suggest this is incorrect. Is a new AM instantiated for *each* MR job?

Also, you mentioned priorities. I can't find any details on the scheduling semantic associated with priorities. I'm assuming those are supposed to convey the scheduler ordering for requests from a particular AM? I.e. all priority 1 containers should be scheduled before the first priority 2 container? This made no sense to me before, but it makes more sense if there is a new AM for each job.","30/Nov/11 06:40;tlipcon;Hi Patrick. Yes, there is an AM-per-job in the current implementation. There's some folks I know looking at bundling full workflows into a single AM, but they would still be fairly short-lived and associated with a single submitting user.","30/Nov/11 07:04;mahadev;@Patrick,
 Yes the priorities are for the requests by a single AM. There are no priorities across jobs(/AM's). A DAG of MR jobs in a single AM is very much a possibility and we have been talking to PIG/Oozie folks for such a possibility, but as of now an AM maps to a single job.","01/Dec/11 22:57;pwendell@gmail.com;@Mahedev and others

A follow up question. In the MR1 code, there was a way to assign a priority to a specific job. This let users whose jobs all go to the same queue give more importance to certain jobs over others and it is used in the fair scheduler.

The MR2 code does seem to have similar support, i.e. per-AM priorities. For instance, in ApplicationSubmissionContext we see:

{noformat} 
  /**
   * Set the <code>Priority</code> of the application.
   * @param priority <code>Priority</code> of the application
   */
  @Public
  @Stable
  public void setPriority(Priority priority);
{noformat} 

The MR AM also seems to support per-job prioritization, though it doesn't look like this code is called anywhere.
{noformat}
  /**
   * Set the priority of a running job.
   * @param priority the new priority for the job.
   * @throws IOException
   */
  public void setPriority(JobPriority priority) 
      throws IOException, InterruptedException {
{noformat}

Such priorities are *not*, however, passed downstream to the scheduler as far as I can see (they aren't reflected in SchedulerApp class). I can just patch the code to pass these to the scheduler, but I'm curious whether there is something I'm missing here, especially given the last comment which seems to contradict that priorities can be applied across jobs.","01/Jan/12 21:52;pwendell@gmail.com;This JIRA has been chunked up into smaller JIRAs to make the patch submission and review more sane.

I've attached a patch for the first of these sub-tasks.
https://issues.apache.org/jira/browse/MAPREDUCE-3600","24/Jan/12 06:37;pwendell@gmail.com;This is patch aggregating all sub-ticket patches and also bringing up to date with trunk.

This patch is ready for review, thanks.","16/Mar/12 18:07;phunt;Patrick unfortunately while the patch applies to the current trunk, it does not compile. Could you update? Thanks.","27/Mar/12 06:43;pwendell@gmail.com;This patch should apply cleanly to trunk, thanks.","29/Mar/12 16:33;ahmed.radwan;Thanks Patrick.

The patch successfully applies and compiles on trunk. The included new tests also successfully run.

I'll postpone specific code comments, since I have some basic questions: 

The old fair scheduler used slots as the basic allocation units, and calculating shares and other features (like preemption) relied on this model. In MR2 we have containers with variable resource attributes. Currently the only supported attribute is memory, but it is planned to add more attributes (network bandwidth, cpu, etc.). So it is not clear for me how this mapping is handled. Are you envisioning combing such attributes for calculating shares, how is this done? and how preemption will be done in such case, for example in the old MR1, the most recent slot was killed. But now slots are not equal and this complicate the choices. I am just using preemption as an example. Can you please elaborate on that?

I also see that the patch doesn't include the web ui components. I think postponing them to follow-up work is fine. But if someone wants to test the patch on a real cluster, can you please highlight the steps needed to setup a cluster using the new fair scheduler and if there are changes in setup compared to the old MR1 version.","29/Mar/12 17:24;pwendell@gmail.com;Hey Ahmed,

MR2 does support multiple resource types, but as you point out, currently only memory is implemented. Right now this scheduler basically treats memory similarly to the way slots were handled in the old fair scheduler, with the only key difference being that memory can be allocated in variable size chunks.

There is no widely accepted definition of what ""fairness"" means with multiple resource types, but there have been proposals from the academic community (see http://static.usenix.org/events/nsdi11/tech/full_papers/Ghodsi.pdf) which could ultimately be factored into the scheduler if and when multiple resource types are introduced. Having several resource dimensions complicates the scheduling decision substantially and will require major changes to existing schedulers (such as the capacity scheduler) as well. In all of these cases, existing approaches depend on a single fungible resource about which you can express minimum capacities (or shares), target capacities (or shares), etc. These semantics don't make sense with multiple resource types.

Right now the preemption logic preempts lowest priority containers first. This is the best general-purpose metric I could think of for choosing what to preempt.

I will include a write-up about deployment in the scheduler's current form as well in an upcoming patch.","06/Apr/12 20:15;tomwhite;Overall this looks like a great addition. Initial review feedback:

* It looks like what were called ""pools"" in the MR1 version are now ""queues"", to fit with the term that the capacity scheduler uses. Is that correct? If so, then the fair scheduler code should use the same term throughout.
* What's the locking order for the fair scheduler classes and the classes that call them? It might be worth documenting it somewhere in the code.
* I noticed that not all the unit tests from MR1 have been ported. Are you planning on including the rest? 
* Configuration strings are duplicated and spread through different classes. How about creating a FairSchedulerConfiguration class to hold all the constants (like capacity scheduler does)?
* Seeing that the configuration is changing compared to MR1, is it worth changing the allocations file to be in Hadoop configuration format? Having one less format to deal with would be an improvement IMO.
* Annotate all classes as @Private @Unstable. Some are not annotated at all, even though they should be considered private.
* PoolSchedulable.assignContainer has a log line which is called frequently (""Node offered to pool""), so should really be debug level.
* The Scheduler UI link gives a 500 error - I know you plan on fixing this, but I would have expected a 404.
* Nit: PoolManager has two license headers
* Nit: Resources class spelling: ""Mutliply""
* Nit: SchedulerApp - schedulingOpportunities comment has been changed from javadoc comment in MR1 - change back?

What testing have you done? I tried it on a pseudo distributed cluster and successfully ran a job with the fair scheduler.

This will need some documentation (in hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/) - are you planning on doing that as a separate JIRA? It would also be useful to call out the configuration differences compared to the MR1 fair scheduler. (E.g. I noticed that yarn.scheduler.fair.user-as-default-queue is new.)
","10/Apr/12 18:01;ahmed.radwan;Hi Patrick,

There are also these comments/nits: 
- RMContainer.java: Added a unused ""import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl;""
- Many of the inline docs have spelling mistakes. Please review them.
- SchedulerApp.java: The following was removed from the docs: [Each time the scheduler
   asks the application for a task at this priority, it is incremented,
   and each time the application successfully schedules a task, it
   is reset to 0.], please add it back.
- Add javadocs and setup and configuration docs.","15/Apr/12 05:09;pwendell@gmail.com;Hey Tom/Ahmed,

Thanks for the comments - I'm addressing them now. For the documentation pointer (hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/) - I'm unfamiliar with the markup being used in these documents (it looks like the Velocity engine maybe?). Is there any way to render these files locally as I'm writing some preliminary docs for the FS?

- Patrick","15/Apr/12 06:57;atm;bq. For the documentation pointer (hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/) - I'm unfamiliar with the markup being used in these documents (it looks like the Velocity engine maybe?). 

Take a look at this page for info on the markup: http://maven.apache.org/doxia/references/apt-format.html

bq. Is there any way to render these files locally as I'm writing some preliminary docs for the FS?

From the repo root, `cd hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site' and run `mvn site'. You can then look at the rendered page by browsing to ""file:///<path to repo>/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/target/site/<file name>.html""","17/Apr/12 07:46;pwendell@gmail.com;Thanks for the feedback. About to attach a patch with the following changes:

- Updated the terminology from “pool” to “queue” to be consistent. 
- I added annotations to all classes.
- Fixed the debug → log statement
- I modified the web UI to give a more precise error message in the logs, but it still has a generic 500 in the web page. MRv2 uses a custom built MVC framework based on Guice dependency injection. I’m embarrassed to say that I spend 40 minutes trying to figure out how to make it render a 404 and couldn’t. It seems like it’s not easy to trigger a 404 from within a view, rather than at the URL router. 
- Extra license header removed
- Noted spelling errors fixed.
- There is extensive test coverage in the MRS FS I’d like to get there eventually but probably not with this first commit. I tried to included tests of most of the core functionality. We can add this as a JIRA, though.
- I consolidated the configuration constants as you suggested. For now I’m going to leave the queue manifest file as its own thing unless this is a deal-breaker. There is a lot of logic in parsing that file which I’d prefer to leave as-is since it’s very well tested. While the term names have changed it’s essentially identical configuration to the MR1 FS, so leaving things as-is should ease the transition for people running current clusters. It’s also a fundamentally hierarchical type of configuration, and the apache conf is all flat key/value space which will be much more clunky.

I haven’t done much testing other than on a single node. One reason I want to get this in is that some are waiting on having something in trunk to start testing. The build is stable and the unit tests pass, so I hope that is sufficient to get this in there with the caveat that things may be brittle when run on a real cluster.
 
I also added an updated version of the documentation (even the MR1 version wasn’t totally up-to-date with changes that have been made in the configuration options). This will need some more work going forward but it should give people to the basic configuration/installation info they need to get started testing this.","17/Apr/12 07:59;pwendell@gmail.com;New patch including response to feedback and documentation file.","20/Apr/12 00:46;tomwhite;Patrick, thanks for addressing the feedback and also for adding documentation. I think it's fine to improve test coverage and revisit the configuration format in follow on JIRAs, so I'm +1 for committing this. I'm going to run this through Jenkins now.","20/Apr/12 02:36;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12522923/MAPREDUCE-3451.v3.patch.txt
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified test files.

    -1 javadoc.  The javadoc tool appears to have generated 2 warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 21 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests:
                  org.apache.hadoop.yarn.server.TestContainerManagerSecurity
                  org.apache.hadoop.yarn.server.resourcemanager.security.TestApplicationTokens
                  org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService
                  org.apache.hadoop.yarn.server.resourcemanager.resourcetracker.TestNMExpiry
                  org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization
                  org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs
                  org.apache.hadoop.mapred.TestClientRedirect
                  org.apache.hadoop.mapreduce.security.TestJHSSecurity

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2259//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2259//artifact/trunk/hadoop-mapreduce-project/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-resourcemanager.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2259//console

This message is automatically generated.","20/Apr/12 16:59;ahmed.radwan;Thanks Patrick for the comments, +1","25/Apr/12 11:34;vicaya;bq. It seems like it’s not easy to trigger a 404 from within a view, rather than at the URL router.

You can trigger a 404 or any http response code anywhere (preferably in controller code as view is not guaranteed to be fully buffered for long responses by design) by calling Controller#setStatus (via context().setStatus in views (if it's not yet flushed) and then throw an WebAppException (do not use generic RuntimeException). You can bind ErrorPage.class in a webapp to any view class if you want a custom error page.","09/May/12 01:11;pwendell@gmail.com;Just added a patch addressing the legitimate findbugs and javadoc warnings. There were some findbugs warnings about concurrency that seemed like false positives - not sure if there is a way to suppress them.

I can't re-create the core test failures locally... they seem related to port contention issues and Kerberos issues around the hostname of the Hudson machine. I don't see any direct relationship between these test failures and the code in this patch. Could we at least re-run this in Hudson to make sure that these tests are deterministically failing?","09/May/12 01:12;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12526086/MAPREDUCE-3451.v4.patch.txt
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified test files.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 9 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2369//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2369//artifact/trunk/trunk/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-resourcemanager.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2369//console

This message is automatically generated.","09/May/12 06:17;pwendell@gmail.com;Okay looks like that patch fixed everything except for the findbugs, there are some member variables which are written only while holding a lock but can be read outside of the lock and findbugs doesn't like this. I consider these false positives - any idea how to suppress?
","09/May/12 10:56;qwertymaniac;Per MAPREDUCE-3812, I've tweaked your patch a bit to make it use the new configs for min/max RM allocation (memory for containers).

One thing I noticed when tweaking your patch, is that there are quite a bit of trailing white-spaces and possibly a few occurrences of tab+space mix you may wanna clean up.

The diff I've added to your patch overall is below:

{code}
diff --git a/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerConfiguration.java b/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerConfiguration.java
index 35eafc5..e888be8 100644
--- a/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerConfiguration.java
+++ b/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerConfiguration.java
@@ -4,6 +4,7 @@ import java.io.File;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.yarn.api.records.Resource;
+import org.apache.hadoop.yarn.conf.YarnConfiguration;
 import org.apache.hadoop.yarn.server.resourcemanager.resource.Resources;
 
 public class FairSchedulerConfiguration extends Configuration {
@@ -13,15 +14,7 @@ public class FairSchedulerConfiguration extends Configuration {
   
   protected static final String ALLOCATION_FILE = CONF_PREFIX + ""allocation.file"";
   protected static final String EVENT_LOG_DIR = ""eventlog.dir"";
-  
-  /** The minimum size container the scheduler will ever allocate. */
-  protected static final String MINIMUM_MEMORY_MB = CONF_PREFIX + ""minimum-allocation-mb"";
-  protected static final int    DEFAULT_MINIMUM_MEMORY_MB = 512;
-  
-  /** The maximum size container the scheduler will ever allocate. */
-  protected static final String MAXIMUM_ALLOCATION_MB = CONF_PREFIX + ""maximum-allocation-mb"";
-  protected static final int    DEFAULT_MAXIMUM_ALLOCATION_MB = 10240;
-  
+
   /** Whether to use the user name as the queue name (instead of ""default"") if
    * the request does not specify a queue. */
   protected static final String  USER_AS_DEFAULT_QUEUE = CONF_PREFIX + ""user-as-default-queue"";
@@ -60,17 +53,21 @@ public class FairSchedulerConfiguration extends Configuration {
     super(conf);
     addResource(FS_CONFIGURATION_FILE);
   }
-    
+
   public Resource getMinimumMemoryAllocation() {
-    int mem = getInt(MINIMUM_MEMORY_MB, DEFAULT_MINIMUM_MEMORY_MB);
+    int mem = getInt(
+        YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB,
+        YarnConfiguration.DEFAULT_RM_SCHEDULER_MINIMUM_ALLOCATION_MB);
     return Resources.createResource(mem);
   }
-  
+
   public Resource getMaximumMemoryAllocation() {
-    int mem = getInt(MAXIMUM_ALLOCATION_MB, DEFAULT_MAXIMUM_ALLOCATION_MB);
+    int mem = getInt(
+        YarnConfiguration.RM_SCHEDULER_MAXIMUM_ALLOCATION_MB,
+        YarnConfiguration.DEFAULT_RM_SCHEDULER_MAXIMUM_ALLOCATION_MB);
     return Resources.createResource(mem);
   }
-  
+
   public boolean getUserAsDefaultQueue() {
     return getBoolean(USER_AS_DEFAULT_QUEUE, DEFAULT_USER_AS_DEFAULT_QUEUE);
   }
{code}

bq. Okay looks like that patch fixed everything except for the findbugs, there are some member variables which are written only while holding a lock but can be read outside of the lock and findbugs doesn't like this. I consider these false positives - any idea how to suppress?

I didn't take a look at which specific mem-var you're discussing here, but what happens if one reads it when its being updated? Does your locking mech cover this scenario too? I remember making the same mistake a while ago at https://issues.apache.org/jira/browse/MAPREDUCE-1347?focusedCommentId=13003257&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13003257 but that may be irrelevant here.

In case you wish to add it to ignores, see file {{hadoop-mapreduce-project/hadoop-yarn/dev-support/findbugs-exclude.xml}} and add appropriate entries for your discovered warnings to suppress them. I'd recommend suppressing them after the review/over another ticket and just leave as a comment for now that this may not be harmful as it looks.","09/May/12 11:14;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12526140/MAPREDUCE-3451.v5.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified test files.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 9 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2371//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2371//artifact/trunk/trunk/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-resourcemanager.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2371//console

This message is automatically generated.","14/May/12 22:25;tomwhite;Rather than simply porting the Fair Scheduler to MR2, I think we should consider whether the Capacity Scheduler can support the majority of use cases. Having a single scheduler would simplify things for users and administrators. Just like we merged the features of the MapReduce MultipleOutputs and MultipleOutputFormat libraries, moving to a single main scheduler would reduce confusion.

Capacity Scheduler lacks a few features that FS has - but there are JIRAs for these (see MAPREDUCE-3210, MAPREDUCE-3938, MAPREDUCE-4257). Also, note that this doesn't preclude having other schedulers, since they are pluggable in MR2, so having other schedulers for experimentation, other use cases etc is still supported.
","14/May/12 23:16;matei;I think the fair scheduler can be one of the alternative schedulers. Why not leave it in? Apart from the features it implements, another major advantage is that it will be compatible with fair scheduler config files that people have set up before moving to MR2. This was one of the main reasons why people have been asking us for it -- they want to be sure that their cluster will share resources the same way as before.","15/May/12 20:53;tomwhite;Matei - I'm not against this going in (although we should put it in a separate module), but I thought it would be a good time to have a discussion about supporting a single main scheduler in YARN. If we could combine features from the various schedulers in MR1 to YARN then we would have less code to support and test, and it would be less confusing for users. What do you think? Do you agree that this would be a useful goal?","15/May/12 21:21;matei;Having a single very feature-rich scheduler is certainly useful, but that's a separate discussion. My point was just that some people want compatibility with the MR1 Fair Scheduler as a feature -- not having that is an obstacle to their adopting MR2.

I also think it would be hard to design a single scheduler that will support all the features people might want in the future, for the same reason that it was hard to get different development groups to build a single scheduler in MR1. You just can't anticipate all the needs, and how various features will interact -- for example, Yahoo! had to remove preemption from the capacity scheduler because it complicated the design. We are also planning to add some features to the Fair Scheduler ourselves, such as multi-resource fairness, that might require major changes to other schedulers even if we were to wait on them adopting the existing features.","19/May/12 07:03;pwendell@gmail.com;I'm not sure I understand what it would mean to have a single featureful scheduler.

There is already an abstraction layer built into YARN to allow multiple scheudling policies. Aside from the logic specific to the policy itself, the code for dealing with resource management is re-used amongst schedulers. That is, when someone is using the Fair Scheduler (vs the Capacity or FIFO scheduler) they are using mostly the same RM code save for the scheduling logic itself. Furthermore, several shared classes are used by both the Fair and Capacity schedulers, such as the Queue interface and the SchedulerApp class - these common classes are ""tested"" by users of both schedulers.

The alternative suggested seems to be having a monolithic scheduler class that can enact different policies depending on some configuration option. Any sensible implementation of that approach would abstract out the scheduling policy and I think you'd get what's already there now.

Or maybe the idea is that everyone using Hadoop will want the same scheduling policy (modulo minor configurations like timeouts, capacities, etc.). That seems unlikely to me given the diversity of Hadoop deployments and the fact that both of the currently available schedulers in MR1 are widely used.

Also, just a note - adding preemption to the capacity scheduler will not make it equal the Fair Scheduler. These are fundamentally different approaches which will become substantailly more different when multiple resource types are added to the YARN RM stack. 

I strongly agree that having single implementations of common classes is a win in terms of support and testability - but I think this implementation mostly acheives that goal. ","12/Jul/12 00:09;pwendell@gmail.com;Bringing this back to trunk again and adding one small change to make config files more backwards incompatible.","12/Jul/12 00:13;pwendell@gmail.com;Forgot to include some whitespace fixes in last patch","12/Jul/12 00:25;pwendell@gmail.com;So I spent some time offline talking with Tom and I think we're ready to move forward with committing this to MR2. The primary goals are:

1) Let people who use the MR1 fair scheduler have a drop in equivalent for MR2
- and -
2) Keep as much code shared as possible between MR2 schedulers given that the overlap in substantial functionality

I think we're clearly accomplishing (1) and well on our way to (2), but subsequent patches might find more opportunity for code re-use.

This also doesn't include some necessities like the Web UI and some nice-to-have extensions like hierarchical queues. That said I'd like to get this committed as a starting point build from there, given that it's already a pretty large commit.","12/Jul/12 00:45;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12536147/MAPREDUCE-3451.v7.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 9 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2576//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2576//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-resourcemanager.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2576//console

This message is automatically generated.","12/Jul/12 00:46;tomwhite;Thanks for summarizing the goals here Patrick. I agree that having a single scheduler is not realistic at this point. 

The latest change (and test) supporting the ""pool"" element in the allocation XML file for backwards compatibility looks good to me.

+1 (pending Jenkins)","12/Jul/12 14:41;acmurthy;Patrick I took a brief look, mostly looks great! I agree having different schedulers is very useful.

Minor nits: I don't think it's worth modifying common infrastructure such as SchedulerApp right now - maybe you can just extend them where necessary?","12/Jul/12 14:52;acmurthy;In fact, we should probably make SchedulerApp etc. an interface and have different implementations for CS, FS etc. to minimize interference. Thoughts?","12/Jul/12 17:02;pwendell@gmail.com;Hey Arun - I think extending it is a better move for now.

If we decided to move SchedulerApp to an interface, 95% of the implementation would be the same between the FS/CS. The only differences are in the way delay scheduling works for the FairScheduler.

As far as I can tell - the idea behind SchedulerApp is to deal with application lifecycle management from the perspective of the scheduler. That's helpful to have and made writing the FS simpler. This is likely to be useful for future schedulers well. In terms of maximizing code sharing I would be in favor of keeping it collectively managed.","12/Jul/12 18:10;pwendell@gmail.com;This patch extends rather than modifies the SchedulerApp class for use in the Fair Scheduler.

I cleaned up some tabs/whitespace issues in SchedulerApp.java and those are retained in this patch. But there are no code changes to SchedulerApp.java.","12/Jul/12 23:51;pwendell@gmail.com;This patch moves a TestFSSchedulerApp into the fair/ test subdirectory.

It also reverts whitespace fixes in SchedulerApp.java (so that file is now totally unchanged from trunk). That is out of scope for this JIRA.","13/Jul/12 00:42;tucu00;+1","13/Jul/12 00:48;tucu00;Nice job, thanks Patrick. Committed to trunk and branch-2.","13/Jul/12 00:58;hudson;Integrated in Hadoop-Common-trunk-Commit #2460 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2460/])
    MAPREDUCE-3451. Port Fair Scheduler to MR2 (pwendell via tucu) (Revision 1361020)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1361020
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/resource/Resources.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerEventType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AllocationConfigurationException.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AppSchedulable.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSQueueSchedulable.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerApp.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerConfiguration.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerEventLog.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FifoAppComparator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/NewJobWeightBooster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/QueueManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/Schedulable.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/SchedulingAlgorithms.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/SchedulingMode.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/WeightAdjuster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RmController.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerApp.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/FairScheduler.apt.vm
","13/Jul/12 01:04;hudson;Integrated in Hadoop-Hdfs-trunk-Commit #2526 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2526/])
    MAPREDUCE-3451. Port Fair Scheduler to MR2 (pwendell via tucu) (Revision 1361020)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1361020
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/resource/Resources.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerEventType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AllocationConfigurationException.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AppSchedulable.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSQueueSchedulable.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerApp.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerConfiguration.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerEventLog.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FifoAppComparator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/NewJobWeightBooster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/QueueManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/Schedulable.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/SchedulingAlgorithms.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/SchedulingMode.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/WeightAdjuster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RmController.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerApp.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/FairScheduler.apt.vm
","13/Jul/12 01:51;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #2479 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2479/])
    MAPREDUCE-3451. Port Fair Scheduler to MR2 (pwendell via tucu) (Revision 1361020)

     Result = FAILURE
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1361020
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/resource/Resources.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerEventType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AllocationConfigurationException.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AppSchedulable.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSQueueSchedulable.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerApp.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerConfiguration.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerEventLog.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FifoAppComparator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/NewJobWeightBooster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/QueueManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/Schedulable.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/SchedulingAlgorithms.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/SchedulingMode.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/WeightAdjuster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RmController.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerApp.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/FairScheduler.apt.vm
","13/Jul/12 03:17;acmurthy;bq. As far as I can tell - the idea behind SchedulerApp is to deal with application lifecycle management from the perspective of the scheduler. 

I wish I had a chance to respond to Patrick before this was committed... IAC, we should plan to allow the schedulers to deviate in significant ways (which is the whole point of having multiple schedulers) and, as a result, minimizing interference is a key goal.

Maybe we should open a jira to make SchedulerApp an interface?

I originally kept it common across FifoScheduler and CapacityScheduler since, for the degenerate case of a single queue, they are identical (currently) and planned to pull it out as a separate one once they deviated.","13/Jul/12 04:11;pwendell@gmail.com;We have several pieces of follow-up around this, and one of them can be looking at moving SchedulerApp to an interface (this would involve changing the other two schedulers as well to some extent).

I'd like to see that change occur in tandem with better documentation for common scheduling components like SchedulerApp. The design goals of the RM scheduling code are currently opaque to anyone trying to add new code. Better documentation would make it clearer how to hook in a new scheduler the right way.","13/Jul/12 11:41;hudson;Integrated in Hadoop-Hdfs-trunk #1102 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1102/])
    MAPREDUCE-3451. Port Fair Scheduler to MR2 (pwendell via tucu) (Revision 1361020)

     Result = FAILURE
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1361020
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/resource/Resources.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerEventType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AllocationConfigurationException.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AppSchedulable.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSQueueSchedulable.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerApp.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerConfiguration.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerEventLog.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FifoAppComparator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/NewJobWeightBooster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/QueueManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/Schedulable.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/SchedulingAlgorithms.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/SchedulingMode.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/WeightAdjuster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RmController.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerApp.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/FairScheduler.apt.vm
","13/Jul/12 14:05;hudson;Integrated in Hadoop-Mapreduce-trunk #1135 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1135/])
    MAPREDUCE-3451. Port Fair Scheduler to MR2 (pwendell via tucu) (Revision 1361020)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1361020
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/resource/Resources.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainer.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerEventType.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AllocationConfigurationException.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AppSchedulable.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSQueueSchedulable.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerApp.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerConfiguration.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerEventLog.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FifoAppComparator.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/NewJobWeightBooster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/QueueManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/Schedulable.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/SchedulingAlgorithms.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/SchedulingMode.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/WeightAdjuster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RmController.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerApp.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/FairScheduler.apt.vm
","13/Jul/12 17:12;acmurthy;Unfortunately we've introduced findbugs warnings, Alejandro you missed them?

I'll file another jira.","13/Jul/12 17:44;acmurthy;I'm seeing another problem after I did a 'git pull' today after this commit.

My IDE complains bitterly that TestFSSchdulerApp has the wrong package declaration.

[~tucu00]: Should we revert this and fix the package problem and the findbugs warnings?","13/Jul/12 17:45;acmurthy;bq. My IDE complains bitterly that TestFSSchdulerApp has the wrong package declaration.

Also, my IDE complains that Queue needs an import.","13/Jul/12 17:50;acmurthy;Digging more - looks like I don't see the fair-scheduler tests show up in hadoop-yarn-server-resourcemanager-3.0.0-SNAPSHOT-tests.jar.

Is that the reason why we don't see a compilation failure even though my IDE complains?

Some maven work remaining, perhaps?","13/Jul/12 17:52;acmurthy;bq. Digging more - looks like I don't see the fair-scheduler tests show up in hadoop-yarn-server-resourcemanager-3.0.0-SNAPSHOT-tests.jar.

Spoke too soon, I see the following:

{noformat}
$ jar -tvf ./hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/target/hadoop-yarn-server-resourcemanager-3.0.0-SNAPSHOT-tests.jar | grep -i fair
     0 Fri Jul 13 10:37:20 PDT 2012 org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/
   324 Fri Jul 13 10:37:20 PDT 2012 org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler$1.class
  1463 Fri Jul 13 10:37:20 PDT 2012 org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler$MockClock.class
 25975 Fri Jul 13 10:37:20 PDT 2012 org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.class
{noformat}

However, I don't see TestFSSchedulerApp, not sure why I don't see a compilation failure given the wrong package declaration.

Patrick - any idea?","13/Jul/12 18:03;kkambatl;Arun, in the last version of the patch, Patrick moved TestFSSchdulerApp from o.a.h.y.s.rm.scheduler to o.a.h.y.s.rm.scheduler.fair. He might have forgotten to update the package name accordingly. From what I understand, because of the wrong package name, the class file is still created one level above.","13/Jul/12 18:04;kkambatl;Do you suggest creating a clean-up JIRA and addressing these issues?","13/Jul/12 19:43;pwendell@gmail.com;Yes the error is related to the last minute movement of that test into the Fair package.

Karthik - could you quickly patch this so we can get the build stabilized? If you can fix the findbugs quickly that would be great too - as i said earlier these are false positives but we might be able to coerce findbugs into not spouting warnings.","13/Jul/12 19:45;pwendell@gmail.com;It's a one line change to the package header. The findbugs issues are discussed further up in this Jira and were also discussed during the last round of reviews.","13/Jul/12 20:25;acmurthy;Patrick - the last comment/advice was to supress them (from Harsh). Why weren't they?

Now, all patch builds are failing complaining about the findbugs warnings... ","13/Jul/12 20:33;tucu00;I'm on amending the patch","13/Jul/12 23:06;hudson;Integrated in Hadoop-Common-trunk-Commit #2466 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2466/])
    MAPREDUCE-3451. Amendment, excluding findbugs warnings (tucu) (Revision 1361436)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1361436
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/dev-support/findbugs-exclude.xml
","13/Jul/12 23:10;hudson;Integrated in Hadoop-Hdfs-trunk-Commit #2532 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2532/])
    MAPREDUCE-3451. Amendment, excluding findbugs warnings (tucu) (Revision 1361436)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1361436
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/dev-support/findbugs-exclude.xml
","13/Jul/12 23:54;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #2486 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2486/])
    MAPREDUCE-3451. Amendment, excluding findbugs warnings (tucu) (Revision 1361436)

     Result = FAILURE
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1361436
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/dev-support/findbugs-exclude.xml
","14/Jul/12 00:03;zhihyu@ebaysf.com;HBASE-6395 has been created for putting TestFSSchedulerApp in the right package.","14/Jul/12 00:06;zhihyu@ebaysf.com;MAPREDUCE-4445 is the correct JIRA, sorry about this.","14/Jul/12 11:40;hudson;Integrated in Hadoop-Hdfs-trunk #1103 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1103/])
    MAPREDUCE-3451. Amendment, excluding findbugs warnings (tucu) (Revision 1361436)

     Result = FAILURE
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1361436
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/dev-support/findbugs-exclude.xml
","14/Jul/12 14:04;hudson;Integrated in Hadoop-Mapreduce-trunk #1136 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1136/])
    MAPREDUCE-3451. Amendment, excluding findbugs warnings (tucu) (Revision 1361436)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1361436
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/dev-support/findbugs-exclude.xml
","13/Mar/13 07:15;vkruglikov;[~pwendell@gmail.com]:
FairScheduler.apt.vm description for the ""minResources"" queue property doesn't say what the units are: megabytes or bytes? For example, mapreduce.map.memory.mb is in megabytes. Could you please clarify the units for ""minResources""? Many thanks!","17/Mar/13 06:16;qwertymaniac;Hi Vitaly,

The minResources (and also maxResources) are to be set as megabytes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Grouping using hashing instead of sorting,MAPREDUCE-1639,12460425,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,jsensarma,jsensarma,27/Mar/10 13:57,02/May/13 02:29,12/Jan/21 09:52,,,,,,,,,,,,,,,,1,,,,,"most applications of map-reduce care about grouping and not sorting. Sorting is a (relatively expensive) way to achieve grouping. In order to achieve just grouping - one can:

- replace the sort on the Mappers with a HashTable - and maintain lists of key-values against each hash-bucket.
- key-value tuples inside each hash bucket are sorted - before spilling or sending to Reducer. Anytime this is done - Combiner can be invoked.
- HashTable is serialized by hash-bucketid. So merges (of either spills or Map Outputs) works similar to today (at least there's no change in overall compute complexity of merge)

Of course this hashtable has nothing to do with partitioning. it's just a replacement for map-side sort.

--

this is (pretty much) straight from the MARS project paper: http://www.cse.ust.hk/catalac/papers/mars_pact08.pdf. They report a 45% speedup in inverted index calculation using hashing instead of sorting (reference implementation is NOT against Hadoop though).",,aaa,aah,acmurthy,anty,ashutoshc,atm,avik_dey@yahoo.com,binlijin,cdouglas,cutting,davelatham,dcapwell,decster,dhruba,djmdata,edmazur,eli,gates,gchen,gemini5201314,hammer,he yongqiang,jerrychenhf,jothipn,kaykay.unique,liangly,lianhuiwang,mahadev,nemon,nourl,philip,qwertymaniac,ravidotg,rekhajoshm,revans2,sandyr,schen,srivas,sseth,tlipcon,tomwhite,vicaya,wangmeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-3235,HADOOP-7761,,,,,,,,,,,,,,,,MAPREDUCE-2454,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-03-27 16:27:16.93,,,false,,,,,,,,,,,,,,,,,,92098,,,,,Tue Dec 11 01:51:26 UTC 2012,,,,,,,"0|i0e90f:",81222,,,,,,,,,,,,,,,,,,,,,"27/Mar/10 16:27;omalley;Three comments spring to mind:
  1. A completely new data path is very expensive.
  2. A much easier way to achieve their result is to include the hash in the key as the first 4 bytes of the serialization, then have a comparator that uses the first 4 bytes as memcmp before it uses the dispatched raw compare method.
  3. Having a library of memcmp'able keys would be even better. Basically, you'd need to implement org.apache.hadoop.io.memcmp.(Int|Long|String|Array) that implemented Writable and a marker interface.","27/Mar/10 17:34;jsensarma;realize this is non-trivial.

> 2. A much easier way to achieve their result is to include the hash in the key as the first 4 bytes of the serialization, then have a comparator that uses the first 4 bytes as memcmp before it uses the dispatched raw compare method.

not sure about this. it doesn't change the fact that all keys are being compared against each other. just because we compare fewer bytes doesn't mean that the number of comparisons goes down. it's still n.logn. the cost of each comparison is lower though.

if we did a radix sort (first) on the leading 4 (hashed) bytes - then yeah - it changes things. but at that point we are taking (pretty much) the same approach as the paper and talking about a different data path.","27/Mar/10 18:41;jsensarma;following up on the previous comment - reading the paper again closely - it's no longer clear to me that the paper actually implements what i filed or whether it's closer to how Owen interpreted it. But i think it's true that sorting only within hash buckets only is fundamentally lower complexity than sorting everything together.

---

i am also wondering how different this is from current data path. today we anyway bucket map outputs into one bucket per reducer and sort therein. this is simply saying that we have lots of buckets per reducer (not one) - we are still sorting inside each of those - but because we are sorting much smaller sets - the complexity is lowered. 

so on the mapper side - we can (sort of) just pretend that the number of reducers is much much higher. Except when it's time to send off the data to the reducer (or spill) - we send a bunch of buckets. but this is not transparent to the merge code - it now has to merge taking the bucketid into consideration. if we prefixed each map-output key with the bucketid - then it's transparent to the merge code. so definitely some changes required - but maybe tractable?
","28/Mar/10 04:20;dhruba;So, then are you saying that thsi is equivalent to just increasing the number of reducers? And make each reducer receive the records from one bucket.","28/Mar/10 06:23;jsensarma;no - the number of reducers is determined by other criteria (people here are expert on determining how to come up with that). we obviously can't have millions of reducers. but we can have millions of hash buckets that are sent in (large) groups to a much much smaller number of reducers.

i explained in terms of larger number of reducers to explore similarities (if any) to current data path.","28/Mar/10 17:11;vicaya;IMHO, it'd be easier to implement such grouping using a special group record reader (inputformat) and mapper (that uses a hash map to group things and walk the map to emit grouped key value pairs, where values can be counts or lists, at the end of the map method) than rearchitecting the current data path.","28/Mar/10 18:51;jsensarma;@Luke - like this line of thinking. I am familiar with Hive - it already does map side partial aggregates for (most) group-bys and emits them (what you mentioned). However - for (most) joins and cluster-by - it depends on the sorting provided by the map-reduce framework. We could alter the these query plans to execute this algorithm. the downside is pretty obvious: the benefits would only be available to Hive users (even as the cost of implementation remains fairly high). it would not be reusable by Pig for example - nor streaming users, etc.

extrapolating ur idea a bit - we could try to implement this as a library - something that intercepts the output of the mapper (and combiner) and the input to the reduce function (that could be inserted into the data path via config settings). (There's already a configurable output collector interface on the map output side. not sure about reducer input). We could turn off map-side sorting in the map-reduce framework itself. the main concern i would have is that it would seem too hard for these output collectors to implements spills (/merges) to (/from) disk. Perhaps if that functionality can be extracted out of the map-reduce core and provided as a library that these output collectors can use - then it's all feasible.","19/Oct/11 22:24;acmurthy;This is a great candidate for MR2.

It's a new pipeline which would be the most efficient though:

The output collector would hash rather than sort and spill in order of keys, thus keeping the combiner optional.

The twist is that you wouldn't do a 2nd or 3rd or n-th level merge in the map. Just the segments out and get the reduce to think there are more segments than #maps (additional index at the top). Most of the times, each map-output fits in memory of the reduce and thus you wouldn't seek anymore than today. The 2+ level merges don't change in the reduce.

Thoughts?","20/Oct/11 00:23;he yongqiang;something that may help get more discussions:

we are trying to experiment on a new output collector. Here are some of our thoughts:
1) group key, value in a memoryblock which is basically start-end pointer to the big kvbuffer. One memoryblock must belong to one reducer.
2) use quicksort to sort data in memoryblock
3) use binaryinsert sort when doing spill. in this phase, since memoryblocks are grouped by reducer already, so this will not sort memoryblocks across reducers

On group by and join, if not enforced sorting, only a grouping is needed, no need for a global sort.
On the mapper side:
have an hashtable for memoryblocks. And use the hash to decide which memory block to go. this will only help reduce number of sort across memoryblocks, but will not eliminate them. This is because of memory constrain(in which case we need to borrow memory from other memory block) and collision.

On the reduce side:
all mappers are applying the same rule, so can add some metadata for each mapper's output to help reducer side decide whether or not need to compare.
","20/Oct/11 00:25;he yongqiang;with 1) 2) and 3), in some testcases, we are seeing 20%-40% CPU saving on the mapper side. And it helps a lot to reduce the cpu used for mem sort. But we are definitely doing more tests on this. ","20/Oct/11 00:32;tlipcon;Would be curious to see what the CPU impact of introducing a faster raw comparator would be. See HBASE-4012 for one optimization that would be easy to try out.","20/Oct/11 01:14;he yongqiang;yeah, actually we tried that with the native comparator code from Baidu (see patches for HCE), and the difference is not very big, sometimes is worse (maybe because of some jni cost, but havn't look into it). ","20/Oct/11 01:45;tlipcon;The JNI cost makes sense, but the linked HBase JIRA doesn't use JNI. It uses sun.misc.unsafe calls which are actually JVM intrinsics (ie they get directly compiled into assembly, rather than going through the whole calling-convention + safepoint shenanigans that JNI does)","21/Oct/11 05:56;decster;bq. we are trying to experiment on a new output collector. Here are some of our thoughts:

Nice work, I'm very interested in how it is done exactly:) 
Actually I'm considering further optimizations, not just grouping, but to do ""foldl"" style aggregation operations directly in HashTable liked data structure at map output collector stage and reducer side.
It seems hyracks already to that, and google mentioned this in the paper ""Tenzing A SQL Implementation On The MapReduce Framework"".

","21/Oct/11 06:00;tlipcon;Just added MAPREDUCE-3235 and HADOOP-7761 as related JIRAs. The combination of those got a 40% CPU speedup on terasort in my tests.","22/Oct/11 08:12;he yongqiang;@Binglin, cool. Can you generate a patch based on the facebook hadoop github repository when you are done? ","22/Oct/11 08:15;he yongqiang;@Binglin, we will first try to deploy the code internally, and then will try to push the code to fb hadoop github (or sent you offline when it is almost done), and maybe you can do more improvements on that.","22/Oct/11 09:48;decster;@YongQiang 
Thanks! 
Originally, I plan to support grouping in nativetask, but after optimizing sort, I found the sort phrase no longer a bottleneck anymore, especially in large jobs with many reduce tasks. In fact the current sort implementation is not optimized at all(just use stl::sort), so replacing sort with grouping may not have a big effect after sort is optimized. Above all, I prefer hash aggregation, the bad thing about it is it must change combiner/reducer api, this won't be a problem for the under dev nativetask, but is much complicated in java, I will create a issue for discussion.
I prefer sent to me offline, can't wait to see :)
","22/Oct/11 12:46;decster;I created MAPREDUCE-3247 for the discussion about hashtable based join/aggregation.
","25/Jun/12 02:22;gemini5201314;spark-and-shark also said it has a hash based reduce faster then hadoop sort on hadoop summit 2012 presentation.","25/Jun/12 07:16;gemini5201314;I guess main point is we need a per-chunk comparison instead of a per-record comparison whether is based on hash (like this jira suggested) or minor range (like google tenzing's block shuffle).","11/Dec/12 01:49;jerrychenhf;+1 I think this feature is valuable and I would take time to work on this. The hash based algorithm can both used for group by and for join. Both of them are not requiring a global sort.","11/Dec/12 01:51;jerrychenhf;MAPREDUCE-2454 is dealing with a new feature which support pluggable sort alogrithm. I think these feature would better based on that piece of work.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set mapreduce scheduler to capacity scheduler for RPM/Debian packages by default,MAPREDUCE-2476,12506344,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,eyang,eyang,eyang,05/May/11 22:18,02/May/13 02:29,12/Jan/21 09:52,05/May/11 23:12,0.20.203.1,,,,,,,,build,,,,,,0,,,,,Hadoop RPM/Debian package is default to use the default scheduler.  It would be nice to setup the packages to use capacity scheduler instead.,"Redhat 5.5, Java 6",aah,acmurthy,atm,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-6255,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-05-05 22:25:57.738,,,false,,,,,,,,,,,,,,,,,,72493,,,,,Thu May 05 23:12:12 UTC 2011,,,,,,,"0|i0jivz:",111990,,,,,,,,,,,,,,,,,,,,,"05/May/11 22:19;eyang;This enhancement depends on HADOOP-6255 rpm/debian package builds.","05/May/11 22:25;tlipcon;Why should the RPM/Deb install have a different default than the tarball?

If Capacity Scheduler should be the default, why is it part of contrib?","05/May/11 22:33;knoguchi;In addition to Todd's point, do many users really need the features from capacity scheduler (and/or fair scheduler) ? 
","05/May/11 22:49;acmurthy;-1

It's fine to ask the user the option for the scheduler: default, capacity, fair.

It can be improved further to ask for queues, capacities, max-capacities etc. for capacity-scheduler.

Similarly for pools etc. for fair-share.","05/May/11 22:51;tlipcon;(on the other hand I would probably support a long term vision of combining some of the features of capacity and fairshare and making the One Scheduler To Rule Them All part of core)","05/May/11 23:12;acmurthy;As I mentioned, we shouldn't do this.

I'm happy to have the configure script ask the user his option for the scheduler. The default should remain the 'fifo' scheduler.

I'm closing this jira. Please use a different one for enhancements along the line I suggested. Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MR portion of HADOOP-7214 - Hadoop /usr/bin/groups equivalent,MAPREDUCE-2473,12506145,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,atm,atm,atm,04/May/11 08:00,02/May/13 02:29,12/Jan/21 09:52,13/May/11 00:01,0.23.0,,,,,0.23.0,,,jobtracker,,,,,,0,,,,,,,aah,atm,cutting,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-7214,,,,"04/May/11 22:53;atm;mapreduce-2473.0.patch;https://issues.apache.org/jira/secure/attachment/12478227/mapreduce-2473.0.patch","05/May/11 06:22;atm;mapreduce-2473.1.patch;https://issues.apache.org/jira/secure/attachment/12478252/mapreduce-2473.1.patch","05/May/11 19:12;atm;mapreduce-2473.2.patch;https://issues.apache.org/jira/secure/attachment/12478314/mapreduce-2473.2.patch","11/May/11 07:09;atm;mapreduce-2473.3.patch;https://issues.apache.org/jira/secure/attachment/12478777/mapreduce-2473.3.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2011-05-04 23:05:19.395,,,false,,,,,,,,,,,,,,,,,,150212,Reviewed,,,,Mon May 16 20:07:13 UTC 2011,,,,,,,"0|i09dtz:",52674,"Introduces a new command, ""mapred groups"", which displays what groups are associated with a user as seen by the JobTracker.",,,,,,,,,,,,,,,,,,,,"04/May/11 22:53;atm;Patch which uses the {{RefreshUserMappingsProtocol}}. This patch is dependent upon the latest patch posted to HADOOP-7214.","04/May/11 23:05;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12478227/mapreduce-2473.0.patch
  against trunk revision 1099590.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The patch appears to cause tar ant target to fail.

    -1 findbugs.  The patch appears to cause Findbugs (version 1.3.9) to fail.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:


    -1 contrib tests.  The patch failed contrib unit tests.

    -1 system test framework.  The patch failed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/216//testReport/
Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/216//console

This message is automatically generated.","05/May/11 05:03;tlipcon;It seems unfortunate that we're lumping in the ""getGroups"" API (an action intended for non-admin usage) with the refresh methods (admin actions). Unfortunately the ACLs for this whole Protocol are determined by ""security.refresh.user.mappings.protocol.acl"" which is a misnomer when it also affects the group command.

Any thoughts on a way around this?","05/May/11 05:16;atm;{quote}
It seems unfortunate that we're lumping in the ""getGroups"" API (an action intended for non-admin usage) with the refresh methods (admin actions). Unfortunately the ACLs for this whole Protocol are determined by ""security.refresh.user.mappings.protocol.acl"" which is a misnomer when it also affects the group command.

Any thoughts on a way around this?
{quote}

Certainly. We could have a separate protocol interface for refreshing versus fetching user group mappings. It seemed logical to me to lump the two together, since they both cover user -> group mapping, but I don't feel very strongly about this.","05/May/11 05:19;tlipcon;I agree conceptually, but since our authorization here is service-level instead of more finer grained, we either need to split up the Protocols, or split up the authorization configuration.","05/May/11 06:22;atm;Updated patch addressing Todd's comments.","05/May/11 06:35;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12478252/mapreduce-2473.1.patch
  against trunk revision 1099590.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The patch appears to cause tar ant target to fail.

    -1 findbugs.  The patch appears to cause Findbugs (version 1.3.9) to fail.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:


    -1 contrib tests.  The patch failed contrib unit tests.

    -1 system test framework.  The patch failed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/217//testReport/
Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/217//console

This message is automatically generated.","05/May/11 06:45;atm;D'oh! I missed a license header. Fixed in this patch.","05/May/11 18:15;tlipcon;TestGetGroups.setUpNameNode and tearDownNameNode should be renamed to reflect that they setup/teardown the JT, not the NN.
Also, In the Common patch, I don't see GetGroupsTestBase included.","05/May/11 19:12;atm;Thanks for the comments, Todd. Updated patch attached.","05/May/11 19:25;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12478314/mapreduce-2473.2.patch
  against trunk revision 1099847.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The patch appears to cause tar ant target to fail.

    -1 findbugs.  The patch appears to cause Findbugs (version 1.3.9) to fail.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:


    -1 contrib tests.  The patch failed contrib unit tests.

    -1 system test framework.  The patch failed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/220//testReport/
Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/220//console

This message is automatically generated.","11/May/11 01:04;tlipcon;+1, but please re-attach patch so Hudson can re-run now that HADOOP-7214 is committed.","11/May/11 07:09;atm;Rebased patch against trunk.","11/May/11 09:04;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12478777/mapreduce-2473.3.patch
  against trunk revision 1101741.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/227//testReport/
Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/227//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/227//console

This message is automatically generated.","13/May/11 00:01;tlipcon;Committed to trunk. Thanks, Aaron!","14/May/11 15:38;hudson;Integrated in Hadoop-Mapreduce-trunk #679 (See [https://builds.apache.org/hudson/job/Hadoop-Mapreduce-trunk/679/])
    ","16/May/11 20:07;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #675 (See [https://builds.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/675/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Collecting cpu and memory usage for MapReduce tasks,MAPREDUCE-220,12423952,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,schen,hong.tang,hong.tang,27/Apr/09 23:19,02/May/13 02:29,12/Jan/21 09:52,20/Aug/10 17:42,,,,,,0.22.0,,,task,tasktracker,,,,,0,,,,,It would be nice for TaskTracker to collect cpu and memory usage for individual Map or Reduce tasks over time.,,aaa,aah,acmurthy,amareshwari,atm,aw,cdouglas,cutting,decster,dhruba,eli,hammer,jrideout,liangly,lianhuiwang,luoli,mahadev,matei,olgan,philip,ravidotg,rksingh,sreekanth,srivas,sumanshg,tshiran,yhemanth,zhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-1167,,,,,,,,,,,,,,,,,MAPREDUCE-2777,,,,,,,,,,,,,,,,,,,,,"16/Jun/10 22:34;schen;MAPREDUCE-220-20100616.txt;https://issues.apache.org/jira/secure/attachment/12447289/MAPREDUCE-220-20100616.txt","04/Aug/10 19:53;schen;MAPREDUCE-220-20100804.txt;https://issues.apache.org/jira/secure/attachment/12451256/MAPREDUCE-220-20100804.txt","07/Aug/10 04:17;schen;MAPREDUCE-220-20100806.txt;https://issues.apache.org/jira/secure/attachment/12451489/MAPREDUCE-220-20100806.txt","09/Aug/10 19:50;schen;MAPREDUCE-220-20100809.txt;https://issues.apache.org/jira/secure/attachment/12451619/MAPREDUCE-220-20100809.txt","11/Aug/10 22:52;schen;MAPREDUCE-220-20100811.txt;https://issues.apache.org/jira/secure/attachment/12451838/MAPREDUCE-220-20100811.txt","13/Aug/10 01:56;schen;MAPREDUCE-220-20100812.txt;https://issues.apache.org/jira/secure/attachment/12451976/MAPREDUCE-220-20100812.txt","17/Aug/10 19:25;schen;MAPREDUCE-220-20100817.txt;https://issues.apache.org/jira/secure/attachment/12452306/MAPREDUCE-220-20100817.txt","19/Aug/10 00:14;schen;MAPREDUCE-220-20100818.txt;https://issues.apache.org/jira/secure/attachment/12452472/MAPREDUCE-220-20100818.txt","29/Apr/10 21:21;schen;MAPREDUCE-220-v1.txt;https://issues.apache.org/jira/secure/attachment/12443235/MAPREDUCE-220-v1.txt","28/Apr/10 20:31;schen;MAPREDUCE-220.txt;https://issues.apache.org/jira/secure/attachment/12443120/MAPREDUCE-220.txt","19/Aug/10 00:14;schen;ant-test-patch.log;https://issues.apache.org/jira/secure/attachment/12452470/ant-test-patch.log","19/Aug/10 00:14;schen;ant-test.log;https://issues.apache.org/jira/secure/attachment/12452471/ant-test.log",,,,,,,,,,,,,,,,,,,,,,,12.0,,,,,,,,,,,,,,,,,,,,2009-11-06 22:17:32.789,,,false,,,,,,,,,,,,,,,,,,36863,Reviewed,,,,Fri Oct 29 02:04:26 UTC 2010,,,,,,,"0|i0iwvz:",108422,Collect cpu and memory statistics per task.,,,,,,,,,,,,,,,,,,,,"27/Apr/09 23:22;hong.tang;Such information may be useful in several ways:
- Visualizing the resource usage of a user job over time.
- Identifying user application hotspots.
- Identifying scheduling anomaly in MapReduce framework.","06/Nov/09 22:17;schen;We can make TaskTracker reports its usage on CPU, memory, bandwidth to JobTracker. JobTracker can use the information for scheduling tasks and profiling jobs.

One way to do this is to first make ProcfsProcessTree to collect the utilization information (CPU, mem...) and write the information in TaskTrackerStatus.taskReports and send them with the heartbeats. Then we can aggregate these information in JobInProgress to do job profiling and scheduling.","17/Nov/09 20:31;dhruba;Are we proposing that we add the following metrics to the heartbeat message?

A1. virtual memory used by each task (in bytes)
A2. physical memory  used by each task (in bytes)
A3. cpu used by each task (as a percentage of total CPU on that machine)

B1. available physical memory on this machine (in bytes)
B2. available cpu on this machine (as a percentage of total CPU on that machine)","17/Nov/09 20:55;schen;Dhruba and I discussed again. We think these information should be transmitted

A1. virtual memory used by each task (in bytes)
A2. physical memory used by each task (in bytes)
A3. cumulative cpu time used by each task (in millisecond)

B1. available physical memory on this machine (in bytes)
B2. cumulative used cpu time (for all cores) since the machine is up (in millisecond)
B3. cpu speed on this machine (in Hz)
B4. # of cpu cores on the machine

The cpu % can be obtained by looking at the difference of the reported cumulative cpu time.","18/Nov/09 00:43;schen;I created a sub-task for this one for collecting the TaskTracker resource in MAPREDUCE-1218.
I will factor out the codes for TaskTracker resource collecting there and leave the scheduling related codes here.","18/Nov/09 00:50;schen;I posted the above comment in the wrong place. It's supposed to go to MAPREDUCE-961. Sorry for the confusion and spam.","18/Nov/09 00:54;dhruba;I am proposing that we hold off doing anything to this JIRA until MAPREDUCE-901 is committed.

In the meantime, the items marked B1 - B4 can be done as part of MAPREDUCE-1218. The B1-B4 are not task related metrics (rather, they are TaskTracker related) and are not dependent on TaskMetrics changes proposed in MAPREDUCE-901.","18/Nov/09 04:09;hong.tang;+1 on holding off this Jira until MAPREDUCE-901 is committed.

+1 on moving B* to MAPREDUCE-1218.

I'd also propose A4. Total number of child processes in the process tree rooted from the main task process. (Assuming A1, A2, A3 are cumulative metrics on all sub-processes launched by the main task process).","18/Nov/09 04:43;vinodkv;bq. B2. cumulative used cpu time (for all cores) since the machine is up (in millisecond)
bq. I'd also propose A4. Total number of child processes in the process tree rooted from the main task process.
What can these two stats possibly be used for?","18/Nov/09 18:18;olgan;It would be very useful for profiling purposes if applications could get resource utilization information via counters. Either detailed information for each map/reduce or min/max/average could be useful.

- Average CPU utilization
- Max memory usage
- Average inbound and outbound I/0. (Not sure if it is possible to obtain this information on per process basis.)","21/Nov/09 09:22;hong.tang;bq. What can these two stats possibly be used for?
This would allow us to tell whether a sudden decrease/increase of cpu or memory usage is caused by the spawning of new processes.","23/Nov/09 19:09;schen;bq. What can these two stats possibly be used for?

B2. can also allow us to compute the current CPU usage (by taking difference)?
+1 on Hong's idea of collecting A4. total number of child processes.","21/Apr/10 18:01;dhruba;hi folks, we would like to start work on this one. In the earlier discussion, we said that a pre-requisite is to refactor all the metric reporting via MAPREDUCE-901. But 901 is not moving forward. Given that fact, is it ok with people if we start working on this JIRA using the existing reporting framework even before M901 is done?","21/Apr/10 18:08;acmurthy;Dhruba, I think that makes sense.

Having said that, if we do manage to get MAPREDUCE-901 committed before this gets in, would it be reasonable to ask for a bit of re-work on this one?","21/Apr/10 18:15;dhruba;Agreed. we can start work on this one, ge it reviewed by the community, and by the time it is ready, if M-901 is already comitted, then we rafactor this patch to be compatible with M-901. On the other hand, if M-901 is not committed by the time this one is ready, then we do not hold this one up. Sounds fair?","21/Apr/10 18:19;acmurthy;Precisely. Thanks!","21/Apr/10 18:25;schen;Thanks, Dhruba and Arun.
I will start working on this one now.","28/Apr/10 20:31;schen;This patch implements the feature that transmit the per task CPU and memory information through TaskTracker heartbeat.
Here's a summary of the changes:

1. Add three methods in LinuxResourceCalculatorPlugin to get the cumulative CPU time, physical memory and virtual memory of a process (aggregated all the subprocesses)..
2. Add a method called resourceStatusUpdate in TaskTracker to update the resource status in TaskStatus.taskReports before sending heartbeat.
3. Add a nested class called ResourceStatus in TaskStatus to store the CPU and memory information.
3. Modify TestTTResource to check if the per task resource information actually transmitted.","29/Apr/10 04:43;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12443120/MAPREDUCE-220.txt
  against trunk revision 938805.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/155/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/155/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/155/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/155/console

This message is automatically generated.","29/Apr/10 21:21;schen;The problem found in findbug is because I made the method resourceUpdate() synchronized.
This is unnecessary. I have remove it.","29/Apr/10 21:59;schen;Rerun the failed contrib test TestSimulatorDeterministicreplay. It succeed on my dev box.

{code}
 b/c/m/t/TEST-org.apache.hadoop.mapred.TestSimulatorDeterministicReplay.txt                                                                                     
Testsuite: org.apache.hadoop.mapred.TestSimulatorDeterministicReplay
Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 31.101 sec
------------- Standard Output ---------------
Job job_200904211745_0002 is submitted at 103010
Job job_200904211745_0002 completed at 141990 with status: SUCCEEDED runtime: 38980
Job job_200904211745_0003 is submitted at 984078
Job job_200904211745_0004 is submitted at 993516
Job job_200904211745_0003 completed at 1011051 with status: SUCCEEDED runtime: 26973
Job job_200904211745_0005 is submitted at 1033963
Done, total events processed: 595469
Job job_200904211745_0002 is submitted at 103010
Job job_200904211745_0002 completed at 141990 with status: SUCCEEDED runtime: 38980
Job job_200904211745_0003 is submitted at 984078
Job job_200904211745_0004 is submitted at 993516
Job job_200904211745_0003 completed at 1011051 with status: SUCCEEDED runtime: 26973
Job job_200904211745_0005 is submitted at 1033963
Done, total events processed: 595469
------------- ---------------- ---------------
{code}
","30/Apr/10 12:39;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12443235/MAPREDUCE-220-v1.txt
  against trunk revision 939505.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/363/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/363/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/363/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/363/console

This message is automatically generated.","03/May/10 20:06;schen;Vinod: I think you are very familiar with this part of the codes. Is it possible that you can help me review the patch? Thanks.

Arun: Let me know if there is anything that might cause trouble to MAPREDUCE-901.","04/May/10 16:07;acmurthy;TaskStatus isn't really a 'public' api (MAPREDUCE-1623 & HADOOP-5073).

Thus TaskStatus.ResourceStatus shouldn't be exposed directly to the end-user. We probably should convert the fields in ResourceStatus into Counters and use that as the primary interface for the end-user, also we should store them into JobHistory etc.","04/May/10 17:10;vinodkv;bq. We probably should convert the fields in ResourceStatus into Counters and use that as the primary interface for the end-user, also we should store them into JobHistory etc.
I second that. It will also solve two other issues with the patch:
 - the cpu and memory usage details of each task are sent in every heartbeat, making it bulky. Translating them into Counters will make them to be sent only once every minute
 - with Counters, we get for free the logging into JobHistory as well displaying on the web UI.

Leaving that aside, I have one more comment on the TT side: For getting the cpu/memory usage of a task, we construct the process-tree of the task repeatedly every time a heartbeat is sent.
 - For one, if we go the Counters way, we only need to do the calculations every once a minute.
 - Otherwise, the process-trees for all tasks are now constructed by both by TaskMemoryManager and the TT main thread. It can become costly depending on the size of the process-tree. There is an opportunity for refactoring this, I guess - may be a single class which maintains all the process-trees (TaskMemoryManager.ProcessTreeInfo?) and the corresponding statistics, within a given precision, time-wise.

Scott?","04/May/10 17:15;dhruba;I like the idea of sending this information via Counters.

This data could be used by schedulers to make decisions on what/when to schedule new tasks or preempt existing tasks. For this use-case, it would be nice if we can send them to the JT more frequently that 1 minute. any ideas here?

","04/May/10 23:20;schen;Hey guys, Thanks for the help.

I am not familiar with the counters. But from Arun and Vinod's comments I can the see the benefits:
1. Reuse of the counter logging and transmitting
2. Easier to expose to end users
This is really good!

But as Dhruba mentioned, we want to use this information for scheduling.
So measuring it and then sending it with the heart beat ensures the scheduler gets the latest information.
One minute may be too slow for the scheduling.

The other question I have is that 
Using counters, can we aggregate using other method (e.g. max) rather than just increment values?

My original plan is to report these information in this issue and aggregate them into job level status in MAPREDUCE-1739.
And I am planning to generate these fields after aggregation:
1. Total CPU cycles (# of giga-cycles)
2. Total Memory occupied time (GB-sec)
3. Maximum peak memory on one task (GB)
4. Maximum peak CPU on one task (GHz)
Is it possible to get these fields by using the counters?

I will read the relavent codes and think more about it.
Maybe there's a way to obtain both benefit.

Vinod: I also feel that there are lots of redundant creation/computation of processTree.
Maybe we should refactor the codes and use one thread to compute it and expose the information to others.

","07/May/10 01:09;schen;I had some discussion with Arun. The problem with the Counter is that it can only be incremented.
So it is difficult to use to transmit CPU and memory information (this goes up and down).
We filed another JIRA MAPREDUCE-1762 to allow setValue() in Counter.
Then we may use Counters to send these information.

What do you think?","07/May/10 23:47;schen;Had some offline discussion with Dhruba.
We can make COUNTER_UPDATE_INTERVAL configurable here (it is currently hard coded to 1 min).
Then we can reconfigure it to submit newer information to the scheduler in our case.
This should solve our problem.
","16/Jun/10 22:34;schen;Update patch. Move the resource information in TaskCounter.
We created the following three values in TaskCounter.
{code}
CPU_MILLISECONDS
PHYSICAL_MEMORY_BYTES
VIRTUAL_MEMORY_BYTES
{code}
and added a method called Task.updateResourceCounters() which is used by Task.updateCounters().
","17/Jun/10 02:57;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12447289/MAPREDUCE-220-20100616.txt
  against trunk revision 955198.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/576/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/576/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/576/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/576/console

This message is automatically generated.","18/Jun/10 00:30;schen;The failed contrib test is TestSimulatorDeterministicReplay.testMain.
It is a know issue in MAPREDUCE-1834.

In the patch we put task cumulative CPU time, current physical memory and current virtual memory in task counters.
So it will be aggregated in JobInProgress.getJobCounter(). We will get the total CPU time and current total memory usage.
They will go to both web ui and history as part of the counters.
We can access the task counters to obtain these information in place like task scheduler too.

@Vinod: I didn't do the refactoring of the ProcTree. Because LinuxResourceCalculatorPlugin is now called by the task (updateCounters is in Task.java). It is in a different process than TaskTracker so we can not reuse the ProcTree. But directory /proc/ is in memory, this may not be so bad in terms of performance. What do you think?","18/Jun/10 00:46;aw;(You know, it is a shame this was called ProcfsBasedProcessTree with the disclaimer that it only works on Linux.  It probably should be renamed LinuxProcfsBasedProcessTree so that other operating systems with /proc could work.  I suppose the alternative is to hack this code to be multi-OS aware)","23/Jun/10 21:48;schen;Allen: I agree. LinuxProcfsBasedProcessTree is a better name.","12/Jul/10 11:43;evan65;why not collect disk i/o and bandwidth for scheduling as well?","12/Jul/10 22:41;schen;@Evan: That's a very good idea. We can file another JIRA on this one. What do you think?","13/Jul/10 01:17;evan65;@Scott: I've already make a java tool for MR profiling through Linux OS tools, which is independent from Hadoop. However, the overhead of network monitor, tcpdump, is really high. When running gridmix2, tcpdump will cost 20% cpu in one core. Disk monitor also encountered some problems. So, I am not so sure that the MR performance is influenced by all that factors---cpu, memory, disk, network. I'd like to complete my base experiment first. Could you give me some advice about network and disk monitor?","13/Jul/10 01:38;schen;@Evan: This sounds like a good experiment.

The CPU and memory collected in this JIRA is obtained by parsing /proc/ directory.
It is very good because /proc/ is in memory so the overhead is small. 
However, there is no per process IO and network information in /proc/.
And like you mentioned, using tools like tcpdump can be very expensive.

Another approach to do this is by counting the {non,rack,data}-local bytes fetched from HDFS and fetched/served for map output.
This way we can estimate the IO and network traffic from these numbers.
The drawback of this approach is that this doesn't capture IO and network that is not introduced by the framework.
People can write user script which does lots of IO. That will not be captured by this.
Thoughts?
","13/Jul/10 06:40;srivas;We've found that disk bandwidth is virtually unlimited compared to other factors, esp network, thus measuring/collecting it is not worthwhile for scheduling. More interesting is disk-ops-per-second-per-drive. It identifies  bad data layout immediately (ie, one disk will be very hot even though it might be transferring very little data).

Unfortunately, using ops / second / disk  to schedule work is still not very useful, since bad data layout will not change because we schedule less.

Network is a big bottleneck. But bytes-in/bytes-out per unit of time is not representative of a problem. IF we had some measure of the congestion, we could use it to increase/decrease scheduling locality (eg, if network gets congested, reduce %-age of non-local tasks).  We need to know round-trip times under ""normal"" vs ""congested"" situations., dropped packet counts, retransmit counts, etc. to figure out metrics for congestion. (Perhaps add some sockopts to tell us this? TCP knows this, after all)

CPU/memory/swapping still seem to be most useful therefore.


","13/Jul/10 06:44;dhruba;+1 to srivas's proposal. let this jira focus on cpu/memory metrics. And then maybe continue the discussion about disk bandwidth in another jira.

Evan:  If this is acceptable to you, can you pl create a new jira for it? Thanks.","03/Aug/10 04:57;philip;Scott,

Quick question: have you tried this patch with JVM re-use enabled?  On my quick-reading, this patch doesn't handle that case; I don't know if it's a real problem or not.

Cheers,

-- Philip","03/Aug/10 18:52;schen;Hey Philip,

We haven't try test this under the case of JVM re-use. But I think you are right about this.
We need to do some more work for this case.

We can still get the correct PID in JVM reuse case. Because we use
{code}
String pid = System.getenv().get(""JVM_PID"");
{code}
which is invoked from Task.updateCounters().
So we should be able to get the correct PID for the task no matter JVM is reused or not.

The problem is the cumulated CPU time. Because the process may be used by another task for a while.
One way to solve this is to send only the current value instead of cumulated value.
Does this sound correct to you?

Scott","04/Aug/10 03:22;philip;Hi Scott,

You could also ""reset"" the counters to 0 when the new task is started (sort of like a ""tare"" button on a scale).  If resourceCalculator.getProcCumulativeCpuTime() was rather resourceCalculator.getCumulativeCpuTimeDelta() [cumulative CPU time since last call], you could use counter.incr() for the CPU usage.

It's also worth mentioning that the memory usage here is the last-known memory usage value.  It's not byte-seconds (which wouldn't be that useful), nor is it maximum memory.  That seems useful, but it's a bit unintuitive.

{noformat}
+    long cpuTime = resourceCalculator.getProcCumulativeCpuTime();
+    long pMem = resourceCalculator.getProcPhysicalMemorySize();
+    long vMem = resourceCalculator.getProcVirtualMemorySize();
+    counters.findCounter(TaskCounter.CPU_MILLISECONDS).setValue(cpuTime);
+    counters.findCounter(TaskCounter.PHYSICAL_MEMORY_BYTES).setValue(pMem);
+    counters.findCounter(TaskCounter.VIRTUAL_MEMORY_BYTES).setValue(vMem);
{noformat}","04/Aug/10 19:53;schen;Update based on Philip's suggestion.
We obtain a initial CPU cumulative time when task is initialized and we subtract it when reporting the CPU time.","06/Aug/10 16:51;eli;Hey Scott, 

Latest patch looks good to me.  I assume the redundant calls to getProcessTree be handled in MR-901, worth returning the values as a tuple in the mean time? Out of curiosity for the test why did the map and reduce sleeps time need to be bumped to 5s? Wouldn't anything >1s pass?

Thanks,
Eli ","07/Aug/10 04:06;schen;Hey Eli,

I think returning a tuple is a very good idea. I updated the patch based on this suggestion.

For the sleep time, when I tested it I increase it because I thought I have to wait some time for the update counters.
But the counters will be updated at the very beginning of the task. So I changed it back in the patch.
Thanks for the review,

Scott
","09/Aug/10 18:39;eli;Looks good.     Minor nit: I might rename ProcResourceStatus to something like ProcResourceValues.  Also, this inner class technically needs interface annotations (private and unstable). Sanjay and Tom can correct me if I'm wrong but I don't think we decided that classes inherit the annotations of the outer class.","09/Aug/10 19:50;schen;Thanks for the comment again, Eli.
I have changed ProcResourceStatus to ProcResourceValues. It's better.
Also I added the interface annotations.","09/Aug/10 20:36;eli;+1   

Latest patch looks good to me. Thanks Scott.  ","11/Aug/10 16:24;acmurthy;Scott, sorry for coming in late. 

I have a nit: we seem to create a new ProcfsBasedProcessTree each time - wouldn't it be easier to re-use the object? Create it once and re-use it each time?","11/Aug/10 22:29;schen;Thanks, Arun. I will update the patch soon.","11/Aug/10 22:54;schen;Update to address Arun's comment.","11/Aug/10 23:26;eli;Caching the process tree this way works with JVM re-use?","12/Aug/10 18:21;schen;Hey Eli,

I think it will still work. The process tree will be initialized in Task.initialized().
So it will get the correct process id.

Scott","13/Aug/10 01:27;acmurthy;Scott the patch looks good, but you need to generate it with --no-prefix.

Once it passes through hudson I'll commit. Thanks!","13/Aug/10 01:31;schen;ah, I see. Thanks, Arun.
That's why Hudson never respond to my patch. I will submit the patch again with --no-prefix.","14/Aug/10 01:33;acmurthy;Hudson might be stuck. Can you please attach the output of 'ant test' and 'ant test-patch' here? Thanks.","16/Aug/10 18:11;schen;Hey Arun,
Thanks, I will run the tests and attach them.
Scott","19/Aug/10 00:14;schen;Upload the result for ""ant test"" and ""ant test-patch"".

Update the patch: In Task.java, move the new added two variables to the location commented ""Fields"".","20/Aug/10 17:42;acmurthy;I just committed this. Thanks Scott!","20/Aug/10 17:48;schen;Thanks for the help :)","29/Oct/10 02:04;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #523 (See [https://hudson.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/523/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Capturing interim progress times, CPU usage, and memory usage, when tasks reach certain progress thresholds",MAPREDUCE-2037,12472746,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,dking,dking,dking,27/Aug/10 21:05,02/May/13 02:29,12/Jan/21 09:52,12/Aug/11 21:05,,,,,,0.23.0,,,,,,,,,1,,,,,"We would like to capture the following information at certain progress thresholds as a task runs:

   * Time taken so far
   * CPU load [either at the time the data are taken, or exponentially smoothed]
   * Memory load [also either at the time the data are taken, or exponentially smoothed]

This would be taken at intervals that depend on the task progress plateaus.  For example, reducers have three progress ranges -- [0-1/3], (1/3-2/3], and (2/3-3/3] -- where fundamentally different activities happen.  Mappers have different boundaries, I understand, that are not symmetrically placed.  Data capture boundaries should coincide with activity boundaries.  For the state information capture [CPU and memory] we should average over the covered interval.

This data would flow in with the heartbeats.  It would be placed in the job history as part of the task attempt completion event, so it could be processed by rumen or some similar tool and could drive a benchmark engine.",,aah,acmurthy,bcwalrus,cdouglas,ddas,decster,eli,hammer,hong.tang,lianhuiwang,philip,rajesh.balamohan,ranjit,ravidotg,tlipcon,vicaya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-2039,,,,,,,,,,,,,,,MAPREDUCE-901,,MAPREDUCE-220,,,,"12/Aug/11 01:18;acmurthy;MAPREDUCE-2037.patch;https://issues.apache.org/jira/secure/attachment/12490202/MAPREDUCE-2037.patch","11/Aug/11 23:44;acmurthy;MAPREDUCE-2037.patch;https://issues.apache.org/jira/secure/attachment/12490193/MAPREDUCE-2037.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2010-09-07 05:51:47.6,,,false,,,,,,,,,,,,,,,,,,64992,,,,,Tue Oct 18 06:36:18 UTC 2011,,,,,,,"0|i09ehr:",52781,"Capture intermediate task resource consumption information:
* Time taken so far
* CPU load [either at the time the data are taken, or exponentially smoothed]
* Memory load [also either at the time the data are taken, or exponentially smoothed]

This would be taken at intervals that depend on the task progress plateaus. For example, reducers have three progress ranges - [0-1/3], (1/3-2/3], and (2/3-3/3] - where fundamentally different activities happen. Mappers have different boundaries that are not symmetrically placed [0-9/10], (9/10-1]. Data capture boundaries should coincide with activity boundaries. For the state information capture [CPU and memory] we should average over the covered interval.
",,,,,,,,,,,,,,,,,,,,"07/Sep/10 05:51;vinodkv;I didn't realize before but MAPREDUCE-220 captures the cpu/memory load at the time of task completion. So the core functionality is already there in trunk.

But the load at the time of task completion isn't really a useful stat. +1 for either exponential smoothing or a simpler capturing of highest,lowest and average loads for cpu and memory.","07/Sep/10 22:42;hong.tang;-1 on using EWMA to capture CPU usage. It is more useful to track the aggregated cpu tick counter as raw data, and we can always calculate EWMA from that later, but not vice versa. It'd be also useful to capture the number of threads that are included in the calculation. So each entry looks like the following: <time, cpu-ticker-counter, #threads>. I'd also like to capture CPU MHz number for the task tracker so that I can know if we are saturating the CPU.","16/Sep/10 01:54;dking;Benchmarks to support more realistic validation of putative scheduler improvements would benefit from a gridmix3-like tool that can simulate the CPU usage patterns of the tasks of the emulated jobs.  That includes both the average loads of the various tasks, and also the time variation.  In order to develop this information, we need to capture the CPU usage of each task over time.

Fortunately, on linux systems, there's a way to capture this.  The {{/proc/n/stat}} information appears to capture everything I need.

I would plumb this using {{LinuxResourceCalculatorPlugin}} and {{TaskStatus}} .

The information will be placed in the job history files, in the task attempt end records.  This might be placed as a coded character string with a few dozen characters.","16/Sep/10 23:49;dking;Worker tasks seldom have multiple threads.  Streaming and its friends spawn a task, and of course users can write whatever code they want, but most tasks burn their CPU time in their sole thread.

Of course, when we do have streaming we need to capture the info from the slave task...
","13/Oct/10 00:11;dking;The information I need flows with the counters as a result of MAPREDUCE-220 .

Counters ship from the {{TaskTracker}} to the {{JobTracker}} in the heartbeat -- but prior to 901 this doesn't happen every heartbeat.  Since the purpose of this code is to approximate a graph of resource use v. progress from snapshots that come with the heartbeats, we'll have to interpolate more aggressively and will lose accuracy when some counter instances are not shipped.

With 901, the framework data will be shipped every time.","14/Oct/10 21:53;dking;The information we need is already being gathered.  Moving it where it needs to go is a small matter of plumbing.

The info flows into the job tracker via counters.  I'll need to check whether accessing the counters will be too expensive to do within {{heartbeat}} [inside the {{JobTracker}} lock].  I might end up creating a queue of unprocessed counter blocks that can be handled in another thread.","11/Aug/11 23:44;acmurthy;Patch ported from y-merge branch for ensuring we can merge MAPREDUCE-279 to trunk. Credit, of course, goes to Dick.","12/Aug/11 01:18;acmurthy;Minor update, messed up diff previously.","12/Aug/11 21:05;acmurthy;I just committed this after manually running tests. Thanks Dick!","12/Aug/11 21:11;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #764 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/764/])
    MAPREDUCE-2037. Capture intermediate progress, CPU and memory usage for tasks. Contributed by Dick King.

acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1157253
Files : 
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapreduce/jobhistory/AvroArrayUtils.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/MapTaskAttemptInfo.java
* /hadoop/common/trunk/mapreduce/src/java/mapred-default.xml
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/TaskInProgress.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/Counters.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapreduce/jobhistory/Events.avpr
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapreduce/jobhistory/TaskAttemptUnsuccessfulCompletionEvent.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/StatePeriodicStats.java
* /hadoop/common/trunk/mapreduce/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapreduce/jobhistory/ReduceAttemptFinishedEvent.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapreduce/server/jobtracker/JTConfig.java
* /hadoop/common/trunk/mapreduce/src/test/mapred/org/apache/hadoop/mapred/TestTaskPerformanceSplits.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/ZombieJob.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/ReduceAttempt20LineHistoryEventEmitter.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/TaskAttemptInfo.java
* /hadoop/common/trunk/mapreduce/src/test/mapred/org/apache/hadoop/mapreduce/jobhistory/TestJobHistoryEvents.java
* /hadoop/common/trunk/mapreduce/CHANGES.txt
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/CumulativePeriodicStats.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/ReduceTaskAttemptInfo.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/TaskAttempt20LineEventEmitter.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/JobInProgress.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/PeriodicStatsAccumulator.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/ProgressSplitsBlock.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/LoggedTaskAttempt.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapreduce/jobhistory/MapAttemptFinishedEvent.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/MapAttempt20LineHistoryEventEmitter.java
","14/Aug/11 13:00;hudson;Integrated in Hadoop-Mapreduce-trunk #754 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/754/])
    MAPREDUCE-2037. Capture intermediate progress, CPU and memory usage for tasks. Contributed by Dick King.

acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1157253
Files : 
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapreduce/jobhistory/AvroArrayUtils.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/MapTaskAttemptInfo.java
* /hadoop/common/trunk/mapreduce/src/java/mapred-default.xml
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/TaskInProgress.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/Counters.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapreduce/jobhistory/Events.avpr
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapreduce/jobhistory/TaskAttemptUnsuccessfulCompletionEvent.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/StatePeriodicStats.java
* /hadoop/common/trunk/mapreduce/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapreduce/jobhistory/ReduceAttemptFinishedEvent.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapreduce/server/jobtracker/JTConfig.java
* /hadoop/common/trunk/mapreduce/src/test/mapred/org/apache/hadoop/mapred/TestTaskPerformanceSplits.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/ZombieJob.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/ReduceAttempt20LineHistoryEventEmitter.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/TaskAttemptInfo.java
* /hadoop/common/trunk/mapreduce/src/test/mapred/org/apache/hadoop/mapreduce/jobhistory/TestJobHistoryEvents.java
* /hadoop/common/trunk/mapreduce/CHANGES.txt
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/CumulativePeriodicStats.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/ReduceTaskAttemptInfo.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/TaskAttempt20LineEventEmitter.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/JobInProgress.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/PeriodicStatsAccumulator.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/ProgressSplitsBlock.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/LoggedTaskAttempt.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapreduce/jobhistory/MapAttemptFinishedEvent.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/MapAttempt20LineHistoryEventEmitter.java
","15/Aug/11 18:01;hudson;Integrated in Hadoop-Common-trunk-Commit #742 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/742/])
    MAPREDUCE-2037. Capture intermediate progress, CPU and memory usage for tasks. Contributed by Dick King.

acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1157253
Files : 
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapreduce/jobhistory/AvroArrayUtils.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/MapTaskAttemptInfo.java
* /hadoop/common/trunk/mapreduce/src/java/mapred-default.xml
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/TaskInProgress.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/Counters.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapreduce/jobhistory/Events.avpr
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapreduce/jobhistory/TaskAttemptUnsuccessfulCompletionEvent.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/StatePeriodicStats.java
* /hadoop/common/trunk/mapreduce/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapreduce/jobhistory/ReduceAttemptFinishedEvent.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapreduce/server/jobtracker/JTConfig.java
* /hadoop/common/trunk/mapreduce/src/test/mapred/org/apache/hadoop/mapred/TestTaskPerformanceSplits.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/ZombieJob.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/ReduceAttempt20LineHistoryEventEmitter.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/TaskAttemptInfo.java
* /hadoop/common/trunk/mapreduce/src/test/mapred/org/apache/hadoop/mapreduce/jobhistory/TestJobHistoryEvents.java
* /hadoop/common/trunk/mapreduce/CHANGES.txt
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/CumulativePeriodicStats.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/ReduceTaskAttemptInfo.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/TaskAttempt20LineEventEmitter.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/JobInProgress.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/PeriodicStatsAccumulator.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapred/ProgressSplitsBlock.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/LoggedTaskAttempt.java
* /hadoop/common/trunk/mapreduce/src/java/org/apache/hadoop/mapreduce/jobhistory/MapAttemptFinishedEvent.java
* /hadoop/common/trunk/mapreduce/src/tools/org/apache/hadoop/tools/rumen/MapAttempt20LineHistoryEventEmitter.java
","18/Oct/11 06:36;acmurthy;Editorial pass over hadoop-0.23 content.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We need a benchmark to model system behavior in the face of tasks with time-variant performance,MAPREDUCE-2063,12473936,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,dking,dking,dking,12/Sep/10 17:51,02/May/13 02:29,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"This benchmark would accept descriptions of task performance, with the times of reaching the [say] deciles of progress reported.",,ddas,hong.tang,ranjit,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-2037,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,149968,,,,,2010-09-12 17:51:36.0,,,,,,,"0|i0e833:",81072,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Large-scale Automated Framework,MAPREDUCE-1774,12463953,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,cos,cos,cos,07/May/10 05:32,02/May/13 02:29,12/Jan/21 09:52,26/Jun/10 20:27,0.21.0,,,,,0.21.0,,,test,,,,,,0,,,,,This is MapReduce part of HADOOP-6332,,lianhuiwang,sharadag,sreekanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-1646,MAPREDUCE-1713,HADOOP-6786,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/10 00:40;cos;MAPREDUCE-1774.patch;https://issues.apache.org/jira/secure/attachment/12447409/MAPREDUCE-1774.patch","17/Jun/10 18:30;cos;MAPREDUCE-1774.patch;https://issues.apache.org/jira/secure/attachment/12447371/MAPREDUCE-1774.patch","16/Jun/10 19:15;cos;MAPREDUCE-1774.patch;https://issues.apache.org/jira/secure/attachment/12447263/MAPREDUCE-1774.patch","14/Jun/10 20:59;cos;MAPREDUCE-1774.patch;https://issues.apache.org/jira/secure/attachment/12447065/MAPREDUCE-1774.patch","12/Jun/10 01:30;cos;MAPREDUCE-1774.patch;https://issues.apache.org/jira/secure/attachment/12446937/MAPREDUCE-1774.patch","12/Jun/10 01:23;cos;MAPREDUCE-1774.patch;https://issues.apache.org/jira/secure/attachment/12446936/MAPREDUCE-1774.patch","03/Jun/10 18:44;cos;MAPREDUCE-1774.patch;https://issues.apache.org/jira/secure/attachment/12446271/MAPREDUCE-1774.patch","03/Jun/10 00:30;cos;MAPREDUCE-1774.patch;https://issues.apache.org/jira/secure/attachment/12446207/MAPREDUCE-1774.patch","01/Jun/10 21:06;cos;MAPREDUCE-1774.patch;https://issues.apache.org/jira/secure/attachment/12446063/MAPREDUCE-1774.patch","26/May/10 22:15;cos;MAPREDUCE-1774.patch;https://issues.apache.org/jira/secure/attachment/12445589/MAPREDUCE-1774.patch","21/May/10 22:17;cos;MAPREDUCE-1774.patch;https://issues.apache.org/jira/secure/attachment/12445213/MAPREDUCE-1774.patch","07/May/10 06:11;cos;MAPREDUCE-1774.patch;https://issues.apache.org/jira/secure/attachment/12443926/MAPREDUCE-1774.patch",,,,,,,,,,,,,,,,,,,,,,,12.0,,,,,,,,,,,,,,,,,,,,2010-06-12 07:16:00.471,,,false,,,,,,,,,,,,,,,,,,36964,Reviewed,,,,Sat Jun 26 20:27:00 UTC 2010,,,,,,,"0|i02ptb:",13783,,,,,,,,,,,,,,,,,,,,,"07/May/10 06:11;cos;Same as for HDFS-1134: initial version of source code forward patch. No build changes are included yet.","21/May/10 22:17;cos;This patch version has all correct build modifications in please. However, because of the code changes between MR in 0.20 and in the trunk aspects aren't binding anymore and this needs to be fixed.","26/May/10 22:15;cos;Workaround for {{test-patch}} version setting. Fixing too wide a mask to include aspect files.","01/Jun/10 21:06;cos;Changes similar to latest HDFS ones: moving tests to {{src/test/system/test}} for better build handling; changing Common artifacts from {{core}} to {{common}}, etc.
Can't fully verify the patch at the moment because aspects are still broken and need more work there.","03/Jun/10 00:30;cos;Merging in Sreekanth's modifications of aspects addressing changes in 0.22 MR API.
Also, rename of {{core}} to {{common}} was lost somehow, so I had to re-merge this particular modification once more.
At the moment all code is being properly woven, tests are mostly compiled except for {{TestFileOwner}} which fails with follow message:
{noformat}
    [javac] .../H0.22/git/mapreduce/src/test/system/test/org/apache/hadoop/mapred/TestFileOwner.java:183: cannot find symbol
    [javac] symbol  : variable TASKJARDIR
    [javac] location: class org.apache.hadoop.mapred.TaskTracker
    [javac]           if (filename.equals(TaskTracker.TASKJARDIR)) {
    [javac]                                          ^
{noformat}
However, the brief check shows that the symbol is present in the woven class file. 
","03/Jun/10 18:44;cos;Two missed classes for {{testjar}}; {{system-test.conf}} is renamed to {{{{system-test-mapred.conf}}","12/Jun/10 01:23;cos;This version of the patch has all issues with the build resolved. Template for POM files are also created and instrumented build is assembled properly.

The only issue I see at the moment that tests can not connect to 0.20 cluster. It seems like the changes in the aspects couldn't cause that, so there should be another reason for this. Will keep looking and/or try to get together and start 0.22 cluster to make sure the this is a real problem.

Patch is pretty much ready for verification.","12/Jun/10 01:23;cos;Let's check.","12/Jun/10 01:30;cos;Coupla more javadoc and import corrections.","12/Jun/10 07:16;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12446937/MAPREDUCE-1774.patch
  against trunk revision 953879.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 105 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 2226 javac compiler warnings (more than the trunk's current 2219 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 3 release audit warnings (more than the trunk's current 0 warnings).

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/238/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/238/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/238/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/238/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/238/console

This message is automatically generated.","14/Jun/10 20:59;cos;- This patch addresses audit warnings caused by missing Apache license boiler plate in a couple of places.
- Javac warnings are caused by using deprecated {{JobConf}} and {{JobContext}} in two new classes from {{testjar}} package. While this is a valid issue I am not sure if it has to fought considering 2K+ of similar warnings all over the MR code. 
- Core tests failures are old: they are around for at least 6 days and this patch hasn't cause any ones
Contrib test failure seems irrelevant (a Mumak testcase {{TestSimulatorDeterministicReplay}} timing out for over 10 days).
","16/Jun/10 19:15;cos;Includes Sreekanth fix for the connectivity problem. It has been caused by property names change between MR 0.20 and 0.22
- mapred.job.tracker -> mapreduce.jobtracker.address
- mapred.task.tracker.report.address ->mapreduce.tasktracker.report.address
But because MR lacks a container similar to {{DFSConfigKeys}} there's *no way* to track such change by Java means. Except by knowing by heart ;(

Tests are running against a real 0.22 cluster now! Some issues are addressed by HADOOP-6828 (another neat change). A couple more of issues will addressed in the next iteration. The patch is almost ready at the moment!","17/Jun/10 18:30;cos;With another Sreekanth fix jobhistory is retrieved properly now.
Tests are running. The only issue I am seeing at the moment is incorrect use of daemon scripts which needs to be addressed separately (HADOOP-6829)

I think the patch is ready for the commit as the MR part of the framework works. Some of the test issues should be addressed separate. Longer this patch stays out higher the changes it will get stale. ","17/Jun/10 18:38;cos;Let's see once more.","17/Jun/10 22:38;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12447371/MAPREDUCE-1774.patch
  against trunk revision 955543.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 103 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 2226 javac compiler warnings (more than the trunk's current 2219 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/251/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/251/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/251/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/251/console

This message is automatically generated.","18/Jun/10 00:39;cos;Test failure is unrelated. The number of warnings is increasing because of the some deprecated APIs usage. They are all used over the place and perhaps their elimination should be a matter of separate JIRA.
","18/Jun/10 00:40;cos;Same as before but all string literals are replaced with proper named constants from container classes.","21/Jun/10 06:05;sharadag;Retrying Hudson","21/Jun/10 09:36;sharadag;Tested binary-system, jar-system and jar-test-system. Worked fine.
Tried running 'test-system' against a real deployed cluster. The test client could contact the cluster but some of the test cases are failing for me. The framework is working fine. I am ok fixing testcases in subsequent issues. +1 on committing this.
","21/Jun/10 10:41;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12447409/MAPREDUCE-1774.patch
  against trunk revision 956335.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 103 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 2226 javac compiler warnings (more than the trunk's current 2219 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/580/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/580/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/580/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/580/console

This message is automatically generated.","22/Jun/10 00:41;cos;I have committed this both to trunk and 0.21 branch. Thanks Sharad, Sreekanth, and everybody else!","22/Jun/10 02:52;cos;This is going to be closed as soon as the subtasks are fixed.","26/Jun/10 20:27;cos;This has been committed and all subtasks are resolved. Closing it as fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[herriot] Ability to restart a single node for pushconfig,MAPREDUCE-1889,12467678,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,balajirg,balajirg,balajirg,23/Jun/10 08:39,02/May/13 02:29,12/Jan/21 09:52,,,,,,,,,,test,,,,,,0,,,,,"Right now the pushconfig is supported only at a cluster level, this jira will introduce the functionality to be supported at node level. 
",,balajirg,cos,iyappans,vinaythota,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/10 17:34;balajirg;restartDaemon.txt;https://issues.apache.org/jira/secure/attachment/12447961/restartDaemon.txt","05/Jul/10 11:22;balajirg;restartDaemon_1.txt;https://issues.apache.org/jira/secure/attachment/12448683/restartDaemon_1.txt","09/Jul/10 08:36;balajirg;restartDaemon_y20.patch;https://issues.apache.org/jira/secure/attachment/12449067/restartDaemon_y20.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2010-06-28 10:59:56.915,,,false,,,,,,,,,,,,,,,,,,149838,,,,,Fri Jul 09 08:36:20 UTC 2010,,,,,,,"0|i0e8iv:",81143,,,,,,,,,,,herriot,,,,,,,,,,"28/Jun/10 10:59;vinaythota;- Add the return type deatils in java doc section.
{noformat}
+  
+  /**
+   * getTTClient will give access to one of many TTClient present 
+   */
+  public TTClient getTTClient() {
{noformat}

- there's no need to assign {{null]] to a variable and then immediately assign a real object to it.
{noformat}
+  private void writeConfToFile(String configFile, String localDirPath,
+      Configuration conf) throws IOException{      
+    File xmlFileObj = null;
+    String confXMLFile = null;
+    
+    localFolderObj = new File(localDirPath);
+    if (!localFolderObj.exists()) {
+      localFolderObj.mkdir();
+    }
+    confXMLFile = localDirPath + File.separator + configFile;
+    xmlFileObj = new File(confXMLFile);
{noformat}

-Incomplete java doc details. 
{noformat}
   /**
+   * Restart only one daemon as opposed to all the daemons
+   * @param client points to the daemon that will restarted. 
+   * @throws IOException
+   */
+  public void restart(AbstractDaemonClient client) throws IOException {
{noformat}
{noformat}
+  
+  /**
+   * Get a RemoteProcess given the hostname
+   */
+  RemoteProcess getDaemonProcess(String hostname);
 {noformat}","05/Jul/10 11:22;balajirg;Implemented vinay's review comment, on getting +1 will generate patches for forward porting. ","05/Jul/10 11:28;vinaythota;Patch looks good. +1 for patch.","07/Jul/10 02:24;cos;- technically, you might end up with a situation where the same host is having two different daemons, say JT and TT. Or NN and second DN. I believe in such situation this new method {{+  public RemoteProcess getDaemonProcess(String hostname) { }} will be deterministic. That's why we have {{HadoopDaemonInfo}} class with a role for any daemon. Perhaps, the method should have an extra parameter and return only daemons with a specific role.
- JavaDoc hasn't been changed for the changes of the signature
{[+  String pushConfig(String localDir) throws IOException;}}
Also, will changes of the method signature affect existing tests?

And please name the pachs {{something.patch}} instead of .txt or else.","09/Jul/10 08:36;balajirg;Absolutely valid comments, and I have implemented them. The role is a required field. Also I have modified the JIRA MAPREDUCE-1854 and provided the new patch to use the new methods. Once I get +1 for y20 I will generate patch for trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[herriot] Automate health script system test,MAPREDUCE-1854,12466673,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,balajirg,balajirg,balajirg,10/Jun/10 17:34,02/May/13 02:29,12/Jan/21 09:52,,,,,,,,,,test,,,,,,0,,,,,"1. There are three scenarios, first is induce a error from health script, verify that task tracker is blacklisted. 
2. Make the health script timeout and verify the task tracker is blacklisted. 
3. Make an error in the health script path and make sure the task tracker stays healthy. ",Herriot framework,balajirg,cos,iyappans,vinaythota,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,432000,432000,,0%,432000,432000,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-1882,,,,,,,,,,,,,,,,,MAPREDUCE-1889,,,,"10/Jun/10 17:39;balajirg;health_script_5.txt;https://issues.apache.org/jira/secure/attachment/12446777/health_script_5.txt","24/Jun/10 17:43;balajirg;health_script_7.txt;https://issues.apache.org/jira/secure/attachment/12447962/health_script_7.txt","05/Jul/10 12:26;balajirg;health_script_trunk.txt;https://issues.apache.org/jira/secure/attachment/12448691/health_script_trunk.txt","05/Jul/10 12:08;balajirg;health_script_y20.txt;https://issues.apache.org/jira/secure/attachment/12448688/health_script_y20.txt","09/Jul/10 08:28;balajirg;health_script_y20_1.patch;https://issues.apache.org/jira/secure/attachment/12449063/health_script_y20_1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,2010-06-11 06:18:36.965,,,false,,,,,,,,,,,,,,,,,,149809,,,,,Thu Jul 22 00:44:48 UTC 2010,,,,,,,"0|i0e8kv:",81152,,,,,,,,,,,herriot,,,,,,,,,,"10/Jun/10 17:39;balajirg;The first patch for review. ","11/Jun/10 06:18;cos;Some comments:
- changing visibility from 'package private' to 'public' for testing purpose isn't advisable. Consider injecting a getter with public access
- same here: 
{noformat}
 -  static class TaskTrackerHealthStatus implements Writable {
 +  public static class TaskTrackerHealthStatus implements Writable {
{noformat}
- AbstractTestCase sounds like a utility methods' class to me. Unless a common parent is really required from some design perspective I won't recommend clog class hierarchy with unnecessary inheritance. 
- using hard-coded paths like {{/tmp/}} restricts tests applicability. Would it be better to use configurable location, i.e. mapred data directory or something?
- JUnit v3 imports +import junit.framework.Assert
- using {{StringBuffer}} to append a couple of tokens and convert the result to a new {{String}} seems excessive. Why not to use {{String}}?
{noformat}
+    StringBuffer localFile = new StringBuffer();
+    localFile.append(scriptDir).append(File.separator).append(scriptName);
+    cmdArgs.add(localFile.toString());
{noformat}
- remove commented out lines of code which seem like a debugging leftovers
- try to generate patch with '--no-prefix' to avoid extra prefixes in the file paths
- this JavaDoc seems incomplete
{noformat}
+   * This directly calls the JobTracker public with no modifications
+   * @param trackerID uniquely indentifies the task tracker
+   * @return
+   * @throws IOException is thrown in case of RPC error
{noformat}
- there some unused imports
- are changes in AbstractDaemonCluster.java related to this patch?
- looks like the change in DaemonProtocolAspect.aj is unrelated to this patch, isn't it? ;)
- same about ClusterProcessManager, HadoopDaemonRemoteCluster, and RemoteProcess

As you're clearly using Git (this isn't SVN - it is a great SCM system!) for the development work try to have a separate branch for any JIRA you're working on. But this you'll avoid any mess and accidental inclusion of irrelevant files. 

- writing new script every time we need to do some sort of ssh command looks bad. I have a couple alternative thoughts:
** using pure Java ssh client like JSch
** creating a wrapper around ssh command using Shell class (in case the above is impossible because of license issues or something)

I think this is enough for the starter :)","11/Jun/10 06:21;cos;Looks like license of JSch shouldn't be an issue because BSD license is generally acceptable for Apache software (think Ant)","11/Jun/10 07:09;balajirg;changing visibility from 'package private' to 'public' for testing purpose isn't advisable. Consider injecting a getter with public access 

The tasktrackerStatus is a writable object, should'nt the inner class of writable object be
public for others to use, I understand we don't want to change permission of jobtracker or
tasktracker, but tasktrackerStatus is a writable object which can be streamed across the wire,
but if you still insist I will have aspect hooks to make the inner class methods public. 

using StringBuffer to append a couple of tokens and convert the result to a new String seems excessive. Why not to use String? 
I used stringbuffer for appends, since each time a new string object is created, and temporary
object is discarded a new string object is created, and I thought if I use string then people
will comment why not use stringbuffer since it avoids all the temporary object creations :(. 

Do agree with you AbstractTestCase methods looks more of a helper, will move it a static
helper than have it part of parent hierarchy, I was in a dilemma on how to do this myself.
My real intention of having abstract parent class is have common functionality that can be
shared among all the test cases, which can be reused easily. 

The changes to the abstractdaemoncluster are related to the patch, because the health script automation 
validates that a single task tracker should be stopped/started instead of stopping and starting the
whole cluster. 

I do agree DaemonProtocolAspect.aj is not related to the patch, it was same fix vinay mentioned,
but I had to have that code to make the test case pass. So I will remove this from my subsequent patch. 
Also I will have to put an dependency on the vinay's jira meaning I can only check in my patch 
after he completes his changes and checks it in. 
 Also I am planning to break the individual  test methods to seperate classes since the test occasionally timesout. 

The clusterprocessmanager, defined the iterface method to get a daemon process given the hostname, 
HadoopDaemonRemoateCluster provides the implemetation, for the interface. The pushConfig in 
Remote Process was not returning the newConf Dir, now I need to return that since health script
selectively restarts a particual daemon. So it is all related to the health script patch. ","14/Jun/10 19:53;cos;bq. The tasktrackerStatus is a writable object, should'nt the inner class of writable object be public for others to use.
You might be right. However, this field has package-private access. And I believe this has been done for a reason. I am not an expert on MR's internals to tell you one way or another. However, from the common standpoint such widening of permissions isn't advisable.

.bq My real intention of having abstract parent class is have common functionality that can be shared
If in the future we'll see that the number of such shared functions is growing and it become useful to move them all to the common parent we might do just that. However, two functions don't like a good justification to me.

Thanks for the explanations on the Common classes' modifications. They all make sense. I guess these changes will have to end up in a separate JIRA though.

What about getting rid of the script wrappers for ssh functionality and sleep? ","15/Jun/10 10:03;balajirg;What about getting rid of the script wrappers for ssh functionality and sleep? 
   I will come with script wrapper for copying and deleting the script files in remote nodes, but you do see the need to have sleep and echo ERROR message in a script, this is because tasktracker code picks up a health script and runs it in regular interval which is configurable. So to induce the error I change the health script location, and copy the error inducing health script, and put back everything to normal state at the end of the test. So I don't think I can do script wrapper for sleep, echo error. ","15/Jun/10 18:11;cos;Yeah, I think  you right about the 'sleep-echo' thing.
And I still suggest to take a look at JSch or a similar library for pure Java SSH wrapper.","16/Jun/10 11:33;balajirg;Regarding Jsch I read about it after you point that out, I totally agree with you that this seems to be better solution than running a shell command from the java code using the Shell.java helper class. Cos my thoughts on this can we make this is a seperate JIRA and replace all the places where we use ssh and execute remote command replace it with Jsch, rather than doing it for particular instance alone, since if I do this for health script alone  there will be certain funcationality using Shell command execution, and others using Jsch. If you agree I will raise a seperate JIRA. ","17/Jun/10 01:45;cos;Right, please open a separate JIRA for JSch integration. I want to make sure that no ssh wrapper scripts are going into the Herriot code anymore after this particular one.","24/Jun/10 17:43;balajirg;Addresses all the code review comments. ","29/Jun/10 21:07;cos;- Consider lowering log levels in {{JobTrackerAspect.aj}}. I think there's no need to print clearly debug messages with *INFO* level.
- {{+   * @return int}} isn't very informative.
- this is very C-like programming style:
{noformat}
+  /**
+   * This gets the value of all task trackers windows in the tasktracker page.
+   *
+   * @param none,
+   * @return int[] of  all the tasks that ran, in the sequence given below
+   * ""since_start"", ""total_tasks""
+   * ""since_start"",""succeeded_tasks""
+   * ""last_hour"", ""total_tasks""
+   * ""last_hour"", ""succeeded_tasks""
+   * ""last_day"", ""total_tasks""
+   * ""last_day"", ""succeeded_tasks""
+   */
{noformat}
why don't you use an object container instead? It will make code like this 
{noformat}
+    int totalTasksSinceStartBeforeJob = ttAllInfo[0];
+    int succeededTasksSinceStartBeforeJob = ttAllInfo[1];
+    int totalTasksLastHourBeforeJob = ttAllInfo[2];
+    int succeededTasksLastHourBeforeJob = ttAllInfo[3];
{noformat}
much clearer and readable.
- bad choice of class name {{TestTaskTrackerInfoFirst}} as well as {{TestTaskTrackerInfoSecond}}
- some of the tests are commented out {{//@Test}}. Please consider to take them away all together if they aren't going to be used. You'd better add them later in a separate JIRA.
- in the second test class waiting and log levels in {{waitForTTStop()}} and {{waitForTTStart()}} seem to be inconsistent.
","29/Jun/10 21:09;cos;Please disregard my last comment - this was intended for MAPREDUCE-1871","30/Jun/10 03:28;balajirg;Cos is it a +1 for this jira, can I commit ?","30/Jun/10 16:44;cos;- Please remove 
{noformat}
+        LOG.error(""Exit code in shell command exe ""+exitCode+"" ""+errMsg.toString());
{noformat}
from the Shell.java in the Common code.
- Also, this seems to be the patch for y20 branch. Please provide one for trunk.
","05/Jul/10 12:08;balajirg;Removed changes to Shell.java this patch is for yahoo y20. Will submit another patch for trunk. ","05/Jul/10 12:26;balajirg;This patch is for trunk. ","07/Jul/10 02:29;cos;- This script seems to be a test related thing {{src/test/system/scripts/healthScriptError}} So, shall it be the part of framework scripts?
- inconsistent formatting:
{noformat}
+  private void deleteFileOnRemoteHost(String path, String hostname) 
+  {
{noformat}
and
{noformat}
+  private void verifyTTBlackList(Configuration conf, TTClient client, String 
+      errorMessage) throws IOException{   
{noformat}

Looks good otherwise.","07/Jul/10 06:03;balajirg;how about creating src/test/system/fw and src/test/system/tc directories and have the scripts be present in two different directories. ","09/Jul/10 08:28;balajirg;I have refactored the test cases into 3 since at times as a whole the test case timesout, also since the Role was added to dependent JIRA MAPREDUCE-1889, the test cases where refactored . I will generated patch for trunk once I get +1 for y20. ","12/Jul/10 04:35;cos;bq. how about creating src/test/system/fw and src/test/system/tc directories and have the scripts be present in two different directories. 

Well, in the trunk tests are separated from the rest of the framework. Thus, their scripts should be with them.","21/Jul/10 12:59;iyappans;Remove the LOG.info statement from nodeHealthStatus method, as it is inside a while loop. Rest of the code is Ok for 20.1.xxx. ","21/Jul/10 13:09;balajirg;The info statement I added intentionally it gives me an idea of the status of the task tracker, there is a delay in the while loop for 3 secs, to the log statement does not swamp the test output. ","22/Jul/10 00:44;cos;bq. Well, in the trunk tests are separated from the rest of the framework. Thus, their scripts should be with them.

So, what about the scripts location? Any comments?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Collecting CPU and memory usage for MapReduce jobs,MAPREDUCE-1739,12463256,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,schen,schen,schen,28/Apr/10 20:43,02/May/13 02:29,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"MAPREDUCE-220 collects CPU and memory usage for each task.
We can aggregate them to get the information per job. Such information can be used for scheduling, profiling or charging the users based on the resource they consumed.

Here are some information that should be useful:
1. Total CPU cycles (# of giga-cycles)
2. Total Memory occupied time (GB-sec)
3. Maximum peak memory on one task (GB)
4. Maximum peak CPU on one task (GHz)

Thoughts?",,cdouglas,dhruba,farseeing,hammer,johanoskarsson,lianhuiwang,philip,srivas,yhemanth,yvesbastos,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-1808,,,,,,,,,,,,,,,,,MAPREDUCE-220,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,149720,,,,,2010-04-28 20:43:06.0,,,,,,,"0|i0e8sv:",81188,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The trace generator should test operation on the current format by building a test case that runs an ad hoc cluster,MAPREDUCE-1479,12455950,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,dking,dking,10/Feb/10 22:57,02/May/13 02:29,12/Jan/21 09:52,,0.22.0,,,,,,,,tools/rumen,,,,,,0,,,,,"Rumen's trace generator, built in final form when we install MAPREDUCE-1309 , has features that can parse a history event log in the current format.  The testing story on that feature is not good.  We can improve it by writing a test case that builds a local single-node cluster, runs a carefully crafted map/reduce job on that cluster, runs the trace generator on the resulting job tracker log, and finally tests the trace for correctness.",,hong.tang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-1309,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,149541,,,,,Wed Feb 10 22:59:54 UTC 2010,,,,,,,"0|i0e97z:",81256,,,,,,,,,,,,,,,,,,,,,"10/Feb/10 22:59;dking;There's little point in testing older versions of the trace builder.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sqoop should support CLOB and BLOB datatypes,MAPREDUCE-1446,12455111,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,kimballa,kimballa,kimballa,02/Feb/10 20:20,02/May/13 02:29,12/Jan/21 09:52,18/Mar/10 23:18,,,,,,,,,,,,,,,0,,,,,Sqoop should allow import of CLOB and BLOB based data.,,alexlod,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-1445,,,,"02/Feb/10 22:11;kimballa;MAPREDUCE-1446.2.patch;https://issues.apache.org/jira/secure/attachment/12434601/MAPREDUCE-1446.2.patch","02/Feb/10 21:18;kimballa;MAPREDUCE-1446.patch;https://issues.apache.org/jira/secure/attachment/12434591/MAPREDUCE-1446.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2010-02-09 02:54:11.682,,,false,,,,,,,,,,,,,,,,,,149516,Reviewed,,,,Sat Mar 20 06:45:37 UTC 2010,,,,,,,"0|i0jgn3:",111626,,,,,,,,,,,,,,,,,,,,,"02/Feb/10 21:18;kimballa;Attaching a patch which provides this functionality. The main challenge of BLOB and CLOB data is that it can result in very large records -- larger than will fit in memory all at once. The current patch proposes a mechanism to have two serializations for CLOB/BLOB data:

* Data less than 16MB will be stored inline in the record bodies
* Data greater than 16MB will be stored in separate files in HDFS; the records will contain only a pointer to the file. This will then be accessed through an InputStream interface so that users can buffer in as much data as is appropriate.

The latter of these two mechanisms is unimplemented, but placeholders have been left in the code where necessary. The boundary size (16MB) is also a load-time parameter. It is currently hardcoded, but it would be trivial to allow users to configure this to their own liking based on their datasets, hardware, etc.","02/Feb/10 22:11;kimballa;Better BLOB testcase for Oracle","09/Feb/10 02:54;bohanchen;We tested the patch with less than 16MB CLOB, and it works for us.  Thanks for the great work of Aaron!","18/Feb/10 04:16;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12434601/MAPREDUCE-1446.2.patch
  against trunk revision 911234.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 12 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/326/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/326/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/326/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/326/console

This message is automatically generated.","18/Mar/10 23:18;tomwhite;I've just committed this. Thanks Aaron!","19/Mar/10 17:07;hudson;Integrated in Hadoop-Mapreduce-trunk #263 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/263/])
    . Sqoop should support CLOB and BLOB datatypes. Contributed by Aaron Kimball.
","20/Mar/10 06:45;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #285 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/285/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add AvroInputFormat and AvroOutputFormat so that hadoop can use Avro Serialization,MAPREDUCE-815,12431762,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,kimballa,ravidotg,ravidotg,30/Jul/09 04:38,02/May/13 02:29,12/Jan/21 09:52,21/May/10 22:16,,,,,,,,,,,,,,,2,,,,,MapReduce needs AvroInputFormat similar to other InputFormats like TextInputFormat to be able to use avro serialization in hadoop. Similarly AvroOutputFormat is needed.,,acmurthy,atm,aw,brentworden,cdouglas,cutting,cwilkes,cwsteinbach,eyang,hammer,jbooth,jkreps,johanoskarsson,jrideout,kimballa,klbostee,mahadev,omalley,philip,romainr,sgoyal,sharadag,tomwhite,zhong,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-895,,,,,,,AVRO-493,,,,,,,,,,,,,,,,,,MAPREDUCE-1360,,,,,,,,,,,,,,,,,HADOOP-6497,,,,"14/Jan/10 22:14;kimballa;MAPREDUCE-815.2.patch;https://issues.apache.org/jira/secure/attachment/12430305/MAPREDUCE-815.2.patch","14/Jan/10 23:58;kimballa;MAPREDUCE-815.3.patch;https://issues.apache.org/jira/secure/attachment/12430317/MAPREDUCE-815.3.patch","15/Jan/10 00:30;kimballa;MAPREDUCE-815.4.patch;https://issues.apache.org/jira/secure/attachment/12430326/MAPREDUCE-815.4.patch","15/Jan/10 01:09;kimballa;MAPREDUCE-815.5.patch;https://issues.apache.org/jira/secure/attachment/12430332/MAPREDUCE-815.5.patch","14/Jan/10 04:38;kimballa;MAPREDUCE-815.patch;https://issues.apache.org/jira/secure/attachment/12430224/MAPREDUCE-815.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,2009-10-20 22:36:57.213,,,false,,,,,,,,,,,,,,,,,,149061,,,,,Fri May 21 22:16:20 UTC 2010,,,,,,,"0|i0jeov:",111310,,,,,,,,,,,,,,,,,,,,,"30/Jul/09 05:00;ravidotg;This could have something like

  public class AvroInputFormat extends
    FileInputFormat<AvroReflectSerializable, AvroReflectSerializable> {

  @Override
  public RecordReader<AvroReflectSerializable, AvroReflectSerializable> 
    createRecordReader(InputSplit split,
                       TaskAttemptContext context) {
    return new AvroRecordReader();
  }
//...
}



and 

public class AvroRecordReader extends
    RecordReader<AvroReflectSerializable, AvroReflectSerializable> {
//implements the methods of RecordReader for KEY and VALUE of avro types
}



Does this look fine ?","20/Oct/09 22:36;cutting;This issue depends on the shuffle using the metadata-based API (MAPREDUCE-1126).","15/Dec/09 06:08;jrideout;What is the current line of thought on how keys and values will interact with the schema for an avro file? Is the intention that there would be a master schema that encapsulated the key/values similar to:

{code}
{ ""type"" : ""record"",
  ""fields"" : [
  { ""name"" : ""KEY"", ""type"" : ""record"" },
  { ""name"" : ""VALUE"", ""type"" : ""record"" }
]}
{code}

What about files created without this ""master"" schema; would the key return a null object? Byte offset in a schema of type ""long"" ?","15/Dec/09 17:30;cutting;> What is the current line of thought on how keys and values will interact with the schema for an avro file?

I think, similar toTextInputFormat and TextOutputFormat, only one of key and value will be useful and the other will be null.  It doesn't matter much which.  If input keys have the Avro datum and values are null then the default identity mapper can be used to sort data, while if input values contain the Avro datum and keys are null then InverseMapper must be specified.
","15/Dec/09 20:58;jrideout;> only one of key and value will be useful and the other will be null.
Thanks Doug, that makes sense.

What about the AvroOutputFormat? Does the same condition apply? I can see ignoring keys for the output of the reducer, but what about the output of a map?","15/Dec/09 21:08;cutting;> What about the AvroOutputFormat?

I suggest we treat it similarly: require either keys or values to be null.

An output format could combine output keys and values into a compound record, and one could define an input format that splits each input datum into a separate key and value, but I don't think the basic AvroOutputFormat should do this.  If we add a MapFile-like abstraction for Avro, then its input and output formats should probably do this.
","12/Jan/10 01:01;kimballa;Now that MAPREDUCE-1126 is in, I'm going to attack this and complete the loop.

Given that TextInputFormat yields a semi-arbitrary key and encapsulates the file contents in the value, I plan to follow suit here -- the value produced by the AvroRecordReader will contain the next object in the file. 

As for output: I think that it's best to leave the output format accepting a single value only (rather than explicitly making a hybrid of key and value pair). Users can implement their own UnionAvroOutputFormat (or whatever) if they need both, but I think the basic version should only do the most straightforward thing. I plan to make this write the user's key to the file, and drop the value. That way InverseMapper -> IdentityReducer should emit it all in sorted order.

","12/Jan/10 17:46;cutting;Aaron, this sounds good.  A few questions:
 - If, in the InputFormat we populated the key rather than the value, then one would not even need to specify InverseMapper: by default, MapReduce would simply partition and sort Avro data.  Making values optional in both input and output seems more consistent, but does break compatibility with TextInputFormat.  Thoughts?
 - In the OutputFormat, should we check if values are non-null or just drop them?  Just dropping them may cause some confusion, but is probably useful in many cases, so I guess we err towards utility?","12/Jan/10 18:08;tomwhite;bq. If, in the InputFormat we populated the key rather than the value, then one would not even need to specify InverseMapper: by default, MapReduce would simply partition and sort Avro data.

I like this. It is the approach I was taking on MAPREDUCE-252. ","12/Jan/10 18:20;cutting;FWIW, the file-position-as-text-map-input-key convention came from the original Google MapReduce paper, but I don't think its ever proven useful.","12/Jan/10 22:07;kimballa;Doug:

* I agree; I'll make this be the key. The value will be the byte offset.
* My current implementation gives a log message at level WARN the first time a non-null value is received; it then ignores the value and continues operating.

","14/Jan/10 04:38;kimballa;Attaching a patch that provides AvroInputFormat/AvroOutputFormat.

AvroInputFormat allows you to set its input schema in the job configuration. It provides static methods for this functionality. Depending on the input serialization metadata it can choose to deserialize to generic, reflect, or specific-based classes. 

This patch includes unit tests for both of these classes.

I have also extended the jobdata API to allow you to set output serialization metadata (vs. simple class-name-only metadata) in the same fashion as MAPREDUCE-1126 allowed you to set intermediate serialization metadata. This deprecates the old methods like {{JobConf.setOutputKeyClass()}}. Note that now the PipesMapRunner/PipesReducer, MapFileOutputFormat, and SequenceFileOutputFormat rely on these deprecated APIs. MAPREDUCE-1360 will require a Hadoop-core-project JIRA that allows SequenceFile to handle non-class-based serialization; that will update at least the SequenceFile IF/OF APIs. Handling Pipes is a separate issue.

This cannot be submitted to the patch queue until a small change is made to the Hadoop-core API (issue is linked), and Hadoop is upgraded across the board to Avro 1.3. I'll mark this patch-available when that happens.","14/Jan/10 17:38;cutting;This looks great!  A few nits:
 - in javadoc comments, use ""@deprecated use #foo()""  to link to the new implementation
 - AvroSeekableStream is likely to be reused by other applications that use Avro with HDFS.  it might be named AvroFSInput.  it might better belong in common than in mapreduce.
 - why use LongWritable?  Could we instead use java.lang.Long?  Or perhaps just null for these values?  Does anyone ever make use of the position?  If not, let's use null.  If we can avoid a dependency on Writable here that'd be good.  or does this provide some important compatibility?
 - i don't think SYNC_DISTANCE is needed: DataFileWriter syncs automatically every 100k or so.
","14/Jan/10 19:11;kimballa;The only reason I could think of to use the position would be building some sort of index over an avro file. I think this probably doesn't make much sense here. That having been said, we can't use null or we'll break the identity mapper. (The MapOutputBuffer expects non-null keys and values only. A {{context.write(k, null)}} from the mapper will throw NullPointerException.) 

This is why writables included NullWritable, I think. We could add a type e.g. ""Empty"" which implements AvroReflectSerializable and whose toString method returns the empty string; this would work fairly transparently I think and be entirely avro-based.
","14/Jan/10 19:22;cutting;> That having been said, we can't use null or we'll break the identity mapper.

It seems to me that we should be able to pass null end-to-end as a value.  If we can't, then we perhaps haven't yet removed all of the Writable assumptions, no?","14/Jan/10 22:14;kimballa;New patch per code-review.

* HADOOP-6492 has been updated with {{org.apache.hadoop.fs.AvroFSInput}}; AvroRecordReader now makes use of this.
* AvroInputFormat/AvroRecordReader now always returns a null value (while Writables can't handle nulls, I've confirmed that Avro can accept a null schema for a value and this works in-shuffle)
* AvroRecordReader now traps {{java.util.NoSuchElementException}} in {{nextKeyValue()}} to gracefully handle EOF
* Tests now include a test which does not set an explicit schema, but instead uses the Generic interface to read it from the file
* javadoc comments updated
* {{SYNC_DISTANCE}} removed.","14/Jan/10 22:37;cutting; - those javadoc @deprecated lines don't look right.  did you run javadoc & look at the output?
 - AvroInputFormat's value field isn't used and could be removed.
 - should we add an end-to-end test somewhere that runs an mr job with avro input, avro comparison, and avro output? new issue? localrunner would probably suffice.","14/Jan/10 23:58;kimballa;New patch; fixed javadoc. Also includes an avro end-to-end test in TestAvroInputFormat.","15/Jan/10 00:30;kimballa;end-to-end test was accidentally excluded from patch #4. New patch includes this.","15/Jan/10 00:52;cutting;In the end-to-end-test validation, you don't need seekable and can read the entire file with something like:

{code}
reader = new DataFileStream<Integer>(istream, datumReader);
for (int value : reader) { ... }
{code}

This will save a number of lines and provide a better example of non-split Avro file usage.
","15/Jan/10 01:09;kimballa;Now uses DataStreamReader","27/Jan/10 03:24;cdouglas;It looks like some changes to JobTracker were accidentally included in the latest patch","18/May/10 16:33;cutting;I'd like to close this as redundant with AVRO-493.  This will be included in the upcoming 1.4.0 release of Avro which will support mapreduce over Avro data files using Hadoop 0.20 or greater.  Objections?","21/May/10 22:13;cutting;Marking this a duplicate of AVRO-493.","21/May/10 22:16;cutting;Closing this as a duplicate. This can be re-opened if someone objects.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compression and output splitting for Sqoop,MAPREDUCE-1017,12436262,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,kimballa,kimballa,kimballa,22/Sep/09 00:29,02/May/13 02:29,12/Jan/21 09:52,22/Oct/09 14:58,,,,,,,,,,,,,,,0,,,,,"Sqoop ""direct mode"" writing will generate a single large text file in HDFS. It is important to be able to compress this data before it reaches HDFS. Due to the difficulty in splitting compressed files in HDFS for use by MapReduce jobs, data should also be split at compression time.
",,aaa,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/09 23:32;kimballa;MAPREDUCE-1017.2.patch;https://issues.apache.org/jira/secure/attachment/12422162/MAPREDUCE-1017.2.patch","15/Oct/09 20:08;kimballa;MAPREDUCE-1017.3.patch;https://issues.apache.org/jira/secure/attachment/12422269/MAPREDUCE-1017.3.patch","19/Oct/09 20:43;kimballa;MAPREDUCE-1017.4.patch;https://issues.apache.org/jira/secure/attachment/12422608/MAPREDUCE-1017.4.patch","22/Sep/09 00:33;kimballa;MAPREDUCE-1017.patch;https://issues.apache.org/jira/secure/attachment/12420242/MAPREDUCE-1017.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2009-10-14 01:41:23.133,,,false,,,,,,,,,,,,,,,,,,37370,Reviewed,,,,Thu Oct 29 17:40:02 UTC 2009,,,,,,,"0|i02tan:",14347,,,,,,,,,,,,,,,,,,,,,"22/Sep/09 00:33;kimballa;This patch introduces two new features/arguments to Sqoop:

* Data can be compressed via {{\-\-compress}} / {{\-z}}. This will enable gzipping of text inputs
* Users can specify the approximate maximum file size used in direct mode with {{\-\-direct-split-size}}, which takes an argument in bytes, of the approximate file size to generate. After writing a record which surpasses this boundary, a new file is opened. Because Sqoop uses buffered writers, this file size is approximate, though Sqoop guarantees that new files will only be opened on record boundaries.

The compression argument applies to non-direct-mode imports as well. Sqoop will now use a compression codec for writing text files when using a MapReduce-based import. Sqoop used to call {{SequenceFileOutputFormat.setCompressionEnabled(true)}}by default; this will now only be the case if the user explicitly requests compression.","22/Sep/09 00:37;kimballa;In addition to the unit tests added in the {{org.apache.hadoop.sqoop.io}} package, I also performed a larger-scale test of this functionality. A 1.5 GB table was imported from MySQL to HDFS; the data was highly redundant, so compression shrank the files considerably, as well improved as the import time. The arguments {{\-z \-\-direct-split-size 25000000}} was given, so that it would generate approximately 25 MB files. This worked, and three files were generated. I verified using {{head}} and {{tail}} that the files did not lose any records and that records did not span multiple files.

I also verified that {{\-\-direct-split-size}} worked without compression, which it does. ","13/Oct/09 21:50;kimballa;Recycling; hudson seems to have dropped this from the queue.","14/Oct/09 01:41;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12420242/MAPREDUCE-1017.patch
  against trunk revision 824750.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 8 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/70/console

This message is automatically generated.","14/Oct/09 23:32;kimballa;New patch sync'd to trunk.","15/Oct/09 02:30;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422162/MAPREDUCE-1017.2.patch
  against trunk revision 825083.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 8 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/170/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/170/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/170/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/170/console

This message is automatically generated.","15/Oct/09 05:19;kimballa;Hudson-failing test passes locally; should be unrelated (Sqoop is a contrib, after all). ","15/Oct/09 11:06;tomwhite;Overall this looks like a good addition. A few comments:
* HdfsSplitOutputStream doesn't seem to be HDFS-specific, so should be renamed (to BlockSplitOutputStream?). In principle it could be used with another block-based filesystem, like S3FileSystem.
* HdfsSplitOutputStream uses CountingOutputStream to keep track of how many bytes have been written. Could you use FSDataOutputStream#getPos() for this? (Also, there's a CountingOutputStream in Apache Commons IO, which we already depend on.)
* It would be good to support more than just gzip compression in HdfsSplitOutputStream. The machinery in org.apache.hadoop.io.compress should make this relatively straightforward.
* It would be good to have a unit test for HdfsSplitOutputStream.
* Formatting nits: there are a few redundant imports, and some lines are greater than 80 characters.","15/Oct/09 18:58;kimballa;Thanks for the review. Some responses:

* You're correct about the name of HdfsSplitOutputStream. Will change.
* Thanks for the pointer about existing counting streams.
* I agree that this should eventually support multiple compression codecs, but the burden is on the application to select the correct codec based on the intended file extension. That would add even more code to this ticket; I'll move toward support for additional codecs (bz2, etc.) in a subsequent JIRA.
* Do you think a separate test is necessary for HdfsSplitOutputStream? This is tested through TestSplittableBufferedWriter. The SplittableBufferedWriter and SplitOutputStream classes are pretty tightly coupled -- SplittableBufferedWriter does virtually nothing but wrap the OutputStream in a BufferedWriter. {{testSplittingTextFile()}}, for example, only passes because of {{HdfsSplitOutputStream.openNextFile()}}.
* I'll clean up the formatting a bit in the next patch.

Will submit a new patch soon.
","15/Oct/09 20:08;kimballa;New patch implementing CR suggestions.","15/Oct/09 22:53;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422269/MAPREDUCE-1017.3.patch
  against trunk revision 825469.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 8 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/173/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/173/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/173/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/173/console

This message is automatically generated.","19/Oct/09 20:43;kimballa;New patch includes changes to documentation to reflect new arguments added by this feature.","19/Oct/09 23:29;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422608/MAPREDUCE-1017.4.patch
  against trunk revision 826767.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 8 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/188/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/188/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/188/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/188/console

This message is automatically generated.","22/Oct/09 14:58;tomwhite;I've just committed this. Thanks Aaron!","22/Oct/09 16:09;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #93 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/93/])
    . Compression and output splitting for Sqoop. Contributed by Aaron Kimball.
","29/Oct/09 17:40;hudson;Integrated in Hadoop-Mapreduce-trunk #127 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/127/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make ProcfsBasedProcessTree collect rss memory information,MAPREDUCE-1167,12439448,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,schen,schen,schen,29/Oct/09 19:43,02/May/13 02:29,12/Jan/21 09:52,17/Nov/09 20:25,0.22.0,,,,,0.21.0,,,tasktracker,,,,,,0,,,,,"Right now ProcfsBasedProcess collects only virtual memory. We can make it collect rss memory as well.
Later we can use rss in TaskMemoryManagerThread to obtain better memory management.",,acmurthy,dhruba,hong.tang,rksingh,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-1201,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-961,,,,,,,,,,,,,,,,,,,,,"03/Nov/09 00:15;schen;MAPREDUCE-1167-v2.patch;https://issues.apache.org/jira/secure/attachment/12423865/MAPREDUCE-1167-v2.patch","10/Nov/09 01:29;schen;MAPREDUCE-1167-v3.patch;https://issues.apache.org/jira/secure/attachment/12424424/MAPREDUCE-1167-v3.patch","12/Nov/09 00:36;schen;MAPREDUCE-1167-v4.patch;https://issues.apache.org/jira/secure/attachment/12424681/MAPREDUCE-1167-v4.patch","12/Nov/09 19:57;schen;MAPREDUCE-1167-v5.patch;https://issues.apache.org/jira/secure/attachment/12424758/MAPREDUCE-1167-v5.patch","29/Oct/09 19:46;schen;MAPREDUCE-1167.patch;https://issues.apache.org/jira/secure/attachment/12423608/MAPREDUCE-1167.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,2009-11-06 10:51:36.55,,,false,,,,,,,,,,,,,,,,,,149325,Reviewed,,,,Thu Dec 03 18:42:37 UTC 2009,,,,,,,"0|i0jfyn:",111516,,,,,,,,,,,,,,,,,,,,,"29/Oct/09 19:46;schen;This patch makes ProcfsBasedProcessTree collect rss. The corresponding unit test is included.","03/Nov/09 00:15;schen;I added some minor change which makes cumulativeRssmem() returns rss memory in bytes rather than page size.","06/Nov/09 10:51;vinodkv;Quickly looked at your patch. Few questions:
 - Can you find out and put some details as to how standard is {{getconf}} command? Given {{ProcfsBasedProcessTree}} is for Linux only, we are only concerned about Linux distributions. I can find it on my Ubuntu dev box, but RHEL?
 - I think we should keep track of rssmem everywhere in terms of bytes. This would inclue ProcessInfo.rssMem;
 - The behaviour when PAGESIZE is -ve should be changed. The patch gives out negative rss sizes. Instead we should throw exceptions in {{ProcessInfo.getRss()}}, {{ProcfsBasedProcessTree.getCumulativeRssMem()}} etc.","06/Nov/09 19:05;schen;Thanks, Vinod.
1. I will find out what is the standard way to obtain PAGESIZE.
2. I will use bytes everywhere for rss. I agree it is more clear that way.
3. Throwing exception in getCumulativeRssMem() is also a good suggestion. I will follow this one as well.","08/Nov/09 08:57;schen;I did some survey. getconf is defined in POSIX.
http://linux.die.net/man/1/getconf (in the bottom)
So it should be supported on different Linux versions.","10/Nov/09 01:28;schen;1. Rename rssmem to rssmemPage to help clarify.
2. Throws Exception when PAGESIZE is not available.

@Vinod: After reviewing the code, I think it is better to use page number in ProcessInfo because ProcessInfo is simply a parsed version of /proc/PID directory. It should be consistent with what's in /proc. I changed the field's name from rssmem to rssmemPage and also the getters' names. I think this should be able to help clarify. Also ProcessInfo is a private class. I think as long as our public methods all use bytes it should be fine.","12/Nov/09 00:36;schen;1. Change TestProcfsBasedProcessTree.ProcessStatInfo so that it will not affect TestTaskTrackerMemoryManager.
2. Remove the change in TestTaskTrackerMemoryManager
3. I make getCumulativeRssmem return 0 if PAGE_SIZE is not available

The reason for 3 is because getCumulativeVmem and getCumulativeRssmem should be consistent.
When /proc/ is not available, getCumulativeVmem will return 0 instead of throwing Exception.
It is good to make them follow the same behavior.
And these situations should not happen if the system is linux.","12/Nov/09 03:23;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12424681/MAPREDUCE-1167-v4.patch
  against trunk revision 834284.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/136/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/136/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/136/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/136/console

This message is automatically generated.","12/Nov/09 06:46;vinodkv;The patch looks OK. Only minor comments:
 - ProcessTreeDump is printing Rss size in pages but the header reads RSSMEM_USAGE(BYTES). This should be fixed.
 - In TestProcfsBasedProcessTree, Long.ParseLong(String) is used in many places. This is costly, you can use the long value directly if long type is needed, otherwise Long.valueOf(long) if Long type is. For example, see +408 after applying your patch.
 - TestProcfsBasedProcessTree failed. You need to modify the pattern at TestProcfsBasedProcessTree.java +188.

bq. The reason for 3 is because getCumulativeVmem and getCumulativeRssmem should be consistent. When /proc/ is not available, getCumulativeVmem will return 0 instead of throwing Exception.
I think this was wrongly done and should be changed. Will file a new issue. We can keep whatever you've done for now.","12/Nov/09 19:57;schen;Thanks, Vinod.

I have fixed the issues. I tested this on my mac so it did not go through testProcessTree() on my machine. I have tested it on a linux dev box this time. I have also globally replaced all parseLong() in TestProcfsBasedProcessTree. 

I agree with you on the return 0 behavior. It should be filed in another issue.","12/Nov/09 23:06;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12424758/MAPREDUCE-1167-v5.patch
  against trunk revision 835237.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/239/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/239/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/239/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/239/console

This message is automatically generated.","13/Nov/09 04:23;vinodkv;The contrib test failure is tracked at MAPREDUCE-1124.

+1 for the latest patch.

This is good to go. Can you ask someone to commit this?","17/Nov/09 18:00;schen;Thanks for all the help, Vinod.

I will ask Dhruba to see if he can commit this one.","17/Nov/09 18:04;dhruba;I will commit this in a short while.","17/Nov/09 20:18;dhruba;I just committed this. Thanks Scott!","17/Nov/09 20:25;dhruba;I just committed this. Thanks Scott!","17/Nov/09 20:47;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #121 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/121/])
    . ProcfsBasedProcessTree collects rss memory information.
(Scott Chen via dhruba)
","03/Dec/09 18:42;schen;@Vinod, Could you help me review MAPREDUCE-1201? It is quite similar to this one. 
I think you should be able to give a good review. Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task resource utilization reporting for profiling and scheduling,MAPREDUCE-1195,12440055,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,,schen,schen,06/Nov/09 19:44,02/May/13 02:29,12/Jan/21 09:52,06/Nov/09 22:17,,,,,,,,,,,,,,,0,,,,,"We can make TaskTracker reports its usage on CPU, memory, bandwidth to JobTracker. JobTracker can use the information for scheduling tasks and profiling jobs. 

One way to do this is to first make ProcfsProcessTree to collect the utilization information (CPU, mem...) and write the information in TaskTrackerStatus.taskReports and send them with the heartbeats. Then we can aggregate these information in JobInProgress to do job profiling and scheduling.",,acmurthy,aw,cos,dhruba,schen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-1167,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-11-06 21:17:24.826,,,false,,,,,,,,,,,,,,,,,,72467,,,,,Fri Nov 06 22:17:02 UTC 2009,,,,,,,"0|i0jg0v:",111526,,,,,,,,,,,,,,,,,,,,,"06/Nov/09 21:17;acmurthy;How is this different from MAPREDUCE-220 ?","06/Nov/09 22:15;schen;Hi Arun, Thanks for pointing this out. I will work on that one and move my comments there.","06/Nov/09 22:17;schen;It is this same as MAPREDUCE-220. Thanks Aruns for pointing this out.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Web service interface to the JobTracker,MAPREDUCE-445,12425362,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,,omalley,omalley,13/May/09 19:22,02/May/13 02:29,12/Jan/21 09:52,10/Jul/12 16:53,,,,,,,,,,,,,,,2,,,,,I think we need to provide a web services interface to submit and track jobs. This will simplify cross-version and non-Java access to JobTracker functionality.,,aaa,acmurthy,castagna,cutting,hammer,jsensarma,qwertymaniac,tomwhite,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,YARN-2332,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-05-13 21:05:22.949,,,false,,,,,,,,,,,,,,,,,,148801,,,,,Tue Jul 10 16:53:41 UTC 2012,,,,,,,"0|i08ayn:",46375,,,,,,,,,,,,,,,,,,,,,"13/May/09 21:05;steve_l;+1 to a RESTy long-haul API with status served as Atom feeds. 

","13/May/09 21:12;steve_l;Thought it was familiar","14/May/09 05:25;hammer;Hey Owen, this is a dupe of https://issues.apache.org/jira/browse/HADOOP-5633","19/Jun/09 19:25;jsensarma;i am a little surprised this is considered dup of 5633. what i would love to see is a web service api to submit jobs that is jobtracker agnostic. that way we can control what jobs go where (in the (not so uncommon) case of multiple map-red clusters attached to the same hdfs). (so almost EMR style)","22/Jun/09 09:53;steve_l;It's all the same need

For me
* Works long-haul, which either means slow networks or complex networks
* Server can be NAT-ed to the front, no require for 1:1 match of IP addresses on both sides
* Not brittle against Hadoop versions (i.e. stable and version checking (in the URL?)
* uses HTTPs with various authentication opens (certificates as well as passwords)
* has a java client
* has a client in something like python just to test x-platform nature
* has an ant task that uses that java client so you can easily submit work
* Usable by humans too (gui, list jobs, form submit, &c)

I would like to get involved in this, once I've got other patches checked in. It is a need of mine.","10/Jul/12 16:53;qwertymaniac;Resolving as dupe of MAPREDUCE-454 as both seem to target the same thing. Please reopen if I missed something.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enforce RSS memory limit in TaskMemoryManagerThread,MAPREDUCE-1181,12439777,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Invalid,,schen,schen,03/Nov/09 18:56,02/May/13 02:29,12/Jan/21 09:52,08/Nov/09 09:03,0.20.1,,,,,0.20.1,,,tasktracker,,,,,,0,,,,,"TaskMemoryManagerThread will periodically check the rss memory usage of every task. If the memory usage exceeds the specified threshold, the task will be killed. Also if the total rss memory of all tasks exceeds (total amount of memory - specified reserved memory). The task with least progress will be killed to recover the reserved rss memory.

This is similar to the virtual memory limit provided by TaskMemoryManagerThread. But now the limit is for rss memory. This new feature allow us to avoid page swapping which is prone to error.

The following are the related configurations
mapreduce.reduce.memory.rss.mb   // RSS memory allowed for a reduce task
mapreduce.map.memory.rss.mb       // RSS memory allowed for a map task
mapreduce.tasktracker.reserved.memory.rss.mb     // RSS memory reserved (not for tasks) on a tasktracker",,acmurthy,hong.tang,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-1167,,,,"03/Nov/09 18:59;schen;MAPREDUCE-1181.patch;https://issues.apache.org/jira/secure/attachment/12423938/MAPREDUCE-1181.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2009-11-06 11:26:32.912,,,false,,,,,,,,,,,,,,,,,,72525,,,,,Fri Nov 06 18:57:43 UTC 2009,,,,,,,"0|i0jfzr:",111521,,,,,,,,,,,,,,,,,,,,,"03/Nov/09 19:01;schen;Uses the RSS memory gauged by ProcfsBasedProcessTree provided by MAPREDUCE-1167","06/Nov/09 11:26;vinodkv;bq. This new feature allow us to avoid page swapping which is prone to error.
Can you elaborate on this? RSS unlike vmem is a very dynamic entity for a process, and depends not just on this process but others too. So I am not sure if trying to shoot down tasks based on their memory usage will work well.

bq. This new feature allow us to avoid page swapping which is prone to error.
Explain this too?

The original intention why the feature of killing tasks via the TaskMemoryManager was added was to prevent nodes from going down. If tasks use too much virtual memory (rss *AND* swap), OS will not have any way of recovering itself. And we have seen instances of this where nodes go down completely because of this.

On the other hand, I am not too sure too much rss usage results in similar effects. Did you see such drastic instances? If not and if you are concerned about thrashing only, then a better way of controlling this may be to not even schedule tasks if total rss usage is to the brim. Thoughts?
","06/Nov/09 18:57;schen;Hi Vinod,

Thanks for the comment.

After investigating this and doing some experiments for the past few days. I agree with you. It is more reliable to monitor tasks using virtual memory than using physical memory because virtual memory is not as dynamic as RSS and we can stop the task before the physical memory goes high.

And I also agree with your second point. It is better to use the total RSS usage in scheduling rather than here.

Now I think this feature is not necessary. But it is still good to keep the feature that allows the ProcfsBasedProcessTree collects RSS. It can be used for job profiling later. I will continue working on that one. My plan is to make ProcfsBaedProcessTree collect RSS usage and number of CPU jiffies for all tasks and submit through heartbeat by TaskTrackerStatus.taskReports.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add SASL DIGEST-MD5 authentication to TaskUmbilicalProtocol,MAPREDUCE-1335,12444201,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,kzhang,kzhang,kzhang,26/Dec/09 02:14,02/May/13 02:29,12/Jan/21 09:52,03/Feb/10 03:07,,,,,,0.21.0,,,,,,,,,0,,,,,Use job token as the credential for Task to local TaskTracker authentication over RPC.  ,,cutting,ddas,ghelmling,hammer,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-6419,,,,"26/Dec/09 02:17;kzhang;m6419-11.patch;https://issues.apache.org/jira/secure/attachment/12428967/m6419-11.patch","30/Jan/10 01:50;kzhang;m6419-15.patch;https://issues.apache.org/jira/secure/attachment/12431849/m6419-15.patch","31/Jan/10 12:54;kzhang;m6419-19.patch;https://issues.apache.org/jira/secure/attachment/12431904/m6419-19.patch","01/Feb/10 23:49;kzhang;m6419-20.patch;https://issues.apache.org/jira/secure/attachment/12434456/m6419-20.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2010-02-03 02:30:56.542,,,false,,,,,,,,,,,,,,,,,,149446,Reviewed,,,,Thu Feb 04 17:20:18 UTC 2010,,,,,,,"0|i0jgdz:",111585,,,,,,,,,,,,,,,,,,,,,"26/Dec/09 02:17;kzhang;Attaching a preliminary patch for review.","30/Jan/10 01:50;kzhang;A new patch that matches the new patch in Common. It also includes a new test on the TaskUmbilicalProtocol itself.","31/Jan/10 12:54;kzhang;uploading a new patch that turns on Kerberos authentication for JobTracker RPC protocols (based on HADOOP-6419).

Ran unit tests on local MAC OS X box and found TestMiniMRChildTask failed. However, ran the same test multiple times on a Linux (ucdev) box and was successful.","31/Jan/10 12:58;kzhang;Wasn't able to run test-patch.","01/Feb/10 23:49;kzhang;attaching a new patch that matches latest Common changes.","03/Feb/10 02:30;tlipcon;Can we either get this committed or HADOOP-6419 reverted? MapReduce trunk build is currently broken.","03/Feb/10 02:41;ddas;I am going to commit this one soon. ","03/Feb/10 03:07;ddas;+1 (also, ran tests/findbugs/javadoc manually). 
I just committed this. Thanks, Kan!","03/Feb/10 03:48;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #226 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/226/])
    . Adds SASL Kerberos/Digest authentication in MapReduce. Contributed by Kan Zhang.
","04/Feb/10 17:20;hudson;Integrated in Hadoop-Mapreduce-trunk #226 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/226/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Concrete implementation of MultiFileInputFormat ,MAPREDUCE-214,12409454,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,enis,enis,enis,28/Nov/08 16:23,02/May/13 02:29,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,There has been a demand for a concrete implementation for MultiFileInputFormat. We should include one as a library. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-4565,,,,"28/Nov/08 16:25;enis;recordPerFileIF_v1.patch;https://issues.apache.org/jira/secure/attachment/12394909/recordPerFileIF_v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,148602,,,,,Fri Nov 28 16:27:12 UTC 2008,,,,,,,"0|i0ivj3:",108202,,,,,,,,,,,,,,,,,,,,,"28/Nov/08 16:25;enis;This patch introduces RecordPerFileInputFormat, which extends MultiFileInputFormat and returns a recordReader for <Text, FSDataInputStreamWrapper>. The key's are file names, values are wrappers to FSDataInputStreams of files. 

","28/Nov/08 16:27;enis;This issue depends on HADOOP-4565. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In Streaming, allow different mappers for different subsets of the input",MAPREDUCE-605,12383243,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,arkady,arkady,26/Nov/07 17:29,02/May/13 02:29,12/Jan/21 09:52,,,,,,,,,,contrib/streaming,,,,,,0,,,,,"The command line may look like this:

-mapper mapper-command-1 -input dir11   -mapper mapper-command-2 -input dir22  input -dir21

meaning that map phase will apply mapper-command-1 to part files from dir11, and the part files from dir22 and dir21 will be processed by mapper-command-2
then all will be shuffled and processed by a single reducer.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-372,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,148933,,,,,2007-11-26 17:29:00.0,,,,,,,"0|i0isnb:",107735,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Web UI to MR2 Fair Scheduler,MAPREDUCE-3603,12536406,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,,pwendell@gmail.com,pwendell@gmail.com,27/Dec/11 01:03,16/Apr/13 23:30,12/Jan/21 09:52,16/Apr/13 23:30,,,,,,,,,scheduler,,,,,,0,,,,,,,atm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,YARN-145,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,222089,,,,,Thu Jan 05 21:18:00 UTC 2012,,,,,,,"0|i0e6c7:",80789,,,,,,,,,,,,,,,,,,,,,"05/Jan/12 21:18;pwendell@gmail.com;This will probably be a good amount of work, so I'm making a standalone ticket (was under MAPREDUCE-3600). It will require writing Hamlet code to plugin to the existing Scheduler view of the RM GUI.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement a FlumeJava-like library for operations over parallel collections using Hadoop MapReduce,MAPREDUCE-1849,12466603,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Implemented,,hammer,hammer,09/Jun/10 21:40,22/Mar/13 19:36,12/Jan/21 09:52,22/Mar/13 19:36,,,,,,,,,,,,,,,5,,,,,The API used internally at Google is described in great detail at http://portal.acm.org/citation.cfm?id=1806596.1806638.,,acmurthy,anhi,ashutoshc,atm,aw,brocknoland,cdouglas,cutting,cwensel,dcapwell,ddas,devaraj,drew.farris,eli,gates,gguardin,hammer,hong.tang,iholsman,ijuma,jake.mannix,jghoman,johanoskarsson,julienledem,lianhuiwang,matei,ogrisel,philip,qwertymaniac,rahul.sharma,romainr,schubertzhang,sharadag,svenkat,thkoch,tomwhite,vicaya,xuwenhao,yhemanth,yozh,zhangguancheng,zjffdu,zshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-06-09 23:57:07.782,,,false,,,,,,,,,,,,,,,,,,56874,,,,,Fri Mar 22 19:36:29 UTC 2013,,,,,,,"0|i0amkf:",59929,,,,,,,,,,,,,,,,,,,,,"09/Jun/10 21:48;hammer;h2. Data Model
* ""The central class of the FlumeJava library is *PCollection<T>*, a (possibly huge) immutable bag of elements of type T.""
** Can be unordered (collection) or ordered (sequence)
** Could be created with an underlying Java Collection<T> for local execution
** recordsOf() can be used to indicate how to read the elements of the collection (cf. Pig's LoadFunc or Hive's SerDe)
* Second central class: *PTable<K, V>*
** Immutable multi-map with keys of class K and values of class V
** Subclass of PCollection<Pair<K, V>> 

h2. Operators
* parallelDo(PCollection<T>): PCollection<S>; runs S doFunc(T) over each element
* groupByKey(PTable<Pair<K,V>>): PTable<Pair<K, Collection<V>>>: turns a multi-map into a uni-map
* combineValues(PTable<Pair<K, Collection<V>>): PTable<Pair<K, V>>: does the reduction
* flatten(): logical view of multiple PCollections as one PCollection
* writeToRecordFileTable() to flush the output of a pipeline to a table","09/Jun/10 23:57;jake.mannix;+1 from this casual observer over from Mahout-land (nobody ever seems to believe me that this would make Hadoop programmers soooooo much more efficient).

I've written a half-baked, bug-ridden, inefficient version of this several times in the past, and it would be *so* useful to have done right.

An api which essentially wrapped a SequenceFile<K,V> and allowed you to do things like

  Path dataPath = new Path(""hdfs://foo/bar"");
  PTable<K,V> data = new PTable<K,V>(dataPath);
  LightWeightMap<K,V,KOUT,VOUT> map = new MyMapper();
  PTable<KOUT,VOUT> transformedData = data.parallelDo(map);

etc. would be awesome.

Of course, the real trick is writing a good optimizer which can figure out how to squish together separate M/R steps into one (for example, parallelDo() returns a PCollection, which you might then do groupByKey() on, but these could often easily be combined into the Map and Reduce steps of a single job).","10/Jun/10 00:32;tdunning;Another sweet trick for this would be to allow multiple modes of execution that are all efficiently implemented.  These should include local threaded, non-redundant distributed map-reduce (a la Twister) and full-on Hadoop.  That gives highest speed for small jobs, medium speed for medium jobs at the cost of task failure = job resubmit and full scalability and reliability for the largest jobs.

Right now, anything but full scale hadoop execution is the red-headed child and gets no love.

","10/Jun/10 14:52;omalley;I haven't read the paper yet, but can you summarize how this differs from Pig? Those operators all map into Pig's operators one to one. Pig also supports join, which is *really* nice to have automated support for.","10/Jun/10 15:28;hammer;Owen: sure. They provide ""derived operators"" as well, like count(), join(), and top(). The main difference from Pig seems to be allowing users to work in Java. In fact, the Google team initially implemented their approach in a new language called Lumberjack, but mentions that, among other things, the implementation of a new language was a lot of work, and most importantly, novelty is an obstacle to adoption. They settled on Java and seem to have had some internal success.","10/Jun/10 16:05;hammer;Some things you get for free from being a Java library: control flow (branching, looping, etc.), composability (functions, classes, packages), IDE support, etc. Having PigLatin execute on top of something like FlumeJava could be interesting","10/Jun/10 16:37;jake.mannix;[quote]
The main difference from Pig seems to be allowing users to work in Java.
[quote]

To add my $0.02: FlumeJava lets the developers work in an object-oriented language, *period*.  The difference between writing a Pig ""script"", or a SQL (or Hive variant therof) ""query"" and being able to seamlessly integrate distributed primitives (primitive not meaning java primitive, but ""basic building block"") in a standard java program is *amazing*

The real comparison is between FlumeJava and *Cascading*, which also lets you stay in java-land, and has a query-plan optimizer.  I'm no expert in Cascading, but it seems the primitives in Cascading are ""verbs"" related to flows, while FlumeJava really settles on a DistributedDataSet (PCollection, for them) as the object which has methods, and can be passed to methods of other (either distributed or normal) objects.  I don't know if that is clearly better, but it certainly seems more in line with the way most people program in java.","10/Jun/10 17:53;vicaya;I had some experience with Cascading in production code. One of the major benefits of being a java library from my POV is easy unit testing of various user defined operations, which is inconvenient in most DSLs. OTOH, Cascading forces you to define data-flows explicitly (which is not so bad, if you have nice FlowBuilder utility class).

FlumeJava, IMO, actually captures the essence of MapReduce originated from functional programming. The immutable P* collections and side-effect free (no global effect) DoFn's allows many optimization opportunities a la Haskell's lazy evaluation (deferred evaluation in the paper.) However the lack of type inference and closure in Java makes the usage much more verbose than necessary. I think similar libraries could be better implemented in Scala.","10/Jun/10 22:32;ogrisel;I am not sure closure such as scala's would work on a distributed, multi JVM setup such as Hadoop. Otherwise I agree with Luke's POV.","10/Jun/10 22:51;jghoman;bq. I am not sure closure such as scala's would work on a distributed, multi JVM setup such as Hadoop. Otherwise I agree with Luke's POV.
Matei and the Spark guys got it working quite well: http://www.cs.berkeley.edu/~matei/spark/  ","10/Jun/10 23:50;jake.mannix;While I agree that doing cool Hadoop via functional JVM languages like Scala and Clojure are great ideas, I think part of the point of the findings of this paper (and the point of this particular JIRA ticket) is concerning a simple, object-oriented Java API which has ""distributed primitives"" that the typical java programmer can easily understand and integrate with their current code with minimal effort.
","11/Jun/10 17:47;cutting;I think this is better done as a separate project.  Our goal in the MapReduce project should be to provide a low-level execution engine for higher-level APIs like this.  Wherever possible we should strive to reduce the amount of user-level code in the base mapreduce system.  This permits user-code to be versioned independently from the critical fault-tolerant base system.  The base system should focus on reliability and performance, not on high-level features.","11/Jun/10 18:10;vicaya;Agree that this should really be a separate project, as it's at a much higher level than mapreduce per se and it should be able to utilize features from a non-mapreduce framework, say, an alternative BSP implementation.","30/Jul/10 21:08;tdunning;See http://tdunning.blogspot.com/2010/07/new-grool.html

I have started a github with an eager, local execution approximate clone of FlumeJava.  My thought is to work out the API design before moving to parallel execution.

My current implementation has a word count example and has flushed out at least one issue in the API design (which google seems to have gotten right, btw).  It could use an Avro expert to look at it to guide how to integrate Avro into the type structure. ","04/Aug/10 16:43;tdunning;The github implementation of Plume now supports local evaluation in an eager as opposed to lazy fashion.  Avro file reading is working in at least one example.  The execution plan optimizer is beginning to work.  We have an emulated map-reduce framework working to support mocking up the full map-reduce execution.  Nobody is working on the Hadoop interface yet so if there is a volunteer for that, they would be very welcome.

Anybody who would like to contribute is welcome as long as they are willing to Apache license their contributions.

See http://github.com/tdunning/Plume for the source,  http://tdunning.blogspot.com/2010/07/new-grool.html for some discussion.","04/Aug/10 19:52;vicaya;IMO, this kind of ""fluent""/LINQish interface might be better built on top of pig (preferably pig libraries, a la LINQ for SQL etc.) as you don't want/need to reinvent much of the scheduling/optimization in these existing work.","11/Nov/10 18:38;tdunning;Pig is not a suitable framework for this because it imposes very high overhead due to a very wide and complex API and no abstract syntax layer.

Contrarily, Plume is moving along very nicely.  We have a preliminary optimizer that actually does some important optimizations that Pig doesn't do.

Check it out: 
{quote}
See http://github.com/tdunning/Plume for the source, http://tdunning.blogspot.com/2010/07/new-grool.html for some discussion.
{quote}","10/Oct/11 18:06;tlipcon;Josh Wills has implemented this as a project on github: https://github.com/cloudera/crunch/","10/Oct/11 18:06;hammer;Josh Wills from Cloudera has implemented the FlumeJava API in an open source project called Crunch: http://www.cloudera.com/blog/2011/10/introducing-crunch/. The code is currently on Github: https://github.com/cloudera/crunch. Once we've built a community of contributors, we plan to take the project to the Apache Incubator. If you're interested in programmatic workflow construction from Java, we'd welcome your contributions!","19/Oct/12 08:47;rahul.sharma;This is now available in Apache incubator at http://incubator.apache.org/crunch/. Pls pour in your thoughts/suggestions.","22/Mar/13 19:36;qwertymaniac;Crunch is now a top level Apache project (Apache Crunch): http://crunch.apache.org.

Resolving as Implemented (long time pending, issue gone stale here).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add experimental support for MR AM to schedule CPUs along-with memory,MAPREDUCE-4520,12597871,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,acmurthy,acmurthy,acmurthy,07/Jul/12 00:43,15/Feb/13 13:10,12/Jan/21 09:52,09/Jan/13 05:32,,,,,,2.0.3-alpha,,,,,,,,,0,,,,,,,acmurthy,adferguson,hudson,jlowe,kasha,kkambatl,nroberts,revans2,sseth,tgraves,tlipcon,tomwhite,tucu00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-4936,,,,,,,,"29/Dec/12 09:17;acmurthy;MAPREDUCE-4520.patch;https://issues.apache.org/jira/secure/attachment/12562659/MAPREDUCE-4520.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2013-01-09 05:37:12.014,,,false,,,,,,,,,,,,,,,,,,253824,,,,,Mon Jan 14 15:04:28 UTC 2013,,,,,,,"0|i0e5hb:",80650,,,,,,,,,,,,,2.0.3-alpha,,,,,,,,"29/Dec/12 09:17;acmurthy;Straight-fwd, very simple patch introducing yarn.app.mapreduce.am.resource.cpu-vcores for AM and mapreduce.(map,reduce).cpu.vcores for tasks.","09/Jan/13 05:32;acmurthy;I just committed this trivial patch post YARN-2.","09/Jan/13 05:37;hudson;Integrated in Hadoop-trunk-Commit #3200 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3200/])
    MAPREDUCE-4520. Added support for MapReduce applications to request for CPU cores along-with memory post YARN-2. Contributed by Arun C. Murthy. (Revision 1430688)

     Result = SUCCESS
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1430688
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/JobImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java
","09/Jan/13 10:47;hudson;Integrated in Hadoop-Yarn-trunk #91 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/91/])
    MAPREDUCE-4520. Added support for MapReduce applications to request for CPU cores along-with memory post YARN-2. Contributed by Arun C. Murthy. (Revision 1430688)

     Result = SUCCESS
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1430688
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/JobImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java
","09/Jan/13 13:12;hudson;Integrated in Hadoop-Hdfs-trunk #1280 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1280/])
    MAPREDUCE-4520. Added support for MapReduce applications to request for CPU cores along-with memory post YARN-2. Contributed by Arun C. Murthy. (Revision 1430688)

     Result = FAILURE
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1430688
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/JobImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java
","09/Jan/13 13:20;hudson;Integrated in Hadoop-Mapreduce-trunk #1308 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1308/])
    MAPREDUCE-4520. Added support for MapReduce applications to request for CPU cores along-with memory post YARN-2. Contributed by Arun C. Murthy. (Revision 1430688)

     Result = FAILURE
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1430688
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/JobImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java
","10/Jan/13 15:28;tomwhite;Arun, looks like you forgot to get a review and Jenkins run for this?","14/Jan/13 03:50;acmurthy;Tom, sorry, I've been traveling from India and hence the late response.

I forgot to add I ran 'test-patch' locally on 12/29 since the patch was originally blocked on YARN-2. My bad.

The patch is trivial and just adds a experimental config variables and was around for a while before I pushed it to make hadoop-2.0.3-alpha. If you see any issues please let me know and I'll fix them. Thanks.","14/Jan/13 15:04;tomwhite;Arun, even ""trivial"" code changes require a review before being committed. 

The change breaks existing unit tests. Please see MAPREDUCE-4936.

Also, since this is adding a new feature it should have new tests (e.g. something that exercises the new mapreduce.map.cpu.vcores and mapreduce.reduce.cpu.vcores properties).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The Map tasks logs should have the value of input split it processed,MAPREDUCE-3678,12538536,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,qwertymaniac,bejoyks,bejoyks,16/Jan/12 17:35,15/Feb/13 13:10,12/Jan/21 09:52,09/Oct/12 13:45,1.0.0,2.0.0-alpha,,,,1.2.0,2.0.3-alpha,,mrv1,mrv2,,,,,0,,,,,"It would be easier to debug some corner in tasks if we knew what was the input split processed by that task. Map reduce task tracker log should accommodate the same. Also in the jobdetails web UI, the split also should be displayed along with the Split Locations. 

Sample as
Input Split
hdfs://myserver:9000/userdata/sampleapp/inputdir/file1.csv - <split no>/<offset from beginning of file>

This would be much beneficial to nail down some data quality issues in large data volume processing.
",,acmurthy,cdouglas,hudson,jdonofrio,qwertymaniac,tlipcon,tomwhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-2076,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/12 18:21;qwertymaniac;MAPREDUCE-3678-branch-1.patch;https://issues.apache.org/jira/secure/attachment/12537724/MAPREDUCE-3678-branch-1.patch","24/Jul/12 18:25;qwertymaniac;MAPREDUCE-3678.patch;https://issues.apache.org/jira/secure/attachment/12537726/MAPREDUCE-3678.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2012-01-16 17:47:32.812,,,false,,,,,,,,,,,,,,,,,,224040,Reviewed,,,,Wed Oct 10 14:00:50 UTC 2012,,,,,,,"0|i00sl3:",2563,A map-task's syslogs now carries basic info on the InputSplit it processed.,,,,,,,,,,,,,,,,,,,,"16/Jan/12 17:47;qwertymaniac;Task's own logs are the best place for this, not the daemons.

The reason it is tedious to do/maintain at the framework level is that not all InputSplits may be FileSplits, and formats that do use FileSplits may use them in different ways as well (CombineFileIF, for instance).

The InputSplit interface by itself is path-agnostic.","17/Jan/12 04:03;acmurthy;AFAIK MR1 already shows this in taskdetails.jsp - we need to add this to MR2.

Also, AFAIK, I thought MR1 task-logs had this info logged, something I see missing in MR2 also.","17/Jan/12 06:33;bejoyks;Ya it is available in taskdetails.jsp . But when we have a large number of jobs running on our cluster in a matter of half an hour the jobs would be in history and in in jobtaskshistory.jsp there are only the following values
-Task Id	
-Start Time	
-Finish Time
-Error

Can we have one more filed here similar to status in  taskdetails.jsp that would show the input split it processed as well.

Once the job is in history viewer currently do we have any option to find this information?
","24/Jul/12 18:03;qwertymaniac;Hi Arun,

bq. AFAIK MR1 already shows this in taskdetails.jsp - we need to add this to MR2.

But this state is wiped away if the task sets a status. So I don't find it reliable :(

bq. Also, AFAIK, I thought MR1 task-logs had this info logged, something I see missing in MR2 also.

We do not log this at all. I'll post patches that target both.","24/Jul/12 18:04;qwertymaniac;bq. Once the job is in history viewer currently do we have any option to find this information?

Unsure about this one, we can probably handle via another JIRA if its important to know via JH too (minus userlogs, i.e.). I'll file a new one after completing up the patches.","24/Jul/12 18:21;qwertymaniac;Patch for branch-1.","24/Jul/12 18:25;qwertymaniac;Patch for trunk attached.","24/Jul/12 18:44;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537726/MAPREDUCE-3678.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2653//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2653//console

This message is automatically generated.","07/Oct/12 12:55;qwertymaniac;Hi,

If no one has any objections to these INFO log additions, I'll commit it in in a couple of days.

This helps projects such as Pig, Hive, etc. without any changes on their end.","09/Oct/12 11:23;tomwhite;+1","09/Oct/12 13:44;hudson;Integrated in Hadoop-Hdfs-trunk-Commit #2897 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2897/])
    MAPREDUCE-3678. The Map tasks logs should have the value of input split it processed. Contributed by Harsh J. (harsh) (Revision 1396032)

     Result = SUCCESS
harsh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1396032
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java
","09/Oct/12 13:45;qwertymaniac;Thanks Tom. I committed this to trunk, branch-2 and branch-1.","09/Oct/12 13:45;hudson;Integrated in Hadoop-Common-trunk-Commit #2835 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2835/])
    MAPREDUCE-3678. The Map tasks logs should have the value of input split it processed. Contributed by Harsh J. (harsh) (Revision 1396032)

     Result = SUCCESS
harsh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1396032
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java
","09/Oct/12 14:32;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #2858 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2858/])
    MAPREDUCE-3678. The Map tasks logs should have the value of input split it processed. Contributed by Harsh J. (harsh) (Revision 1396032)

     Result = FAILURE
harsh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1396032
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java
","10/Oct/12 12:59;hudson;Integrated in Hadoop-Hdfs-trunk #1191 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1191/])
    MAPREDUCE-3678. The Map tasks logs should have the value of input split it processed. Contributed by Harsh J. (harsh) (Revision 1396032)

     Result = SUCCESS
harsh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1396032
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java
","10/Oct/12 14:00;hudson;Integrated in Hadoop-Mapreduce-trunk #1222 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1222/])
    MAPREDUCE-3678. The Map tasks logs should have the value of input split it processed. Contributed by Harsh J. (harsh) (Revision 1396032)

     Result = SUCCESS
harsh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1396032
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TextPartioner for hashing Text with good hashing function to get better distribution,MAPREDUCE-4839,12618452,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Not A Problem,hsn,hsn,hsn,01/Dec/12 20:33,01/Jan/13 10:38,12/Jan/21 09:52,01/Jan/13 10:38,,,,,,,,,,,,,,,0,,,,,partitioner for Text keys using util.Hash framework for hashing function,,cutting,hsn,qwertymaniac,revans2,tgraves,vicaya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/12 20:35;hsn;textpartitioner1.txt;https://issues.apache.org/jira/secure/attachment/12555646/textpartitioner1.txt","06/Dec/12 00:22;hsn;textpartitioner2.txt;https://issues.apache.org/jira/secure/attachment/12556180/textpartitioner2.txt","06/Dec/12 12:12;hsn;textpartitioner3.txt;https://issues.apache.org/jira/secure/attachment/12556265/textpartitioner3.txt","18/Dec/12 19:23;hsn;textpartitioner4.txt;https://issues.apache.org/jira/secure/attachment/12561545/textpartitioner4.txt","19/Dec/12 22:02;hsn;textpartitioner6.txt;https://issues.apache.org/jira/secure/attachment/12561791/textpartitioner6.txt","19/Dec/12 23:10;hsn;textpartitioner7.txt;https://issues.apache.org/jira/secure/attachment/12561808/textpartitioner7.txt","26/Dec/12 21:26;hsn;textpartitioner8.txt;https://issues.apache.org/jira/secure/attachment/12562408/textpartitioner8.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,2012-12-05 23:27:05.734,,,false,,,,,,,,,,,,,,,,,,293250,,,,,Tue Jan 01 10:38:11 UTC 2013,,,,,,,"0|i0sw53:",166684,,,,,,,,,,,,,,,,,,,,,"05/Dec/12 23:27;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12555646/textpartitioner1.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3096//console

This message is automatically generated.","06/Dec/12 02:57;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12556180/textpartitioner2.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 2014 javac compiler warnings (more than the trunk's current 2013 warnings).

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3099//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3099//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3099//console

This message is automatically generated.","06/Dec/12 12:34;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12556265/textpartitioner3.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3101//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3101//console

This message is automatically generated.","18/Dec/12 11:18;vicaya;Don't you think that the test should verify that configured hash fn is used as expected (at least the default and an arbitrary configuration)?","18/Dec/12 13:27;hsn;it can not be tested that way without powermock which was rejected.","18/Dec/12 15:24;revans2;I have not really looked at this in too much detail, but in the partitioner you are converting key to a String to convert it to UTF-8 bytes.  Text was designed to store the data in UTF-8 internally instead of UCS-2 like String does.  I think you could just call key.getBytes() directly.  I think the only difference is that malformed and unmappable bytes will be replaced in the returned String, instead of leaving the bytes as is.","18/Dec/12 19:43;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12561545/textpartitioner4.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3134//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3134//console

This message is automatically generated.","19/Dec/12 19:47;vicaya;I agree with Bobby that we should just use key.getBytes() directly.","19/Dec/12 19:56;hsn;patch v4 is using that","19/Dec/12 20:32;vicaya;v4 still shows:
{code}
+    int hashval = hashfn.hash(key.toString().getBytes());
{code}
to me.","19/Dec/12 20:34;vicaya;ie., toString is not necessary.","19/Dec/12 22:24;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12561791/textpartitioner6.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core:

                  org.apache.hadoop.mapreduce.lib.partition.TestTextPartitioner

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3142//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3142//console

This message is automatically generated.","19/Dec/12 23:18;vicaya;It seems to me that you really don't need mock for testing hash configurations in this case. You can use a real configuration object directly.","19/Dec/12 23:23;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12561808/textpartitioner7.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3144//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3144//console

This message is automatically generated.","19/Dec/12 23:33;hsn;i didnt get it. How do you know that p.getPartition() called Hash function if you can not use powermock and hook into it?","19/Dec/12 23:42;vicaya;I mean you can assert the real value of partition is this case. If numReduceTasks is large enough, the likelihood the value is the same for the tested hashes are small, which IMO is good enough of a test.","26/Dec/12 12:20;hsn;check if different hashes returns different partitions","26/Dec/12 12:54;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12562377/partitioner8.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3172//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3172//console

This message is automatically generated.","26/Dec/12 21:27;hsn;attached wrong patch, retry.","26/Dec/12 21:54;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12562408/textpartitioner8.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3175//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3175//console

This message is automatically generated.","01/Jan/13 10:38;hsn;do not need this in hadoop core",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create reduce input merger plugin in ReduceTask.java and pass it to Shuffle,MAPREDUCE-4812,12616999,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,masokan,masokan,masokan,20/Nov/12 19:51,17/Dec/12 23:15,12/Jan/21 09:52,15/Dec/12 20:50,2.0.2-alpha,,,,,,,,,,,,,,1,,,,,"This is part of MAPREDUCE-2454.  This further breaks down MAPREDUCE-4808
",,acmurthy,avnerb,cdouglas,cutting,gortsleigh,lakshman,masokan,sandyr,tenduy,tgraves,tucu00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-4049,,,,,,,,,,,,,,,,,,,,,"29/Nov/12 21:14;masokan;COMBO-mapreduce-4809-4812.patch;https://issues.apache.org/jira/secure/attachment/12555404/COMBO-mapreduce-4809-4812.patch","24/Nov/12 23:27;masokan;COMBO-mapreduce-4809-4812.patch;https://issues.apache.org/jira/secure/attachment/12554779/COMBO-mapreduce-4809-4812.patch","29/Nov/12 21:14;masokan;mapreduce-4812.patch;https://issues.apache.org/jira/secure/attachment/12555403/mapreduce-4812.patch","24/Nov/12 23:27;masokan;mapreduce-4812.patch;https://issues.apache.org/jira/secure/attachment/12554778/mapreduce-4812.patch","23/Nov/12 04:08;masokan;mapreduce-4812.patch;https://issues.apache.org/jira/secure/attachment/12554716/mapreduce-4812.patch","21/Nov/12 22:28;masokan;mapreduce-4812.patch;https://issues.apache.org/jira/secure/attachment/12554592/mapreduce-4812.patch","20/Nov/12 23:53;masokan;mapreduce-4812.patch;https://issues.apache.org/jira/secure/attachment/12554427/mapreduce-4812.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,2012-11-20 19:59:45.196,,,false,,,,,,,,,,,,,,,,,,258913,,,,,Mon Dec 17 23:15:53 UTC 2012,,,,,,,"0|i0l5p3:",121573,,,,,,,,,,,,,,,,,,,,,"20/Nov/12 19:59;tucu00;Thanks Asokan, this makes sense, it will reduce further the size of MAPREDUCE-4808.

I'm also marking this as a related to MAPREDUCE-4049 as this change will affect the ShuffleContext object. I think we should commit this one and then MAPREDUCE-4049 on top of it.
 ","20/Nov/12 23:53;masokan;Hi Arun & Alejandro,
  I am uploading a patch that implements the bare-bones requirements for the reduce input merger plugin.  I will create a simple mock test for the plugin and post it with some protection changes so that the test will run.

Arun, in this patch an instance of {{ReduceInputMerger}} plugin is passed to {{Shuffle}} constructor as you asked for.  I figured out a way to implement sort avoidance plugin.  However, I need MAPREDUCE-4049(shuffle plugin) to be committed before that.

Thanks.
-- Asokan","21/Nov/12 02:51;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12554427/mapreduce-4812.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3044//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3044//console

This message is automatically generated.","21/Nov/12 18:10;tucu00;Asokan, it looks good. Similar to what has been suggested/done in MAPREDUCE-4049 the MergerManager.initialize() method should receive a context with all parameter. This Context could be a static inner class in the  ReduceInputMerger interface. Also the initialize() method should be renamed to init() to be consistent with the shuffle plugin.","21/Nov/12 19:17;masokan;Hi Alejandro,
  Do you want me to do similar thing for MapOutputCollector interface(change initialize() to init() and create a Context though there are only three parameters)?
Thanks.

-- Asokan
","21/Nov/12 19:32;tucu00;Asokan, that is a good point, it didn't seem obvious there before because the params passed are just 3. Please do. Thx","21/Nov/12 22:28;masokan;Hi Alejandro,
  I have created static context class and changed the method name from {{initialize()}} to {{init()}} as you suggested. It will receive only a context object.

Please review and give your comments.

Thanks.
-- Asokan","21/Nov/12 23:13;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12554592/mapreduce-4812.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3052//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3052//console

This message is automatically generated.","22/Nov/12 20:35;tucu00;Analogous comments like in MAPREDUCE-4807. 

It looks good, you indicated you'd be creating a patch with a test. I'll wait for it for +1ing it.

Minor naming nits:

* The property *mapreduce.job.reduce.merge.classs*, I think a better name would be *mapreduce.job.reduce.input.merger.classs*

* The ReduceMergerContext inner class, it could simply be called Context (as the outer class  fully defines it)","23/Nov/12 04:08;masokan;Hi Alejandro,
  Thanks for your comments.  I am uploading the patch incorporating the name changes.  However, I cannot even create a mock plugin test unless I create a combo of MAPREDUCE-4809 and MAPREDUCE-4812.  I will do that and post a patch with a mock plugin test.

-- Asokan","23/Nov/12 04:24;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12554716/mapreduce-4812.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3062//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3062//console

This message is automatically generated.","23/Nov/12 21:48;tucu00;In the case of the local run, the ReduceTask is hardcoded using the Merger, the ReduceInputMerger impl is not being used. Is that intentional?","23/Nov/12 23:13;masokan;Hi Alejandro,
  You are right.  That is intentional.  I will do it as part of MAPREDUCE-4808.

-- Asokan
","23/Nov/12 23:30;masokan;Created a combo patch of 4812 and 4809 and added a mock plugin test for {{ReduceInputMerger.}}

-- Asokan
","24/Nov/12 00:24;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12554755/COMBO-mapreduce-4812-4809.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3065//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3065//console

This message is automatically generated.","24/Nov/12 03:17;tucu00;+1","24/Nov/12 23:27;masokan;Hi Alejandro,
  I fixed a minor problem in the incremental patch file mapreduce-4812.patch.  I am also uploading the combo patch file.  I renamed it to be consistent with the sequence of patches applied to it from left to right.

Thanks.
-- Asokan","25/Nov/12 03:15;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12554779/COMBO-mapreduce-4809-4812.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3068//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3068//console

This message is automatically generated.","26/Nov/12 16:47;tucu00;+1","29/Nov/12 21:14;masokan;Hi Alejandro and Arun,
  I simplified the patch somewhat.  Please take a look.

Thanks,
-- Asokan","29/Nov/12 21:44;tucu00;+1 pending jenkins run with the combo in trunk.","29/Nov/12 22:15;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12555404/COMBO-mapreduce-4809-4812.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3084//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3084//console

This message is automatically generated.","29/Nov/12 22:51;tucu00;[~acmurthy], patch looks good to me, I'll wait till tomorrow morning to commit in case you find something, or if you +1 it before then.","30/Nov/12 11:56;acmurthy;[~tucu00] I'm going to need more time to review this, thanks.","04/Dec/12 14:20;tucu00;Arun, as I have not heard from you I'll be committing this patch later this morning.","04/Dec/12 17:45;acmurthy;Is there a hurry? I'll need a couple more days, I have several comments already.","04/Dec/12 18:07;acmurthy;[~tucu00] we are introducing new interfaces to an incredibly important piece of the code (see issues like MAPREDUCE-3721 & MAPREDUCE-4842). Further this is an area where you don't have a history. So, I'd appreciate if you could stop rushing me. 

I've already spent a HUGE amount of time on this reviewing, see MAPREDUCE-2454, MAPREDUCE-4807 & MAPREDUCE-4809. This is close, let's get this right. 

Thanks.","06/Dec/12 03:07;masokan;Hi Arun,
  I have some ideas to fix the problem in MAPREDUCE-4842.  I posted my comments there.  Please take a look.

Thanks.

-- Asokan","07/Dec/12 06:34;acmurthy;Asokan, I see the same interface in MAPREDUCE-4808 too which I have concerns about. 

Can you please help me understand what patch I should be reviewing?","07/Dec/12 19:43;acmurthy;Alejandro - since you +1ed this jira, can you pls explain the rationale for the apis I've asked in MAPREDUCE-4808?

Maybe you and Asokan had an offline conversation?","07/Dec/12 21:28;tucu00;
MAPREDUCE-4812 (this JIRA) makes the MergerManager is made pluggable, nothing else.

In the current code, in the local case where Shuffle is not involved, the MergerManager is not being used; instead the Merger class is used directly.

With MAPREDUCE-4808, the MergerManager is augmented to also handle the local case by adding a new method and moving the Merger.merge() invocation to it.

MAPREDUCE-4808 introduces the merge pluggability for the local case when shuffle is not in the picture.

My understanding is that Asokan split the pluggability of MergeManager from augmenting its  functionality to handle the local case to keep the changes focused.

I'm OK with folding both in a single JIRA if you think it makes more sense..

","07/Dec/12 21:45;masokan;Hi Arun,
  Sorry I did not get back sooner.  The intention of {{ReduceInputMerger}} interface is to have a pluggable {{MergeManager}} implementation.  For a non-local job, {{Shuffle}} and {{MergeManager}} interact and synchronize with each other using the three methods {{waitForInMemoryMerge(),}} {{reserve(),}} and {{close()}}.  So in order to use the {{Shuffle}} these methods are captured in {{ReduceInputMerger}} interface.  I renamed {{waitForInMemoryMerge()}} to a generic name {{waitForResource()}} since the plugin implementation may not have the concept of in-memory merge.
Since the return value from {{reserve()}} is {{MapOutput}}, I did some refactoring of {{MapOutput}} so that plugin can return its own implementation of it.  I kept the refactoring done on {{MapOutput}} in MAPREDUCE-4808.  With just MAPREDUCE-4812, an external plugin is not possible, but it has the core part of the concepts so that it is easy to review just {{ReduceInputMerger}} design.  Similarly, for a local job the input is coming from local files.  I enhanced {{ReduceInputMerger}} with one more method for this.  It is also kept in MAPREDUCE-4808.

Hope I explained well.  Please let me know if you have any more questions.

Thanks.

-- Asokan
","15/Dec/12 20:50;acmurthy;For now I'll close this as a dup of MAPREDUCE-4808, we are having same discussions in both places","17/Dec/12 23:15;masokan;Hi Arun,
  That is fine with me.  I will post a patch for MAPREDUCE-4808.

-- Asokan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce large output segments directly from remote host,MAPREDUCE-4586,12604944,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,cdouglas,cdouglas,25/Aug/12 00:17,11/Dec/12 18:54,12/Jan/21 09:52,,,,,,,,,,task,,,,,,0,,,,,"For some jobs, copying large output segments to the local host is inefficient. The reduce can construct iterators on remote hosts, provided the stream is restartable. This should reduce task latency by amortizing the cost of the data transfer over the entire reduce, rather than paying it upfront.",,cdouglas,jlowe,sho.shimauchi,sseth,vicaya,wind5shy,yvesbastos,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-12-11 18:54:41.671,,,false,,,,,,,,,,,,,,,,,,253812,,,,,Tue Dec 11 18:54:41 UTC 2012,,,,,,,"0|i0e5en:",80638,,,,,,,,,,,,,,,,,,,,,"25/Aug/12 00:26;cdouglas;As a first pass, writing these data to HDFS would provide the connection fault recovery and restart without putting an unreasonable burden on the NameNode. Since there are no quotas on intermediate data generally, tracking larger spills can throttle jobs with large spills (e.g., a directory quota on a shared spill space can limit checkpoint proliferation from (MAPREDUCE-4584)). More general handling of intermediate data and/or restartable/seekable streams from the shuffle service would also be admissible solutions.

Large segments occur naturally, but MAPREDUCE-4585 will make them more common.

We will also need to be mindful of leaving connections open on the remote host, as this could create new contention for threads serving these data.","11/Dec/12 18:54;vicaya;Separate namespace for these data (hence separate namenodes in HDFS2) would help as well. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add hash aggregation style data flow and/or new API,MAPREDUCE-3247,12528372,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,decster,decster,22/Oct/11 12:44,11/Dec/12 01:59,12/Jan/21 09:52,,0.23.0,,,,,,,,task,,,,,,1,api,perfomance,,,"In many join/aggregation like queries run on top of mapreduce, sort is not need, in fact a hash table based join/aggregation is more efficient, this is described in ""Tenzing A SQL Implementation On The MapReduce Framework"" in detail. There are two ways to support hash table based join/aggregation in hadoop mapreduce:

# Only support no sort, the framework do nothing, just pass partitioned k/v pair from mapper to reducer
   The upper application use hash table in their mapper & reducer to do aggregation, and emit all hashtable enties in cleanup() of mapper/reducer, this is how Google did in Tenzing. The main problem is memory control of hashtable.

# Add new ""fold"" API, it can coexist with combiner/reducer API, user can use mapper-combiner-reducer or ""mapper-folder"" (maybe a bad name, welcome to propose a better name..)
   Like foldl in functional programming: folder should have the semantic:
     foldl folder z (x:xs)  =   foldl folder (folder z x) xs
   In this way, upper applications only need to provide folder, underlying framework create and maintains hashtable for key/value pairs, it can be managed & optimized by the framework. For example, in mapper side, we can pre emit entire hashtable or use some policies like cache algorithm to emit part of k/v pairs to free some memory, if the memory consumption reach io.sort.mb

",,aah,acmurthy,ahmed.radwan,anty,cdouglas,cutting,davelatham,decster,eli,eli2,gemini5201314,hammer,he yongqiang,jerrychenhf,jlowe,kiranmr,lianhuiwang,mahadev,mgong@vmware.com,nourl,ozawa,sandyr,sergeant,srivas,tlipcon,tomwhite,varun_saxena,wangmeng,zhihyu@ebaysf.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-12-11 01:59:16.254,,,false,,,,,,,,,,,,,,,,,,214232,,,,,Tue Dec 11 01:59:16 UTC 2012,,,,,,,"0|i0e6rb:",80857,,,,,,,,,,,,,,,,,,,,,"11/Dec/12 01:59;jerrychenhf;Binglin, I noticed that you create this bug from MAPREDUCE-1639, while I think this two bugs are more or less similar. And also there are a lot other things related are going on such as MAPREDUCE-2454 and MAPREDUCE-4049.

If you are not working on this, I would like to take time to work on this feature.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modify Security Conditional that check for KERBEROS,MAPREDUCE-4853,12618972,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,robsparker,robsparker,robsparker,06/Dec/12 00:15,07/Dec/12 00:28,12/Jan/21 09:52,,,,,,,,,,security,,,,,,0,,,,,"To support PLAIN authentication, checks should disallow certain types (TOKEN for token delegation) instead of allowing only KERBEROS",,aw,daryn,eli,robsparker,tgraves,tucu00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,296241,,,,,2012-12-06 00:15:54.0,,,,,,,"0|i14787:",232634,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reducer Channels,MAPREDUCE-4776,12615151,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,craigm,craigm,07/Nov/12 12:51,07/Nov/12 12:51,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"A Google paper on LDA from 2009 -- which can be found at http://plda.googlecode.com/files/aaim.pdf -- describes what it terms ""reducer channels"". This is similar to MultipleOutputs, but where the collect() in the map task specifies a name of a set of reducers, and the key values are forwarded to the appropriate set of reducers. This infers also separate combiners and partitioning for each reduce channel. 

It strikes me that while the same affect may be achievable in Hadoop by using special keys, this formulation may be more natural. It would better facilitate data operations where passes over large data could be condensed into single maps with multiple sets of reducers, resulting in lesser mapping jobs.

(For instance, see Figure 2 of the paper, where there are two channels: one for data, one for the model.)

I note that from the documentation of MultipleOutputs: ""When named outputs are used within a Mapper implementation, key/values written to a name output are not part of the reduce phase, only key/values written to the job OutputCollector are part of the reduce phase.""

The proposed change would address this limitation of MultipleOutputs.",,acmurthy,billie,cdouglas,craigm,eli,jdonofrio,jlowe,qwertymaniac,revans2,sandyr,tgraves,tlipcon,wind5shy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,255735,,,,,2012-11-07 12:51:10.0,,,,,,,"0|i0fudj:",90519,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add queue metrics with buckets for job run times,MAPREDUCE-3773,12540594,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,omalley,omalley,omalley,31/Jan/12 18:27,22/Oct/12 18:47,12/Jan/21 09:52,09/Mar/12 23:37,,,,,,0.23.3,2.0.2-alpha,,jobtracker,,,,,,0,,,,,"It would be nice to have queue metrics that reflect the number of jobs in each queue that have been running for different ranges of time.

Reasonable time ranges are probably 0-1 hr, 1-5 hr, 5-24 hr, 24+ hrs; but they should be configurable.",,hudson,revans2,sseth,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,YARN-179,,,,,,,,"29/Feb/12 16:09;omalley;mr-3773-trunk.patch;https://issues.apache.org/jira/secure/attachment/12516575/mr-3773-trunk.patch","29/Feb/12 08:23;omalley;mr-3773-trunk.patch;https://issues.apache.org/jira/secure/attachment/12516532/mr-3773-trunk.patch","01/Feb/12 20:00;omalley;mr-3773.patch;https://issues.apache.org/jira/secure/attachment/12512788/mr-3773.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2012-02-06 16:41:09.955,,,false,,,,,,,,,,,,,,,,,,226007,,,,,Tue Aug 21 12:41:06 UTC 2012,,,,,,,"0|i07y9z:",44320,,,,,,,,,,,,,,,,,,,,,"01/Feb/12 20:00;omalley;This is the patch for branch-1. I still need to forward port it to trunk.","06/Feb/12 16:19;omalley;This patch applies to 0.23 and trunk.

It renames the property to yarn.resourcemanager.metrics.runtime.buckets

I had to make the CapacityScheduler Configurable so that it could get the YarnConfiguration. I also needed to add the YarnConfiguration to QueueMetrics.forQueue so   that the QueueMetrics object would have access to the YarnConfiguration.","06/Feb/12 16:41;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12513435/mr-3773-trunk.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 3 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests:
                  org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService
                  org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps
                  org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched
                  org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterLauncher
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestNodesPage
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits
                  org.apache.hadoop.yarn.server.resourcemanager.TestRM
                  org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueParsing
                  org.apache.hadoop.yarn.server.resourcemanager.TestResourceManager
                  org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCResponseId
                  org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices
                  org.apache.hadoop.yarn.server.resourcemanager.TestApplicationCleanup

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1788//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1788//artifact/trunk/hadoop-mapreduce-project/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-resourcemanager.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1788//console

This message is automatically generated.","06/Feb/12 19:45;omalley;Fixed findbugs warnings and add missing Configurable interface from CapacityScheduler.","06/Feb/12 19:46;omalley;resubmitting","06/Feb/12 20:14;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12513479/mr-3773-trunk.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests:
                  org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1792//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1792//artifact/trunk/hadoop-mapreduce-project/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-resourcemanager.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1792//console

This message is automatically generated.","28/Feb/12 18:29;omalley;fix a couple of findbugs issues that I missed last time.","28/Feb/12 18:53;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12516366/mr-3773-trunk.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests:
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestNodesPage
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueParsing
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched
                  org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1949//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1949//console

This message is automatically generated.","29/Feb/12 04:46;omalley;Fix npe problem in unit test.","29/Feb/12 05:05;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12516517/mr-3773-trunk.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1960//console

This message is automatically generated.","29/Feb/12 05:31;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12516518/mr-3773-trunk.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests:
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestNodesPage
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueParsing
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1961//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1961//console

This message is automatically generated.","29/Feb/12 05:34;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12516518/mr-3773-trunk.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests:
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestNodesPage
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueParsing

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1963//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1963//console

This message is automatically generated.","29/Feb/12 07:49;omalley;fix some unit tests.","29/Feb/12 08:12;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12516527/mr-3773-trunk.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 21 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests:
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueParsing
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1964//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1964//console

This message is automatically generated.","29/Feb/12 08:14;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12516527/mr-3773-trunk.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 21 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests:
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueParsing

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1965//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1965//console

This message is automatically generated.","29/Feb/12 08:45;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12516532/mr-3773-trunk.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 21 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests:
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1966//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1966//console

This message is automatically generated.","29/Feb/12 08:54;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12516532/mr-3773-trunk.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 21 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests:
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1967//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1967//console

This message is automatically generated.","29/Feb/12 17:08;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12516575/mr-3773-trunk.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 24 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in .

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1969//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1969//console

This message is automatically generated.","29/Feb/12 17:09;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12516575/mr-3773-trunk.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 24 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in .

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1970//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1970//console

This message is automatically generated.","09/Mar/12 23:20;acmurthy;+1, the patch looks good. Thanks Owen.

Could you please comment on the testing you did on this patch? Thanks.","09/Mar/12 23:27;omalley;I ran it on a 1.0 cluster and a 0.23 cluster and watched a job move between buckets as it ran.","09/Mar/12 23:28;acmurthy;Ok, sounds good - pushing this in. Thanks!","09/Mar/12 23:37;acmurthy;I just committed this. Thanks Owen!","09/Mar/12 23:45;hudson;Integrated in Hadoop-Hdfs-trunk-Commit #1937 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/1937/])
    MAPREDUCE-3773. Add queue metrics with buckets for job run times. Contributed by Owen O'Malley. (Revision 1299100)

     Result = SUCCESS
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1299100
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueMetrics.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TimeBucketMetrics.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacitySchedulerContext.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestQueueMetrics.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestApplicationLimits.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestParentQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestQueueParsing.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebApp.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesCapacitySched.java
","09/Mar/12 23:46;hudson;Integrated in Hadoop-Common-0.23-Commit #668 (See [https://builds.apache.org/job/Hadoop-Common-0.23-Commit/668/])
    Merge -c 1299100 from trunk to branch-0.23 to fix MAPREDUCE-3773. Add queue metrics with buckets for job run times. Contributed by Owen O'Malley. (Revision 1299101)

     Result = SUCCESS
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1299101
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueMetrics.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TimeBucketMetrics.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacitySchedulerContext.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestQueueMetrics.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestApplicationLimits.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestParentQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestQueueParsing.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebApp.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesCapacitySched.java
","09/Mar/12 23:49;hudson;Integrated in Hadoop-Hdfs-0.23-Commit #659 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Commit/659/])
    Merge -c 1299100 from trunk to branch-0.23 to fix MAPREDUCE-3773. Add queue metrics with buckets for job run times. Contributed by Owen O'Malley. (Revision 1299101)

     Result = SUCCESS
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1299101
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueMetrics.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TimeBucketMetrics.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacitySchedulerContext.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestQueueMetrics.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestApplicationLimits.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestParentQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestQueueParsing.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebApp.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesCapacitySched.java
","09/Mar/12 23:52;hudson;Integrated in Hadoop-Common-trunk-Commit #1862 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/1862/])
    MAPREDUCE-3773. Add queue metrics with buckets for job run times. Contributed by Owen O'Malley. (Revision 1299100)

     Result = SUCCESS
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1299100
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueMetrics.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TimeBucketMetrics.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacitySchedulerContext.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestQueueMetrics.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestApplicationLimits.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestParentQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestQueueParsing.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebApp.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesCapacitySched.java
","10/Mar/12 00:26;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #1871 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/1871/])
    MAPREDUCE-3773. Add queue metrics with buckets for job run times. Contributed by Owen O'Malley. (Revision 1299100)

     Result = ABORTED
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1299100
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueMetrics.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TimeBucketMetrics.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacitySchedulerContext.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestQueueMetrics.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestApplicationLimits.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestParentQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestQueueParsing.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebApp.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesCapacitySched.java
","10/Mar/12 00:26;hudson;Integrated in Hadoop-Mapreduce-0.23-Commit #676 (See [https://builds.apache.org/job/Hadoop-Mapreduce-0.23-Commit/676/])
    Merge -c 1299100 from trunk to branch-0.23 to fix MAPREDUCE-3773. Add queue metrics with buckets for job run times. Contributed by Owen O'Malley. (Revision 1299101)

     Result = ABORTED
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1299101
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueMetrics.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TimeBucketMetrics.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacitySchedulerContext.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestQueueMetrics.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestApplicationLimits.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestParentQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestQueueParsing.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebApp.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesCapacitySched.java
","10/Mar/12 12:52;hudson;Integrated in Hadoop-Hdfs-0.23-Build #193 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/193/])
    Merge -c 1299100 from trunk to branch-0.23 to fix MAPREDUCE-3773. Add queue metrics with buckets for job run times. Contributed by Owen O'Malley. (Revision 1299101)

     Result = UNSTABLE
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1299101
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueMetrics.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TimeBucketMetrics.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacitySchedulerContext.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestQueueMetrics.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestApplicationLimits.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestParentQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestQueueParsing.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebApp.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesCapacitySched.java
","10/Mar/12 12:53;hudson;Integrated in Hadoop-Hdfs-trunk #980 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/980/])
    MAPREDUCE-3773. Add queue metrics with buckets for job run times. Contributed by Owen O'Malley. (Revision 1299100)

     Result = SUCCESS
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1299100
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueMetrics.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TimeBucketMetrics.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacitySchedulerContext.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestQueueMetrics.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestApplicationLimits.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestParentQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestQueueParsing.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebApp.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesCapacitySched.java
","10/Mar/12 13:17;hudson;Integrated in Hadoop-Mapreduce-0.23-Build #221 (See [https://builds.apache.org/job/Hadoop-Mapreduce-0.23-Build/221/])
    Merge -c 1299100 from trunk to branch-0.23 to fix MAPREDUCE-3773. Add queue metrics with buckets for job run times. Contributed by Owen O'Malley. (Revision 1299101)

     Result = FAILURE
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1299101
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueMetrics.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TimeBucketMetrics.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacitySchedulerContext.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestQueueMetrics.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestApplicationLimits.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestParentQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestQueueParsing.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebApp.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesCapacitySched.java
","10/Mar/12 13:57;hudson;Integrated in Hadoop-Mapreduce-trunk #1015 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1015/])
    MAPREDUCE-3773. Add queue metrics with buckets for job run times. Contributed by Owen O'Malley. (Revision 1299100)

     Result = SUCCESS
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1299100
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueMetrics.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TimeBucketMetrics.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacitySchedulerContext.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestQueueMetrics.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestApplicationLimits.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestParentQueue.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestQueueParsing.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebApp.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesCapacitySched.java
","20/Aug/12 16:37;revans2;Apparently with the split to 2.0 this never made it in to branch-0.23.  I just pulled it in.","21/Aug/12 12:41;hudson;Integrated in Hadoop-Hdfs-0.23-Build #350 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/350/])
    MAPREDUCE-3773. Add queue metrics with buckets for job run times. Contributed by Owen O'Malley. (Revision 1375094)

     Result = SUCCESS
bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1375094
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueMetrics.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TimeBucketMetrics.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacitySchedulerContext.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestQueueMetrics.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestApplicationLimits.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestParentQueue.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestQueueParsing.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebApp.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesCapacitySched.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backport Gridmix and Rumen features from trunk to Hadoop 0.20 security branch,MAPREDUCE-3118,12525140,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,ravidotg,ravidotg,ravidotg,29/Sep/11 05:51,17/Oct/12 18:27,12/Jan/21 09:52,18/Oct/11 14:48,1.1.0,,,,,1.1.0,,,contrib/gridmix,tools/rumen,,,,,0,,,,,Backporting all the features and bugfixes that went into gridmix and rumen of trunk to hadoop 0.20 security branch. This will enable using all these gridmix features and run gridmix/rumen on the history logs of 0.20 security branch.,,mattf,mholderba,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/11 10:40;ravidotg;gridmix_rumen_backports.v2.4.patch;https://issues.apache.org/jira/secure/attachment/12496988/gridmix_rumen_backports.v2.4.patch","30/Sep/11 11:55;ravidotg;gridmix_rumen_backports.v2.5.patch;https://issues.apache.org/jira/secure/attachment/12497144/gridmix_rumen_backports.v2.5.patch","11/Oct/11 08:34;ravidotg;gridmix_rumen_backports.v2.6.patch;https://issues.apache.org/jira/secure/attachment/12498534/gridmix_rumen_backports.v2.6.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2011-10-05 20:06:15.236,,,false,,,,,,,,,,,,,,,,,,39251,Reviewed,,,,Wed Oct 17 18:27:25 UTC 2012,,,,,,,"0|i09bun:",52353,Backports latest features from trunk to 0.20.206 branch.,,,,,,,,,,gridmix rumen backport,,1.1.0,,,,,,,,"29/Sep/11 06:31;ravidotg;System tests patches are not part of this for now. They will be added by Vinay (may be later).

Backporting patches (features and bugfixes) are from the following JIRAs:

MAPREDUCE-2596: [Gridmix] Summarize Gridmix runs.
MAPREDUCE-2107: [Gridmix] Total heap usage emulation in Gridmix.
MAPREDUCE-2106: [Gridmix] Cumulative CPU usage emulation in Gridmix.
MAPREDUCE-2543: [Gridmix] High-Ram feature emulation testcase.
MAPREDUCE-2408: [Gridmix] Compression emulation in Gridmix.
MAPREDUCE-2137: Provide mapping between jobs of trace file and the corresponding simulated cluster's jobs in Gridmix.
MAPREDUCE-2407: Make GridMix emulate usage of distributed cache files in simulated jobs.
MAPREDUCE-2416: Remove the restriction of specifying group names in users-list file for Gridmix in RoundRobinUserResolver mode.
MAPREDUCE-1931: Gridmix documentation.
MAPREDUCE-2417: Fix Gridmix in RoundRobinUserResolver mode to map testing/proxy users to unique users in a trace.
MAPREDUCE-2095: Fix Gridmix to run from compressed traces.
MAPREDUCE-1989: Fix error message in gridmix when user resolver is set and no user list is given.
MAPREDUCE-1979: Fix 'Output directory already exists' error in gridmix when gridmix.output.directory is not defined.
MAPREDUCE-1975: Fix unnecessary InterruptedException log in gridmix.
\\
\\
\\
MAPREDUCE-2078: Fix TraceBuilder to generate traces when a globbed job history path is given.
MAPREDUCE-2153: Rumen should bring in job configuration properties into the trace file.
MAPREDUCE-2104: [Rumen] Add Cpu, Memory and Heap usages to TraceBuilder's output.
MAPREDUCE-1978: Rumen TraceBuilder should provide recursive input folder scanning.
MAPREDUCE-1918: Add documentation to Rumen.
MAPREDUCE-2000: Rumen is not able to extract counters for Job history logs from Hadoop 0.20.
MAPREDUCE-1982: Fix Rumen's TraceBuilder to extract job name from either of configuration properties mapreduce.job.name and mapred.job.name.
MAPREDUCE-1925: Fix failing TestRumenJobTraces.
\\
\\
This backporting also needs minor changes to mapreduce task progress reporting because some of these gridmix features rely on the task progress values.","29/Sep/11 10:40;ravidotg;Attaching patch that incorporates all the features and bugfixes mentioned in previous comment.","30/Sep/11 11:55;ravidotg;Attaching new patch adding src.test.data to build-contrib.xml (which is missing in 20-security branch) to make TestGridmixSubmission to run properly.","05/Oct/11 20:06;mattf;Hi Ravi, it will be great to see these improvements in 0.20.206.  Thanks.","06/Oct/11 03:13;amar_kamat;@Matt I have taken up the review of the latest patch. 

@All Vinay has tested this patch on a test cluster and seems like all the latest Gridmix features work fine. Vinay also backported the system tests he wrote for trunk to make sure that all the features are tested thoroughly.","07/Oct/11 07:06;amar_kamat;I see some MRJobConfig fields in DistributedCacheEmulator's javadoc. Can this be changed to their new locations maybe in JobContext or DistributedCache?
Rest of the patch looks good to me.

@Ravi: Can you upload test-patch and ant-test status along with the final patch?","11/Oct/11 08:34;ravidotg;Incorporated Amar's review comments.

Unit tests and test-patch passed on my local machine.","12/Oct/11 04:14;amar_kamat;Folks,
This patch adds a _getProgress()_ API to public classes and interfaces like:
1. Progress.java 
2. TaskInputOutputContext.java 
3. Reporter.java

This API provides the task's current progress and is immensely useful of Gridmix. We tend to believe that this might get classified as a backward incompatible change. We want to be sure about the incompatibility side of the story and make a call accordingly. 

Kindly let us know your thoughts/comments regarding the same.","18/Oct/11 08:12;ravidotg;Please come back if there are any issues. We are planning to commit this soon.","18/Oct/11 11:29;amar_kamat;Folks,
We ran the wordcount and Gridmix3 from Hadoop 0.20.205 on a Hadoop cluster running a MR-3118 patched version of 0.20.205. All the jobs ran fine. We didn't see any compatibility issues. I will commit this patch.","18/Oct/11 14:48;amar_kamat;I just committed this to the 0.20.206 branch. Great job Ravi! Thanks Vinay for helping with the testing.","17/Oct/12 18:27;mattf;Closed upon release of Hadoop-1.1.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add support for encrypted shuffle,MAPREDUCE-4417,12598111,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,tucu00,tucu00,tucu00,10/Jul/12 00:15,11/Oct/12 17:48,12/Jan/21 09:52,26/Jul/12 13:26,2.0.0-alpha,,,,,2.0.2-alpha,,,mrv2,security,,,,,0,,,,,"Currently Shuffle fetches go on the clear. While Kerberos provides comprehensive authentication for the cluster, it does not provide confidentiality. 

When processing sensitive data confidentiality may be desired (at the expense of job performance and resources utilization for doing encryption).
",,atm,cutting,efan,eric14,hammer,junping_du,larsgeorge,lianhuiwang,mayank_bansal,omalley,qwertymaniac,sseth,sureshms,tgraves,tlipcon,tomwhite,tucu00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/12 16:54;tucu00;MAPREDUCE-4417-branch-1.patch;https://issues.apache.org/jira/secure/attachment/12537582/MAPREDUCE-4417-branch-1.patch","25/Jul/12 21:42;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12537902/MAPREDUCE-4417.patch","23/Jul/12 16:55;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12537583/MAPREDUCE-4417.patch","22/Jul/12 16:02;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12537503/MAPREDUCE-4417.patch","20/Jul/12 23:14;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12537424/MAPREDUCE-4417.patch","20/Jul/12 22:09;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12537419/MAPREDUCE-4417.patch","20/Jul/12 21:16;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12537412/MAPREDUCE-4417.patch","20/Jul/12 18:42;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12537381/MAPREDUCE-4417.patch","20/Jul/12 03:27;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12537291/MAPREDUCE-4417.patch","20/Jul/12 00:50;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12537273/MAPREDUCE-4417.patch","19/Jul/12 23:24;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12537262/MAPREDUCE-4417.patch","18/Jul/12 16:34;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12537017/MAPREDUCE-4417.patch","18/Jul/12 05:22;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12536950/MAPREDUCE-4417.patch","17/Jul/12 23:10;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12536907/MAPREDUCE-4417.patch","17/Jul/12 23:04;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12536904/MAPREDUCE-4417.patch","17/Jul/12 20:50;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12536884/MAPREDUCE-4417.patch","17/Jul/12 18:48;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12536861/MAPREDUCE-4417.patch","17/Jul/12 16:45;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12536844/MAPREDUCE-4417.patch","17/Jul/12 04:01;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12536772/MAPREDUCE-4417.patch","17/Jul/12 00:28;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12536752/MAPREDUCE-4417.patch","16/Jul/12 21:11;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12536713/MAPREDUCE-4417.patch","16/Jul/12 19:23;tucu00;MAPREDUCE-4417.patch;https://issues.apache.org/jira/secure/attachment/12536699/MAPREDUCE-4417.patch",,,,,,,,,,,,,22.0,,,,,,,,,,,,,,,,,,,,2012-07-11 03:17:55.833,,,false,,,,,,,,,,,,,,,,,,240995,Reviewed,,,,Thu Jul 26 13:26:43 UTC 2012,,,,,,,"0|i017nj:",5009,,,,,,,,,,,,,,,,,,,,,"11/Jul/12 03:17;eric14;What is the driving use case?  

I'd suggest that anyone who wants the data encrypted on the wire, will want it encrypted at rest on both sides as well.  The data is as vulnerable there.  

I wonder if we can come up with an approach that just introduces new plugins and doesn't add any hadoop code?  The right thing is probably to use the compression codecs to encrypt on the way to disk.

thoughts?
","11/Jul/12 16:04;tucu00;@eric14, 

The driving use case is to avoid data spoofing while on the wire.

Agree, encrypting data at both sides is the obvious follow up to this JIRA in order to have end to end over the wire confidentiality.

In current Hadoop, as you suggest, you can use compression codecs to do encryption on both sides.

However, you can not do that for the shuffle. Thus this JIRA to tackle the shuffle case first.

Of course, this functionality would be disabled by default, even if Kerberos security is enabled. You'll need to set another knob to enable shuffle encryption.

Hope this clarifies.

","11/Jul/12 16:28;eric14;Anyone I've talked to who has been concerned about over the wire has also raise the on disk issue.  So, would it be better to put the encryption where we write to disk, where we already compress?  It seems like this might be less invasive and would be more complete.

It has downsides if you do lots of spills, but it is much more complete.  The compaction issue can be addressed by collation work folks are already playing with down the road.

---

Do you already have an HDFS solution in place?  This only covers a fraction of the data traffic.","11/Jul/12 16:37;atm;bq. Do you already have an HDFS solution in place? This only covers a fraction of the data traffic.

Just filed: HDFS-3637","11/Jul/12 17:07;tucu00;@eric14, my bad you can use a codec in the shuffle, when looking into this I've discarded that option, let me remember exactly why and I'll follow up.","13/Jul/12 05:11;tucu00;When looking at encryption on the wire for the shuffle the alternatives that popped up where transport encryption (HTTPS) and data/spills encryption (doable via a codec).

Using HTTPS requires improving the Fetcher/ShuffleHandler (Netty/JDK-URL) to use HTTPS and configuring certificates. It is a well understood/standard/proven technology and gives you end to end confidentiality, integrity, server authentication (and optionally client authentication), in an out of box manner without room to get things wrong. The server certificates private keys are out of reach from job tasks (they are used by the NM, similar to Kerberos keytabs). 

Using a codec, requires (leveraging a existing plugin point) a compression codec implementation that adds cipher-streams wrappers to the original streams and in addition could delegate to a real compression codec (in order not to lose compression if doing encryption). This requires us choosing a Cipher implementation by hand (which I'm not an expert on) and I'm not sure which one would be the best choice and what are the weaknesses of each one of them (http://en.wikipedia.org/wiki/Stream_cipher#Comparison_Of_Stream_Ciphers). Using a cipher on its own will provide confidentiality but it would not provide integrity or man-in-the-middle protection (unless we end up implementing something like TLS). In addition, both ends are controlled by job tasks, thus it becomes the responsibility of the user to create/distribute/protect the secrets that are basis of confidentiality. In addition, with the codec approach the HTTP shuffle requests/response headers go in the clear which could enable a man-in-the-middle attach.
","16/Jul/12 19:23;tucu00;patch with complete implementation. Introducing an SSLFactory class in common so it can be used by follow up HADOOP-8581. Patch does not have documentation yet. I'll work on that next.","16/Jul/12 20:36;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12536699/MAPREDUCE-4417.patch
  against trunk revision .

    -1 @author.  The patch appears to contain 2 @author tags which the Hadoop community has agreed to not allow in code contributions.

    +1 tests included.  The patch appears to include 6 new or modified test files.

    -1 javac.  The applied patch generated 2115 javac compiler warnings (more than the trunk's current 2066 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 5 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle:

                  org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays
                  org.apache.hadoop.io.file.tfile.TestTFileByteArrays
                  org.apache.hadoop.mapreduce.security.ssl.TestEncryptedShuffle

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2602//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2602//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2602//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-shuffle.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2602//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-core.html
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2602//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2602//console

This message is automatically generated.","16/Jul/12 21:11;tucu00;patch addressing test-patch complains.","16/Jul/12 22:27;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12536713/MAPREDUCE-4417.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified test files.

    -1 javac.  The applied patch generated 2115 javac compiler warnings (more than the trunk's current 2066 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle:

                  org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays
                  org.apache.hadoop.io.file.tfile.TestTFileByteArrays
                  org.apache.hadoop.mapred.TestFileInputFormatPathFilter
                  org.apache.hadoop.mapred.TestFieldSelection
                  org.apache.hadoop.mapred.TestBlockLimits
                  org.apache.hadoop.mapred.TestMiniMRClasspath
                  org.apache.hadoop.mapred.TestTextOutputFormat
                  org.apache.hadoop.mapred.TestSequenceFileInputFormat
                  org.apache.hadoop.mapreduce.TestMROutputFormat
                  org.apache.hadoop.mapreduce.lib.chain.TestChainErrors
                  org.apache.hadoop.mapreduce.lib.aggregate.TestMapReduceAggregates
                  org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers
                  org.apache.hadoop.mapred.jobcontrol.TestJobControl
                  org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileAsTextInputFormat
                  org.apache.hadoop.mapred.TestMiniMRBringup
                  org.apache.hadoop.mapreduce.TestValueIterReset
                  org.apache.hadoop.mapreduce.lib.output.TestMRMultipleOutputs
                  org.apache.hadoop.mapred.TestMiniMRChildTask
                  org.apache.hadoop.mapred.TestMapRed
                  org.apache.hadoop.mapred.lib.TestMultipleOutputs
                  org.apache.hadoop.mapred.TestReporter
                  org.apache.hadoop.mapred.TestCollect
                  org.apache.hadoop.mapred.TestReduceFetch
                  org.apache.hadoop.mapred.TestNetworkedJob
                  org.apache.hadoop.mapred.TestTaskCommit
                  org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter
                  org.apache.hadoop.mapred.TestClusterMRNotification
                  org.apache.hadoop.mapreduce.TestMapReduce
                  org.apache.hadoop.mapred.TestReduceFetchFromPartialMem
                  org.apache.hadoop.mapred.TestJobCounters
                  org.apache.hadoop.mapreduce.lib.db.TestDataDrivenDBInputFormat
                  org.apache.hadoop.mapred.TestMiniMRClientCluster
                  org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter
                  org.apache.hadoop.mapreduce.lib.input.TestMultipleInputs
                  org.apache.hadoop.mapred.TestFileOutputCommitter
                  org.apache.hadoop.mapred.TestLazyOutput
                  org.apache.hadoop.mapred.TestLocalMRNotification
                  org.apache.hadoop.mapred.TestJobCleanup
                  org.apache.hadoop.mapreduce.TestMapReduceLazyOutput
                  org.apache.hadoop.mapred.TestSpecialCharactersInOutputPath
                  org.apache.hadoop.mapred.lib.TestMultithreadedMapRunner
                  org.apache.hadoop.mapreduce.lib.chain.TestSingleElementChain
                  org.apache.hadoop.mapred.TestLineRecordReader
                  org.apache.hadoop.mapred.TestUserDefinedCounters
                  org.apache.hadoop.mapred.TestMapOutputType
                  org.apache.hadoop.mapred.lib.aggregate.TestAggregates
                  org.apache.hadoop.mapreduce.lib.jobcontrol.TestMapReduceJobControl
                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat
                  org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader
                  org.apache.hadoop.mapred.lib.TestChainMapReduce
                  org.apache.hadoop.mapred.TestClusterMapReduceTestCase
                  org.apache.hadoop.mapred.join.TestDatamerge
                  org.apache.hadoop.io.TestSequenceFileMergeProgress
                  org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileAsBinaryInputFormat
                  org.apache.hadoop.mapred.jobcontrol.TestLocalJobControl
                  org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileInputFilter
                  org.apache.hadoop.mapred.TestJavaSerialization
                  org.apache.hadoop.mapred.lib.TestKeyFieldBasedComparator
                  org.apache.hadoop.mapreduce.lib.join.TestJoinDatamerge
                  org.apache.hadoop.mapred.lib.TestLineInputFormat
                  org.apache.hadoop.mapreduce.lib.fieldsel.TestMRFieldSelection
                  org.apache.hadoop.mapred.TestJobSysDirWithDFS
                  org.apache.hadoop.mapred.TestComparators
                  org.apache.hadoop.mapreduce.lib.input.TestNLineInputFormat
                  org.apache.hadoop.mapred.TestMultipleTextOutputFormat
                  org.apache.hadoop.mapreduce.lib.partition.TestMRKeyFieldBasedComparator
                  org.apache.hadoop.mapreduce.lib.output.TestMRSequenceFileAsBinaryOutputFormat
                  org.apache.hadoop.mapreduce.lib.map.TestMultithreadedMapper
                  org.apache.hadoop.mapred.TestJobName
                  org.apache.hadoop.mapred.TestFileOutputFormat
                  org.apache.hadoop.mapreduce.security.TestJHSSecurity
                  org.apache.hadoop.mapred.TestMapProgress
                  org.apache.hadoop.mapreduce.lib.chain.TestMapReduceChain

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2603//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2603//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-shuffle.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2603//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-core.html
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2603//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2603//console

This message is automatically generated.","17/Jul/12 00:28;tucu00;getting (or trying) to get rid of findbugs. looking into the other errors that I don't see them failing locally","17/Jul/12 02:35;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12536752/MAPREDUCE-4417.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified test files.

    -1 javac.  The applied patch generated 2115 javac compiler warnings (more than the trunk's current 2066 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle:

                  org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays
                  org.apache.hadoop.io.file.tfile.TestTFileByteArrays
                  org.apache.hadoop.mapred.TestFileInputFormatPathFilter
                  org.apache.hadoop.mapred.TestFieldSelection
                  org.apache.hadoop.mapred.TestBlockLimits
                  org.apache.hadoop.mapred.TestMiniMRClasspath
                  org.apache.hadoop.mapred.TestTextOutputFormat
                  org.apache.hadoop.mapred.TestSequenceFileInputFormat
                  org.apache.hadoop.mapreduce.TestMROutputFormat
                  org.apache.hadoop.mapreduce.lib.chain.TestChainErrors
                  org.apache.hadoop.mapreduce.lib.aggregate.TestMapReduceAggregates
                  org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers
                  org.apache.hadoop.mapred.jobcontrol.TestJobControl
                  org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileAsTextInputFormat
                  org.apache.hadoop.mapred.TestMiniMRBringup
                  org.apache.hadoop.mapreduce.TestValueIterReset
                  org.apache.hadoop.mapreduce.lib.output.TestMRMultipleOutputs
                  org.apache.hadoop.mapred.TestMiniMRChildTask
                  org.apache.hadoop.mapred.TestMapRed
                  org.apache.hadoop.mapred.lib.TestMultipleOutputs
                  org.apache.hadoop.mapred.TestReporter
                  org.apache.hadoop.mapred.TestCollect
                  org.apache.hadoop.mapred.TestReduceFetch
                  org.apache.hadoop.mapred.TestNetworkedJob
                  org.apache.hadoop.mapred.TestTaskCommit
                  org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter
                  org.apache.hadoop.mapred.TestClusterMRNotification
                  org.apache.hadoop.mapreduce.TestMapReduce
                  org.apache.hadoop.mapred.TestReduceFetchFromPartialMem
                  org.apache.hadoop.mapred.TestJobCounters
                  org.apache.hadoop.mapreduce.lib.db.TestDataDrivenDBInputFormat
                  org.apache.hadoop.mapred.TestMiniMRClientCluster
                  org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter
                  org.apache.hadoop.mapreduce.lib.input.TestMultipleInputs
                  org.apache.hadoop.mapred.TestFileOutputCommitter
                  org.apache.hadoop.mapred.TestLazyOutput
                  org.apache.hadoop.mapred.TestLocalMRNotification
                  org.apache.hadoop.mapred.TestJobCleanup
                  org.apache.hadoop.mapreduce.TestMapReduceLazyOutput
                  org.apache.hadoop.mapred.TestSpecialCharactersInOutputPath
                  org.apache.hadoop.mapred.lib.TestMultithreadedMapRunner
                  org.apache.hadoop.mapreduce.lib.chain.TestSingleElementChain
                  org.apache.hadoop.mapred.TestLineRecordReader
                  org.apache.hadoop.mapred.TestUserDefinedCounters
                  org.apache.hadoop.mapred.TestMapOutputType
                  org.apache.hadoop.mapred.lib.aggregate.TestAggregates
                  org.apache.hadoop.mapreduce.lib.jobcontrol.TestMapReduceJobControl
                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat
                  org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader
                  org.apache.hadoop.mapred.lib.TestChainMapReduce
                  org.apache.hadoop.mapred.TestClusterMapReduceTestCase
                  org.apache.hadoop.mapred.join.TestDatamerge
                  org.apache.hadoop.io.TestSequenceFileMergeProgress
                  org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileAsBinaryInputFormat
                  org.apache.hadoop.mapred.jobcontrol.TestLocalJobControl
                  org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileInputFilter
                  org.apache.hadoop.mapred.TestJavaSerialization
                  org.apache.hadoop.mapred.lib.TestKeyFieldBasedComparator
                  org.apache.hadoop.mapreduce.lib.join.TestJoinDatamerge
                  org.apache.hadoop.mapred.lib.TestLineInputFormat
                  org.apache.hadoop.mapreduce.lib.fieldsel.TestMRFieldSelection
                  org.apache.hadoop.mapred.TestJobSysDirWithDFS
                  org.apache.hadoop.mapred.TestComparators
                  org.apache.hadoop.mapreduce.lib.input.TestNLineInputFormat
                  org.apache.hadoop.mapred.TestMultipleTextOutputFormat
                  org.apache.hadoop.mapreduce.lib.partition.TestMRKeyFieldBasedComparator
                  org.apache.hadoop.mapreduce.lib.output.TestMRSequenceFileAsBinaryOutputFormat
                  org.apache.hadoop.mapreduce.lib.map.TestMultithreadedMapper
                  org.apache.hadoop.mapred.TestJobName
                  org.apache.hadoop.mapred.TestFileOutputFormat
                  org.apache.hadoop.mapreduce.security.TestJHSSecurity
                  org.apache.hadoop.mapred.TestMapProgress
                  org.apache.hadoop.mapreduce.lib.chain.TestMapReduceChain

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2608//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2608//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2608//console

This message is automatically generated.","17/Jul/12 04:01;tucu00;and not taking care of most of the javac warnings. There are a bunch of them because of direct use of SunX509 classes, but there is not alternative for this.","17/Jul/12 04:31;tucu00;I've meant 'and now ...'","17/Jul/12 05:09;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12536772/MAPREDUCE-4417.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified test files.

    -1 javac.  The applied patch generated 2108 javac compiler warnings (more than the trunk's current 2066 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle:

                  org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays
                  org.apache.hadoop.io.file.tfile.TestTFileByteArrays
                  org.apache.hadoop.mapred.TestFileInputFormatPathFilter
                  org.apache.hadoop.mapred.TestFieldSelection
                  org.apache.hadoop.mapred.TestBlockLimits
                  org.apache.hadoop.mapred.TestMiniMRClasspath
                  org.apache.hadoop.mapred.TestTextOutputFormat
                  org.apache.hadoop.mapred.TestSequenceFileInputFormat
                  org.apache.hadoop.mapreduce.TestMROutputFormat
                  org.apache.hadoop.mapreduce.lib.chain.TestChainErrors
                  org.apache.hadoop.mapreduce.lib.aggregate.TestMapReduceAggregates
                  org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers
                  org.apache.hadoop.mapred.jobcontrol.TestJobControl
                  org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileAsTextInputFormat
                  org.apache.hadoop.mapred.TestMiniMRBringup
                  org.apache.hadoop.mapreduce.TestValueIterReset
                  org.apache.hadoop.mapreduce.lib.output.TestMRMultipleOutputs
                  org.apache.hadoop.mapred.TestMiniMRChildTask
                  org.apache.hadoop.mapred.TestMapRed
                  org.apache.hadoop.mapred.lib.TestMultipleOutputs
                  org.apache.hadoop.mapred.TestReporter
                  org.apache.hadoop.mapred.TestCollect
                  org.apache.hadoop.mapred.TestReduceFetch
                  org.apache.hadoop.mapred.TestNetworkedJob
                  org.apache.hadoop.mapred.TestTaskCommit
                  org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter
                  org.apache.hadoop.mapred.TestClusterMRNotification
                  org.apache.hadoop.mapreduce.TestMapReduce
                  org.apache.hadoop.mapred.TestReduceFetchFromPartialMem
                  org.apache.hadoop.mapred.TestJobCounters
                  org.apache.hadoop.mapreduce.lib.db.TestDataDrivenDBInputFormat
                  org.apache.hadoop.mapred.TestMiniMRClientCluster
                  org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter
                  org.apache.hadoop.mapreduce.lib.input.TestMultipleInputs
                  org.apache.hadoop.mapred.TestFileOutputCommitter
                  org.apache.hadoop.mapred.TestLazyOutput
                  org.apache.hadoop.mapred.TestLocalMRNotification
                  org.apache.hadoop.mapred.TestJobCleanup
                  org.apache.hadoop.mapreduce.TestMapReduceLazyOutput
                  org.apache.hadoop.mapred.TestSpecialCharactersInOutputPath
                  org.apache.hadoop.mapred.lib.TestMultithreadedMapRunner
                  org.apache.hadoop.mapreduce.lib.chain.TestSingleElementChain
                  org.apache.hadoop.mapred.TestLineRecordReader
                  org.apache.hadoop.mapred.TestUserDefinedCounters
                  org.apache.hadoop.mapred.TestMapOutputType
                  org.apache.hadoop.mapred.lib.aggregate.TestAggregates
                  org.apache.hadoop.mapreduce.lib.jobcontrol.TestMapReduceJobControl
                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat
                  org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader
                  org.apache.hadoop.mapred.lib.TestChainMapReduce
                  org.apache.hadoop.mapred.TestClusterMapReduceTestCase
                  org.apache.hadoop.mapred.join.TestDatamerge
                  org.apache.hadoop.io.TestSequenceFileMergeProgress
                  org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileAsBinaryInputFormat
                  org.apache.hadoop.mapred.jobcontrol.TestLocalJobControl
                  org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileInputFilter
                  org.apache.hadoop.mapred.TestJavaSerialization
                  org.apache.hadoop.mapred.lib.TestKeyFieldBasedComparator
                  org.apache.hadoop.mapreduce.lib.join.TestJoinDatamerge
                  org.apache.hadoop.mapred.lib.TestLineInputFormat
                  org.apache.hadoop.mapreduce.lib.fieldsel.TestMRFieldSelection
                  org.apache.hadoop.mapred.TestJobSysDirWithDFS
                  org.apache.hadoop.mapred.TestComparators
                  org.apache.hadoop.mapreduce.lib.input.TestNLineInputFormat
                  org.apache.hadoop.mapred.TestMultipleTextOutputFormat
                  org.apache.hadoop.mapreduce.lib.partition.TestMRKeyFieldBasedComparator
                  org.apache.hadoop.mapreduce.lib.output.TestMRSequenceFileAsBinaryOutputFormat
                  org.apache.hadoop.mapreduce.lib.map.TestMultithreadedMapper
                  org.apache.hadoop.mapred.TestJobName
                  org.apache.hadoop.mapred.TestFileOutputFormat
                  org.apache.hadoop.mapreduce.security.TestJHSSecurity
                  org.apache.hadoop.mapred.TestMapProgress
                  org.apache.hadoop.mapreduce.lib.chain.TestMapReduceChain

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2609//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2609//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2609//console

This message is automatically generated.","17/Jul/12 16:45;tucu00;And now with testcases passing (the encrypted shuffle testcase was leaving a core-site.xml that was being picked up by other testcases)

Forgot to mention before, all this work is based on an initial implementation by Tom White.","17/Jul/12 17:57;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12536844/MAPREDUCE-4417.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified test files.

    -1 javac.  The applied patch generated 2108 javac compiler warnings (more than the trunk's current 2066 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle:

                  org.apache.hadoop.io.file.tfile.TestTFileByteArrays
                  org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays
                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2613//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2613//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2613//console

This message is automatically generated.","17/Jul/12 17:58;tucu00;test failures seem unrelated. javac warnings are because of use of SunX509 classes.","17/Jul/12 18:48;tucu00;And now with documentation.","17/Jul/12 19:57;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12536861/MAPREDUCE-4417.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified test files.

    -1 javac.  The applied patch generated 2108 javac compiler warnings (more than the trunk's current 2066 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site:

                  org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays
                  org.apache.hadoop.io.file.tfile.TestTFileByteArrays
                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2614//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2614//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2614//console

This message is automatically generated.","17/Jul/12 20:50;tucu00;Same as last patch, just adding a comment about the performance impact (per Devaraj's suggestion):

*Using encrypted shuffle will incurs in a significant performance impact. Users should profile this and potentially reserve 1 or more cores for encrypted shuffle.*
","17/Jul/12 21:43;tomwhite;This looks good to me (although, as Alejandro mentioned, I have worked on an earlier version of this, so someone else should review it too). A few minor things I noticed:

* SSLFactory is in a mapreduce package, but in the common project. Just move it to org.apache.hadoop.security.ssl?
* Mark SSLFactory.resolvePropertyName with the VisibleForTesting annotation.
* ReloadingX509TrustManager allows 'this' to escape in its constructor. Perhaps give it a separate initialization method to start the reloader.","17/Jul/12 21:55;tucu00;Thanks for the review Tom. I'll integrate your changes. After a chat with Devaraj and other with you, I'll do some refactoring in how the keystores are produced to enable plugin alterante implementations (a follow up JIRA will be for creating the certs in teh keystore using the jobtoken secrets).","17/Jul/12 21:57;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12536884/MAPREDUCE-4417.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified test files.

    -1 javac.  The applied patch generated 2108 javac compiler warnings (more than the trunk's current 2066 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site:

                  org.apache.hadoop.io.file.tfile.TestTFileByteArrays
                  org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays
                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2615//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2615//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2615//console

This message is automatically generated.","17/Jul/12 23:04;tucu00;addressing Tom's comments and making the keystores pluggable (to later enable other mechanisms -such as jobtoken- to generate certificates on the fly).","17/Jul/12 23:10;tucu00;previously missed Tom's last suggestion (forgot to 'git add -u' before creating the patch)","18/Jul/12 00:17;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12536907/MAPREDUCE-4417.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified test files.

    -1 javac.  The applied patch generated 2108 javac compiler warnings (more than the trunk's current 2066 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site:

                  org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays
                  org.apache.hadoop.io.file.tfile.TestTFileByteArrays
                  org.apache.hadoop.security.ssl.TestSSLFactory
                  org.apache.hadoop.mapreduce.security.ssl.TestEncryptedShuffle
                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2617//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2617//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2617//console

This message is automatically generated.","18/Jul/12 05:22;tucu00;fixing testcases that failed after latest refactoring due to incorrect setup.","18/Jul/12 06:36;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12536950/MAPREDUCE-4417.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified test files.

    -1 javac.  The applied patch generated 2108 javac compiler warnings (more than the trunk's current 2066 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site:

                  org.apache.hadoop.security.ssl.TestSSLFactory
                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2618//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2618//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2618//console

This message is automatically generated.","18/Jul/12 16:34;tucu00;again, missed to add a file git cache before cutting the previous patch. Now we should be fine.","18/Jul/12 17:47;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537017/MAPREDUCE-4417.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified test files.

    -1 javac.  The applied patch generated 2108 javac compiler warnings (more than the trunk's current 2066 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site:

                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2622//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2622//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2622//console

This message is automatically generated.","19/Jul/12 00:41;tlipcon;- the reformatting of ssl-client.xml.example and ssl-server.xml.example makes it a little hard to read the diff. Is it necessary to reindent, etc?

- Style:
{code}
+ * if the trust certificates keystore file changes, the trustmanager
+ * is refreshed with the new trust certificate entries (using a
+ * {@link ReloadingX509TrustManager} trustmanager).
{code}

Formatting can be improved here - eg TrustManager is a java class, so should probably {@link} it to make it clear you're talking about a specific class and not an abstract concept. (as someone who doesn't know the SSL APIs well, it would make it easier to read)

----

{code}
+    SSLFactory.Mode mode =
+      SSLFactory.Mode.valueOf(conf.get(SSLFactory.SSL_FACTORY_MODE));
{code}
Why are we passing the factory mode through the configuration, instead of just making it a parameter for init()? It seems a little fragile/unnecessary, and a bit confusing since it's not a parameter that the user sets.

----

{code}
+    String keystoreType =
+      conf.get(resolvePropertyName(mode, SSL_KEYSTORE_TYPE_TPL), ""jks"");
{code}

What's jks? You also use the term ""jks"" in the conf files, but I don't know what it refers to (again, as an SSL n00b). Improvements:
-- in the config file where you say ""default value is 'jks'"", add ""which enables the blah blah type key store"" and some reference to what it means?
-- in the code, add a constant SSL_KEYSTORE_TYPE_DEFAULT, and javadoc with a pointer to what jks is.


----
{code}
+      String keystoreLocation = conf.get(
+        resolvePropertyName(mode, SSL_KEYSTORE_LOCATION_TPL), """");
+      keystorePassword = conf.get(
+        resolvePropertyName(mode, SSL_KEYSTORE_PASSWORD_TPL), """").toCharArray();
+
+      LOG.debug(mode.toString() + "" KeyStore: "" + keystoreLocation);
+
+      InputStream is = new FileInputStream(keystoreLocation);
{code}

If this property isn't set, you'll end up passing an empty string to the FileInputStream constructor, which will end up giving a hard-to-diagnose message. Check whether {{keystoreLocation.isEmpty()}}, and if it is, throw an appropriate exception including the config name.

Same goes for {{trustStoreLocation}}

----

Style nit: you have several javadocs for getters which are redundant, eg:
{code}
+  /**
+   * Returns the trustmanagers for trusted certificates.
+   *
+   * @return the trustmanagers for trusted certificates.
+   */
{code}
No need to repeat yourself twice - just have the @return line in the javadoc and not the line above it, IMO.

----
{code}
+   * @param type type of truststore file, typically 'JKS'.
{code}
Elsewhere in the code you have ""jks"" (lower case). Is it case sensitive?

----
{code}
+    } catch (Exception ex) {
+      trustManagerRef.set(null);
+      LOG.warn(""Could not load truststore, using empty one : "" + ex.toString(),
+               ex);
+    }
{code}
Why should you use an empty one? If the user configures a path to a trust store, and then starts up but the store can't be found, I don't think we should ignore their config. Better to bail out on startup. Then all of the null checks later on in this file could be removed.

----
{code}
+      FileInputStream in = new FileInputStream(file);
+      try {
+        ks.load(in, password.toCharArray());
+        lastLoaded = file.lastModified();
{code}
I think you need to set {{lastLoaded}} _before_ opening the file. Otherwise there's a race where you can miss a change to the file.

----
{code}
+    } catch (Exception ex) {
+      throw new RuntimeException(ex);
+    }
{code}
Maybe use {{Throwables.propagateIfPossible}} here to propagate IOException and GeneralSecurityException first? Seems strange to throw RTE for an IOE when you declare that the method throws IOE.

----
{code}
+  @SuppressWarnings({""InfiniteLoopStatement""})
+  public void run() {
{code}
There's really no way we can get a cleanup hook here to stop the thread at shutdown?

----
{code}
+        } catch (Exception ex) {
+          trustManagerRef.set(null);
+          LOG.warn(""Could not load truststore, using empty one : "" +
+                   ex.toString(),  ex);
+        }
{code}

If it fails to reload, why not stick to the previous version of the reference instead of falling back to empty?

----
{code}
+ * This SSLFactory uses a {@link ReloadingX509TrustManager} intance,
+ * which reloads public keys if the truststore file changes.
+ * <p/>
+ * This factory is used to configure HTTPS in Hadoop HTTP based endpoints, both
+ * client & server.
{code}
Typo: 'intance'
Style: don't abbreviate the word ""and"" as '&' -- it's invalid javadoc and also just harder to read.

----
{code}
+  public enum Mode { CLIENT, SERVER }
{code}

This can be {{static}} right? Also, since it's an inner class, you need an {{@InterfaceAudience.Private}} on it, too, or else it shows up in the public javadoc. (unfortunately the annotation doesn't get inherited from its outer class)

----
{code}
+  public static final String SSL_ENABLED =
+    ""hadoop.ssl.enabled"";
...
+  public static final String KEYSTORES_FACTORY_CLASS =
+    ""hadoop.ssl.keystores.factory.class"";
{code}
Style: rename all these constants to end in {{_KEY}} so it's clear it's the conf keys and not the values themselves.

----
{code}
+    Configuration sslConf = new Configuration(false);
+    sslConf.setBoolean(SSL_REQUIRE_CLIENT_CERT, requireClientCert);
+    String sslConfResource;
+    if (mode == Mode.CLIENT) {
+        sslConfResource = conf.get(SSL_CLIENT_CONF, ""ssl-client.xml"");
+    } else {
+      sslConfResource = conf.get(SSL_SERVER_CONF, ""ssl-server.xml"");
+    }
+    sslConf.addResource(sslConfResource);
{code}
Move this into a private method {{readSslConfiguration(mode)}}? Also, indentation is off in one line here.

----
- Extract the creation of SSLHostnameVerifier into a new method, as well.

----
{code}
+<property>
+  <name>hadoop.ssl.enabled</name>
+  <value>false</value>
+  <description>Whether encrypted shuffle is enabled</description>
+</property>
{code}
If this is specific to encrypted shuffle, the name should reflect that, and it should be in mapred-default.xml, not core-default.xml

I wonder: is there a use case for having this setting per-job in some clusters? Either way, it should definitely be an MR config and not a core config.

----
{code}
+    The keystores factory to use for retriving certificates.
{code}
Typo: retriving

----
{code}
+
+public class KeyStoreUtil {
{code}
Rename to KeyStoreTestUtil, since this is a test-only class.

{code}
+    FileOutputStream out = new FileOutputStream(filename);
+    ks.store(out, password.toCharArray());
+    out.close();
{code}
Need try..finally. A few other places later in this same file that need this fix.

----

{code}
+    // Wait so that the file modification time is different
+    Thread.sleep((tm.getReloadInterval() + 2) * 1000);
{code}
You have this in a bunch of places in the test - but if you set the last modified time of the file, as you do elsewhere in the test, then you shouldn't have to sleep, except for waiting for it to _notice_ the reload. If you change the reload interval to be specified in millis instead of seconds, then you could set it to 10ms or so for the tests and these tests would run a lot faster.

----
- In TestSSLFactory, you use Assert.fail() in a lot of places after catching an Exception. Instead, just let the exception fall through which will fail the test, with the advantage that we'll actually have the stack trace of the exception instead of an unexplained failure message. In the cases where you expect an exception, use {{GenericTestUtils.assertExceptionContains}} to check the text.

----
{code}
+        writeFuture = ch.write(new ChunkedFile(spill, info.startOffset,
+                                               info.partLength, 8192));
{code}
What's 8192 here? Need a constant or config. If it's a buffer size, I'd think 64K or 128K would probably perform better, based on my general experience with java IO.

----

- In the docs, under the ssh-client configuration, it references ssl-server.xml in one spot.
- Typo: ""trutsstore"" in one place.
- Typo: ""will incurs in a significant""

----

General comment: what's the point of client certificates here? They're not a secret, since all users share them. I would think they'd need to be shipped with the job in the distributed-cache, if the use case is for cross-cluster authentication in tools like distcp, since different users may want to distcp from different clusters, and also have different access controls.","19/Jul/12 23:24;tucu00;@todd, thanks for the detailed review.

I've integrated most of your comments.

* The javadoc style for 'Returns BLAH' and then '@return  BLAH' is Sun javadoc sytle.

* keystore type is case insensitive, 'jks' is the same as 'JKS'. Still I've lowercased that javadoc.

* the ReloadingX509TrustManager will work with an empty keystore if the keystore file is not avail at initialization time, and if the keystore file becomes available later one, it will be loaded. WARNs are logged while the file is not present, so it won't go unnoticed.

* added a init()/destroy() methods where appropriate to be able to shutdown the reload thread gracefully.

* If reload() fails to reload the new keystore, it assumes there are not certs and runs empty until the next reload attempt. Seems a safer assumption that continuing running with obsolete keys.

* While hadoop.ssl.enabled only applies to shuffle, the intention is to use it for the rest of the HTTP endpoints. Thus, a single know would enable SSL. That is why the name of the property and its location (in core-default.xml)

* Regarding having it per job, This would require having shuffler serving both HTTP and HTTPS and denying the endpoint the job is not configured to use. This would require the shuffler to have access to that piece of job configuration. I'd say it is out of scope of this patch, and it could be a future improvement.

* In the TestSSLFactory, the Assert.fail() statements, are sections the test should not make it; they are used for negative tests.

* Client certs are disabled by default. If they are per job, yes they could be shipped via DC. This would require a alternate implementation of the KeyStoresFactory, thus the mechanism is already in place.
","19/Jul/12 23:46;tlipcon;bq. The javadoc style for 'Returns BLAH' and then '@return BLAH' is Sun javadoc sytle.
Ew. That's disgusting. Oh well.

bq. the ReloadingX509TrustManager will work with an empty keystore if the keystore file is not avail at initialization time, and if the keystore file becomes available later one, it will be loaded. WARNs are logged while the file is not present, so it won't go unnoticed.

WARNs in the logs are often not noticed. Don't you think it's simpler to just fail if the conf is not present? If someone configures this and doesn't create the file (or the file is unreadable due to a permissions error), I think it's friendlier to fail fast. Otherwise they'll just end up seeing strange downstream issues like client certs not being properly trusted, which will be more difficult to root-cause back to the trust store configuration without log spelunking.

bq. If reload() fails to reload the new keystore, it assumes there are not certs and runs empty until the next reload attempt. Seems a safer assumption that continuing running with obsolete keys.

My worry here is that people might be using a conf management system to push out the key store files. If the reload happens to trigger right in the middle of a conf mgmt update, and the update is non-atomic, it will see an invalid keystore. I wouldn't want the TT to revert to an empty key store until the next reload interval in that case.

bq. While hadoop.ssl.enabled only applies to shuffle, the intention is to use it for the rest of the HTTP endpoints. Thus, a single know would enable SSL. That is why the name of the property and its location (in core-default.xml)

Given it doesn't currently affect the other HTTP endpoints, I find this very confusing. Why not make a separate config for now, and then once it affects more than just the shuffle, you can change the default for {{mapred.shuffle.use.ssl}} to {{${hadoop.use.ssl}}} to pick up the system-wide default.

bq. In the TestSSLFactory, the Assert.fail() statements, are sections the test should not make it; they are used for negative tests.
I get that. But, if the test breaks, you'll end up with a meaningless failure, instead of a message explaining why it failed. If you let the exception fall through, then the failed unit test would actually have a stack trace that explains why it failed, which aids in debugging.

bq. Client certs are disabled by default. If they are per job, yes they could be shipped via DC. This would require a alternate implementation of the KeyStoresFactory, thus the mechanism is already in place.
Does it need an alternate implementation? The distributed cache files can be put on the classpath already, in which case the existing keystore-loading code should be able to find them. The only change would be in the documentation -- explaining that the client should ship the files via distributed cache rather than putting them in HADOOP_CONF_DIR. Why wouldn't that be enough?","20/Jul/12 00:50;tucu00;Thanks Todd, attached patch that takes care of all your comments but the last one. So test-patch runs. I'll update the docs to explain the client cert stuff properly.","20/Jul/12 00:50;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537262/MAPREDUCE-4417.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified test files.

    -1 javac.  The applied patch generated 2048 javac compiler warnings (more than the trunk's current 2006 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site:

                  org.apache.hadoop.mapred.TestMiniMRClientCluster
                  org.apache.hadoop.mapreduce.v2.TestMROldApiJobs
                  org.apache.hadoop.mapred.TestJobCounters
                  org.apache.hadoop.mapred.TestClusterMapReduceTestCase
                  org.apache.hadoop.mapred.TestJobName
                  org.apache.hadoop.mapreduce.v2.TestMiniMRProxyUser
                  org.apache.hadoop.mapred.TestClusterMRNotification
                  org.apache.hadoop.mapred.TestReduceFetch
                  org.apache.hadoop.mapreduce.TestChild
                  org.apache.hadoop.mapred.TestLazyOutput
                  org.apache.hadoop.mapred.TestReduceFetchFromPartialMem
                  org.apache.hadoop.mapreduce.v2.TestMRJobs
                  org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers
                  org.apache.hadoop.mapreduce.v2.TestMRJobsWithHistoryService
                  org.apache.hadoop.mapred.TestJobSysDirWithDFS
                  org.apache.hadoop.mapreduce.TestMapReduceLazyOutput
                  org.apache.hadoop.mapred.TestMiniMRClasspath
                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2629//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2629//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-shuffle.html
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2629//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2629//console

This message is automatically generated.","20/Jul/12 03:27;tucu00;adding to docs a note about client certs. And a missed hunk to ShuffleHandler.","20/Jul/12 03:42;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537273/MAPREDUCE-4417.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified test files.

    -1 javac.  The applied patch generated 2048 javac compiler warnings (more than the trunk's current 2006 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site:

                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2631//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2631//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-shuffle.html
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2631//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2631//console

This message is automatically generated.","20/Jul/12 04:12;tucu00;test failure seems unrelated.","20/Jul/12 06:48;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537291/MAPREDUCE-4417.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified test files.

    -1 javac.  The applied patch generated 2048 javac compiler warnings (more than the trunk's current 2006 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site:

                  org.apache.hadoop.mapreduce.TestMapReduceLazyOutput
                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2632//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2632//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-shuffle.html
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2632//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2632//console

This message is automatically generated.","20/Jul/12 16:19;tucu00;Findbugs issue is a false positive, due to the ShuffleHandler start() method being syncrhonized (wherethe buffer variable gets instantiated), I'll add the corresponding findbug exclusion before committing.

Test failures seem unrelated.
","20/Jul/12 18:42;tucu00;minor correction in testcase where a conf.writeXml was outside of the try block.","20/Jul/12 18:52;tlipcon;- The {{AtomicBoolean running}} doesn't need to be an atomic boolean, since it's already volatile. You can just use a volatile boolean here.
- in {{loadTrustManager}}, you need to get {{file.lastModified}} before you even open the {{FileInputStream}}. Otherwise if the file is replaced in between opening the stream and you getting the mtime, you'll read the old version of the file but think you read the new one (assuming an atomic rename-over-old-file replacement)

Otherwise looks good to me.","20/Jul/12 19:57;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537381/MAPREDUCE-4417.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified test files.

    -1 javac.  The applied patch generated 2048 javac compiler warnings (more than the trunk's current 2006 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site:

                  org.apache.hadoop.mapreduce.v2.TestMRJobs
                  org.apache.hadoop.mapreduce.v2.TestSpeculativeExecution
                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2638//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2638//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-shuffle.html
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2638//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2638//console

This message is automatically generated.","20/Jul/12 21:16;tucu00;@todd, addressing your last to comments. THX!","20/Jul/12 21:21;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537412/MAPREDUCE-4417.patch
  against trunk revision .

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2640//console

This message is automatically generated.","20/Jul/12 22:09;tucu00;rebasing patch as the revert for MAPREDUCE-4423 created some conflicts.","20/Jul/12 23:16;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537419/MAPREDUCE-4417.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified test files.

    -1 javac.  The applied patch generated 2048 javac compiler warnings (more than the trunk's current 2006 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site:

                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat
                  org.apache.hadoop.mapred.TestClusterMapReduceTestCase

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2641//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2641//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-shuffle.html
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2641//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2641//console

This message is automatically generated.","20/Jul/12 23:21;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537424/MAPREDUCE-4417.patch
  against trunk revision .

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2642//console

This message is automatically generated.","22/Jul/12 16:02;tucu00;rebasing patch to trunk and resolving conflict in index documentation page. Also, simplified some initialization logic in the testcases.","22/Jul/12 17:16;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537503/MAPREDUCE-4417.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified test files.

    -1 javac.  The applied patch generated 2049 javac compiler warnings (more than the trunk's current 2007 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site:

                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2645//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2645//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-shuffle.html
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2645//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2645//console

This message is automatically generated.","23/Jul/12 16:54;tucu00;backport for branch-1.","23/Jul/12 16:55;tucu00;minor corrections to the docs for trunk.","23/Jul/12 17:06;tucu00;The patch for branch-1, opens an additional port with SSL just for encrypted shuffle, the shuffle servlet (MapOutputServlet) refuses to serve shuffle over the clear HTTP endpoint if SSL is enable:

{code}
      if (shuffleSsl && !request.isSecure()) {
        response.sendError(HttpServletResponse.SC_FORBIDDEN,
          ""Encrypted Shuffle is enabled, shuffle is only served over HTTPS"");
        return;
      }
{code}
","23/Jul/12 18:06;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537583/MAPREDUCE-4417.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified test files.

    -1 javac.  The applied patch generated 2049 javac compiler warnings (more than the trunk's current 2007 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site:

                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2649//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2649//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-shuffle.html
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2649//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2649//console

This message is automatically generated.","23/Jul/12 19:02;eric14;Why would we add this to 1?  I understand mainline, but not anything else at this point.  I don't think this is a complete approach.  It's going to take additional work to finish this.  Why add the complexity to a stabilized code line?","23/Jul/12 19:03;eric14;PS I thought we had a process for adding things to 1, which was to propose them during next release planning.","23/Jul/12 19:45;tucu00;@eric14, you are correct, on its own is an incomplete feature and it will require more work. I've posted the patch for branch-1 in case somebody is interested. ","23/Jul/12 22:24;tlipcon;bq. PS I thought we had a process for adding things to 1, which was to propose them during next release planning.

That process was proposed but it hasn't been followed at all. There have been plenty of new features going into branch-1 without prior discussion.","25/Jul/12 20:06;tlipcon;bq. -1 findbugs. The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.
Are you going to update the findbugs exclude file for this warning? We can't commit until this comes back clean.


Otherwise the latest trunk patch looks good.","25/Jul/12 21:35;tucu00;yep, updating findbugs exclusion as part of the commit.

thx","25/Jul/12 21:42;tucu00;patch now includes findbugs exclusion.","25/Jul/12 23:03;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537902/MAPREDUCE-4417.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified test files.

    -1 javac.  The applied patch generated 2049 javac compiler warnings (more than the trunk's current 2007 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site:

                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2661//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2661//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2661//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2661//console

This message is automatically generated.","26/Jul/12 00:22;tucu00;findbugs warning is in *org.apache.hadoop.fs.FileUtil.symLink* which is not related to this JIRA work.","26/Jul/12 00:36;tlipcon;Sorry, that findbugs was my issue due to an accidental commit - I reverted it. +1 from my side assuming Tom and co are still good with it.","26/Jul/12 01:46;tomwhite;+1 for the latest patch.","26/Jul/12 13:26;tucu00;committed to trunk and branch-2.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint shuffle aggregation as map output,MAPREDUCE-4585,12604943,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,cdouglas,cdouglas,25/Aug/12 00:17,27/Aug/12 19:19,12/Jan/21 09:52,,,,,,,,,,task,,,,,,0,,,,,"Map output collected during the shuffle can be spilled and written as a composite of map outputs. Particularly if the job employs a combiner, this checkpoint can provide fault tolerance and improve job throughput by aggregating intermediate output. The latter is especially helpful for jobs with multiple waves of reduces.",,cdouglas,curino,jlowe,lianhuiwang,ozawa,sriramsrao,sseth,wind5shy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Aug/12 19:14;curino;shufflecheckpoint.pdf;https://issues.apache.org/jira/secure/attachment/12542651/shufflecheckpoint.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-08-25 00:27:29.781,,,false,,,,,,,,,,,,,,,,,,253813,,,,,Mon Aug 27 19:19:48 UTC 2012,,,,,,,"0|i0e5ev:",80639,,,,,,,,,,,,,,,,,,,,,"25/Aug/12 00:27;curino;We have experimented with a prototype that simulates map output collation by spilling collected output from idle reducers and rescheduling them. Reducers are rescheduled in order of pending work (i.e., reducers with many segements to merge are scheduled first). We are currently piggybacking on fault recovery for rescheduling, but we will improve on this before submission. Our experiments show no regressions even in unfavorable conditions (uniform distribution of map output, single waves of reducers, etc.). We are currently investigating the benefits to larger jobs, per [1].

[1] http://research.yahoo.com/files/yl-2012-002.pdf
","27/Aug/12 19:14;curino;Shuffle Checpoint-restart explained","27/Aug/12 19:19;curino;The image posted shows an example of a run where checkpointing-restart helps ""hiding"" part of the shuffling costs, by allowing (two in this example) reducers to take turns over the only available slot, and thus getting to the point in which all maps have completed with most of the shuffling done. The basic behavior would have the reducer one sitting on the slot until all maps complete, and thus reduce 2 paying the full cost of shuffling ""after"" map5 completed and a new slot is available. We believe this should help for non-ideal schedules (multiple waves of maps/reduces which are not perfectly aligned) and to handle skew.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable context-specific and stateful serializers in MapReduce,MAPREDUCE-1462,12455395,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,omalley,omalley,omalley,05/Feb/10 07:36,07/Jul/12 12:56,12/Jan/21 09:52,,,,,,,,,,task,,,,,,2,,,,,"Although the current serializer framework is powerful, within the context of a job it is limited to picking a single serializer for a given class. Additionally, Avro generic serialization can make use of additional configuration/state such as the schema. (Most other serialization frameworks including Writable, Jute/Record IO, Thrift, Avro Specific, and Protocol Buffers only need the object's class name to deserialize the object.)

With the goal of keeping the easy things easy and maintaining backwards compatibility, we should be able to allow applications to use context specific (eg. map output key) serializers in addition to the current type based ones that handle the majority of the cases. Furthermore, we should be able to support serializer specific configuration/metadata in a type safe manor without cluttering up the base API with a lot of new methods that will confuse new users.",,aah,acmurthy,alexlod,cdouglas,davelatham,ddas,decster,eli,gates,gemini5201314,hammer,hong.tang,jdonofrio,jghoman,jrideout,kimballa,philip,qwertymaniac,romainr,schen,sharadag,tlipcon,tomwhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/10 00:46;tomwhite;MAPREDUCE-1462-common.patch;https://issues.apache.org/jira/secure/attachment/12435636/MAPREDUCE-1462-common.patch","12/Feb/10 00:46;tomwhite;MAPREDUCE-1462-mr.patch;https://issues.apache.org/jira/secure/attachment/12435637/MAPREDUCE-1462-mr.patch","05/Feb/10 08:34;omalley;h-1462.patch;https://issues.apache.org/jira/secure/attachment/12434941/h-1462.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2010-02-05 17:49:46.686,,,false,,,,,,,,,,,,,,,,,,149528,,,,,Sat Jul 07 12:56:34 UTC 2012,,,,,,,"0|i0e99b:",81262,,,,,,,,,,,,,,,,,,,,,"05/Feb/10 08:34;omalley;Ok this patch is a rough sketch of the way we could refactor the serialization interface. 

The RootSerilizationFactory provides a factory to look up a serialization given an object's class.

Serializations contain a serialize and deserialize method along with the Input/OutputStream to write the object to. They also contain a serializeSelf/deserializeSelf method that serializes the metadata for that serializer. By having the serializer handle and parse the metadata itself, it means the framework doesn't need to support each individual serializers' information and it doesn't lose all type safety of the string to string maps where a single mispelling can cause an attribute to not be found or an extra space can cause parse errors.

There is a subtype of Serialization named TypedSerialization that is the base class for all of the serializations that use the object's class as their metadata. This would include the Writable, Thrift, ProtocolBuffers, and Avro Specific serializers.

For containers such as SequenceFile and T-File, the file would contain the name of the serializer class and the serializer class' metadata. That is enough to reconstruct the serialization and deserialize the objects. For writing a SequenceFile or T-File, you can specify the types as currently and let the root serialization factory pick the serializers or you can provide them explicitly.

In terms of how this would hook to MapReduce, the job would have the map outputs' class name configured as currently, but it would just require that the actual type be assignable to the declared type. (ie. you can set the map output type to Object and pass anything.) 

There would be an enumeration of the contexts and a method to set a serialization for a specific context:

{noformat}
public enum SerializationContext {
   DEFAULT, 
   MAP_OUTPUT_KEY, 
   MAP_OUTPUT_VALUE, 
   REDUCE_OUTPUT_KEY, 
   REDUCE_OUTPUT_VALUE, 
   INPUT_SPLIT
};
{noformat}

and the Job/JobContext would get a new setter/getter for getting the Serialization for each context. If the user doesn't specify a given context, it will use the default. If the default isn't specified, it will use the root serialization factory for the assignable type.

","05/Feb/10 17:49;tomwhite;Owen, thanks for posting your design. I've reproduced my comments on the design which I made on MAPREDUCE-1126 here for convenience:

* The changes to the serialization API are not backwards compatible, so a new package of serializer types would need creating. Is this really necessary to achieve Avro integration?
* I'm not sure why we need to serialize serializations. The patch in MAPREDUCE-1126 avoids the need for this by using a simple string mechanism for configuration. Having an opaque binary format also makes it difficult to retrieve and use the serialization from other languages (e.g. C++ or other Pipes languages). My latest patch on MAPREDUCE-1126 is language-neutral in this regard.
* Adding a side file for the context-serializer mapping complicates the implementation. It's not clear what container file would be used for the side file (Avro container, custom?). I understand that putting framework configuration in the job configuration may not be desirable, but it has been done in the past so I don't know why it is being ruled out here. I would rather have a separate effort (and discussion) to create a ""private"" job configuration (not accessible by user code) for such configuration (above and beyond the configuration needed for serialization).
* The user API is no shorter than the one proposed in MAPREDUCE-1126. Compare:
{code}
Schema keySchema = ...
AvroGenericSerialization serialization = new AvroGenericSerialization();
serialization.setSchema(keySchema);
job.set(SerializationContext.MAP_OUTPUT_KEY, serialization);
{code}
with
{code}
Schema keySchema = ...
AvroGenericData.setMapOutputKeySchema(job, keySchema);
{code}
","05/Feb/10 18:10;cutting;> Serializations contain a serialize and deserialize method along with the Input/OutputStream to write the object to.

Where would these be stored?  Are you proposing we add another file to each job?  Currently we have conf+splits+jar.  Do we really want tasks to have to open more files?

This proposes fundamental changes to job submission that should be addressed elsewhere.  We can achieve the goals of this issue using the existing mechanisms, as in Tom & Aaron's patch to MAPREDUCE-1126.  Changing job submission in the way you suggest should be discussed separately, in MAPREDUCE-1183.

> it doesn't lose all type safety of the string to string maps where a single misspelling can cause an attribute to not be found or an extra space can cause parse errors

We have long used string maps for Hadoop configuration.  These strings are generally written by programs that use constants to avoid misspellings.  This approach has long served HTTP, SMTP and unix enviroment variables as a language-independent means of specifying parameters.  If you want to change this, it should be addressed systematically in MAPREDUCE-1183.

> the job would have the map outputs' class name configured as currently,

So we'd have a class name in the job configuration too?  That seems redundant and inconsistent with your misspelling concerns.

> the Job/JobContext would get a new setter/getter for getting the Serialization for each context.

HADOOP-6420 already provided a mechanism for this purpose.  Tom & Aaron's patch for HADOOP-1126 use this mechanism with a constant per context rather than an enum.  Would you prefer that an enum was used?  Perhaps you could suggest that there?
","05/Feb/10 18:18;acmurthy;{quote}
Where would these be stored? Are you proposing we add another file to each job? Currently we have conf+splits+jar. Do we really want tasks to have to open more files?

This proposes fundamental changes to job submission that should be addressed elsewhere. We can achieve the goals of this issue using the existing mechanisms, as in Tom & Aaron's patch to MAPREDUCE-1126. Changing job submission in the way you suggest should be discussed separately, in MAPREDUCE-1183.
{quote}

*Iff* there is consensus that this is the the model being proposed in MAPREDUCE-1183, we could start that journey in this patch, no? Why do we need to do the work twice i.e. first put in the conf, then move it to the (serialized) job-description file (say, job.data) via MAPREDUCE-1183?


","07/Feb/10 09:17;omalley;{quote}
The changes to the serialization API are not backwards compatible, so a new package of serializer types would need creating. Is this really necessary to achieve Avro integration?
{quote}

No, it is not necessary. I believe it to be a much cleaner interface. The current interface defines the metadata for serializers as a Map<String,String>. The metadata is *not* supposed to be user facing but defined by each particular serialization to control its own serialization and deserialization. For opaque data that is not intended to be interpreted by the user, isn't a binary blob a better way to communicate the intent than a Map<String,String>?

{quote}
I'm not sure why we need to serialize serializations.
{quote}

The goal is to reduce the chance that user's will make a mistake in calling the API. With your patch on 1126, all of the application's control over serialization is done indirectly through static methods that reach into the Job's configuration and set the map. So roughly,

1. user calls to serializer-specific configuration code, which manually sets the configuration with the metadata.
2. to get the serializer, the frameworks gets the metadata from the configuration, and looks through the list of serializations for the first one that will accept that metadata. The selected serializer gets the metadata and hopefully does the right thing.

I think it is much clearer and less error-prone, if the framework has a method that takes a serializer and uses that to get the metadata. The serializer serialization is really just methods to read and write serialization specific metadata.

Under your 1126 patch, my hypothetical magic serialization looks like:

{code}
public class MyMagicSerialization extends SerializerBase {
  public static void setMapOutputKeyMagicMetdata(Job job, Other metadata) { ... }
  public static void setMapOutputValueMagicMetdata(Job job, Other metadata) { ... }
  public static void setReduceOutputKeyMagicMetdata(Job job, Other metadata) { ... }
  public static void setReduceOutputValueMagicMetdata(Job job, Other metadata) { ... }
  public boolean accept(Map<String,String> metadata) { ... }
  ...
}
{code}

The first thing to notice is that my serialization, which has does *not* depend on MapReduce needs a bunch of methods that are very concretely tied to MapReduce. Furthermore, it is difficult to extend by adding new contexts. If HBase needs to use this serializer they need to add a new method to the *serializer* for their context.

Now look at the equivalent in my scheme:
{code}
public class MyMagicSerialization extends Serialization {
  public void setMMagicMetdata(Other metadata) { ... }
  ...
}
{code}

It does not depend on MapReduce and the way that MapReduce may use it. On the other hand, I do want a method to set the serializer for each context:

{code}
public class Job extends JobContext {
  enum Context {MAP_OUT_KEY, MAP_OUT_VALUE,  REDUCE_OUT_KEY, REDUCE_OUT_VALUE};
  public void setSerialization(Context context, Serialization serialization);
...
}
{code}

I've pulled all of the MapReduce specific code into MapReduce's Job class. That is a much better place for it to be. Furthermore, if MapReduce adds a new context, it only means changing Job by adding a new value to an enum and not adding new methods to all of the serializers. That is a big win.

Other nice changes are:
* having serialize/deserialize methods rather than objects represents the real semantics in that they should not be storing state between calls (ie. the bug that hit the original java serialization).
* it also means that the merge code can be given a single object that it reuse for both serialization and deserialization rather than one of each
* the new api also means that you can serialize/deserialize to a new stream without recreating the object

As to where the serialized metadata is stored, I don't care nearly as much. It might make sense to stick the encoded bytes into the configuration, write it into a new file, or add it to the input split file. That matters much less to me than getting APIs that are clean, understandable, and extensible.","12/Feb/10 00:46;tomwhite;In order to help understand the problem better I've created a demonstration patch that uses the SerializationContext-based user API, while retaining the Serialization code that exists in common. (In fact, I had to make some changes to the Serialization code so that it can retain its metadata in an instance variable.)

Here's what the configuration looks like for the user:

{code}
Schema keySchema = Schema.create(Schema.Type.STRING);
Schema valSchema = Schema.create(Schema.Type.LONG);
job.setSerialization(Job.SerializationContext.MAP_OUTPUT_KEY,
           new AvroGenericSerialization(keySchema));
job.setSerialization(Job.SerializationContext.MAP_OUTPUT_VALUE,
           new AvroGenericSerialization(valSchema));
{code}","10/Jan/11 21:07;nidaley;Too late for 0.22.  Moving Fix Version from 0.22 to 0.23.","07/Jul/12 12:56;qwertymaniac;(Not a blocker for any release, so unmarking as one for the moment)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shipping Profiler Libraries by DistributedCache,MAPREDUCE-4365,12595748,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,,jay23jack,jay23jack,25/Jun/12 07:20,28/Jun/12 16:14,12/Jan/21 09:52,28/Jun/12 01:11,1.0.3,,,,,,,,,,,,,,0,,,,,"Hadoop profiling is great for performance tuning and debugging, but currently we can only use Java built-in profilers such as HProf, and for other profilers we need to install them on all slave nodes first, which is inconvenient for large clusters and sometimes impossible for production clusters. 

Supporting shipping profiler libraries using DistributedCache will solve this problem. For example, in mapred.task.profile.params, we specify a profiler library from the DistributedCache using special place holders such as <foo.jar>, and Hadoop can look at the DistributedCache to replace <foo.jar> with the localized path before launching the child jvm.

",,acmurthy,jay23jack,kam_iitkgp,lianhuiwang,miomir,revans2,sseth,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-06-25 13:59:46.973,,,false,,,,,,,,,,,,,,,,,,247987,,,,,Thu Jun 28 16:14:29 UTC 2012,,,,,,,"0|i0964n:",51425,,,,,,,,,,,,,,,,,,,,,"25/Jun/12 13:59;revans2;Why can't you just use specify your distributed cache entry with something like hdfs://path/to/profiler.lib#profiler-link.lib?  I know it is a little ugly but it will add a symbolic name link named profiler-link.lib in the current working directory of your task to wherever it is in the distributed cache.

You can do this to with a tgz or zip.

hdfs://path/to/archive.tgz#profiler

Now if you want to access lib/profiler.so from inside of archive.tgz you would use a path of profiler/lib/profiler.so.","25/Jun/12 18:40;jay23jack;Hi Robert,

I don't quite understand your approach, because we need to provide the
path of the profiler libraries to the TaskTracker instead of the
tasks. So if the libraries appear in the task' working directory, how
can the TaskTracker find it when launching the task? And currently TT
doesn't look into the profile parameters to see if there is any
distributed cache entry.","25/Jun/12 19:01;acmurthy;Jie - You can just add the profiler params to mapred.(map,reduce).child.java.opts and the TT will add it to the tasks' jvm launch cmd.","25/Jun/12 19:26;revans2;Jie,

I am confused too.  Do you want to profile the task or the task tracker?  If you want to profile the task you can do a combination of what I said and what Arun is saying.

{noformat}
hadoop jar ... -Dmapred.map.child.java.opts=""... -agentlib:yjpagent"" -Dmapred.reduce.child.java.opts=""... -agentlib:yjpagent"" -Dmapred.child.env=""... LD_LIBRARY_PATH=yourkit/bin/linux-x86-32"" -archive '/path/to/yourkit.tgz#yourkit' ...
{noformat}

This is just thrown together from memory and from http://www.yourkit.com/docs/80/help/agent.jsp so some of the parameter options may be wrong, but it should point you down the correct path.","26/Jun/12 23:42;jay23jack;Thanks Arun and Robert. 

I meant profiling tasks and actually I'm using [Hadoop profiling|http://hadoop.apache.org/common/docs/r0.20.2/mapred_tutorial.html#Profiling] by setting mapred.task.profile.{maps|reduces} so Hadoop will automatically send back the profiling output files.

The reason why your approach couldn't work is that, currently it is task's responsibility to set up the symlink for the distributed cache, so when TT launches the task, the symlink is not set up yet. Note TaskRunner#setupWorkDir is called in Child#main.

So one solution is to create the symlink before launching tasks, or we can replace the distributed cache entry found in the profiling parameters with the localized path for this particular problem?","28/Jun/12 01:11;jay23jack;One way is to include the profiler library into the job jar and use relative path like ""../../foo.library"" to locate it.

Thanks Deveraj, Sid, Vinod and everyone!","28/Jun/12 16:14;revans2;@Jie on 1.0 that may work, but I don't know if we are exploding the job.jar for 2.0.  I think we need to have a JIRA for creating the symlinks before launching at least in 2.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Recovery of ResourceManager,MAPREDUCE-2713,12514747,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,mahadev,sharadag,sharadag,20/Jul/11 06:01,18/Jun/12 17:06,12/Jan/21 09:52,18/Jun/12 17:06,0.23.1,,,,,,,,mrv2,,,,,,0,,,,,ResourceManager needs to recover from crashes to the state where it left off. All running applications should be able to join back the restarted RM. All running containers should not be affected and continue to run.,,atm,bikassaha,devaraj,eli,qwertymaniac,revans2,tomwhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-06-18 17:06:40.864,,,false,,,,,,,,,,,,,,,,,,67425,,,,,Mon Jun 18 17:06:40 UTC 2012,,,,,,,"0|i0jj8f:",112046,,,,,,,,,,,,,,,,,,,,,"18/Jun/12 17:06;bikassaha;Resolving as dup of MAPREDUCE-4326",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide NullOutputCommitter,MAPREDUCE-3471,12532641,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,kam_iitkgp,kam_iitkgp,24/Nov/11 18:22,20/Feb/12 08:18,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"Hadoop by default provides only FileOutputCommitter and the same has been used internally except in the NOF.
There are cases where using FOC is not much appropriate.

Example
DBOutputFormat instantiates FOC, though it does nothing.
I think using NOC will be much more appropriate here.

And also it can be used along with other OFs, where taking special care of the task/job output directories may not be required.",,raviteja,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/12 08:18;kam_iitkgp;MAPREDUCE-3471.patch;https://issues.apache.org/jira/secure/attachment/12515226/MAPREDUCE-3471.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,218374,,,,,Mon Feb 20 08:18:07 UTC 2012,,,,,,,"0|i00hfb:",756,,,,,,,,,,,,,,,,,,,,,"20/Feb/12 08:18;kam_iitkgp;I encountered the importance of *NullOutputCommitter*, when I was debugging [MAPREDUCE-3130|https://issues.apache.org/jira/browse/MAPREDUCE-3130]. I fixed this by setting *OutputCommitter* class of FileOutputFormat as *NullOutputCommitter*. Again for setting this, we need to fix [MAPREDUCE-2493|https://issues.apache.org/jira/browse/MAPREDUCE-2493].

Also, OC implementation of NOF returns true for isRecoverySupported(). Why do we need this?

Providing patch, thinking it may be useful.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Speculative Execution: Put to sleep a reducer when queue is full and a mapper needs to speculate,MAPREDUCE-3403,12531518,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,patwhitey2007,patwhitey2007,15/Nov/11 19:10,17/Jan/12 16:42,12/Jan/21 09:52,,0.23.0,,,,,,,,job submission,,,,,,0,,,,,"When forcing multiple reduce tasks to be launched by applying the setNumReduceTasks() method on a Job object, and
running on input data which has one significantly longer map (and consequently reduce) task;

- a speculative reduce task was not launched, even with a longer running reducer only 4 reduce tasks were launched

- the spec launch of map tasks was inhibited by the setNumReduceTasks() method applied, so even with
-Dmapreduce.job.maps.speculative.execution=true we only had 4 map tasks launched. The exact same code with the
setNumReduceTasks() method taken out, and on the same input data set, consistently launched 5 mappers as expected.

Testing info:

3. modified WordCount to force 4 reducers being launched, by adding:

    job.setNumReduceTasks(4); // hardwire 4 reducers for now
    System.out.println(""\nTESTDEBUG: using 4 reduce tasks for now\n\n"");

to the Job object. This causes 4 reduce tasks to be launched, oddly though it inhibits the map task from speculative
launch. So the same job code, without the setNumReduceTasks() method, will launch 5 mappers as described in case #2.
When this method is added, that same job will only launch 4 mappers, as well as 4 reducers, otherwise the job
successfully completes.

output snippet with setNumReduceTasks():

        org.apache.hadoop.mapreduce.JobCounter
                TOTAL_LAUNCHED_MAPS=4
                TOTAL_LAUNCHED_REDUCES=4
                RACK_LOCAL_MAPS=4
                SLOTS_MILLIS_MAPS=190787
                SLOTS_MILLIS_REDUCES=572554","Hadoop version is: Hadoop 0.23.0.1110031628
10 node test cluster",devaraj,revans2,sseth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-01-17 16:20:48.866,,,false,,,,,,,,,,,,,,,,,,217254,,,,,Tue Jan 17 16:34:57 UTC 2012,,,,,,,"0|i0e6lz:",80833,,,,,,,,,,,,,,,,,,,,,"17/Jan/12 16:20;epayne;The reason we see this behavior is because the queue's containers are all in use at the time of the speculation. So, the speculated mapper gets requested, but as long as all of the containers are occupied, the speculated mapper task will never be scheduled.

For e.g., 
|Requested Tasks|Used Containers|Completed Tasks|
|Map0 prime|Map0 (long running||
||Reduce 0|Map1, Map2|
||Reduce 1|Map3|


If all of the containers of a queue are consumed with running mapred task attempts (both maps and reduces) and
speculation of a mapper needs to run, would it be reasonable to have the MRAM kill one of the reduce tasks so the
speculative map task can run?
","17/Jan/12 16:26;epayne;Sorry, I didn't create the proper table in the previous comment.

Let's try that again...

| Requested Task Attempt | Used Containers | Completed Task Attempts |
| Map0 Prime | Map0 (long running | |
| | Reduce0 | Map1, Map2 |
| | Reduce1 | Map3 |

Basically, the situation is that the queue is full and a speculated map task attempt needs to run, which holds up the whole job waiting for the blocked mapper.

I am retooling this Jira to be an enhancement request to explore this issue.

","17/Jan/12 16:34;epayne;What we really may want is to pick a reduce to save its state, put to sleep, and give up its resources. Then, when a container becomes available abain, wake it back up where it left off.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Map/Reduce job with SequenceFileOutputFormat should be able to add user specified metadata to the output file,MAPREDUCE-218,12361519,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,,runping,runping,29/Jan/07 23:45,17/Jan/12 03:51,12/Jan/21 09:52,17/Jan/12 03:51,,,,,,,,,,,,,,,0,,,,,"When creating a map/reduce job with SequenceFileFormat, 
the user would like to add some metada to the output files automatically. 
In particular, if the output value class was a JuteRecord class generated from a Jute IDL, 
we would like to add JUTE_IDL/IDL_STRING as a attribute/value pair of the metadata.
This way, the output files will be self describing: 
When an application that tries to use the files may not have the value class with it. 
But the application can use Jute tool to generate the classes on demand.
Or better yet, the SequenceFile record reader may be able to do that automatically.


",,qwertymaniac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2007-01-29 23:49:34.072,,,false,,,,,,,,,,,,,,,,,,148606,,,,,Tue Jan 17 03:51:11 UTC 2012,,,,,,,"0|i09893:",51769,,,,,,,,,,,,,,,,,,,,,"29/Jan/07 23:49;cutting;Could this be done as a subclass of SequenceFileOutputFormat, RecordOutputFormat?","29/Jan/07 23:52;runping;Certainly. That perhaps the most convenient way to do.
","16/Jan/12 10:40;qwertymaniac;Users may override/extend the SFOF if they want to utilize SequenceFile.Writer's new metadata methods/features.","17/Jan/12 03:51;acmurthy;SequenceFiles already have a metadata sections.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Map directly to HDFS or reduce(),MAPREDUCE-201,12361494,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Not A Problem,,nuggetwheat,nuggetwheat,29/Jan/07 18:54,16/Jan/12 10:37,12/Jan/21 09:52,16/Jan/12 10:37,,,,,,,,,,,,,,,0,,,,,"For situations where you know that the output of the Map phase is already aggregated (e.g. the input is the output of another Map-reduce job and map() preserves the aggregation), then there should be a way to tell the framework that this is the case so that it can pipe the map() output directly to the reduce() function, or HDFS in the case of IdentityReducer.  This will probably require forcing the number of map tasks to equal the number of reduce tasks.  This will save the disk I/O required to generate intermediate files.
",all,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2007-01-29 19:19:51.504,,,false,,,,,,,,,,,,,,,,,,148591,,,,,Mon Jan 16 10:37:43 UTC 2012,,,,,,,"0|i0irpz:",107585,,,,,,,,,,,,,,,,,,,,,"29/Jan/07 19:19;cutting;Note that there is a standard workaround for this: have your map function write output to DFS as a side-effect.  One could write a MapRunnable that automates this, using the job's specified OutputFormat to create RecordWriters, etc.

> This will probably require forcing the number of map tasks to equal the number of reduce tasks.

Actually, the ability to keep a large number of maps (one per block) and a much smaller number of outputs would be the primary reason I can see for adding this feature to the framework, as opposed to using something like the workaround mentioned above.  If we place reduces to nodes or racks where their input dominates (as discussed in HADOOP-939 comments), then this could be implemented by simply specifying a partition method that returns the hash of the map node name.","29/Jan/07 19:58;nuggetwheat;> Actually, the ability to keep a large number of maps (one per block) and a much smaller number of outputs would be the primary reason I can see for adding this feature [...]

Seems like you'd need an atomic record append API to handle this.

> If we place reduces to nodes or racks where their input dominates (as discussed in HADOOP-939 comments), then this could be implemented by simply specifying a partition method that returns the hash of the map node name.

I guess the primary reason for this enhancement would be to avoid writing and subsequently reading intermediate files thereby reducing disk load on the system as a whole.  The map-to-hdfs workaround sounds reasonable.  If you want to just run a reduce without generating intermediate files, then the reduce task needs to be able to pull from HDFS.  Unless I didn't follow your logic correctly on HADOOP-939, it seems like this optimization is orthogonal.
","29/Jan/07 20:29;cutting;> Seems like you'd need an atomic record append API to handle this.

Using the numbers from HADOOP-939, one could eliminate steps (c) and (d), saving 11 out of 27 seconds, by locating reduces near maps.  If you had atomic append then you could save an additional second or two by not buffering things locally.  So there'd be some improvement by eliminating reduce altogether, but not huge.  The reduce would effectively take the place of atomic append.

> Unless I didn't follow your logic correctly on HADOOP-939, it seems like this optimization is orthogonal.

My argument is that locating reduces near maps will substantially help in both this case (no reduce needed) and in  HADOOP-939 (no sort needed), and that it will also help applications which only lean lightly on sort and reduce.  That optimization gets the vast majority of the theoretical speedup with no public API changes, no special control flow, etc. and works for more applications.  So I'd give that higher priority.","29/Jan/07 20:48;nuggetwheat;Ok, I see your reasoning.  Go ahead and downgrade this one or remove it entirely if you think it's not worth doing.
","16/Jan/12 10:37;qwertymaniac;This should've been closed out before but was not. Closing out now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job setup and take down on Nodes,MAPREDUCE-216,12352553,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Not A Problem,omalley,breed,breed,05/Oct/06 22:39,16/Jan/12 10:04,12/Jan/21 09:52,16/Jan/12 10:04,,,,,,,,,,,,,,,0,,,,,"It would be nice if there was a hook for doing job provisioning and cleanup on compute nodes. The TaskTracker implicitly knows when a job starts (a task for the job is received) and pollForTaskWithClosedJob() will explicitly say that a job is finished if a Map task has been run (If only Reduce tasks have run and are finished I don't think pollForTaskWithClosedJob() will return anything will it?), but child Tasks do not get this information.

It would be nice if there was a hook so that programmers could do some provisioning when a job starts and cleanup when a job ends. Caching addresses some of the provisioning, but in some cases a helper daemon may need to be started or the results of queries need to be retrieved and having startJob(), finishJob() callbacks that happen exactly once for each node that runs part of the job would be wonderful.",,eric14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2006-10-06 16:13:32.0,,,false,,,,,,,,,,,,,,,,,,148604,,,,,Mon Jan 16 10:04:45 UTC 2012,,,,,,,"0|i0irfz:",107540,,,,,,,,,,,,,,,,,,,,,"06/Oct/06 16:13;cutting;It sounds like you want to run user code in the TaskTracker.  Right now, user code only runs in per-task child processes.  We also run some in the JobTracker, but would like to get rid of that, so that no long-running daemons run user code, as discussed in the following thread:

http://www.mail-archive.com/hadoop-dev%40lucene.apache.org/msg03967.html

","06/Oct/06 17:48;breed;No. I'm very against running code in the Trackers (as my mail indicates :). The idea would be that you would spawn off a child process at the beginning of a job and kill it at the end. (Or some variation on that theme.)","16/Jan/12 10:04;qwertymaniac;Some level of this can be achieved with the OutputCommitters today. Closing out.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generic 'Sort' Infrastructure for Map-Reduce framework.,MAPREDUCE-221,12345459,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Incomplete,acmurthy,acmurthy,acmurthy,06/Jul/06 12:57,16/Jan/12 09:16,12/Jan/21 09:52,16/Jan/12 09:16,,,,,,,,,,,,,,,0,,,,,"It would be useful to add a generic *sort* infrastructure to the Map-Reduce framework to ease usage.
Specifically the idea to add a fairly generic and powerful *comparator* which can be configured by the user to meet his specific needs.

Spec:
--------
 
  The proposal is to model generic (uber) comparator along the lines of the the standard unix *sort* command. The comparator provides the following (configurable) functionality:

  a) Separator for breaking up the data (stream) into 'columns'.
  b) Multiple key ranges for specifying priorities of 'columns'. (ala --keys/-k option of unix sort i.e. -k 2,3 -k 1,4 etc.)
  c) A variant of a) to let user specify byte range-boundaries without using a separator for 'columns'.
  d) Option to sort 'reverse'.
  e) Option to do a 'stable' sort i.e. don't do a last-ditch comparision of all bytes if all key ranges match.
  f) Option to do 'numeric' comparisions instead of lexicographical comparisions?

  Of course all these are optional with the default behaviour as-is today.

     - * - * -

 Anything more/less?

thanks,
Arun
",,jly,johanoskarsson,lianhuiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2006-07-07 00:58:07.0,,,false,,,,,,,,,,,,,,,,,,148608,,,,,Mon Jan 16 09:16:08 UTC 2012,,,,,,,"0|i0iran:",107516,,,,,,,,,,,,,,,,,,,,,"07/Jul/06 00:58;omalley;This should be done by implementing a new WritableComparator, which can be selected by calling JobConf.setKeyOutputComparator(). It does not need to change the framework's sort code. The configuration should be done via the job conf along the lines of:

conf.set(""comparator.generic.utf8.keys"",  ""2,4,3""); // columns 2, 4, and 3 are the sort key
conf.set(""comparator.generic.utf8.deliminator"", "" ""); // how to split columns
conf.setBoolean(""comparator.generic.utf8.reverse"", true); // sort backwards

I assume this comparator is just limited to keys that are UTF8. The corresponding comparator for Hadoop record io would also make sense, but there the fields would be given by name. For example,

  class Foo {
     int field1;
     int field2;
     ustring field3;
  }

You'd like to set ""comparator.generic.record.keys"" to ""field3,field2"". But the record io generic comparator is obviously a different bug. *smile*

You won't be able to implement a stable sort without a lot of work. Do you have applications that need stable sorts?","07/Jul/06 02:29;eric14;I'd like to see the comparators config explicitly modelled after unix sort unless a better model exists in the java world?  Could we add the spec for how it is configured here?  Maybe a single string with unix sort options on it?

This should not be limited to utf8.  We should be able to split on bytes or lengths and handle binary keys too.  ","16/Jan/12 09:16;qwertymaniac;This has grown stale, and am closing it out.

While most of this can be (rather easily) done with the pluggable APIs we provide today, if you still feel the framework ought to carry this instead in generic fashion (which sort of limits it, given users' many data formats Hadoop crunches on today), please do reopen.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic information fed into Hadoop for controlling execution of a submitted job,MAPREDUCE-1928,12468864,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,ramang,ramang,08/Jul/10 22:10,03/Jan/12 13:43,12/Jan/21 09:52,,0.20.3,,,,,,,,job submission,jobtracker,tasktracker,,,,0,,,,,"Currently the job submission protocol requires the job provider to put every bit of information inside an instance of JobConf. The submitted information includes the input data (hdfs path) , suspected resource requirement, number of reducers etc.  This information is read by JobTracker as part of job initialization. Once initialized, job is moved into a running state. From this point, there is no mechanism for any additional information to be fed into Hadoop infrastructure for controlling the job execution. 
                           The execution pattern for the job looks very much static from this point. Using the size of input data and a few settings inside JobConf, number of mappers is computed. Hadoop attempts at reading the whole of data in parallel by launching parallel map tasks. Once map phase is over, a known number of reduce tasks (supplied as part of  JobConf) are started. 

Parameters that control the job execution were set in JobConf prior to reading the input data. As the map phase progresses, useful information based upon the content of the input data surfaces and can be used in controlling the further execution of the job. Let us walk through some of the examples where additional information can be fed to Hadoop subsequent to job submission for optimal execution of the job. 

I) ""Process a part of the input , based upon the results decide if reading more input is required "" 
    In a huge data set, user is interested in finding 'k' records that satisfy a predicate, essentially sampling the data. In current implementation, as the data is huge, a large no of mappers would be launched consuming a significant fraction of the available map slots in the cluster. Each map task would attempt at emitting a max of  'k' records. With N mappers, we get N*k records out of which one can pick any k to form the final result. 
   This is not optimal as:
   1)  A larger number of map slots get occupied initially, affecting other jobs in the queue. 
   2) If the selectivity of input data is very low, we essentially did not need scanning the whole of data to form our result. 
        we could have finished by reading a fraction of input data, monitoring the cardinality of the map output and determining if 
       more input needs to be processed.  
   
   Optimal way: If reading the whole of input requires N mappers, launch only 'M' initially. Allow them to complete. Based upon the statistics collected, decide additional number of mappers to be launched next and so on until the whole of input has been processed or enough records have been collected to for the results, whichever is earlier. 
 
 
II)  ""Here is some data, the remaining is yet to arrive, but you may start with it, and receive more input later""
     Consider a chain of 2 M-R jobs chained together such that the latter reads the output of the former. The second MR job cannot be started until the first has finished completely. This is essentially because Hadoop needs to be told the complete information about the input before beginning the job. 
    The first M-R has produced enough data ( not finished yet) that can be processed by another MR job and hence the other MR need not wait to grab the whole of input before beginning.  Input splits could be supplied later , but ofcourse before the copy/shuffle phase.
 

III)  "" Input data has undergone one round of processing by map phase, have some stats, can now say of the resources 
        required further"" 
       Mappers can produce useful stats about of their output, like the cardinality or produce a histogram describing distribution of output . These stats are available to the job provider (Hive/Pig/End User) who can 
      now determine with better accuracy of the resources (memory requirements ) required in reduction phase,  and even the number    of  reducers or may even alter the reduction logic by altering the reducer class parameter. 

 
In a nut shell, certain parameters about a job are governed by the input data and  the intermediate results produced and hence need to be overridden as job progresses. Hadoop does not allow such information to be fed dynamically. Hence job execution may not always be optimal. 

I would like to get feedback from the Hadoop community about the above proposal and if any similar effort is already underway. 
If we agree, as a next step I would like to discuss the implementation details that I have worked out end-to-end. 
",,aah,acmurthy,ahmed.radwan,anty,ashutoshc,cdouglas,cutting,devaraj,dhruba,hong.tang,jrideout,jsensarma,lianhuiwang,prasadc,ramang,rvadali,schen,slider,tlipcon,vinodkv,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7257600,7257600,,0%,7257600,7257600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-07-09 19:45:48.107,,,false,,,,,,,,,,,,,,,,,,149869,Incompatible change,,,,Tue Jan 03 13:43:49 UTC 2012,,,,,,,"0|i0e8fb:",81127,,,,,,,,,,,Job Submission,,,,,,,,,,"09/Jul/10 19:45;ted_yu;Case 2 is related to MAPREDUCE-1849
One possibility is to combine the two MapReduces into one during the optimization step.","14/Jul/10 13:46;lordjoe;Another possible use has to do with adjusting parameters to avoid failures. I have an issue where a reducer is running out of memory. If I was aware that 
certain  keys lead to this failure I could take steps such as sampling data rather than processing the whole set do I would add access to data about failures","14/Jul/10 19:27;jsensarma;to add to #1 - we may be able to change the split size based on the observed selectivity of an ongoing job (ie. add splits with larger/smaller size depending on stats from the first set of splits). It's possible that Hadoop may want to do this as part of the basic framework (by exploiting any mechanisms provided here).

This is a huge win for a framework like Hive. It would drastically reduce the amount of wasted work (limit N queries) and spawning unnecessarily large number of mappers (unknown selectivity) - just to name to obvious use cases. 

Can you supply a more concrete proposal in terms of api changes?","03/Jan/12 13:43;masokan;Please take a look at MAPREDUCE-2454 for another approach to solve the problem addressed in this Jira.  The problem in the current Hadoop MR framework is that when the number of reducers is greater than 0, a sort is always performed.  Sorting requires reading the entire input data.  As of now, there is no way to bypass the sort.

MAPREDUCE-2454 makes the sort pluggable and refactors the current sort code so that it is the default plugin.  An external sort plugin called NullSortPlugin is in the works.  It will bypass the sort and just copy the <KEY,VALUE> pairs from the Mapper to the Reducer.  This will enable one to stop a job after a certain number of records are processed without reading the entire input.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Authentication between pipes processes and java counterparts.,MAPREDUCE-1733,12463160,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,jnp,jnp,jnp,27/Apr/10 23:20,12/Dec/11 06:20,12/Jan/21 09:52,22/Jul/10 22:36,,,,,,0.22.0,,,,,,,,,0,,,,,The connection between a pipe process and its parent java process should be authenticated.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/10 01:17;jnp;MR-1733-y20.1.patch;https://issues.apache.org/jira/secure/attachment/12443030/MR-1733-y20.1.patch","07/May/10 23:14;jnp;MR-1733-y20.2.patch;https://issues.apache.org/jira/secure/attachment/12444009/MR-1733-y20.2.patch","09/May/10 08:28;jnp;MR-1733-y20.3.patch;https://issues.apache.org/jira/secure/attachment/12444054/MR-1733-y20.3.patch","12/Jul/10 20:32;jnp;MR-1733.5.patch;https://issues.apache.org/jira/secure/attachment/12449281/MR-1733.5.patch","22/Jul/10 21:20;jnp;MR-1733.6.patch;https://issues.apache.org/jira/secure/attachment/12450219/MR-1733.6.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,2010-07-16 18:15:01.757,,,false,,,,,,,,,,,,,,,,,,37035,Incompatible change,,,,Fri Oct 29 02:03:41 UTC 2010,,,,,,,"0|i02qhb:",13891,This jira introduces backward incompatibility. Existing pipes applications  MUST be recompiled with new hadoop pipes library once the changes in this jira are deployed.  ,,,,,,,,,,,,,,,,,,,,"28/Apr/10 01:17;jnp;Patch for hadoop-20 added.","12/Jul/10 20:32;jnp;Patch for the trunk.","16/Jul/10 18:15;ddas;+1 (subject to hudson)","16/Jul/10 18:27;ddas;Looked like hudson missed this patch. Re-submitting on behalf of Jitendra.","21/Jul/10 21:18;jnp;ant test, javadoc, javac warnings and findbugs were run manually.

TestRumenJobTraces failed which is unrelated to this patch.

javadoc generates warning in Counter.java which is also unrelated.","22/Jul/10 21:20;jnp;Earlier patch had an issue with automatically generated configure scripts. Rest of the patch is exactly same.","22/Jul/10 22:36;ddas;I just committed this. Thanks, Jitendra!","29/Oct/10 02:03;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #523 (See [https://hudson.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/523/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
streaming should support running on background,MAPREDUCE-1517,12456926,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,sinofool,sinofool,sinofool,21/Feb/10 06:01,12/Dec/11 06:19,12/Jan/21 09:52,28/Sep/10 08:19,0.22.0,,,,,0.22.0,,,contrib/streaming,,,,,,0,,,,,"StreamJob submit the job and use a while loop monitor the progress.
I prefer it running on background.

Just add ""&"" at the end of command is a alternative solution, but it keeps a java process on client machine.
When submit hundreds jobs at the same time, the client machine is overloaded.

Adding a -background option to StreamJob, tell it only submit and don't monitor the progress.
",,amareshwari,aw,jrideout,qwertymaniac,sinofool,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/10 05:14;sinofool;contrib-streaming-background-with-test-with-license.patch;https://issues.apache.org/jira/secure/attachment/12455638/contrib-streaming-background-with-test-with-license.patch","27/Sep/10 05:29;sinofool;contrib-streaming-background-with-test-with-license2.patch;https://issues.apache.org/jira/secure/attachment/12455640/contrib-streaming-background-with-test-with-license2.patch","17/Sep/10 08:33;sinofool;contrib-streaming-background-with-test.patch;https://issues.apache.org/jira/secure/attachment/12454833/contrib-streaming-background-with-test.patch","14/Sep/10 01:37;sinofool;contrib-streaming-background.patch;https://issues.apache.org/jira/secure/attachment/12454512/contrib-streaming-background.patch","13/Sep/10 10:12;sinofool;contrib-streaming-background.patch;https://issues.apache.org/jira/secure/attachment/12454442/contrib-streaming-background.patch","17/Sep/10 08:33;sinofool;hadoop-mapred-patch-logs.tar.gz;https://issues.apache.org/jira/secure/attachment/12454834/hadoop-mapred-patch-logs.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,2010-02-22 09:24:56.942,,,false,,,,,,,,,,,,,,,,,,149573,Reviewed,,,,Fri Oct 29 02:04:33 UTC 2010,,,,,,,"0|i0jgtr:",111656,Adds -background option to run a streaming job in background.,,,,,,,,,,,,,,,,,,,,"21/Feb/10 06:05;sinofool;this patch adds -background to contrib-streaming.
","22/Feb/10 09:24;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12436479/contrib-streaming-background.patch
  against trunk revision 912471.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/468/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/468/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/468/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/468/console

This message is automatically generated.","22/Feb/10 20:13;aw;Am I correct that this patch makes -background the default?  I'm pretty certain that'll break things.","24/Feb/10 05:45;sinofool;Allen: the default behavior remains unchanged.
Two tests failed:
1
 org.apache.hadoop.mapred.TestDebugScript.testDebugScript  (from TestDebugScript) 
Timeout occurred. Please note the time in the report does not reflect the time until the timeout.

2
 org.apache.hadoop.mapred.TestMiniMRLocalFS.testWithLocal  (from TestMiniMRLocalFS) 
It seems problem of trunk, MAPREDUCE-1510 also failed with the same message.

Should I resume the progress and make hudson test it again?
","24/Feb/10 14:30;sinofool;resume progress asking hudson re-test it.
","06/Jul/10 10:19;amareshwari;Bochun, Can you update the patch to trunk and upload again?

One comment on the patch :
* Update the -background option in exitUsage() with proper description and specify it as optional.

","13/Sep/10 10:12;sinofool;match current(rev 996456) trunk","13/Sep/10 10:22;sinofool;Amareshwari, A new patch for current trunk is now available and the old files are deleted.","13/Sep/10 11:03;amareshwari;bq. A new patch for current trunk is now available and the old files are deleted. 
Thanks Bochun for the patch. But, next time please do not delete the old patch files because they would be useful for tracking the history.

Changes in the patch look good. Minor nits:
* Can you change the help message for the option to ""Submit the job and don't wait *till* it completes."" Also, log message should say ""Job is running *in* background"".
* Also, can you add the option to the table in src/docs/src/documentation/content/xdocs/streaming.xml in ""Streaming Command Options"" section.
","14/Sep/10 01:37;sinofool;Thanks Amareshwari, text changes are included in this new patch.","14/Sep/10 09:41;amareshwari;Patch looks good.

Can you run test-patch and ant test and upload the results? ","17/Sep/10 06:07;sinofool;test-patch report: 

-1 overall.  

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 system tests framework.  The patch passed system tests framework compile.

","17/Sep/10 08:33;sinofool;Added TestStreamingBackground.

test-patch report is:
-1 overall.  

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 4 release audit warnings (more than the trunk's current 2 warnings).

    +1 system tests framework.  The patch passed system tests framework compile.



and the ""-1 release audit"" is:
$ cat releaseAuditDiffWarnings.txt 

1,2d0
< [rat:report]  !????? /data/home/bochun.bai/code/apache/hadoop-mapred-trunk-testwork/build/hadoop-mapred-PATCH-contrib-streaming-background-with-test.patch/src/contrib/streaming/src/test/org/apache/hadoop/streaming/DelayEchoApp.java
< [rat:report]  !????? /data/home/bochun.bai/code/apache/hadoop-mapred-trunk-testwork/build/hadoop-mapred-PATCH-contrib-streaming-background-with-test.patch/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingBackground.java


The patch logs is attached.","27/Sep/10 05:02;amareshwari;Can you add apache license headers for src test files, both TestStreamingBackground.java and DelayEchoApp.java? You can copy it from any other src file. Then, you won't see release audit warnings.","27/Sep/10 05:14;sinofool;license header added.","27/Sep/10 05:24;amareshwari;Your patch is missing other files. Can you regenerate the patch properly?","27/Sep/10 05:29;sinofool;Previous patch contains only test changes.
Sorry for that.

This is a full patch.","27/Sep/10 07:06;sinofool;test-patch.sh on trunk Revision: 997967

+1 overall.  

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 system tests framework.  The patch passed system tests framework compile.

","28/Sep/10 08:19;amareshwari;+1
All the tests passed with the patch.

I just committed this. Thanks Bochun !","29/Oct/10 02:04;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #523 (See [https://hudson.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/523/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a metrics to track the number of heartbeats processed,MAPREDUCE-1680,12461332,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,dking,hong.tang,hong.tang,07/Apr/10 00:28,12/Dec/11 06:19,12/Jan/21 09:52,06/May/10 02:22,,,,,,0.22.0,,,jobtracker,,,,,,0,,,,,It would be nice to add a metrics that tracks the number of heartbeats processed by JT.,,acmurthy,cdouglas,hammer,hong.tang,lianhuiwang,ravidotg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/10 18:20;dking;ASF.LICENSE.NOT.GRANTED--mapreduce-1680--2010-04-08-for-trunk.patch;https://issues.apache.org/jira/secure/attachment/12441632/ASF.LICENSE.NOT.GRANTED--mapreduce-1680--2010-04-08-for-trunk.patch","13/Apr/10 16:19;dking;ASF.LICENSE.NOT.GRANTED--mapreduce-1680--2010-04-08.patch;https://issues.apache.org/jira/secure/attachment/12441621/ASF.LICENSE.NOT.GRANTED--mapreduce-1680--2010-04-08.patch","08/Apr/10 00:15;dking;mapreduce-1680--2010-04-07.patch;https://issues.apache.org/jira/secure/attachment/12441093/mapreduce-1680--2010-04-07.patch","29/Apr/10 21:57;dking;mapreduce-1680--2010-04-29.patch;https://issues.apache.org/jira/secure/attachment/12443238/mapreduce-1680--2010-04-29.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2010-04-08 01:38:41.166,,,false,,,,,,,,,,,,,,,,,,37070,Reviewed,,,,Thu May 06 02:22:44 UTC 2010,,,,,,,"0|i02qr3:",13935,Added a metric to track number of heartbeats processed by the JobTracker.,,,,,,,,,,,,,,,,,,,,"08/Apr/10 01:38;acmurthy;Some feedback:

# We do not need both JobTrackerInstrumentation.heartbeat and JobTrackerInstrumentation.heartbeats(int), just one of the apis should suffice - say JobTrackerInstrumentation.heartbeat(long increment).
# You'll need to get JobTrackerMetricsInst.doUpdates to send them out by incrementing a metric, say ""heartbeat""","13/Apr/10 16:19;dking;This patch is meant for 0.20 and trunk","13/Apr/10 16:26;dking;I hereby license the attachment ""mapreduce-1680--2010-04-08.patch (3 kB)"" to ASF.

[That checkbox appears to have disappeared from the JIRA file attachment tool.]","13/Apr/10 18:20;dking;The previous patch was intended for 0.20 branches.  This one is intended for Trunk.","26/Apr/10 07:43;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12441632/mapreduce-1680--2010-04-08-for-trunk.patch
  against trunk revision 937922.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/137/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/137/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/137/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/137/console

This message is automatically generated.","27/Apr/10 09:24;amareshwari;Patch looks fine. Can you update the testcase TestJobTrackerInstrumentation.testMetrics() to test the new metric ?","29/Apr/10 21:57;dking;OK, this adds a clause to one of the test cases in {{TestJobTrackerInstrumentation}}, for heartbeats.
","30/Apr/10 03:46;amareshwari;+1 patch looks fine to me.","30/Apr/10 12:25;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12443238/mapreduce-1680--2010-04-29.patch
  against trunk revision 939505.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/159/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/159/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/159/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/159/console

This message is automatically generated.","06/May/10 02:22;cdouglas;+1

I committed this. Thanks, Dick!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobTracker should issue a delegation token only for kerberos authenticated client,MAPREDUCE-1516,12456925,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,jnp,jnp,jnp,21/Feb/10 03:15,12/Dec/11 06:18,12/Jan/21 09:52,11/Jun/10 21:47,,,,,,0.22.0,,,,,,,,,0,,,,,Delegation tokens should be issued only if the client is kerberos authenticated.,,qwertymaniac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/10 23:52;jnp;ASF.LICENSE.NOT.GRANTED--MR-1516.4.patch;https://issues.apache.org/jira/secure/attachment/12441667/ASF.LICENSE.NOT.GRANTED--MR-1516.4.patch","14/Apr/10 20:23;jnpandey;ASF.LICENSE.NOT.GRANTED--MR-1516.5.patch;https://issues.apache.org/jira/secure/attachment/12441765/ASF.LICENSE.NOT.GRANTED--MR-1516.5.patch","15/Apr/10 07:52;jnp;ASF.LICENSE.NOT.GRANTED--MR-1516.6.patch;https://issues.apache.org/jira/secure/attachment/12441794/ASF.LICENSE.NOT.GRANTED--MR-1516.6.patch","21/Feb/10 03:18;jnp;MR-1516.1.patch;https://issues.apache.org/jira/secure/attachment/12436476/MR-1516.1.patch","26/Feb/10 06:41;jnp;MR-1516.2.patch;https://issues.apache.org/jira/secure/attachment/12437131/MR-1516.2.patch","26/Feb/10 23:16;jnp;MR-1516.3.patch;https://issues.apache.org/jira/secure/attachment/12437273/MR-1516.3.patch","08/Jun/10 23:45;jnp;MR-1516.8.patch;https://issues.apache.org/jira/secure/attachment/12446643/MR-1516.8.patch","11/Jun/10 17:56;jnp;MR-1516.9.patch;https://issues.apache.org/jira/secure/attachment/12446885/MR-1516.9.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,2010-04-14 22:50:35.75,,,false,,,,,,,,,,,,,,,,,,149572,,,,,Fri Jun 11 21:47:43 UTC 2010,,,,,,,"0|i0jgtj:",111655,,,,,,,,,,,,,,,,,,,,,"25/Feb/10 21:18;jnp;tests, test-patch, findbugs, javadoc and javac warnings were run manually.","26/Feb/10 23:16;jnp;New patch uploaded accommodating the interface changes in HADOOP-6580","13/Apr/10 23:52;jnp;Updated patch re-based against latest trunk.","13/Apr/10 23:53;jnp;MR-1516.4.patch is submitted for hudson tests.","14/Apr/10 22:50;boryas;1. having two asserts doesn't make sense:
Assert.assertTrue(token != null);
Assert.fail(""Delegation token should not be issued without Kerberos authentication"");

otherwise looks good.","15/Apr/10 07:52;jnp;New patch addressing the comment.","15/Apr/10 07:53;jnp;test-patch results

     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

","15/Apr/10 07:54;jnp;I ran ant tests. All tests passed except TestMiniMRChildTask, which also fails without this patch.","16/Apr/10 00:45;ddas;Sigh .. I wish we hadn't duplicated the methods isAllowedDelegationTokenOp and getConnectionAuthenticationMethod in MR and HDFS, and instead defined something that would address what the methods provide in Common. Could you please take care of this..","09/Jun/10 01:42;jnp;New patch uploaded

test-patch results

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
","11/Jun/10 17:57;jnp;ant test was run manually. All tests pass except TestMiniMRChildTask, which also fails without this patch.","11/Jun/10 21:47;ddas;I just committed the changes in JobTracker.java to the trunk. Although the testcase validates the patch, I didn't commit the testcase since it seems difficult to maintain this testcase in the long run (it switches the authentication method from simple to kerberos in the middle of the testcase).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for Sleep Jobs in gridmix,MAPREDUCE-1594,12458899,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,rksingh,rksingh,rksingh,12/Mar/10 06:02,12/Dec/11 06:18,12/Jan/21 09:52,14/Jul/10 09:24,,,,,,0.22.0,,,contrib/gridmix,,,,,,0,,,,,Support for Sleep jobs in gridmix,,cdouglas,hong.tang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/10 10:44;rksingh;1376-5-yhadoop20-100-3.patch;https://issues.apache.org/jira/secure/attachment/12440717/1376-5-yhadoop20-100-3.patch","07/Apr/10 01:25;hong.tang;1594-diff-4-5.patch;https://issues.apache.org/jira/secure/attachment/12440984/1594-diff-4-5.patch","02/Apr/10 03:51;rksingh;1594-yhadoop-20-1xx-1-2.patch;https://issues.apache.org/jira/secure/attachment/12440571/1594-yhadoop-20-1xx-1-2.patch","04/Apr/10 10:45;rksingh;1594-yhadoop-20-1xx-1-3.patch;https://issues.apache.org/jira/secure/attachment/12440718/1594-yhadoop-20-1xx-1-3.patch","06/Apr/10 05:26;rksingh;1594-yhadoop-20-1xx-1-4.patch;https://issues.apache.org/jira/secure/attachment/12440840/1594-yhadoop-20-1xx-1-4.patch","07/Apr/10 01:25;hong.tang;1594-yhadoop-20-1xx-1-5.patch;https://issues.apache.org/jira/secure/attachment/12440983/1594-yhadoop-20-1xx-1-5.patch","12/Mar/10 17:09;rksingh;1594-yhadoop-20-1xx-1.patch;https://issues.apache.org/jira/secure/attachment/12438625/1594-yhadoop-20-1xx-1.patch","12/Mar/10 17:06;rksingh;1594-yhadoop-20-1xx.patch;https://issues.apache.org/jira/secure/attachment/12438624/1594-yhadoop-20-1xx.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,2010-03-12 07:19:42.683,,,false,,,,,,,,,,,,,,,,,,37077,Reviewed,,,,Wed Jul 14 09:24:21 UTC 2010,,,,,,,"0|i02qsv:",13943,,,,,,,,,,,,,,,,,,,,,"12/Mar/10 06:11;rksingh;Gridmix V3 needs to have support for Sleep jobs . This would help gridmix to run load JT w.r.t to hearbeats as we can have multiple TTs on single node . we can try to match the similar load we have on clusters with less machines.","12/Mar/10 07:19;hong.tang;The main motivation of this extension is to allow us to stress the JT heartbeat handling logic by emulating production size cluster (at least 4K nodes) with an order of magnitude reduction of testing cluster size (several hundred nodes) by running multiple tasktrackers per node. To avoid overwhelming individual nodes, each task would simply sleep for the duration as recorded in the job trace.","12/Mar/10 17:06;rksingh;Attaching the first cut patch for 1594 for yhadoop 20.1xx branch. 
This patch is dependent on 1376 yhadoop 20.1xx patch . 

To make it work in apply ""https://issues.apache.org/jira/secure/attachment/12438576/1376-2-yhadoop-security.patch"" and on top of it apply this patch.","12/Mar/10 17:09;rksingh;Earlier patch was incomplete , attaching the correct patch","16/Mar/10 07:20;hong.tang;- Overall, the approach looks fine although the implementation is still a bit hacky in the sense that users still need to specify input/output directories for running sleep jobs (which should be ignored). But I am fine with it for now as the structure is likely evolving with more extensions to be added to Gridmix.
- Style comment: Consider change JobType to JobCreator - it sounds more natural to call JobCreator.createGridmixJob(...).
- Structure wise, it would be better to rename GridmixJob to LoadJob, and create a common base (probably should be abstract) class for LoadJob and SleepJob and call it GridmixJob that *only* contains the shared parts of LoadJob and SleepJob. E.g. outdir may only belong to LoadJob but not SleepJob. (BTW, are File{Input,Output}Format.set{Input,Output}Path needed for SleepJob.call()?)
- SleepInputFormat.createRecordReader - should return a record reader that produces consecutive keys that match the expected wakeup time for the mapper process. Something like the following:
{noformat}
      return new RecordReader<LongWritable, LongWritable>() {
        long start = -1;
        long slept = 0L;
        long sleep = 0L;
        final LongWritable key = new LongWritable();
        final LongWritable val = new LongWritable();

        @Override
        public boolean nextKeyValue() throws IOException {
          if (start == -1) {
            start = System.nanoTime()/1000000;
          }
          slept += sleep;
          sleep = Math.min(duration - slept, RINTERVAL);
          key.set(slept + sleep + start);
          val.set(duration - slept);
          return slept < duration;
        }

        @Override
        public float getProgress() throws IOException {
          return slept / ((float) duration);
        }

        @Override
        public LongWritable getCurrentKey() {
          return key;
        }

        @Override
        public LongWritable getCurrentValue() {
          return val;
        }

        @Override
        public void close() throws IOException {
          final String msg = ""Slept for "" + duration;
          LOG.info(msg);
        }

        public void initialize(InputSplit split, TaskAttemptContext ctxt) {
        }
      };
{noformat}

Accordingly, SleepMapper.map(...) should be modified as follows:
{noformat}
    public void map(LongWritable key, LongWritable value, Context context)
      throws IOException, InterruptedException {
      context.setStatus(""Sleeping... "" + value.get() + "" ms left"");
      long now = System.nanoTime()/1000000;
      if (now < key.get()) {
        TimeUnit.MILLISECONDS.sleep(key.get()-now);
      }
    }
{noformat}
This is to avoid the actual sleep time deviates from the expected sleep time and the error gets accumulated over many map() calls.
- Similar idea should be applied to SleepReducer too.
- Do I read it right that by default the mapper() updates its progress once every 10 seconds? It is more interesting to make RINTERVAL == Math.min(1sec, totalDuration/20) so that the reported map task progress could be smoother. (Unfortunately the reduce progress may not be very useful).
- Should issue a warning in SleepJob.getSuccessfulAttemptInfo() if no successful attempt is found.
- SleepJob.buildSplits(): should not use InputStriper at all. At the end, you should set locations  of SleepSplit to ""new String[0]"" instead of ""striper.splitFor(inputDir, 512, 3).getLocations()""
","02/Apr/10 03:51;rksingh;Attaching the new patch with hong's comments.
","02/Apr/10 03:54;rksingh;I have implemented all the comments except 

-Structure wise, it would be better to rename GridmixJob to LoadJob, and create a common base (probably should be abstract) class for LoadJob and SleepJob and call it GridmixJob that only contains the shared parts of LoadJob and SleepJob. E.g. outdir may only belong to LoadJob but not SleepJob. (BTW, are File{Input,Output}Format.set{Input,Output}Path needed for SleepJob.call()?)

GridmixJob is created as an abstract class , outdir has been pushed to GridmixJob as SleepJob is also using this. We need File{Input,Output}Format.set{Input,Output}Path for mapreduce . Iam not sure if this is a bug , but it is required.","04/Apr/10 10:44;rksingh;Uploading the new patch to accomodate new changes , Old patch wasnt applying.

Correct order to make it work is 
20.100 + 1376 security patch and on top of it this patch.","05/Apr/10 05:21;hong.tang;- GenerateData should extend from GridmixJob instead of LoadJob. I think we can have a default implementation of buildSplits (as an empty function) in GridmixJob and remove the ""abstract"" keyword.
- The indentation of JobCreator looks weird.
- Replace the following with Configuration.getEnum:
{noformat}
+    Configuration conf, JobCreator defaultPolicy) {
+    String policy = conf.get(GRIDMIX_JOB_TYPE, defaultPolicy.name());
+    return valueOf(policy.toUpperCase());
{noformat}
- unused and extra imports (CommonConfigurationKeys, FsPermission) in TestGridmixSubmission.java.
- There does not seem be any unit tests covering the added feature.
- Avoiding directly setting ""gridmix.job.seq"" in both LoadJob and SleepJob. Instead, refactor the statement to a common method in GridmixJob called setSeqId(Job job). Similarly, adding a method getSeqId(Job job) in GridmixJob and avoid directly calling conf.get(""girdmix.job.seq"", -1) in {GridmixInputFormat, SleepInputFormat}.getSplits(...).
- Should we rename Gridmix{Mapper, Reducer, InputFormat, RecordReader, Split} to Load{Mapper, Reducer, InputFormat, RecordReader, Split}?
- Also suggest to refactor the statement of setting the original job id to a method in GridmixJob.
- There are some comments in LoadJob that are hard to understand.
{noformat}
  // TODO replace with ThreadLocal submitter?

  // not nesc when TL
{noformat}
- reduces in SleepMapper is not used.
- SleepJob hacks GridmixKey to pass along the sleep duration from map tasks to reduce tasks. We should document that in the code and file a jira to fix it.
- The following code does not seem to do what the comments claim to:
{noformat}
      //This is to stop accumulating deviation from expected sleep time
      //over a period of time.
      long now = System.nanoTime() / 1000000;
      if (now < duration) {
        duration = duration - now;
      }
      long slept = 0L;
      long sleep = 0L;
      while (slept < duration) {
        final long rem = duration - slept;
        sleep = Math.min(rem, RINTERVAL);
        context.setStatus(""Sleeping... "" + rem + "" ms left"");
        slept += sleep;
        TimeUnit.MILLISECONDS.sleep(sleep);
      }
{noformat}

Should it be more like the following?
{noformat}
      //This is to stop accumulating deviation from expected sleep time
      //over a period of time.
      long start = System.nanoTime() / 1000000;
      long slept = 0L;
      long sleep = 0L;
      while (slept < duration) {
        final long rem = duration - slept;
        sleep = Math.min(rem, RINTERVAL);
        context.setStatus(""Sleeping... "" + rem + "" ms left"");
        TimeUnit.MILLISECONDS.sleep(sleep);
        slept = System.nanoTime() / 1000000 - start;
      }
{noformat}

- The following seems incorrect:
{noformat}
      final long RINTERVAL = TimeUnit.MILLISECONDS.convert(
        context.getConfiguration().getLong(
          GRIDMIX_SLEEP_INTERVAL, Math.min(
            1, duration / 20)), TimeUnit.SECONDS);
{noformat}
""duration"" is in ms not in seconds. It should be changed to
{noformat}
      final long RINTERVAL = TimeUnit.MILLISECONDS.convert(
        context.getConfiguration().getLong(
          GRIDMIX_SLEEP_INTERVAL, Math.min(
            1, Math.max(1, duration/1000/20))), TimeUnit.SECONDS);
{noformat}
- I cannot find anywhere outdir is used by SleepJob. Did you encounter an error if FOF.setOutputPath is commented out in SleepJob.call()? 
- Both SleepJob and GridmixJob calls FileInputFormat.addInputPath(job, new path(""ignored"")), but one is surrounded with a try-catch block and the other is not. Not sure why. I am also curious to know what would be the error if FIF.addInputPath is not called in both classes.
","06/Apr/10 05:30;rksingh;I have implemented all the comments except few:

- GenerateData should extend from GridmixJob instead of LoadJob. I think we can have a default implementation of buildSplits (as an empty function) in GridmixJob and remove the ""abstract"" keyword.

GenerateData is now extending GridmixJob. But GridmixJob is still abstract as call() method is abstract.And it is implemented by all the derived classes.

- Avoiding directly setting ""gridmix.job.seq"" in both LoadJob and SleepJob. Instead, refactor the statement to a common method in GridmixJob called setSeqId(Job job). Similarly, adding a method getSeqId(Job job) in GridmixJob and avoid directly calling conf.get(""girdmix.job.seq"", -1) in {GridmixInputFormat, SleepInputFormat}.getSplits(...).

getSeqId is not there as {GridmixInputFormat, SleepInputFormat}.getSplits(...). is part of static inner classes and can only access static method. 

- I cannot find anywhere outdir is used by SleepJob. Did you encounter an error if FOF.setOutputPath is commented out in SleepJob.call()?
Removed this code and tested , things work fine.

- Both SleepJob and GridmixJob calls FileInputFormat.addInputPath(job, new path(""ignored"")), but one is surrounded with a try-catch block and the other is not. Not sure why. I am also curious to know what would be the error if FIF.addInputPath is not called in both classes 

I have remove FIF.addInputPath . things are working fine on cluster. I have removed try-catch block and added the exception in the signature of call","06/Apr/10 05:33;rksingh;- SleepJob hacks GridmixKey to pass along the sleep duration from map tasks to reduce tasks. We should document that in the code and file a jira to fix it.

Documented in the code and opened the jira https://issues.apache.org/jira/browse/MAPREDUCE-1675","07/Apr/10 01:28;hong.tang;The latest patch (""1594-yhadoop-20-1xx-1-4.patch"") still has a few minor issues. I fixed them and attached a new patch (1594-yhadoop-20-1xx-1-5.patch). I also attached a separated diff from 1594-yhadoop-20-1xx-1-4.patch (1594-diff-4-5.patch, so that it is easier to see what I have changed):

- Replacing the pattern of ""enum.name().equals()"" to directly enum comparison in various places.
- In GridmixJob, made job field final. Cleaned up the exception logic of PrivilegedExceptionAction.run().
- Move the functionality of setting SeqId and original name in GridmixJob ctor, eliminating the need of setSeqId and setJobId.
- Added another pullDescription method in GridmixJob that takes a JobContext object. (to avoid expose the key ""gridmix.job.seq"").
- The way RINTERVAL is calculated is wrong - when duration is less than 20000, it would lead to RINTERVAL==0. Per offline conversation with Chris, it seems that having a fine granularity for RINTERVAL is pointless since progress is only updated when TT sends heartbeat to JT. So I reverted the way how RINTERVAL is calculated.
- Changed to use System.currentTimeMilis() in SleepJob instead of System.nanoTime() since we are dealing with mili-sec granularity.
- In both TestGridmixSubmission and TestSleepJob, the statements for setting the configuration seems to be useless.
- Some indentation issues in GenerateData.
- Removed unused imports in GridmixJob
- Removed the unused LoadJob ctor.","14/Jul/10 09:24;cdouglas;Fixed in MAPREDUCE-1840",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stress-test tool for HDFS introduced in HDFS-708,MAPREDUCE-1804,12465056,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,shv,shv,shv,20/May/10 19:05,12/Dec/11 06:18,12/Jan/21 09:52,21/May/10 01:11,0.22.0,,,,,0.22.0,,,benchmarks,test,,,,,0,,,,,This issue is to commit the SLive test developed in HDFS-708 to MR trunk.,,zhz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HDFS-708,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/10 19:24;shv;slive.patch.2;https://issues.apache.org/jira/secure/attachment/12445095/slive.patch.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2010-05-20 23:19:50.622,,,false,,,,,,,,,,,,,,,,,,149769,Reviewed,,,,Fri May 21 17:47:43 UTC 2010,,,,,,,"0|i0jhh3:",111761,,,,,,,,,,,,,,,,,,,,,"20/May/10 19:24;shv;Attaching patch from HDFS-708 to let Hudson run it with MR.
I realized running it with HDFS Hudson was not useful, since the patch does not change hdfs.
This patch is the exact copy of Joshua's patch, except for a couple of cosmetic changes and removal of public from two methods.","20/May/10 23:19;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12445095/slive.patch.2
  against trunk revision 946578.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 112 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/196/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/196/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/196/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/196/console

This message is automatically generated.","21/May/10 01:11;shv;I just committed this. Thank you Joshua.","21/May/10 17:47;hudson;Integrated in Hadoop-Mapreduce-trunk #324 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/324/])
    Moving MAPREDUCE-1804 to new features.
MAPREDUCE-1804. Stress-test tool for HDFS introduced in HDFS-708. Contributed by Joshua Harlow.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rumen: a tool to extract job characterization data from job tracker logs,MAPREDUCE-751,12430157,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,dking,dking,dking,12/Jul/09 04:04,24/Nov/11 14:34,12/Jan/21 09:52,28/Aug/09 00:13,,,,,,0.21.0,,,tools/rumen,,,,,,0,,,,," We propose a new map/reduce component, rumen, which can be used to process job history logs to produce any or all of the following:

      * Retrospective info describing the statistical behavior of the
amount of time it would have taken to launch a job into a certain
percentage of the number of mapper slots in the log's cluster, given the
load over the period covered by the log

      * Statistical info as to the runtimes and shuffle times, etc. of
the tasks and jobs covered by the log

      * files describing detailed job trace information, and the
network topology as inferred from the host locations and rack IDs that
arise in the job tracker log.  In addition to this facility, rumen
includes readers for this information to return job and detailed task
information to other tools.

        These other tools include a more advanced version of gridmix, and also includes mumak: see blocked issues.


",,aaa,acmurthy,alexlod,anhi,anthonyr,cdouglas,dking,guanying,guilinsun,hammer,hong.tang,jorda,kimballa,ntolia,omalley,tanjiaqi,vinodkv,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-728,MAPREDUCE-776,,,,,,,,,,,,,,,,,,,,,,,,CHUKWA-342,,,,,,,,,,,,,,,,,,,,,"19/Aug/09 17:47;dking;2009-08-19--1030.patch;https://issues.apache.org/jira/secure/attachment/12417041/2009-08-19--1030.patch","26/Aug/09 22:40;dking;2009-08-26--1513-patch.patch;https://issues.apache.org/jira/secure/attachment/12417823/2009-08-26--1513-patch.patch","23/Jul/09 18:28;dking;mapreduce-751--2009-07-23.patch;https://issues.apache.org/jira/secure/attachment/12414361/mapreduce-751--2009-07-23.patch","28/Aug/09 00:10;cdouglas;mapreduce-751-20090826.patch;https://issues.apache.org/jira/secure/attachment/12417943/mapreduce-751-20090826.patch","27/Aug/09 00:36;hong.tang;mapreduce-751-20090826.patch;https://issues.apache.org/jira/secure/attachment/12417832/mapreduce-751-20090826.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,2009-07-21 07:46:43.527,,,false,,,,,,,,,,,,,,,,,,149016,Reviewed,,,,Thu Nov 24 14:34:42 UTC 2011,,,,,,,"0|i0jefj:",111268,,,,,,,,,,,"rumen,mumakil,job tracker logs",,,,,,,,,,"21/Jul/09 07:46;tanjiaqi;Is this a request for an implementation, or is this a project that's currently midway through implementation with (most of) the planned features, that will be released soon? There is some work on the Chukwa side of things on extracting and modeling job behavior from job history logs, currently for visualization, but we have some work also on Mathematically quantifying the job behavior. It would be interesting to see what the synergies are for using job history data.","21/Jul/09 07:47;tanjiaqi;Swimlanes visualization from the Chukwa project that visualizes job history data. ","21/Jul/09 16:03;dking;This is a project that's almost done.  I expect to add a patch to this issue today.","23/Jul/09 18:28;dking;This is a preliminary patch to gather early feedback on this functionality.

It works, but there are some areas I'm working on -- general code cleanup, mostly.  Its functionality is complete.  Although there are forseeable enhancements, they will be called out in their own JIRAs.","19/Aug/09 17:47;dking;This is the patch that implements Rumen.  It is licensed to Apache.","19/Aug/09 17:50;dking;This patch implements Rumen as described by this issue.  Rumen consumes job tracker log directories and produces the job traces that mumakil and GridMMIX cosume.","19/Aug/09 23:32;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12417041/2009-08-19--1030.patch
  against trunk revision 805324.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 40 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 2239 javac compiler warnings (more than the trunk's current 2232 warnings).

    -1 findbugs.  The patch appears to introduce 8 new Findbugs warnings.

    -1 release audit.  The applied patch generated 217 release audit warnings (more than the trunk's current 202 warnings).

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/495/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/495/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/495/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/495/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/495/console

This message is automatically generated.","26/Aug/09 22:40;dking;This is a new patch for rumen.  It replaces the previous one, incorporating the comments raised by test-patch.

Here is the new test-patch output summary:

{noformat}

     [exec] 
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 38 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     -1 javac.  The applied patch generated 2226 javac compiler warnings (more than the trunk's current 2220 warnings).
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 215 release audit warnings (more than the trunk's current 202 warnings).
     [exec] 
{noformat}

The javac warnings are deprication warnings.  We are using JobConf in this version of rumen.  We expect to fix this in a future release to use the new interface.

The release audit warnings are places we don't have the Apache License.  These are .json input files used in the test cases.  JSON does not define a comment format.  Although some JSON engines have one, obviously if we used one that would kill flexibility for little gain.

I fixed the TestZombieJob code.  These were the tests of the new code that failed.  The other failed tests were in streaming; a known source of test failures.","27/Aug/09 00:11;hong.tang;The patch looks mostly good. +1 except for the following:
- warning found during patch:
{noformat}
patching file ivy.xml
Hunk #1 succeeded at 273 (offset -4 lines).
patching file ivy/libraries.properties
Hunk #1 succeeded at 49 (offset -1 lines).
Hunk #2 succeeded at 69 (offset -1 lines).
{noformat}

- extra unused dependency on org.json in ivy.xml is introduced

- class TreePath needs to be public (including its constructors) because it is referenced by DeepCompare and DeepCompareException.

- constructor   public LoggedNetworkTopology(HashSet<ParsedHost> hosts, String name, int level) {
	refers a package private class ParsedHost. suggest to change this constructors to be package private.

- boolean finalParameter = false;
	unused variables in ParsedConfigFile.java","27/Aug/09 00:38;hong.tang;Added the suggested fixes based on previous submission. Approved by Dick King. :)","27/Aug/09 21:24;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12417832/mapreduce-751-20090826.patch
  against trunk revision 808351.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 37 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 2226 javac compiler warnings (more than the trunk's current 2220 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 215 release audit warnings (more than the trunk's current 202 warnings).

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/530/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/530/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/530/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/530/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/530/console

This message is automatically generated.","28/Aug/09 00:10;cdouglas;(removed name refs)","28/Aug/09 00:13;cdouglas;I committed this. Thanks Dick! 

Thanks also to Guanying Wang, who worked on this","28/Aug/09 16:37;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #3 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/3/])
    . Add Rumen, a tool for extracting statistics from job tracker
logs and generating job traces for simulation and analysis.
Contributed by Dick King and Guanying Wang
","24/Nov/11 14:34;arun_kumar;Q1>Is there any way to create a new trace file from job history logs with custom set of split locations ?

Q2> Can we create new trace files from existing trace files with new values for the attributes like preferred locations ?

Q3> How can i add new attributes / fields (which are not in job history logs) to the job or the tasks in the trace ? (or) Is there any way to generate trace with extra fields ?

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MR-279: Write a shell command application,MAPREDUCE-2719,12514865,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,hitesh,sharadag,sharadag,21/Jul/11 05:01,15/Nov/11 00:49,12/Jan/21 09:52,30/Sep/11 22:27,,,,,,0.23.0,,,mrv2,,,,,,0,,,,,"With nextgen hadoop (mrv2), it is simple to write non-MR applications. Write an AplicationMaster (also corresponding simple client), to submit and run a shell command application in the cluster.",,acmurthy,ahmed.radwan,brocknoland,cutting,eli,hammer,hyunsik.choi,jkreps,mahadev,markus.weimer,raviteja,revans2,romainr,svenkat,tomwhite,vicaya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,YARN-3172,,,,,,,,,,,,,,,,,,,,,"23/Sep/11 00:36;hitesh;MR-2179.1.patch;https://issues.apache.org/jira/secure/attachment/12496205/MR-2179.1.patch","25/Sep/11 07:18;hitesh;MR-2179.2.patch;https://issues.apache.org/jira/secure/attachment/12496382/MR-2179.2.patch","25/Sep/11 22:32;hitesh;MR-2179.3.patch;https://issues.apache.org/jira/secure/attachment/12496416/MR-2179.3.patch","29/Sep/11 21:16;hitesh;MR-2719.4.post3098.patch;https://issues.apache.org/jira/secure/attachment/12497061/MR-2719.4.post3098.patch","30/Sep/11 19:25;hitesh;MR-2719.5.patch;https://issues.apache.org/jira/secure/attachment/12497203/MR-2719.5.patch","30/Sep/11 21:14;hitesh;MR-2719.6.patch;https://issues.apache.org/jira/secure/attachment/12497219/MR-2719.6.patch","21/Sep/11 23:39;hitesh;mr-2719.wip.patch;https://issues.apache.org/jira/secure/attachment/12496048/mr-2719.wip.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,2011-08-29 09:29:14.251,,,false,,,,,,,,,,,,,,,,,,3392,,,,,Mon Oct 03 14:30:36 UTC 2011,,,,,,,"0|i09d3z:",52557,"Adding a simple, DistributedShell application as an alternate framework to MapReduce and to act as an illustrative example for porting applications to YARN.",,,,,,,,,,,,,,,,,,,,"21/Jul/11 05:11;sharadag;Two possible solutions:
1. Launch the shell command *directly* via NodeManager (NM).
This will however require special handling in ResourceManager (RM), as currently all ApplicationMasters (AM) are expected to do the lifecycle invocations (- register, heartbeat, unregister) to RM.
RM would require to be able to work with apps which do not do these lifecycle invocations.

2. Have a java AM which will in turn launch the shell command.
Java AM will do the register, heartbeat, unregister to RM
No change/special logic in RM needed.
This is similar to the pipes/streaming mr job currently.","29/Aug/11 09:29;vinodkv;+1 for (2) as it fits in neatly and doesn't demand any special handling in RM.","13/Sep/11 01:18;hitesh;Approach 2 looks like the better one to pursue. I will be working on this. Will a comment with more details later.

","13/Sep/11 06:47;acmurthy;bq. Approach 2 looks like the better one to pursue.

+1

Thanks for taking this up Hitesh!","21/Sep/11 23:39;hitesh;Initial patch with a client and an application master for a distributed shell sample yarn application.

Still a work in progress and nowhere close to properly tested but basic functionality works. ","22/Sep/11 00:27;acmurthy;Nice!

Good to see this coming along! We should have a hadoop-yarn-simple or some such module eventually...","23/Sep/11 00:36;hitesh;Attaching code with relevant pom files to create a new module. 

Current structure is hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/ 

","23/Sep/11 00:36;hitesh;Tests still pending. ","25/Sep/11 07:18;hitesh;Updated patch with a very simple integration test that deploys and runs the ds app master on the miniyarncluster. ","25/Sep/11 07:36;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12496382/MR-2179.2.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    -1 javadoc.  The javadoc tool appears to have generated 15 warning messages.

    -1 javac.  The applied patch generated 66 javac compiler warnings (more than the trunk's current 60 warnings).

    -1 findbugs.  The patch appears to introduce 14 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests:
                  org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/852//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/852//artifact/trunk/hadoop-mapreduce-project/patchprocess/newPatchFindbugsWarningshadoop-yarn-applications-distributedshell.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/852//console

This message is automatically generated.","25/Sep/11 22:32;hitesh;Addressed patch warnings reported by automated build.

Changed resource allocation to a higher number as the container was getting killed by the monitoring layer causing unit test to fail. ","25/Sep/11 22:34;hitesh;Local build still shows some javadoc warnings:

[WARNING] hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationToken.java:33: warning - Tag @link: reference not found: HttpServletRequest  

- these are not addressed in the patch.

","25/Sep/11 23:02;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12496416/MR-2179.3.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 66 javac compiler warnings (more than the trunk's current 60 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in .

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/854//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/854//console

This message is automatically generated.","25/Sep/11 23:11;hitesh;Additional javac warnings due to: 

[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-yarn-applications-distributedshell:jar:0.24.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.hadoop:hadoop-yarn:${yarn.version}, /Users/Hitesh/dev/hadoop-common/hadoop-mapreduce-project/hadoop-yarn/pom.xml, line 389, column 15
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-yarn-applications:pom:0.24.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.hadoop:hadoop-yarn:${yarn.version}, /Users/Hitesh/dev/hadoop-common/hadoop-mapreduce-project/hadoop-yarn/pom.xml, line 389, column 15

","26/Sep/11 00:54;hitesh;Cancelling patch as the unit test should fail until 3067 is addressed. ","29/Sep/11 21:14;hitesh;New patch after locally merging in changes from 3098. ","30/Sep/11 19:25;hitesh;Latest re-based patch. ","30/Sep/11 19:46;hitesh;Will create additional javac warnings due to apache-rat-plugin warnings.","30/Sep/11 20:32;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12497203/MR-2719.5.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    -1 javac.  The applied patch generated 66 javac compiler warnings (more than the trunk's current 60 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in .

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/913//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/913//console

This message is automatically generated.","30/Sep/11 21:14;hitesh;Address javadoc  warning issue. ","30/Sep/11 21:42;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12497219/MR-2719.6.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 66 javac compiler warnings (more than the trunk's current 60 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in .

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/917//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/917//console

This message is automatically generated.","30/Sep/11 22:27;acmurthy;I just committed this. Thanks Hitesh!","30/Sep/11 22:33;hudson;Integrated in Hadoop-Hdfs-trunk-Commit #1073 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/1073/])
    MAPREDUCE-2719. Add a simple, DistributedShell, application to illustrate alternate frameworks on YARN. Contributed by Hitesh Shah.

acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177864
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/pom.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Client.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/DSConstants.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDistributedShell.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/pom.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/pom.xml
","30/Sep/11 22:35;hudson;Integrated in Hadoop-Common-trunk-Commit #995 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/995/])
    MAPREDUCE-2719. Add a simple, DistributedShell, application to illustrate alternate frameworks on YARN. Contributed by Hitesh Shah.

acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177864
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/pom.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Client.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/DSConstants.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDistributedShell.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/pom.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/pom.xml
","30/Sep/11 23:04;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #1015 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/1015/])
    MAPREDUCE-2719. Add a simple, DistributedShell, application to illustrate alternate frameworks on YARN. Contributed by Hitesh Shah.

acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177864
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/pom.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Client.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/DSConstants.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDistributedShell.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/pom.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/pom.xml
","01/Oct/11 12:48;hudson;Integrated in Hadoop-Hdfs-trunk #817 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/817/])
    MAPREDUCE-2719. Add a simple, DistributedShell, application to illustrate alternate frameworks on YARN. Contributed by Hitesh Shah.

acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177864
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/pom.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Client.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/DSConstants.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDistributedShell.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/pom.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/pom.xml
","01/Oct/11 13:10;hudson;Integrated in Hadoop-Mapreduce-0.23-Build #33 (See [https://builds.apache.org/job/Hadoop-Mapreduce-0.23-Build/33/])
    Merge -r 1177863:1177864 from trunk to branch-0.23 to fix MAPREDUCE-2719.

acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177865
Files : 
* /hadoop/common/branches/branch-0.23
* /hadoop/common/branches/branch-0.23/hadoop-common-project
* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common
* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/docs
* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java
* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/test/core
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/java
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/native
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/secondary
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/test/hdfs
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/.gitignore
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/conf
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/conf/capacity-scheduler.xml.template
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/pom.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Client.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/DSConstants.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDistributedShell.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/pom.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/pom.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/c++
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/block_forensics
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/build-contrib.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/build.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/capacity-scheduler
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/data_join
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/dynamic-scheduler
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/eclipse-plugin
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/fairscheduler
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/index
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/streaming
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/vaidya
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/examples
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/fs
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/hdfs
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/io/FileBench.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/io/TestSequenceFileMergeProgress.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/ipc
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/security/authorize/TestServiceLevelAuthorization.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/test/MapredTestDriver.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/webapps/job
","01/Oct/11 13:20;hudson;Integrated in Hadoop-Mapreduce-trunk #847 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/847/])
    MAPREDUCE-2719. Add a simple, DistributedShell, application to illustrate alternate frameworks on YARN. Contributed by Hitesh Shah.

acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177864
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/pom.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Client.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/DSConstants.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDistributedShell.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/pom.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/pom.xml
","01/Oct/11 13:35;hudson;Integrated in Hadoop-Hdfs-0.23-Build #26 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/26/])
    Merge -r 1177863:1177864 from trunk to branch-0.23 to fix MAPREDUCE-2719.

acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177865
Files : 
* /hadoop/common/branches/branch-0.23
* /hadoop/common/branches/branch-0.23/hadoop-common-project
* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common
* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/docs
* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java
* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/test/core
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/java
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/native
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/secondary
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/test/hdfs
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/.gitignore
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/conf
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/conf/capacity-scheduler.xml.template
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/pom.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Client.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/DSConstants.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDistributedShell.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-applications/pom.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/hadoop-yarn/pom.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/c++
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/block_forensics
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/build-contrib.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/build.xml
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/capacity-scheduler
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/data_join
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/dynamic-scheduler
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/eclipse-plugin
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/fairscheduler
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/index
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/streaming
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/contrib/vaidya
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/examples
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/fs
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/hdfs
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/io/FileBench.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/io/TestSequenceFileMergeProgress.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/ipc
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/security/authorize/TestServiceLevelAuthorization.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/test/MapredTestDriver.java
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/src/webapps/job
","03/Oct/11 14:30;revans2;The merge to 0.23 is busted.  It is looking for a jar with 0.24 in it.  I can file a new JIRA if needed, but it should just be a simple fix.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MR-279: ResourceManager metrics,MAPREDUCE-2434,12504125,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,vicaya,vicaya,vicaya,13/Apr/11 01:18,15/Nov/11 00:49,12/Jan/21 09:52,30/Apr/11 07:25,,,,,,0.23.0,,,mrv2,,,,,,0,,,,,"Hierarchical scheduler metrics, per queue, per user (default off)",,acmurthy,mahadev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/11 04:35;vicaya;mr-2434-metrics-v1.patch;https://issues.apache.org/jira/secure/attachment/12477865/mr-2434-metrics-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2011-04-30 20:35:30.235,,,false,,,,,,,,,,,,,,,,,,150189,,,,,Sat Apr 30 20:35:30 UTC 2011,,,,,,,"0|i09dxz:",52692,I just committed this. Thanks Luke!,,,,,,,,,,,,,,,,,,,,"30/Apr/11 04:35;vicaya;Rebased against:
{noformat}
commit bbb7faabfe2fb0c73c4473f759ec31cf57d04e94
Author: Christopher Douglas <cdouglas@apache.org>
Date:   Fri Apr 29 22:25:09 2011 +0000

    Fix container launch w/ inconsistent credential file naming.
{noformat}","30/Apr/11 18:59;vicaya;Arun, I think you forgot to svn add the new files.","30/Apr/11 20:35;mahadev;Luke, I just committed the new files. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow FairScheduler to control the number of slots on each TaskTracker,MAPREDUCE-2198,12480631,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,schen,schen,schen,23/Nov/10 02:38,15/Nov/11 00:49,12/Jan/21 09:52,15/Sep/11 05:09,0.23.0,,,,,0.23.0,,,contrib/fair-share,,,,,,0,,,,,"We can set the number of slots on the TaskTracker to be high and let FairScheduler handles the slots.
This approach allows us to change the number of slots on each node dynamically.
The administrator can change the number of slots with a CLI tool.

One use case of this is for upgrading the MapReduce.
Instead of restarting the cluster, we can run the new MapReduce on the same cluster.
And use the CLI tool to gradually migrate the slots.
This way we don't lost the progress fo the jobs that's already executed.",,aah,acmurthy,atm,dhruba,eli,jsensarma,lianhuiwang,matei,philip,srivas,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-2108,,,,,,,,,,,,,,,,,,,,,"27/Nov/10 07:34;schen;MAPREDUCE-2198-v2.txt;https://issues.apache.org/jira/secure/attachment/12464776/MAPREDUCE-2198-v2.txt","26/Nov/10 04:30;schen;MAPREDUCE-2198.txt;https://issues.apache.org/jira/secure/attachment/12460484/MAPREDUCE-2198.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2010-11-23 05:26:26.328,,,false,,,,,,,,,,,,,,,,,,60591,,,,,Thu Sep 15 05:09:35 UTC 2011,,,,,,,"0|i0jign:",111921,,,,,,,,,,,,,,,,,,,,,"23/Nov/10 05:26;srivas;The scheduler can easily choose to ignore the slot information while scheduling, correct? Then why is this needed? Are we running two TT's on each node, that talk to different JT's, and ""migrating"" slots from one to the other?
","23/Nov/10 05:49;dhruba;>  Are we running two TT's on each node, that talk to different JT's, and ""migrating"" slots from one to the other?

Precisely. We have a solution that is used to deploy new software to the JT-TT. Earlier, the JT/TT have to be shutdown, new code deployed, and then the cluster is restarted. This means that the cluster was unavailable for a while and currently running jobs all fails when the cluster is shutdown.

The modified approach is to direct all new jobs to the newly created JT instance, and then slowly (and proportionally) migrate slots from the old JT instance to the new JT instance. This allows JT software upgrades without incurring any cluster downtime.","23/Nov/10 05:50;schen;Hey M.C.

Yes, we can set a higher slot limit on TT and let scheduler manage the slots.

bq.  Are we running two TT's on each node, that talk to different JT's, and ""migrating"" slots from one to the other?
Yes. The motivation here is that when deploying new JT and TT. We need to restart the cluster and we lose all the running jobs.
This can be solved by the way you described.

Other use case is that people can experiment with the best slot settings by using the CLI without restarting the cluster.
Right now if you want to change the number of slots, you have to change the conf on every TT and restart.

Scott","23/Nov/10 07:03;jsensarma;what is fairscheduler specific about this? could we not make this a change in the JT directly to scale the slots advertised by TT?

in addition - one bug/feature that we need to fix as part of this where the JT overschedules TTs (when configured to schedule multiple tasks per hbt). this is benign today (TT puts those tasks in unassigned state) - but in this world will not be so benign.","23/Nov/10 17:50;acmurthy;bq. Right now if you want to change the number of slots, you have to change the conf on every TT and restart.

How do you handle heterogeneous clusters? Or will your CLI command be per TT?

Also, as Joydeep pointed out there are several issues with unassigned slots in TTs...","23/Nov/10 17:56;acmurthy;bq. Also, as Joydeep pointed out there are several issues with unassigned slots in TTs...

You also have to worry about piggy-backing of task-cleanup tasks done by the JT...","23/Nov/10 19:39;schen;Hey Arun and Joydeep,

bq. How do you handle heterogeneous clusters? Or will your CLI command be per TT?
Yes, the CLI command will be per TT.

bq. in addition - one bug/feature that we need to fix as part of this where the JT overschedules TTs (when configured to schedule multiple tasks per hbt). this is benign today (TT puts those tasks in unassigned state) - but in this world will not be so benign.
Yes, we should never let JT submit more tasks than TT's limit. Scheduler should incorporate this logic.

bq. You also have to worry about piggy-backing of task-cleanup tasks done by the JT...
I am not very clear about this part. Will read more codes.","23/Nov/10 21:27;schen;Hey Arun,

bq. You also have to worry about piggy-backing of task-cleanup tasks done by the JT...
I read some codes. Now I see your point.
JobTracker.getSetupAndCleanupTasks() is not controlled by the Scheduler.
So we need to put some logic there to make it aware of this task limit. Or we might be over-scheduled.","23/Nov/10 23:15;acmurthy;Right. Also, you need to worry about task-cleanup-tasks i.e. responses to move tasks from COMMIT_PENDING to SUCCESS/KILLED.","24/Nov/10 23:21;matei;Do setup and cleanup tasks do a significant amount of work? I can see two simple ways of dealing with them other than moving control to the scheduler: Either always allow them to run (even if the TT is full on map and reduce slots), or allow them to run but limit the number of such tasks per node to 1 (i.e. have a ""setup slot""). If they do need to do CPU-intensive work, then it makes sense to give control of them to the scheduler.","24/Nov/10 23:38;matei;One other minor comment: if the fair scheduler is over-scheduling task trackers, maybe we should consider that a bug. I don't think it was intended to do that, although in trunk at least, it looks like it may do it if mapAssignCap and reduceAssignCap are set to something less than infinity. (Otherwise, it looks at the number of slots free on the TT and does not assign more than that.) To deal with any sort of race condition that occurs if you lower a slot count while a heartbeat is in progress, I'd suggest making the TT report over-scheduled tasks as killed and drop them.","26/Nov/10 04:31;schen;The patch is ready for review.
Matei: Would you mind help me review this one?","26/Nov/10 04:33;schen;I tried using review board. But I always get 500 error.","26/Nov/10 23:20;matei;The approach in the patch looks good, but I have two questions:

- How can you run the FairSchedulerShell from the command line? It doesn't seem to have a main method (so just using bin/hadoop org.apache.hadoop.mapred.FairSchedulerShell doesn't work), and I don't see it registered as a tool anywhere.

- Will the slot counts set by the scheduler be visible in the JobTracker web UI? It looks like jobtracker.jsp looks at ClusterMetrics and machines.jsp looks at TaskTrackerStatus objects.","27/Nov/10 02:35;schen;Hey Matei, Thanks for the review.

{quote}
How can you run the FairSchedulerShell from the command line? It doesn't seem to have a main method (so just using bin/hadoop org.apache.hadoop.mapred.FairSchedulerShell doesn't work), and I don't see it registered as a tool anywhere.
{quote}
It's my bad. I forgot to put a main method in FairSchedulerShell. I will update it.

{quote}
Will the slot counts set by the scheduler be visible in the JobTracker web UI? It looks like jobtracker.jsp looks at ClusterMetrics and machines.jsp looks at TaskTrackerStatus objects.
{quote}
For the ClusterMetircs, it's OK because we use TaskScheduler.getMaxSlots() to calculate the total slots in JobTracker. But you are right about machines.jsp. I should change it so that it also pulls the information from scheduler.
","27/Nov/10 07:34;schen;Addressed Matie's comments.","28/Nov/10 00:58;matei;The changes look good, but I thought about one other issue: What should we do when we are asked to lower the slots on a node to below the number of running tasks on it? In the current version, the scheduler won't launch tasks on that node until its running task count falls below its slot count. However, if we wanted to use this for rollover, we'd probably want to wait until enough of those tasks are done before giving a slot to the new JobTracker. There are two ways we can do this: Either have the process that's scaling down the cluster watch the running tasks before giving the slots to someone else, or include an API that somehow makes a callback when the number of running tasks has decreased below the target slot count. What are your thoughts on this?

One other thing we may want to support is killing tasks after a timeout if the cluster hasn't scaled down. However, I think this can already be done through the MRAdmin shell command / API.

In either case, we probably need some API to see what's running on the cluster. Some of the commands in MRAdmin might be enough, but we may want to add something there. However, this can be a different JIRA.","28/Nov/10 06:39;dhruba;> Either have the process that's scaling down the cluster watch the running tasks before giving the slots to someone else

I like this idea (instead of having callbacks, keeps the design simple)

","30/Nov/10 23:05;schen;bq. However, I think this can already be done through the MRAdmin shell command / API.
I also prefer using the existing interface and keep this one simpler.","07/Dec/10 18:22;jsensarma;+1 on Matei's previous comments about needing to wait until slots are actually released. We should have another api to request the actual number of [map/reduce] slots in use on any given tracker and only claim slots when they are actually confirmed to be released.","27/Feb/11 04:19;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12464776/MAPREDUCE-2198-v2.txt
  against trunk revision 1074251.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The patch appears to cause tar ant target to fail.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/51//testReport/
Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/51//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/51//console

This message is automatically generated.","07/Sep/11 08:32;acmurthy;Sorry to come in late, the patch has gone stale. Can you please rebase? Thanks.

Given this is not an issue with MRv2 should we still commit this? I'm happy to, but not sure it's useful. Thanks.","15/Sep/11 05:09;schen;Arun: Thanks for the comments. You are right. I guess this is not an issue since we have MRv2. Closing this now.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Gridmix emulate usage of Distributed Cache files,MAPREDUCE-2407,12502685,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,ravidotg,ravidotg,ravidotg,29/Mar/11 09:26,15/Nov/11 00:49,12/Jan/21 09:52,23/May/11 14:37,0.23.0,,,,,0.23.0,,,contrib/gridmix,,,,,,0,,,,,Currently Gridmix emulates disk IO load only. This JIRA is to make Gridmix emulate Distributed Cache load as defined by the job-trace.,,aah,atm,hong.tang,lianhuiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/May/11 08:19;ravidotg;2407.patch;https://issues.apache.org/jira/secure/attachment/12478943/2407.patch","23/May/11 08:17;ravidotg;2407.v1.1.patch;https://issues.apache.org/jira/secure/attachment/12480093/2407.v1.1.patch","20/May/11 08:26;ravidotg;2407.v1.patch;https://issues.apache.org/jira/secure/attachment/12479884/2407.v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2011-05-17 10:15:53.253,,,false,,,,,,,,,,,,,,,,,,150174,Reviewed,,,,Tue May 24 15:44:44 UTC 2011,,,,,,,"0|i09e1j:",52708,Makes Gridmix emulate HDFS based distributed cache files and local file system based distributed cache files.,,,,,,,,,,,,,,,,,,,,"12/May/11 08:19;ravidotg;Attaching patch that adds emulation of distributed cache load in gridmix simulated jobs.

High level details of what this patch does are:

(1) New gridmix configuration property ""gridmix.distributed-cache-emulation.enable"" is added, whose default value is true. Setting it to false disables emulation of distributed cache load. Irrespective of this config property setting, with -generate option, distributed cache files are generated on HDFS by gridmix.
Distributed Cache Emulation is disabled for the case of '-' as input trace(i.e. stdin stream instead of file).
Distributed Cache Emulation is disabled for the case where <iopath> is on local file system.

(2) Behavior of the option -generate is changed. -generate option means (a) generate input data in the directory
<iopath>/input/ and (b) generate distributed cache data needed for emulation of distributed cache load of this
trace file in the directory <iopath>/distributedCache/.
For (a), same old GenerateData MR job is used.
For (b), a new MR job GenerateDistCacheData is added, which is run after GenerateData and before submission of simulated jobs.

With -generate option, (a) existence of <iopath>/input/ directory gives an error, similar to current behavior and
(b) existence of <iopath>/gridmixDistCache/ directory is not an error and leads to generation of only the missing/nonexisting distributed cache files under <iopath>/gridmixDistCache/ for the specific trace file. If all the needed distributed cache files are already
there, then submission of GenerateDistCacheData job is skipped.

Without -generate option, if emulation of distributed cache load is enabled, then gridmix checks if all the needed distributed cache files are available under <iopath>/distributedCache/ and emits an error if any of the expected files are missing.

(3) setupDistCacheEmulation : Read the trace file and build a list of distributed cache file paths and their file sizes. The
file paths are the mapped paths on the simulated cluster(mapped from original cluster's paths to simulated cluster's
paths using
{code}MD5Hash(filePath+timestamp){code} for public distributed cache files
and
{code}MD5Hash(filePath+timestamp+username){code} for private distributed cache files.

This list of mappeed file paths along with the file sizes is written to a special file
<iopath>/distributedCache/_distCacheFiles.txt and the file name can be configured using
""gridmix.distcache.file.list"".

So this means all distributed cache files in the gridmix simulated jobs are public distributed cache files but for each private distributed cache file of a user of the original cluster (i.e. from trace file), there will be a different public distributed cache file on gridmix simulated cluster.

(4) GenerateDistCacheData : The MR job (launched by gridmix if -generate option is seen) that generates distributed cache data files on HDFS. Input to this job is the special file _distCacheFiles.txt that contains the distributed cache file paths and their sizes.
Each map() call generates one distributed cache file.

(5) configureDistCacheFiles : The mapped distributed cache files' paths are configured for the simulated jobs' configrations sothat MapReduce framework takes care of adding the actual distributed cache load equivalent to original cluster's distributed cache load.","17/May/11 10:15;santok;I will take it up from here. Please grant me the commit access. ","17/May/11 13:16;ravidotg;Amar, Would you please review the patch ? Thanks.","20/May/11 07:59;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12478943/2407.patch
  against trunk revision 1125223.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 10 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    -1 release audit.  The applied patch generated 3 release audit warnings (more than the trunk's current 2 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/283//testReport/
Release audit warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/283//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/283//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/283//console

This message is automatically generated.","20/May/11 08:26;ravidotg;Attaching new patch fixing the release audit warning.","20/May/11 11:06;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12479884/2407.v1.patch
  against trunk revision 1125223.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 10 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/284//testReport/
Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/284//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/284//console

This message is automatically generated.","21/May/11 00:16;amar_kamat;The latest patch looks good to me. I have some minor comments (mostly alignment, refactoring and parameter naming) which I have discussed with Ravi offline. I don't want to block the patch just for some minor comments. +1.","23/May/11 08:17;ravidotg;Attaching new patch updating Amar's offline minor comments.","23/May/11 08:27;amar_kamat;Patch looks good to me. +1","23/May/11 10:58;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12480093/2407.v1.1.patch
  against trunk revision 1125599.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 10 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/291//testReport/
Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/291//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/291//console

This message is automatically generated.","23/May/11 14:37;ravidotg;I just committed this to trunk.","23/May/11 14:48;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #695 (See [https://builds.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/695/])
    MAPREDUCE-2407. Make GridMix emulate usage of distributed cache files in simulated jobs.

ravigummadi : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1126499
Files : 
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/PseudoLocalFs.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestPseudoLocalFs.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Gridmix.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateData.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobCreator.java
* /hadoop/mapreduce/trunk/src/docs/src/documentation/content/xdocs/gridmix.xml
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/DebugJobProducer.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestDistCacheEmulation.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixSubmission.java
","24/May/11 15:44;hudson;Integrated in Hadoop-Mapreduce-trunk #689 (See [https://builds.apache.org/hudson/job/Hadoop-Mapreduce-trunk/689/])
    MAPREDUCE-2407. Make GridMix emulate usage of distributed cache files in simulated jobs.

ravigummadi : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1126499
Files : 
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/PseudoLocalFs.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestPseudoLocalFs.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Gridmix.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateData.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobCreator.java
* /hadoop/mapreduce/trunk/src/docs/src/documentation/content/xdocs/gridmix.xml
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/DebugJobProducer.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestDistCacheEmulation.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixSubmission.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure AM Restart and Recovery-on-restart is complete,MAPREDUCE-2692,12514289,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,sharadag,amolkekre,amolkekre,15/Jul/11 22:12,15/Nov/11 00:49,12/Jan/21 09:52,31/Oct/11 04:47,0.23.0,,,,,0.23.0,,,mrv2,,,,,,0,,,,,Need to get AM restart and the subsequent recover after restart to work,,devaraj,srivas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-10-17 05:31:24.668,,,false,,,,,,,,,,,,,,,,,,49527,,,,,Mon Oct 31 04:47:11 UTC 2011,,,,,,,"0|i09d6n:",52569,,,,,,,,,,,,,,,,,,,,,"17/Oct/11 05:31;acmurthy;This umbrella jira isn't a blocker.","31/Oct/11 04:47;acmurthy;Fixed via various sub-tasks. Thanks Sharad!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow TaskScheduler manage number slots on TaskTrackers,MAPREDUCE-2108,12475806,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,schen,schen,schen,04/Oct/10 21:06,15/Nov/11 00:49,12/Jan/21 09:52,15/Sep/11 05:09,0.23.0,,,,,0.23.0,,,capacity-sched,contrib/fair-share,,,,,0,,,,,"Currently the map slots and reduce slots are managed by TaskTracker configuration.
To change the task tracker slots, we need to restart the TaskTrackers.
Also, for a non-uniform cluster, we have to deploy different sets of configuration.

Now JobTracker holds the CPU and memory status of TaskTrackers (MAPREDUCE-1218).
So it makes sense to just let JobTracker.taskScheduler decided the number of slots on each node.
This way we can
1. Change the number of slots dynamically without restarting TaskTracker
2. Use different number of slots based on the resource of a TaskTracker

To achieve this, we need to change the logic that we use totalMapSlots and totalReduceSlots in JobTracker.
I think they are used in WebUI and speculativeCap.

We will need to make JobTracker calculate these numbers from TaskScheduler and TaskTrackerStatus.
TaskScheduler and TaskTracker can both hold their maximum slots. We pick the smaller one.

Thoughts?",,aah,acmurthy,cdouglas,cutting,dhruba,jsensarma,liangly,lianhuiwang,mahadev,matei,philip,srivas,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/10 20:23;schen;MAPREDUCE-2108-v2.txt;https://issues.apache.org/jira/secure/attachment/12464816/MAPREDUCE-2108-v2.txt","23/Nov/10 23:18;schen;MAPREDUCE-2108.txt;https://issues.apache.org/jira/secure/attachment/12460321/MAPREDUCE-2108.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2010-11-23 17:53:57.695,,,false,,,,,,,,,,,,,,,,,,60592,,,,,Thu Sep 15 05:08:41 UTC 2011,,,,,,,"0|i0ji5z:",111873,,,,,,,,,,,,,,,,,,,,,"23/Nov/10 17:53;acmurthy;I like the direction.

I'll warn you that the current implementation of the JT has the notion of 'slot' burned in very deeply into various parts of the codebase, not to mention the TT itself. You might need to do a complete rewrite/refactor of the JT/TT to pull this off. My 2c. Good luck.","23/Nov/10 23:25;schen;Hey Arun,

Thanks. I have made the first patch.
Like you mentioned, the slot may be used in may places and we need to have a careful check.
I will read the code more carefully. I have a feeling that there are still things needs to be changed.

At least in this patch will not change the behavior if we didn't do anything to the scheduler.

Scott","28/Nov/10 05:21;acmurthy;Can you please describe the changes you are making here... ","28/Nov/10 20:21;schen;bq. Can you please describe the changes you are making here...

Sorry for not making this clear. The purpose here is to move the control of maximum slots to TaskScheduler.
This allows TaskScheduler to perform better resource scheduling and allows changing the number of slots on fly.

The changes made in the patch are the following:
1. Add a getMaxSlots(TaskTrackerStatus, TaskType) method to TaskScheduler.
2. Replace TaskTrackerStatus.getMaxSlots() everywhere in the JobTracker with the above method.

This way the JobTracker pulls the ""maximum slots"" information from TaskScheduler.
The default method in TaskScheduler.getMaxSlots() is to simply report TaskTrackerStatus.getMaxSlots(). So it will not change any behavior.
But people can overwrite this method and put more sophisticated logic in it (See MAPREDUCE-2198).
","28/Nov/10 20:23;schen;Update. Replace the maxSlots in machines.jsp.","27/Feb/11 16:53;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12464816/MAPREDUCE-2108-v2.txt
  against trunk revision 1074251.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/63//testReport/
Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/63//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/63//console

This message is automatically generated.","07/Sep/11 08:15;acmurthy;Sorry to come in late, the patch has gone stale. Can you please rebase? Thanks.

Given this is not an issue with MRv2 should we still commit this? I'm happy to, but not sure it's useful. Thanks.","15/Sep/11 05:08;schen;Arun: Thanks for the comments. You are right. I guess this is not an issue since we have MRv2. Closing this now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MR-279: WebApp for Job History,MAPREDUCE-2438,12504324,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,ramach,mahadev,mahadev,14/Apr/11 23:17,15/Nov/11 00:49,12/Jan/21 09:52,15/Apr/11 00:37,,,,,,0.23.0,,,mrv2,,,,,,0,,,,,Add webapp for job history server in MR-279 branch.,,aah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/11 23:26;ramach;yarn-4397204-4.patch;https://issues.apache.org/jira/secure/attachment/12476388/yarn-4397204-4.patch","14/Apr/11 23:53;ramach;yarn-4397204-5.patch;https://issues.apache.org/jira/secure/attachment/12476392/yarn-4397204-5.patch","15/Apr/11 00:34;mahadev;yarn-4397204-6.patch;https://issues.apache.org/jira/secure/attachment/12476397/yarn-4397204-6.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2011-04-14 23:26:09.462,,,false,,,,,,,,,,,,,,,,,,150192,,,,,Fri Apr 15 00:37:28 UTC 2011,,,,,,,"0|i09dxj:",52690,,,,,,,,,,,,,,,,,,,,,"14/Apr/11 23:26;ramach;Suggested fix","14/Apr/11 23:27;ramach;Fix attached for enabling webapp/UI to JobHistoryServer ","14/Apr/11 23:32;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12476388/yarn-4397204-4.patch
  against trunk revision 1092400.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 5 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/168//console

This message is automatically generated.","14/Apr/11 23:53;ramach;revised patch","14/Apr/11 23:55;ramach;updated patch attached. canceling the earlier version","14/Apr/11 23:55;ramach;revised patch","15/Apr/11 00:02;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12476392/yarn-4397204-5.patch
  against trunk revision 1092400.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 5 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/169//console

This message is automatically generated.","15/Apr/11 00:31;mahadev;patch with a minor fix!","15/Apr/11 00:34;mahadev;attaching the right patch with a minor fix.","15/Apr/11 00:37;mahadev;I just committed this. thanks krishna!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add metrics to the fair scheduler,MAPREDUCE-2323,12498477,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,tlipcon,tlipcon,tlipcon,13/Feb/11 03:20,15/Nov/11 00:48,12/Jan/21 09:52,06/Jul/11 05:12,0.23.0,,,,,0.23.0,,,contrib/fair-share,,,,,,0,,,,,"It would be useful to be able to monitor various metrics in the fair scheduler, like demand, fair share, min share, and running task count.",,aah,atm,schen,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/11 03:24;tlipcon;mr-2323-20.txt;https://issues.apache.org/jira/secure/attachment/12470958/mr-2323-20.txt","17/Jun/11 23:57;tlipcon;mr-2323.txt;https://issues.apache.org/jira/secure/attachment/12483015/mr-2323.txt","03/Mar/11 23:31;tlipcon;mr-2323.txt;https://issues.apache.org/jira/secure/attachment/12472627/mr-2323.txt","13/Feb/11 07:25;tlipcon;mr-2323.txt;https://issues.apache.org/jira/secure/attachment/12470959/mr-2323.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2011-02-25 12:31:23.166,,,false,,,,,,,,,,,,,,,,,,36942,Reviewed,,,,Wed Jul 06 14:41:00 UTC 2011,,,,,,,"0|i02pk7:",13742,,,,,,,,,,,,,,,,,,,,,"13/Feb/11 03:24;tlipcon;Here's a quick draft of this feature against our branch 20. Will work on updating to trunk and adding some kind of tests.","13/Feb/11 03:27;tlipcon;Would appreciate feedback on the following:
- is the locking correct for updateMetrics? I take a lock on the scheduler and then the poolmanager, but no other locks.
- could it be finer grain? (do I need the coarse scheduler lock at all? unclear)

- should I add a configuration to disable the job-level metrics and only use pool-level metrics?
- when a user pool becomes empty, should we remove its metrics records?","13/Feb/11 07:25;tlipcon;Here's a patch against trunk with unit tests and docs. Above questions still stand.","25/Feb/11 12:31;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12470959/mr-2323.txt
  against trunk revision 1074251.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:
                  org.apache.hadoop.mapred.TestDebugScript

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/30//testReport/
Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/30//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/30//console

This message is automatically generated.","03/Mar/11 23:31;tlipcon;Slightly improved patch - the main code is the same, but this fixes some flaky behavior in the tests. I previously wasn't unregistering the metrics updater in fairsched's termination method, so different test cases were interacting poorly with eachother.

This should be ready for review.","04/Mar/11 03:38;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12472627/mr-2323.txt
  against trunk revision 1076804.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/116//testReport/
Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/116//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/116//console

This message is automatically generated.","17/Jun/11 23:57;tlipcon;Rebased on trunk. Also fixes a bug where using ""setPool"" twice on a job would cause an NPE, since it would clean up the metrics on the first switch, and then NPE when cleaning it up again","18/Jun/11 02:44;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12483015/mr-2323.txt
  against trunk revision 1137017.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:
                  org.apache.hadoop.cli.TestMRCLI
                  org.apache.hadoop.fs.TestFileSystem

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/406//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/406//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/406//console

This message is automatically generated.","20/Jun/11 17:32;schen;+1 The patch looks good to me.
I think the failed tasks are unrelated.","06/Jul/11 05:12;tlipcon;Committed to trunk. Thanks for reviewing.","06/Jul/11 05:37;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #734 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/734/])
    MAPREDUCE-2323. Add metrics to the fair scheduler. Contributed by Todd Lipcon.

todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1143252
Files : 
* /hadoop/common/trunk/mapreduce/src/contrib/build-contrib.xml
* /hadoop/common/trunk/mapreduce/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/PoolSchedulable.java
* /hadoop/common/trunk/mapreduce/src/docs/src/documentation/content/xdocs/fair_scheduler.xml
* /hadoop/common/trunk/mapreduce/CHANGES.txt
* /hadoop/common/trunk/mapreduce/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/Pool.java
* /hadoop/common/trunk/mapreduce/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/Schedulable.java
* /hadoop/common/trunk/mapreduce/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java
* /hadoop/common/trunk/mapreduce/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/JobSchedulable.java
* /hadoop/common/trunk/mapreduce/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/TestFairScheduler.java
* /hadoop/common/trunk/mapreduce/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/FakeSchedulable.java
* /hadoop/common/trunk/mapreduce/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/PoolManager.java
","06/Jul/11 14:41;hudson;Integrated in Hadoop-Mapreduce-trunk #729 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/729/])
    MAPREDUCE-2323. Add metrics to the fair scheduler. Contributed by Todd Lipcon.

todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1143252
Files : 
* /hadoop/common/trunk/mapreduce/src/contrib/build-contrib.xml
* /hadoop/common/trunk/mapreduce/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/PoolSchedulable.java
* /hadoop/common/trunk/mapreduce/src/docs/src/documentation/content/xdocs/fair_scheduler.xml
* /hadoop/common/trunk/mapreduce/CHANGES.txt
* /hadoop/common/trunk/mapreduce/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/Pool.java
* /hadoop/common/trunk/mapreduce/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/Schedulable.java
* /hadoop/common/trunk/mapreduce/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java
* /hadoop/common/trunk/mapreduce/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/JobSchedulable.java
* /hadoop/common/trunk/mapreduce/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/TestFairScheduler.java
* /hadoop/common/trunk/mapreduce/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/FakeSchedulable.java
* /hadoop/common/trunk/mapreduce/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/PoolManager.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MR-279: Metrics for reserved resource in ResourceManager,MAPREDUCE-2533,12508215,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,vicaya,vicaya,vicaya,24/May/11 16:23,15/Nov/11 00:48,12/Jan/21 09:52,24/May/11 21:14,0.23.0,,,,,0.23.0,,,mrv2,,,,,,0,,,,,Add metrics for reserved resources.,,acmurthy,mahadev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/May/11 17:22;vicaya;mr-2533-reserved-metrics-v1.patch;https://issues.apache.org/jira/secure/attachment/12480282/mr-2533-reserved-metrics-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2011-05-24 20:35:52.51,,,false,,,,,,,,,,,,,,,,,,44129,,,,,Tue May 24 21:14:10 UTC 2011,,,,,,,"0|i09dnj:",52645,,,,,,,,,,,"metrics,mrv2,rm",,,,,,,,,,"24/May/11 17:22;vicaya;Arun: review the reserved resource logic?","24/May/11 20:35;acmurthy;Seems ok - just to clarify: You've added Node.getReservedResource just to get the metric back during Application.unreserve, correct?","24/May/11 21:01;vicaya;bq. You've added Node.getReservedResource just to get the metric back during Application.unreserve, correct?

Yes. The NodeManagerImpl seems to be the place to save it as it also talks about update the reserved resource etc. but not really save it","24/May/11 21:10;acmurthy;+1, lgtm","24/May/11 21:14;acmurthy;I just committed this. Thanks Luke!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement an in-cluster LocalJobRunner,MAPREDUCE-1220,12441117,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,roelofs,acmurthy,acmurthy,18/Nov/09 22:44,15/Nov/11 00:48,12/Jan/21 09:52,18/Oct/11 06:53,,,,,,0.23.0,,,client,jobtracker,,,,,3,,,,,"Currently very small map-reduce jobs suffer from latency issues due to overheads in Hadoop Map-Reduce such as scheduling, jvm startup etc. We've periodically tried to optimize all parts of framework to achieve lower latencies.

I'd like to turn the problem around a little bit. I propose we allow very small jobs to run as a single task job with multiple maps and reduces i.e. similar to our current implementation of the LocalJobRunner. Thus, under certain conditions (maybe user-set configuration, or if input data is small i.e. less a DFS blocksize) we could launch a special task which will run all maps in a serial manner, followed by the reduces. This would really help small jobs achieve significantly smaller latencies, thanks to lesser scheduling overhead, jvm startup, lack of shuffle over the network etc. 

This would be a huge benefit, especially on large clusters, to small Hive/Pig queries.

Thoughts?",,aaa,aah,anty,apurtell,ashutoshc,cdouglas,coderplay,cutting,daijy,ddas,dhruba,dms,eli,gates,hammer,hong.tang,jenvor,jrideout,jsensarma,kbajda,kimballa,liangly,lianhuiwang,mahadev,nidaley,omalley,philip,rksingh,romainr,schen,sharadag,tlipcon,tomwhite,vicaya,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/10 03:07;acmurthy;MAPREDUCE-1220_yhadoop20.patch;https://issues.apache.org/jira/secure/attachment/12435542/MAPREDUCE-1220_yhadoop20.patch","08/Mar/11 16:05;roelofs;MR-1220.v1.trunk-hadoop-common.Progress-dumper.patch.txt;https://issues.apache.org/jira/secure/attachment/12472972/MR-1220.v1.trunk-hadoop-common.Progress-dumper.patch.txt","08/Mar/11 16:18;roelofs;MR-1220.v10e-v11c-v12b.ytrunk-hadoop-mapreduce.delta.patch.txt;https://issues.apache.org/jira/secure/attachment/12472977/MR-1220.v10e-v11c-v12b.ytrunk-hadoop-mapreduce.delta.patch.txt","08/Mar/11 16:18;roelofs;MR-1220.v13.ytrunk-hadoop-mapreduce.delta.patch.txt;https://issues.apache.org/jira/secure/attachment/12472978/MR-1220.v13.ytrunk-hadoop-mapreduce.delta.patch.txt","08/Mar/11 16:20;roelofs;MR-1220.v14b.ytrunk-hadoop-mapreduce.delta.patch.txt;https://issues.apache.org/jira/secure/attachment/12472980/MR-1220.v14b.ytrunk-hadoop-mapreduce.delta.patch.txt","08/Mar/11 16:23;roelofs;MR-1220.v15.ytrunk-hadoop-mapreduce.delta.patch.txt;https://issues.apache.org/jira/secure/attachment/12472981/MR-1220.v15.ytrunk-hadoop-mapreduce.delta.patch.txt","08/Mar/11 22:59;roelofs;MR-1220.v1b.sshot-02-jobdetails.jsp.png;https://issues.apache.org/jira/secure/attachment/12473074/MR-1220.v1b.sshot-02-jobdetails.jsp.png","08/Mar/11 23:06;roelofs;MR-1220.v1b.sshot-03-jobdetailshistory.jsp.png;https://issues.apache.org/jira/secure/attachment/12473075/MR-1220.v1b.sshot-03-jobdetailshistory.jsp.png","09/Sep/10 01:51;roelofs;MR-1220.v2.trunk-hadoop-mapreduce.patch.txt;https://issues.apache.org/jira/secure/attachment/12454177/MR-1220.v2.trunk-hadoop-mapreduce.patch.txt","08/Sep/10 22:04;roelofs;MR-1220.v2.trunk-hadoop-mapreduce.patch.txt;https://issues.apache.org/jira/secure/attachment/12454165/MR-1220.v2.trunk-hadoop-mapreduce.patch.txt","08/Mar/11 22:57;roelofs;MR-1220.v2b.sshot-01-jobtracker.jsp.png;https://issues.apache.org/jira/secure/attachment/12473073/MR-1220.v2b.sshot-01-jobtracker.jsp.png","08/Mar/11 16:09;roelofs;MR-1220.v6.ytrunk-hadoop-mapreduce.patch.txt;https://issues.apache.org/jira/secure/attachment/12472973/MR-1220.v6.ytrunk-hadoop-mapreduce.patch.txt","08/Mar/11 16:10;roelofs;MR-1220.v7.ytrunk-hadoop-mapreduce.delta.patch.txt;https://issues.apache.org/jira/secure/attachment/12472974/MR-1220.v7.ytrunk-hadoop-mapreduce.delta.patch.txt","08/Mar/11 16:15;roelofs;MR-1220.v8b.ytrunk-hadoop-mapreduce.delta.patch.txt;https://issues.apache.org/jira/secure/attachment/12472975/MR-1220.v8b.ytrunk-hadoop-mapreduce.delta.patch.txt","08/Mar/11 16:16;roelofs;MR-1220.v9c.ytrunk-hadoop-mapreduce.delta.patch.txt;https://issues.apache.org/jira/secure/attachment/12472976/MR-1220.v9c.ytrunk-hadoop-mapreduce.delta.patch.txt",,,,,,,,,,,,,,,,,,,,15.0,,,,,,,,,,,,,,,,,,,,2009-11-18 22:48:57.407,,,false,,,,,,,,,,,,,,,,,,60340,,,,,Tue Oct 18 06:53:38 UTC 2011,,,,,,,"0|i0jg2n:",111534,"An efficient implementation of small jobs by running all tasks in the same JVM, there-by effecting lower latency.",,,,,,,,,,,,,,,,,,,,"18/Nov/09 22:48;dhruba;I like this idea. If the configuration that triggers this new behaviour is user-settable, that will be great. ","18/Nov/09 22:52;tlipcon;I like the idea, but am curious what the implementation path would be. The ""Task Type"" stuff is already pretty messy in the JT, so I'm concerned that this will worsen the situation. Will there be a refactor patch prior to this that cleans up the existing functionality before adding more task types?","18/Nov/09 23:17;schen;+1 on the idea. This will be very useful for the hive users.","19/Nov/09 01:09;mahadev;+1 .. seems like a very useful idea.... ","19/Nov/09 01:47;ddas;+1 seems like a good thing. But maybe it is possible to do this without the special task. I am imagining that a combination of jvm-reuse, and, special job attributes (like if the Job's input is less than a dfs block), then send all tasks of that job to the tasktracker, will make it work.","19/Nov/09 03:36;hong.tang;+1 on the direction.

Agree with Dhruba that this should be explicitly enabled until we have more experience.

Not sure we should mandate the serialization of task execution, maybe we can use multiple slots if available on a particular TT? We probably should defer such optimizations after JT refactor is done though.","19/Nov/09 03:45;kimballa;I'm not really sure I see the utility of such a big change. Would this really be that much higher performance than running locally on the client? The client needs to have access to HDFS anyway in order to do things like create the InputSplits for the cluster; so what's the advantage of running a single-threaded process on the cluster? You'd still need to do some of the more heavyweight job-setup operations -- ship the client jar over, spawn a separate JVM (even if it's reused for all map/reduce tasks), set up the task IPC connection to the tasktracker, etc. You'd also be vulnerable to the inherent very large time penalty associated with the ""tasktracker polls"" model of task scheduling that Hadoop uses. 

If the job is really so small that it makes sense to run it in a single thread, then I am suspicious that the overhead described above would be overcome by running in the quasi-locality of the cluster, vs. just staying in the client and starting immediately.

If you do need this behavior, though, then rather than build a significant new amount of internal architecture, it occurs to me that you could probably do this all with ""user level"" code (maybe something that goes in the o.a.h.mapreduce.lib package) as follows: Write a map-only job that uses something like NLineInputFormat to create a single map task. That single map task could then itself be used as a springboard to set up the real job (maybe you've pre-serialized the jobconf.xml in the client and sent it to the singleton map task via the distributed cache) and run it in the existing LocalJobRunner there. I think this approach would be a lot cleaner; thoughts?
","19/Nov/09 06:41;acmurthy;bq. Would this really be that much higher performance than running locally on the client? If the job is really so small that it makes sense to run it in a single thread, then I am suspicious that the overhead described above would be overcome by running in the quasi-locality of the cluster, vs. just staying in the client and starting immediately.

Whilst I agree that applications can do it currently with LocalJobRunner, this jira is all about making it completely transparent to them! 

Imagine you have a large pipeline of jobs, some of which are small and other are not. In this case it would be a nightmare for the owner of the workflow to configure parts of them differently from the others. With this feature jobs would get the benefits without requiring heavy amounts of per-job crafting, capacity-planning etc.

Also lots of real-world clusters have special 'launcher' (or gateway) nodes which really do not have the capacity to run very many jobs via the LocalJobRunner.

----

bq. The client needs to have access to HDFS anyway in order to do things like create the InputSplits for the cluster; so what's the advantage of running a single-threaded process on the cluster?

Short-to-mid term we would all like to see MAPREDUCE-207 get in, this way InputSplits are computed in-cluster... we have talked about the 'setup' task computing the splits - potentially it could then turn around and run the job itself. 
","19/Nov/09 11:35;kimballa;Arun,

My concerns are primarily based around how much code would need to change in the tasktrackers to work with a new type of job runner, etc. As such, I suggested a possible internal implementation model that does not require building a new JobRunner nor changing how child tasks interface with the tasktracker. I understand why you want this to integrate cleanly into user-level job scheduling/configuration. So to that end, please feel free to suggest a clean interface which goes on top of this for the client to work with.

For example, I think that we could add a {{Job.setSingleProcess()}} method which users use to configure a job in this mode; it would then create another {{Job}} internally that uses the process I described above to bootstrap into the real Job. The actual mechanics of how to manage the subprocess are still done in a ""regular"" map task that itself uses the LocalJobRunner. Does this make sense?
","20/Nov/09 18:30;acmurthy;Aaron,

I'm, foremost, trying to sell this idea to the community at large. It is a new (major?) functionality and hence my interest in ensuring people are comfortable about the direction I'm proposing. 

From your first comment I was under the impression that you had reservations about this direction. Maybe I was mistaken about your lack of enthusiasm for the proposal, maybe I put too much emphasis on the implementation aspects when I put forth the proposal. My apologies.

Backing up, I'd appreciate if you could re-state your position on the direction. 

----

To keep things clear, I'll post a separate comment about some of my thoughts on the implementation details once we all agree about the direction. Your thoughts are, as always, welcome there too.","20/Nov/09 18:45;philip;Arun,

I'm definitely +1 on having small jobs trigger one JVM start instead of 3+N (setup, N maps, reduce, cleanup) for small N.

Implementation-wise, I'm wary of adding yet more dimensions and special-cases to the JobTracker.  Does one of these ""quick"" jobs take a map slot, scheduling wise, or does it take both a map and a reduce slot?  Does the framework make the optimization itself, or do users ask for it?

-- Philip","21/Nov/09 07:30;kimballa;Arun,

Thank you for explaining your use cases above more thoroughly. Given the nature of clusters with submission nodes, etc., this does seem like it would be valuable functionality. As such, it can/should be added. +1 on the direction.

Maybe I was jumping the gun on reading your description; you mentioned the LocalJobRunner, so I thought you were off-the-bat proposing to build another *JobRunner. Since we already have three job runners (the usual one, the local one, and the isolation runner), all of which have their own quirks, idiosyncrasies and bugs, I would be nervous about yet another one of these which will have its own slightly deviant semantics, and hope that we could reuse a lot of the existing task deployment code. 

Some questions about the proposal itself:
* Will users be able to set a number of reduce tasks greater than one? Or are you limited to at most one reduce task?
* How does this work with speculative execution, if at all?
* Are all InputSplits associated with the task delivered all-at-once from the JobTracker? Or does this ""super-task"" for the job fetch them one-at-a-time? For that matter, will this be represented in the JobTracker as multiple InputSplits, or just one?
** How is InputSplit locality information regarded? If we want to map over (for example) three small files, and FileInputFormat enumerates three InputSplits, each of which have different locality hints, how does the JobTracker pick where to schedule this job? 
* How do these sorts of jobs interact with the resource allocation algorithms used by the Fair/Capacity Schedulers?

Thanks,
- Aaron
","11/Feb/10 03:07;acmurthy;I spent a long (and happy) weekend building a half-baked *prototype* for this...

Essentially, I've introduced a new kind of task, called ""Uber Task"", half in jest. I've got it to mimic the old local job-runner by running all maps serially and then a single reduce. It needs a lot more work to fix things on the JobTracker, TaskTracker, Scheduler and so on. Most of the effort involved teasing out the framework in the MapTask and ReduceTask to allow several components such as MapOutputBuffer, ReduceValuesIterator etc. to be used as 'pluggable' components. ","12/Feb/10 04:55;tomwhite;bq. Most of the effort involved teasing out the framework in the MapTask and ReduceTask to allow several components such as MapOutputBuffer, ReduceValuesIterator etc. to be used as 'pluggable' components.

Interesting. MAPREDUCE-326 has a proposal for making these components pluggable, which might make the work of this JIRA simpler.","13/Feb/10 09:12;jsensarma;interesting jira! i have been clamoring for something like this internally .. (am totally at the receiving end of these days)

i am still a little dubious about whether this is ambitious enough. particularly - why serial execution? to me local execution == exploiting resources available in a single box. which are 8 core today and on the way up.

my suggestion internally was to have a set of machines running single node hadoop clusters and dispatch to one of those (randomly or using a load-balancer) when job size is small. i am struggling to see why i should like this solution instead.

it does seem that a dispatcher facility at the lowest layer that can juggle between different hadoop clusters is useful generically (across Hive/Pig/native-Hadoop etc.) - but that's not quite the same as what's proposed here - is it?","08/Sep/10 22:04;roelofs;Updated version of Arun's prototype patch; compiles cleanly, but not tested beyond that.","09/Sep/10 01:51;roelofs;Oops, here's the real updated patch.  (Forgot to ""git add"" the two new files, sigh.)","15/Sep/10 04:28;roelofs;I've been looking at what it will take to extend this from Arun's February sketch to something that will actually schedule and run small jobs. At the moment it looks like it will largely avoid JobTracker; most of the action seems to be in JobInProgress (particularly {{initTasks()}} and {{obtainNew*Task()}}), TaskInProgress ({{findNew*Task()}}, {{getTaskToRun()}}, {{addRunningTask()}}), and the scheduler ({{assignTasks()}}).

I haven't asked Arun what he originally had in mind, but it seems that there are two fairly obvious approaches:

 * treat UberTask (or MetaTask or MultiTask or AllInOneTask or ...) as a third kind of Task, not exactly like either MapTask or ReduceTask but a peer (more or less) to both
 * treat UberTask as a variant (subclass) of MapTask

I see the first approach as conceptually cleaner, and some of its implementation details would be cleaner as well, but overall it's harder to implement: there are lots of places where there's map-vs-reduce logic (occasionally with special setup/cleanup cases), and most of it would require modifications. The second approach seems slightly hackish, but it has at least that one big advantage: many of the map-vs-reduce bits of code would not require changes - in particular, I don't believe it would be necessary to touch the schedulers (and we're up to, what, four at this point?) since they'd simply see a job containing one map and no reduces.

Thoughts? (I don't yet have a strong opinion myself, though I'm interested in getting a proof of concept running quickly so I/we can see what works and whether it's a net win in the first place; from that perspective, the latter approach may be better.)

Either way, there will be changes needed in the UI, metrics, and other accounting to surface the tasks-within-a-task details, as well as the obvious configuration-related changes. Retry logic, speculation, etc., are still unclear (to me, anyway).

Stab at a couple of the other upstream questions:

bq. Will users be able to set a number of reduce tasks greater than one? Or are you limited to at most one reduce task?

The latter, at least for now. This is somewhat related to the question of serial execution; i.e., if it's serial, the only reason you would want multiple reduces is if a single one is too big to fit in memory, and if that's the case, it's arguably not a ""small job"" anymore.

bq. i am still a little dubious about whether this is ambitious enough. particularly - why serial execution? to me local execution == exploiting resources available in a single box. which are 8 core today and on the way up.

Well, generally multiple cores =&gt; multiple slots, at which point you let the scheduler figure it out. Chris mentioned that there's a JIRA to extend the LocalJobRunner in this direction (MAPREDUCE-434?), and if the currently proposed version of _this_ one works out, an obvious next step would be to look at similar extensions. But this is still an untested optimization, and all the usual caveats about premature optimization apply.

I don't know how Arun looked at the problem - I'm guessing the motivation was empirical, based on the behavior of Oozie workflows and Pig jobs - but I view it as way to make task granularity a bit more homogeneous by packing multiple, too-small tasks into larger containers. My mental model is that that will tend to make the scheduler more efficient - but also that trying to do too much may begin to work at cross-purposes with the scheduler, i.e., one probably doesn't want to create a secondary scheduler (trying to tune both halves could get very messy).

bq. it does seem that a dispatcher facility at the lowest layer that can juggle between different hadoop clusters is useful generically (across Hive/Pig/native-Hadoop etc.) - but that's not quite the same as what's proposed here - is it?

Nope. But I think other groups are looking at stuff like that.","08/Mar/11 15:44;roelofs;If you've been watching the commits go by on Owen's yahoo-merge branch, about five months' worth of work on this was included.  Unfortunately, I screwed up my most recent push to Yahoo's internal git repo, and as a result, every internal/temporary/debug commit was exposed, which amounts to a lot of noise (> 3 dozen extra commits).

I'll post the ~8 ""real"" patches corresponding to all of that here, along with a dump() method for Progress (in common), which was useful for debugging and may be needed again.  There are also a few screenshots, but I'll probably need to scrub some internal hostnames before posting those.","08/Mar/11 16:05;roelofs;Not sure if this is worthy of its own HADOOP-xxx issue, but it was useful while debugging UberTask's 3-level Progress/phase tree. (Progress needs more help than this, but that's a topic for another day.)","08/Mar/11 16:09;roelofs;Part 1/8.  The ""v6"" (and similar in following patches) matches the commit-comments in the yahoo-merge branch.","08/Mar/11 16:10;roelofs;Part 2/8: MR-1220.v7.ytrunk-hadoop-mapreduce.delta.patch.txt","08/Mar/11 16:15;roelofs;Part 3/8: MR-1220.v8b.ytrunk-hadoop-mapreduce.delta.patch.txt

fix map-only commit bug; fix 4 unit tests

(Part 2/8 simply disabled ubermode by default.)","08/Mar/11 16:16;roelofs;Part 4/8: MR-1220.v9c.ytrunk-hadoop-mapreduce.delta.patch.txt

fix abortTask() cleanup; fix TASK_CLEANUP case; fix 10 more unit tests","08/Mar/11 16:18;roelofs;Part 5/8: MR-1220.v10e-v11c-v12b.ytrunk-hadoop-mapreduce.delta.patch.txt

fix remaining unit tests; disable uber-Chain combo; fix _temporary dir-name bug","08/Mar/11 16:18;roelofs;Part 6/8: MR-1220.v13.ytrunk-hadoop-mapreduce.delta.patch.txt

add support for compressed map outputs","08/Mar/11 16:20;roelofs;Part 7/8:  MR-1220.v14b.ytrunk-hadoop-mapreduce.delta.patch.txt

add counter feedback for users/QA","08/Mar/11 16:23;roelofs;Part 8/8:  MR-1220.v15.ytrunk-hadoop-mapreduce.delta.patch.txt

correct fix for commit bugs (vs. version 12b in ""Part 5/8"" above)

[original comment: correct fix for commit bugs (vs. version 8b)]","08/Mar/11 16:48;roelofs;Status: basically functional; I believe all otherwise-passing unit tests still pass. Unfortunately, because of the duration over which patches were committed (and intervening commits), there's no easy way (that I'm aware of) to merge everything back into one patch. I'm currently working on the ""MR v2"" version (see MAPREDUCE-279), which is much less hackish and shares very little with the version above. I'm not sure this version has a future, but the patches are here if anyone is interested.

Known bugs:

 - ""Re-localization"" is missing. Specifically, because all subtasks run in the same JVM, and Java doesn't have chdir(), there's no clean way to isolate them from each other. If any but the last sub-MapTask does something obnoxious (e.g., delete a distcache symlink or create a file that any other subtask wants to create), things will break.  Obviously this is a problem for an optimization that's supposed to be (mostly) transparent to users.

 - Progress is still broken, apparently. Everything seemed to check out when I had gobs of debugging in there, but it doesn't make it to the UI (including the client) as frequently as it should. No clue what broke.

 - The max-input-size decision criterion (in JobInProgress) should check the default block size (if appropriate) for the actual input filesystem, not use a hardcoded HDFS config that's not necessarily available to tasktracker nodes anyway.

 - The UI changes are incomplete, and there are some 404 and error links in some cases. Basically, the whole idea of masquerading an UberTask as a ReduceTask, yet exposing it to the user in some cases, is awkward, and there are a _lot_ of JSP pages to handle.

There are also some cleanup items (test and potentially enable reduce-only case; fix memory criterion in uber-decision for map-only [and reduce-only] cases; clean up TaskStatus mess; instead of renaming file.out to map_#.out, always use attemptID.out; etc.).  However, those kind of pale in comparison to the overall intrusive grubbiness of the patch. :-/","08/Mar/11 22:57;roelofs;screenshot of top-level (multi-job) JobTracker page

Main addition is the UberTask details under the ""Job Scheduling Information"" column at far right.  The uber stuff gets appended if there's anything else there (as is the case with the capacity scheduler).

Colors: pale yellow for running jobs; pale pink for failed/killed jobs; pale green for successful jobs. (Not uber-specific, but trivial and in the same place as some of the other changes.)","08/Mar/11 22:59;roelofs;screenshot of jobdetails page (uber-job still running)

The ""Job Scheduling information"" line shows up again here, but the top table is also modified, as is the title of the graph. Trivial stuff, but it provides a clue to the user in case the optimization is less transparent than intended.","08/Mar/11 23:06;roelofs;MR-1220.v1b.sshot-03-jobdetailshistory.jsp.png

screenshot of the jobdetailshistory page (uber-job complete)

This was taken before the setup and cleanup tasks were moved inside UberTask. The UI didn't get updated for that, IIRC, so I think it shows lots of zeros on the top and bottom lines now. (Another TODO item...)

Note also the wrappable column heading, courtesy of ""<wbr>"" pseudo-tags. (Browsers optionally break there, but cut-and-paste doesn't pick up a spurious space. Highly recommended for other fat table cells such as counter names and types, job names, hostnames, etc. I should file a separate JIRA...)","18/Oct/11 06:49;acmurthy;Fixed via MAPREDUCE-279.","18/Oct/11 06:53;acmurthy;Duplicate of MAPREDUCE-2405.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Gridmix emulate usage of data compression,MAPREDUCE-2408,12502687,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,amar_kamat,ravidotg,ravidotg,29/Mar/11 09:29,15/Nov/11 00:48,12/Jan/21 09:52,27/May/11 06:26,,,,,,0.23.0,,,contrib/gridmix,,,,,,0,,,,,Currently Gridmix emulates disk IO load only. This JIRA is to make Gridmix emulate load due to data compression as defined by the job-trace.,,atm,hong.tang,lianhuiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-2542,,,,,"26/May/11 17:51;amar_kamat;MR-2408-gridmix-compression-emulation-v1.1.patch;https://issues.apache.org/jira/secure/attachment/12480563/MR-2408-gridmix-compression-emulation-v1.1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2011-05-26 17:46:04.037,,,false,,,,,,,,,,,,,,,,,,150175,Reviewed,,,,Sun May 29 03:16:58 UTC 2011,,,,,,,"0|i09e1b:",52707,"Emulates the MapReduce compression feature in Gridmix. By default, compression emulation is turned on. Compression emulation can be disabled by setting 'gridmix.compression-emulation.enable' to 'false'.  Use 'gridmix.compression-emulation.map-input.decompression-ratio', 'gridmix.compression-emulation.map-output.compression-ratio' and 'gridmix.compression-emulation.reduce-output.compression-ratio' to configure the compression ratios at map input, map output and reduce output side respectively. Currently, compression ratios in the range [0.07, 0.68] are supported. Gridmix auto detects whether map-input, map output and reduce output should emulate compression based on original job's compression related configuration parameters.",,,,,,,,,,,,,,,,,,,,"26/May/11 17:46;amar_kamat;The goal of this jira is to emulate the compression characteristics of a MapReduce job. Emulating compression characteristics involves the following 1. Generating compressible data. The compression characteristics (e.g compression ratio) of the data (map input, map output and reduce output) should be configurable. 2. Extract compression related properties from original job's configuration and history files. Configure the simulated job to mimic the compression behavior using the original job's configuration and history. ","26/May/11 17:51;amar_kamat;Attaching a patch implementing compression emulation support in Gridmix. test-patch and ant tests passed. Manually tested the patch.","26/May/11 17:51;amar_kamat;Running through Hudson.","26/May/11 22:47;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12480563/MR-2408-gridmix-compression-emulation-v1.1.patch
  against trunk revision 1127444.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:
                  org.apache.hadoop.cli.TestMRCLI
                  org.apache.hadoop.tools.TestHadoopArchives
                  org.apache.hadoop.tools.TestHarFileSystem

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/308//testReport/
Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/308//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/308//console

This message is automatically generated.","27/May/11 05:56;ravidotg;Tests failed are not related to this patch. Findbugs warnings reported by Hudson are also not related to this patch.

Patch looks good to me. +1","27/May/11 06:26;amar_kamat;I just committed this to trunk. Thanks Ravi for the review!","27/May/11 06:38;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #703 (See [https://builds.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/703/])
    MAPREDUCE-2408. [Gridmix] Compression emulation in Gridmix. (amarrk)

amarrk : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1128162
Files : 
* /hadoop/mapreduce/trunk/CHANGES.txt
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixRecord.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/CompressionEmulationUtil.java
* /hadoop/mapreduce/trunk/src/docs/src/documentation/content/xdocs/gridmix.xml
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestCompressionEmulationUtils.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/AvgRecordFactory.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/SleepJob.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/RandomTextDataGenerator.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestRandomTextDataGenerator.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Gridmix.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/FileQueue.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateData.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/InputStriper.java
","27/May/11 15:43;hudson;Integrated in Hadoop-Mapreduce-trunk #692 (See [https://builds.apache.org/hudson/job/Hadoop-Mapreduce-trunk/692/])
    MAPREDUCE-2408. [Gridmix] Compression emulation in Gridmix. (amarrk)

amarrk : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1128162
Files : 
* /hadoop/mapreduce/trunk/CHANGES.txt
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixRecord.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/CompressionEmulationUtil.java
* /hadoop/mapreduce/trunk/src/docs/src/documentation/content/xdocs/gridmix.xml
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestCompressionEmulationUtils.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/AvgRecordFactory.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/SleepJob.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/RandomTextDataGenerator.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestRandomTextDataGenerator.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Gridmix.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/FileQueue.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateData.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/InputStriper.java
","28/May/11 18:23;hong.tang;Looks like I missed it before it gets committed. I quickly went through the patch. I like the approach of using a dictionary and empirically match the compression ratio with the dictionary size. However, I believe the compression ratio would be different under different compression codecs (even same codec under different levels). It'd be useful if you could extend CompressionRatioLookupTable so that it takes as input a compression codec (and you may only support the most common few codecs lzo, gzip, and bzip2).","29/May/11 02:47;amar_kamat;Hong,
Thanks a lot for your review. You are right. The compression ratios table will be different for different codecs. The empirical values table in this patch is computed for the default codec (i.e Gzip). We have compiled similar table for LZO and it seems LZO too shows some pattern in that respect. The plan is to add other codecs incrementally. I will open a JIRA to track LZO compression emulation.","29/May/11 03:16;amar_kamat;Opened MAPREDUCE-2542 for tracking LZO codec support in Gridmix.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[rumen] Add a map of jobconf key-value pairs in LoggedJob,MAPREDUCE-2151,12478282,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,hong.tang,hong.tang,hong.tang,26/Oct/10 00:54,15/Nov/11 00:48,12/Jan/21 09:52,24/Jun/11 05:46,,,,,,0.23.0,,,tools/rumen,,,,,,0,,,,,It'd be useful to retain application level configuration settings in LoggedJob.,,ranjit,ravidotg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/10 21:10;hong.tang;mr-2151-yhadoop-20.201-20101029-2.patch;https://issues.apache.org/jira/secure/attachment/12458451/mr-2151-yhadoop-20.201-20101029-2.patch","29/Oct/10 00:20;hong.tang;mr-2151-yhadoop-20.201.patch;https://issues.apache.org/jira/secure/attachment/12458295/mr-2151-yhadoop-20.201.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2010-10-26 05:18:08.826,,,false,,,,,,,,,,,,,,,,,,70123,,,,,Fri Jun 24 05:46:01 UTC 2011,,,,,,,"0|i0jibz:",111900,,,,,,,,,,,,,,,,,,,,,"26/Oct/10 05:18;ravidotg;All the configuration settings that are not same as ""the values that are there in XXX-default.xml"" would be good enough ?","26/Oct/10 05:33;amar_kamat;I think Hong is referring to configuration parameters that are likely to modify the behaviour of the job and tasks (e.g _mapred.child._* , _mapreduce.job._* etc). Hong, is my understanding correct? Also *_-default.xml_ might not be available for reference comparison. Hence for now we might need to identify (i.e handpick) these configuration parameters and add them to _interesting properties_ list.","26/Oct/10 06:11;ravidotg;Hmm. But I guess we need to bring in more and more configuration properties soon.
Created MAPREDUCE-2153 to get other needed configuration properties in to the trace file.

Also created MAPREDUCE-2152 for avoiding TraceBuilder's its own handling of deprecated configuration properties in favour of Configuration object.","26/Oct/10 17:56;hong.tang;bq. I think Hong is referring to configuration parameters that are likely to modify the behaviour of the job and tasks (e.g mapred.child.* , mapreduce.job.* etc).

No, this is not what this jira intends to solve. But this jira could potentially help. Currently Rumen extracts from jobconf.xml some key-values specific to map-reduce layer, and converts them to regular primitive types. I think the extraction of mapred.child.* and mapreduce.job.* etc should continue along this path.

However, we start to think of using Rumen output to analyze performance of frameworks on top of map-reduce. One example is Pig. Pig will add more information in jobconf.xml to describe the features being used, and compile-time statistics, We need to have a mechanism in Rumen to retain such information in an extensible way, and is the primary purpose of this jira.

bq. Also *-default.xml might not be available for reference comparison. 
Correct. That is the main reason we have to make each parsed LoggedJob instance self-contained.

bq. Hmm. But I guess we need to bring in more and more configuration properties soon.
Yes, it will be,  but not unbounded. I think we can support extraction of properties based on exact match or prefixes.

bq. Created MAPREDUCE-2153 to get other needed configuration properties in to the trace file. 
This seems to be in addition to MAPREDUCE-1658. I suggest you roll two jiras into one (closing MR-1658 and roll the work int oMR-2153).

bq. Also created MAPREDUCE-2152 for avoiding TraceBuilder's its own handling of deprecated configuration properties in favour of Configuration object.
The purpose of this jira is to extend the set of key-values to be extracted by jobconf parser and retain them as-is in LoggedJob object. So I believe your point is relatively orthogonal to this jira. FWIW, I am a bit concerned to introduce this dependency between Rumen and MapReduce because I think the handling deprecated conf parameters is not really a core part of MapReduce API and could be dropped in the future (which would lead us to move the code into Rumen - similar to the case of Pre21JobHistoryConstants).","29/Oct/10 00:20;hong.tang;Patch for yahoop hadoop 20.200.","29/Oct/10 00:26;hong.tang;What are included in the patch I uploaded:
- Added a Map<String, String> field named ""configuration"" in LoggedJob.
- Changed JobConfigurationParser to handle extraction based on prefix. Also handle a special case to extract all key-value pairs (by specifying the interested prefix list to be null).
- Added a StringTrie that supports the partial matching needed for JobConfigurationParser.
- Added unit tests for JobConfigurationParser and StringTrie.
- Updated the existing unit tests (the golden files to include the configuration object).","29/Oct/10 06:26;ravidotg;(1) The paths to the new tests seem to be different from existing tests. src/test/org/apache/hadoop/tools/rumen/Test* instead of src/test/mapred/org/apache/hadoop/tools/rumen/. Is this intentional ?

(2) TestRumenJobTraces has testJobConfigurationParser() and this patch added TestJobConfigurationParser.java. Can the old testcase be removed/moved from TestRumenJobTraces ?

(3) Are we not targeting to get these interested configuration properties into trace file ? I don't see a way to specify the interested properties regular expression or the prefix of the interested configuration properties in TraceBuilder. With this patch also, the trace file doesn't contain any new configuration properties as TraceBuilder.run() gets the interested properties from JobConfPropertyNames. Should we add an option to TraceBuilder that takes regular expression and dumps all the config properties that match the regular expression ?

(4) This patch matches only the first part of configuration property(till first "".""). Is there an easy way to make it match more than that ?
Basically, I would like to specify ""mapreduce.job."" as prefix for my interested configuration properties.
Also it would be cool to be able to specify uninterested properties' prefixes like ""mapreduce.jobtracker."" and ""mapreduce.tasktracker."" to get all other config properties other than those matching these 2 patterns.","29/Oct/10 07:26;hong.tang;bq. (1) The paths to the new tests seem to be different from existing tests. src/test/org/apache/hadoop/tools/rumen/Test* instead of src/test/mapred/org/apache/hadoop/tools/rumen/. Is this intentional ? 

No, the tests for trunk are under src/test/mapred. This patch is for yahoo hadoop 0.20.

bq. (2) TestRumenJobTraces has testJobConfigurationParser() and this patch added TestJobConfigurationParser.java. Can the old testcase be removed/moved from TestRumenJobTraces ?

I was not aware of that. I think your suggestion makes sense.

bq. (3) Are we not targeting to get these interested configuration properties into trace file ?

Yes, TraceBuilder needs to be modified to expose the new feature to end user. Will add it.

bq. (4) This patch matches only the first part of configuration property(till first ""."")...

I do not follow your first part. For the second part (exclusion list), it will add significant complexity (now the order of the list may matter). I suggest we wait until some concrete usage case emerge.","29/Oct/10 07:53;ravidotg;>>I do not follow your first part. For the second part (exclusion list), it will add significant complexity (now the order of the list may matter). I suggest we wait until some concrete usage case emerge.

I didn't go through the match() method earlier. But what I wanted to give as input is something like ""mapreduce.job."" as the prefix and get all the config properties that start with ""mapreduce.job."" into the trace file. I think it can be done. Let me see your next patch which adds an option to TraceBuiler and see if anything is missing from my expectations.

I agree that it may be complex to support exclusion list with StringTrie. Let us see that case later if that case is really important.","29/Oct/10 21:10;hong.tang;Patch that addresses ravi's comments.
- Moved a testcase from TestRumenJobTraces to TestJobConfigurationParser.
- Added the plumbing to allow users to add to the list of properties to be extracted from jobconf.

Note that to extract all properties started with ""mapred.*"", one just needs to specify ""mapred"". The prefix (or property name) string is first tokenized by splitting based on ""."", and then each token has to be an exact match of the property string tokens in sequence. So it will not match any properties start with ""mapreduce"".","02/Nov/10 10:35;amar_kamat;Hong,
Isn't it simple to call _Configuration.getValByRegex()_ for each regex specified by the gridmix user and populate the key-val pair maintained in {{JobConfigurationParser}}? ","24/Jun/11 05:46;amar_kamat;This got committed as part of MAPREDUCE-2153.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MR-279: Metrics for NodeManager,MAPREDUCE-2532,12508213,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,vicaya,vicaya,vicaya,24/May/11 16:15,15/Nov/11 00:48,12/Jan/21 09:52,26/May/11 03:00,0.23.0,,,,,0.23.0,,,mrv2,,,,,,0,,,,,Metrics for node manager. Requires a recent (last night) update of hadoop common in the yahoo-merge branch. ,,mahadev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/May/11 16:18;vicaya;mr-2532-nm-metrics-v1.patch;https://issues.apache.org/jira/secure/attachment/12480274/mr-2532-nm-metrics-v1.patch","25/May/11 01:52;vicaya;mr-2532-nm-metrics-v2.patch;https://issues.apache.org/jira/secure/attachment/12480351/mr-2532-nm-metrics-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-05-26 03:00:09.893,,,false,,,,,,,,,,,,,,,,,,44130,Reviewed,,,,Thu May 26 03:00:09 UTC 2011,,,,,,,"0|i09dnr:",52646,,,,,,,,,,,"metrics, mrv2, nm",,,,,,,,,,"25/May/11 01:52;vicaya;Patch v2 added missing change to MiniYarnCluster. All tests are passing with the patch.","26/May/11 03:00;mahadev;I just committed this to MR-279 branch. thanks luke.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mapreduce RPM integration project,MAPREDUCE-2521,12507808,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,eyang,eyang,eyang,19/May/11 20:16,15/Nov/11 00:48,12/Jan/21 09:52,27/May/11 17:04,,,,,,0.23.0,,,build,,,,,,0,,,,,This jira is corresponding to HADOOP-6255 and associated directory layout change. The patch for creating Mapreduce rpm packaging should be posted here for patch test build to verify against mapreduce svn trunk.,"Java 6, RHEL 5.5",aah,atm,gates,szetszwo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/May/11 22:44;eyang;MAPREDUCE-2521-1.patch;https://issues.apache.org/jira/secure/attachment/12479855/MAPREDUCE-2521-1.patch","19/May/11 23:16;eyang;MAPREDUCE-2521-2.patch;https://issues.apache.org/jira/secure/attachment/12479857/MAPREDUCE-2521-2.patch","20/May/11 02:01;eyang;MAPREDUCE-2521-3.patch;https://issues.apache.org/jira/secure/attachment/12479873/MAPREDUCE-2521-3.patch","23/May/11 22:40;eyang;MAPREDUCE-2521-4.patch;https://issues.apache.org/jira/secure/attachment/12480179/MAPREDUCE-2521-4.patch","24/May/11 21:40;eyang;MAPREDUCE-2521-5.patch;https://issues.apache.org/jira/secure/attachment/12480327/MAPREDUCE-2521-5.patch","25/May/11 23:38;eyang;MAPREDUCE-2521-6.patch;https://issues.apache.org/jira/secure/attachment/12480477/MAPREDUCE-2521-6.patch","26/May/11 22:36;eyang;MAPREDUCE-2521-7.patch;https://issues.apache.org/jira/secure/attachment/12480596/MAPREDUCE-2521-7.patch","19/May/11 20:19;eyang;MAPREDUCE-2521.patch;https://issues.apache.org/jira/secure/attachment/12479823/MAPREDUCE-2521.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,2011-05-19 22:13:04.449,,,false,,,,,,,,,,,,,,,,,,72253,Reviewed,,,,Thu Jun 02 16:35:21 UTC 2011,,,,,,,"0|i09dof:",52649,Created rpm and debian packages for MapReduce. ,,,,,,,,,,,,,,,,,,,,"19/May/11 20:19;eyang;Usage:

ant rpm
","19/May/11 22:01;eyang;Store config templates in $PREFIX/share/hadoop/templates, and change related script to use the new location.","19/May/11 22:13;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12479850/HADOOP-6255-mapred-trunk-2.patch
  against trunk revision 1125082.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 15 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/279//console

This message is automatically generated.","19/May/11 22:44;eyang;Attach correct patch.","19/May/11 22:57;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12479823/MAPREDUCE-2521.patch
  against trunk revision 1125082.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 15 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    -1 release audit.  The applied patch generated 4 release audit warnings (more than the trunk's current 2 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/278//testReport/
Release audit warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/278//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/278//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/278//console

This message is automatically generated.","19/May/11 23:16;eyang;Package librecordio to lib.","20/May/11 01:37;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12479855/MAPREDUCE-2521-1.patch
  against trunk revision 1125148.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 15 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    -1 release audit.  The applied patch generated 4 release audit warnings (more than the trunk's current 2 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/280//testReport/
Release audit warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/280//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/280//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/280//console

This message is automatically generated.","20/May/11 02:01;eyang;* Exclude local copy of configuration.xsl and hadoop-metrics2.properties from MAPREDUCE project.","20/May/11 02:18;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12479857/MAPREDUCE-2521-2.patch
  against trunk revision 1125148.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 16 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    -1 release audit.  The applied patch generated 4 release audit warnings (more than the trunk's current 2 warnings).

    -1 core tests.  The patch failed these core unit tests:
                  org.apache.hadoop.mapred.TestJobQueueInformation

    +1 contrib tests.  The patch passed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/281//testReport/
Release audit warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/281//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/281//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/281//console

This message is automatically generated.","20/May/11 04:50;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12479873/MAPREDUCE-2521-3.patch
  against trunk revision 1125148.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 16 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    -1 release audit.  The applied patch generated 4 release audit warnings (more than the trunk's current 2 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/282//testReport/
Release audit warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/282//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/282//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/282//console

This message is automatically generated.","23/May/11 22:40;eyang;Change configuration directory from $PREFIX/conf to $PREFIX/etc/hadoop per Owen's recommendation.  For RPM/deb, it will use /etc/hadoop as default, and create symlink for $PREFIX/etc/hadoop point to /etc/hadoop.","24/May/11 01:31;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12480179/MAPREDUCE-2521-4.patch
  against trunk revision 1126801.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 16 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    -1 release audit.  The applied patch generated 4 release audit warnings (more than the trunk's current 2 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/299//testReport/
Release audit warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/299//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/299//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/299//console

This message is automatically generated.","24/May/11 21:40;eyang;Include example, test, and tool jar files in bin-package target.","25/May/11 01:59;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12480327/MAPREDUCE-2521-5.patch
  against trunk revision 1126801.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 16 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    -1 release audit.  The applied patch generated 4 release audit warnings (more than the trunk's current 2 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/303//testReport/
Release audit warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/303//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/303//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/303//console

This message is automatically generated.","25/May/11 23:38;eyang;Make sure webapps directory is bundled and class path reference is corrected.","26/May/11 09:07;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12480477/MAPREDUCE-2521-6.patch
  against trunk revision 1127444.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 16 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:
                  org.apache.hadoop.cli.TestMRCLI

    +1 contrib tests.  The patch passed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/304//testReport/
Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/304//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/304//console

This message is automatically generated.","26/May/11 22:36;eyang;Minor bug fixes:

- Removed *.debian and *.redhat script from /usr/sbin
- Renamed package from hadoop-mapred to hadoop-mapreduce
- Renamed dependency from hadoop to hadoop-common","27/May/11 01:23;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12480596/MAPREDUCE-2521-7.patch
  against trunk revision 1127444.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 16 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:
                  org.apache.hadoop.cli.TestMRCLI

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/312//testReport/
Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/312//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/312//console

This message is automatically generated.","27/May/11 17:04;omalley;I just committed this. Thanks, Eric!","27/May/11 17:26;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #705 (See [https://builds.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/705/])
    MAPREDUCE-2521. Create RPM and Debian packages for MapReduce. Changes 
deployment layout to be consistent across the binary tgz, rpm, and deb.
(Eric Yang via omalley)

omalley : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1128394
Files : 
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/generateData.sh
* /hadoop/mapreduce/trunk/bin/mapred
* /hadoop/mapreduce/trunk/src/test/mapred/org/apache/hadoop/fs/DFSCIOTest.java
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/webdatasort/webdata_sort.large
* /hadoop/mapreduce/trunk/src/packages/deb/hadoop.control/control
* /hadoop/mapreduce/trunk/src/contrib/mrunit/src/java/org/apache/hadoop/mrunit/package.html
* /hadoop/mapreduce/trunk/src/packages/deb/hadoop.control/prerm
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix2/rungridmix_2
* /hadoop/mapreduce/trunk/src/packages/templates/conf/mapred-site.xml
* /hadoop/mapreduce/trunk/src/packages/rpm/spec/hadoop-mapred.spec
* /hadoop/mapreduce/trunk/src/packages/templates/conf
* /hadoop/mapreduce/trunk/src/packages/deb/hadoop.control/preinst
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/streamsort/text-sort.medium
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/README
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/gridmix-env
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/monsterQuery/monster_query.large
* /hadoop/mapreduce/trunk/src/java/org/apache/hadoop/mapred/pipes/Submitter.java
* /hadoop/mapreduce/trunk/bin/start-mapred.sh
* /hadoop/mapreduce/trunk/src/docs/src/documentation/content/xdocs/mapred_tutorial.xml
* /hadoop/mapreduce/trunk/src/packages/rpm
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/monsterQuery/monster_query.medium
* /hadoop/mapreduce/trunk/src/contrib/raid/bin/stop-raidnode-remote.sh
* /hadoop/mapreduce/trunk/src/packages/deb/hadoop.control/postinst
* /hadoop/mapreduce/trunk/src/test/mapred/org/apache/hadoop/mapred/ReliabilityTest.java
* /hadoop/mapreduce/trunk/src/packages/rpm/init.d/hadoop-tasktracker
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix2/generateGridmix2data.sh
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/javasort/text-sort.large
* /hadoop/mapreduce/trunk/src/docs/src/documentation/content/xdocs/rumen.xml
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/pipesort/text-sort.medium
* /hadoop/mapreduce/trunk/src/packages/deb/init.d
* /hadoop/mapreduce/trunk/src/contrib/fairscheduler/README
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/pipesort/text-sort.large
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/webdatasort/webdata_sort.medium
* /hadoop/mapreduce/trunk/src/packages/deb
* /hadoop/mapreduce/trunk/src/packages/rpm/init.d/hadoop-jobtracker
* /hadoop/mapreduce/trunk/src/packages/rpm/spec
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/webdatascan/webdata_scan.small
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/javasort/text-sort.medium
* /hadoop/mapreduce/trunk/src/contrib/raid/bin/start-raidnode-remote.sh
* /hadoop/mapreduce/trunk/src/packages/deb/init.d/hadoop-tasktracker
* /hadoop/mapreduce/trunk/ivy.xml
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix2/README.gridmix2
* /hadoop/mapreduce/trunk/src/packages/deb/hadoop.control
* /hadoop/mapreduce/trunk/src/contrib/streaming/src/java/org/apache/hadoop/streaming/LoadTypedBytes.java
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/maxent/maxent.large
* /hadoop/mapreduce/trunk/src/c++/librecordio/test/Makefile
* /hadoop/mapreduce/trunk/src/packages
* /hadoop/mapreduce/trunk/src/docs/src/documentation/content/xdocs/fair_scheduler.xml
* /hadoop/mapreduce/trunk/CHANGES.txt
* /hadoop/mapreduce/trunk/src/java/org/apache/hadoop/mapred/LinuxTaskController.java
* /hadoop/mapreduce/trunk/src/contrib/streaming/src/java/org/apache/hadoop/streaming/HadoopStreaming.java
* /hadoop/mapreduce/trunk/src/examples/python/pyAbacus/compile
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/streamsort/text-sort.small
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/webdatascan/webdata_scan.medium
* /hadoop/mapreduce/trunk/src/contrib/vaidya/src/java/org/apache/hadoop/vaidya/vaidya.sh
* /hadoop/mapreduce/trunk/src/contrib/block_forensics/client/BlockForensics.java
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/webdatasort/webdata_sort.small
* /hadoop/mapreduce/trunk/src/contrib/raid/README
* /hadoop/mapreduce/trunk/src/docs/src/documentation/content/xdocs/streaming.xml
* /hadoop/mapreduce/trunk/src/packages/deb/hadoop.control/postrm
* /hadoop/mapreduce/trunk/src/test/system/conf/system-test-mapred.xml
* /hadoop/mapreduce/trunk/src/docs/src/documentation/content/xdocs/capacity_scheduler.xml
* /hadoop/mapreduce/trunk/src/packages/deb/hadoop.control/conffile
* /hadoop/mapreduce/trunk/src/packages/deb/init.d/hadoop-jobtracker
* /hadoop/mapreduce/trunk/src/packages/update-mapred-env.sh
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/monsterQuery/monster_query.small
* /hadoop/mapreduce/trunk/src/contrib/vaidya/src/java/org/apache/hadoop/vaidya/postexdiagnosis/PostExPerformanceDiagnoser.java
* /hadoop/mapreduce/trunk/src/contrib/mumak/bin/mumak.sh
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/webdatascan/webdata_scan.large
* /hadoop/mapreduce/trunk/src/packages/templates
* /hadoop/mapreduce/trunk/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java
* /hadoop/mapreduce/trunk/src/contrib/data_join/src/examples/org/apache/hadoop/contrib/utils/join/README.txt
* /hadoop/mapreduce/trunk/src/contrib/streaming/src/java/org/apache/hadoop/streaming/DumpTypedBytes.java
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/javasort/text-sort.small
* /hadoop/mapreduce/trunk/bin/stop-mapred.sh
* /hadoop/mapreduce/trunk/ivy/libraries.properties
* /hadoop/mapreduce/trunk/src/docs/src/documentation/content/xdocs/vaidya.xml
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix2/gridmix-env-2
* /hadoop/mapreduce/trunk/bin/mapred-config.sh
* /hadoop/mapreduce/trunk/build.xml
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/pipesort/text-sort.small
* /hadoop/mapreduce/trunk/src/examples/python/compile
* /hadoop/mapreduce/trunk/src/packages/rpm/init.d
* /hadoop/mapreduce/trunk/src/benchmarks/gridmix/streamsort/text-sort.large
* /hadoop/mapreduce/trunk/src/c++/pipes/debug/pipes-default-script
","29/May/11 15:43;hudson;Integrated in Hadoop-Mapreduce-trunk #694 (See [https://builds.apache.org/hudson/job/Hadoop-Mapreduce-trunk/694/])
    ","01/Jun/11 13:22;amar_kamat;I cannot compile trunk after this commit. It fails with the error 
{noformat}
build.xml:1310: build/c++/<os-architecture>/include not found.
{noformat}

The trunk compiles fine with this patch reverted. ","01/Jun/11 18:44;eyang;Amar, this is because -Dcompile.c++=true -Dcompile.native=true is not set.  This is intentional to expose previously hidden bug where libraries may not have been packaged.  It is also possible to add failOnError=""false"" on line 1310, and 1316 to package as it's previous state.  If you feel strongly about this change, please open a JIRA.  Thanks.","01/Jun/11 20:06;eli;Same issue as HDFS-2022, the binary target in the build should depend on the copmile-c++ target.","02/Jun/11 03:02;amar_kamat;Eric,
My concern is that when a new user checks out mapreduce trunk and does 'ant binary', the compilation should not fail. Also from the message its not clear as to what went wrong and whether its a user/framework issue. ","02/Jun/11 16:35;eyang;I will make binary target depends on compile-c++ targets in MAPREDUCE-2559.  Thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Gridmix] Add support for HighRam jobs,MAPREDUCE-2543,12508681,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,amar_kamat,amar_kamat,amar_kamat,30/May/11 06:36,15/Nov/11 00:48,12/Jan/21 09:52,02/Jun/11 13:51,,,,,,0.23.0,,,contrib/gridmix,,,,,,0,,,,,Gridmix currently ignores high ram job configuration of the original job. It would be nice if Gridmix configures the simulated job's high ram parameters such that the simulated job has same effect on the job scheduler & task-tracker as the original job.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/May/11 15:04;amar_kamat;mapreduce-2543-v1.2.patch;https://issues.apache.org/jira/secure/attachment/12480945/mapreduce-2543-v1.2.patch","02/Jun/11 10:07;amar_kamat;mapreduce-2543-v1.4.patch;https://issues.apache.org/jira/secure/attachment/12481211/mapreduce-2543-v1.4.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-06-02 12:50:32.538,,,false,,,,,,,,,,,,,,,,,,44122,Reviewed,,,,Wed Jun 29 14:42:52 UTC 2011,,,,,,,"0|i09dlz:",52638,Adds High-Ram feature emulation in Gridmix.,,,,,,,,,,gridmix high-ram,,,,,,,,,,"31/May/11 15:04;amar_kamat;Attaching a patch that adds support for High Ram jobs in Gridmix. Added a unit test-case to test the same. test-patch and test-contrib passed on my box. ","02/Jun/11 10:07;amar_kamat;Attaching a new patch fixing a bug found by Vinay. Incorporated Ravi's offline comments. test-patch and Gridmix unit tests passed.","02/Jun/11 12:50;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12481211/mapreduce-2543-v1.4.patch
  against trunk revision 1130393.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:
                  org.apache.hadoop.cli.TestMRCLI

    +1 contrib tests.  The patch passed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/333//testReport/
Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/333//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/333//console

This message is automatically generated.","02/Jun/11 13:38;amar_kamat;TestMRCLI failure is not related to this patch. This patch was QAed by Vinay and reviewed (offline) by Ravi.","02/Jun/11 13:45;ravidotg;Patch looks fine to me.
+1","02/Jun/11 13:51;amar_kamat;I just committed the latest patch to trunk. Thanks Ravi for the review.","02/Jun/11 14:07;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #709 (See [https://builds.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/709/])
    MAPREDUCE-2543. [Gridmix] High-Ram feature emulation in Gridmix. (amarrk)

amarrk : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1130550
Files : 
* /hadoop/mapreduce/trunk/CHANGES.txt
* /hadoop/mapreduce/trunk/src/docs/src/documentation/content/xdocs/gridmix.xml
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
","02/Jun/11 14:24;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #710 (See [https://builds.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/710/])
    MAPREDUCE-2543. [Gridmix] High-Ram feature emulation testcase. (amarrk)

amarrk : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1130554
Files : 
* /hadoop/mapreduce/trunk/CHANGES.txt
* /hadoop/mapreduce/trunk/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java
","29/Jun/11 14:42;hudson;Integrated in Hadoop-Mapreduce-trunk #722 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/722/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MR-279: Metrics for MRAppMaster,MAPREDUCE-2527,12507941,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,vicaya,vicaya,vicaya,20/May/11 23:58,15/Nov/11 00:48,12/Jan/21 09:52,26/May/11 02:44,0.23.0,,,,,0.23.0,,,mrv2,,,,,,0,,,,,,,aah,cdouglas,mahadev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/May/11 00:01;vicaya;mr-2527-am-metrics-v1.patch;https://issues.apache.org/jira/secure/attachment/12479970/mr-2527-am-metrics-v1.patch","24/May/11 17:54;vicaya;mr-2527-am-metrics-v2.patch;https://issues.apache.org/jira/secure/attachment/12480289/mr-2527-am-metrics-v2.patch","26/May/11 01:38;vicaya;mr-2527-am-metrics-v3.patch;https://issues.apache.org/jira/secure/attachment/12480491/mr-2527-am-metrics-v3.patch","26/May/11 02:25;vicaya;mr-2527-am-metrics-v4.patch;https://issues.apache.org/jira/secure/attachment/12480499/mr-2527-am-metrics-v4.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2011-05-26 02:44:01.644,,,false,,,,,,,,,,,,,,,,,,44132,Reviewed,,,,Thu May 26 02:44:01 UTC 2011,,,,,,,"0|i09dnz:",52647,,,,,,,,,,,"mrv2, metrics, am",,,,,,,,,,"24/May/11 17:54;vicaya;Removed some metrics test workaround, now that hadoop common in yahoo-merge has the right fix from trunk.","26/May/11 01:38;vicaya;Patch v3 rebased against mr-279 head.","26/May/11 02:25;vicaya;Patch v4 fix tests (better isolation) on a Linux box where test order is arbitrary.","26/May/11 02:44;mahadev;I just pushed this to MR-279. thanks luke!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
task logs should specify user vs. system death,MAPREDUCE-1450,12455245,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,,aw,aw,03/Feb/10 23:26,02/Nov/11 17:44,12/Jan/21 09:52,02/Nov/11 17:44,,,,,,,,,,,,,,,0,,,,,"When looking at task attempt logs, it should specify whether the task was killed by Hadoop or by the user.",,aw,hammer,ravidotg,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,149520,,,,,2010-02-03 23:26:40.0,,,,,,,"0|i0jgnz:",111630,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Size-based queuing for capacity scheduler,MAPREDUCE-1998,12471002,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,ramach,aw,aw,06/Aug/10 19:33,02/Nov/11 17:39,12/Jan/21 09:52,02/Nov/11 17:39,,,,,,,,,capacity-sched,,,,,,0,,,,,"On job submission, it would be useful if the capacity scheduler could pick a queue based on the # of maps and reduces.  This way one could have queues based on job-size without users having to pick the queue prior to submission.  ",,aah,acmurthy,hammer,hong.tang,ravidotg,roelofs,srivas,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-08-09 15:46:04.024,,,false,,,,,,,,,,,,,,,,,,149919,,,,,Mon Aug 09 15:46:04 UTC 2010,,,,,,,"0|i0jhyf:",111839,,,,,,,,,,,,,,,,,,,,,"06/Aug/10 19:35;aw;So the way I imagine this working is:

admin configures 3 queues:

jobs with > 1000 tasks
jobs with <999 >100 tasks
jobs with < 99 tasks

user submits a job to default, which triggers sized-based queuing.

hadoop determines map tasks: lets say 100, and user requested 20 reduces.  Total task count: 120.  

JobTracker places job in the middle tier queue since there are 120 tasks.

","09/Aug/10 15:46;acmurthy;An interesting way to look at things - this plays well with MAPREDUCE-1872 too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move jobs between queues post-job submit,MAPREDUCE-2017,12471834,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,,aw,aw,17/Aug/10 18:52,02/Nov/11 17:39,12/Jan/21 09:52,02/Nov/11 17:39,0.21.0,,,,,,,,capacity-sched,,,,,,1,,,,,It would be massively useful to be able to move a job between queues after it has already been submitted.,,aah,atm,liangly,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,149936,,,,,2010-08-17 18:52:27.0,,,,,,,"0|i0ji0n:",111849,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backport MAPREDUCE-220 to Hadoop 20 security branch,MAPREDUCE-2777,12517888,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,amar_kamat,jeagles,jeagles,04/Aug/11 06:22,19/Oct/11 00:26,12/Jan/21 09:52,05/Oct/11 22:39,0.20.205.0,,,,,0.20.205.0,,,,,,,,,0,,,,,,,cutting,mattf,mholderba,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/11 06:16;amar_kamat;mapreduce-2777-v1.3.patch;https://issues.apache.org/jira/secure/attachment/12493004/mapreduce-2777-v1.3.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2011-09-05 06:16:25.091,,,false,,,,,,,,,,,,,,,,,,35825,Reviewed,,,,Wed Oct 19 00:26:10 UTC 2011,,,,,,,"0|i02opr:",13605,Adds cumulative cpu usage and total heap usage to task counters. This is a backport of MAPREDUCE-220 and MAPREDUCE-2469.,,,,,,,,,,,,0.20.205.0,,,,,,,,"05/Sep/11 06:16;amar_kamat;Attaching a patch that backports the MAPREDUCE-220 and MAPREDUCE-2469 features to the hadoop-0.20-security branch. MAPREDUCE-220 and MAPREDUCE-2469 have dependencies which also got selectively backported. Here is the list of JIRA's that got backported:
MR-220, MR-1201, MR-1762, MR-1218 & MR-2469. I have resolved most of the hunks w.r.t parameter naming, missing APIs and missing backports (e.g GC_TIME_MILLIS counters etc). 

We have tested this patch at scale and also benchmarked this patch for regressions. It seems that the overhead is negligible and the newly added counters are correctly captured and recorded.","05/Sep/11 09:33;amar_kamat;test-patch and ant tests passed with the attached patch.","15/Sep/11 18:53;jeagles;+1 patch looks good to me.","28/Sep/11 11:51;amar_kamat;I just committed this to the 0.20 security branch.","28/Sep/11 15:56;amar_kamat;I also committed this to 0.20-205.","28/Sep/11 18:04;mattf;Amar, thank you for providing this backport.  For a non-trivial change like this, please run test-patch locally and paste the results summary into the Jira as a comment (since we don't have HADOOP-7435 yet).  Thanks.","28/Sep/11 18:29;mattf;BTW, that request is because you want this patch in 0.20.205, and I'm just about to cut a release candidate for that release, today.  Thanks, --Matt (RM for 0.20.205)","28/Sep/11 20:51;mattf;In build #38, https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-0.20.205-Build/38/ (r1176934), this patch introduced two new unit test failures not present after the preceding patch:
    org.apache.hadoop.filecache.TestTrackerDistributedCacheManager.testDeleteCache
    org.apache.hadoop.mapred.TestTTMemoryReporting

Reverting from 0.20.205.  Amar, please fix in 0.20-security.  Thank you.","29/Sep/11 01:45;amar_kamat;Matt, 
The 2 failures are due to 
1. TestTrackerDistributedCacheManager : This error seems unrelated to this patch. This patch doesn't change anything in DistributedCache. I checked the stack trace and found NPEs. I will look into it further.
2. TestTTMemoryReporting: This testcase got deleted and rewritten as part of this patch. We only need to delete the stray file.

I uploaded the patch only after test-patch and ant-tests passed. I am re-running them to make sure nothing changed after I generated the last patch.","29/Sep/11 02:00;amar_kamat;test-patch output:
{noformat}
 [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 21 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.8) warnings.
{noformat}","29/Sep/11 02:06;amar_kamat;I re-ran TestTrackerDistributedCacheManager with and without my patch and it passed.
I suggest we commit this patch to 0.20.205.","05/Oct/11 22:38;mattf;Merged back into 0.20-security-205, along with r1177389 to delete the now-empty test file TestTTMemoryReporting.","19/Oct/11 00:26;mattf;Closed upon release of 0.20.205.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A method for finding and tracking jobs from the new API,MAPREDUCE-777,12430991,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,amareshwari,omalley,omalley,21/Jul/09 05:59,18/Oct/11 07:18,12/Jan/21 09:52,18/Sep/09 07:05,,,,,,0.21.0,,,client,,,,,,0,,,,,"We need to create a replacement interface for the JobClient API in the new interface. In particular, the user needs to be able to query and track jobs that were launched by other processes.",,aaa,acmurthy,cdouglas,cutting,cwensel,ddas,dhruba,hammer,henryr,iyappans,kimballa,matei,philip,qwertymaniac,ravidotg,rksingh,sharadag,tlipcon,tomwhite,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-975,MAPREDUCE-864,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/09 23:36;omalley;m-777.patch;https://issues.apache.org/jira/secure/attachment/12417827/m-777.patch","17/Aug/09 11:01;amareshwari;patch-777-1.txt;https://issues.apache.org/jira/secure/attachment/12416762/patch-777-1.txt","16/Sep/09 13:23;amareshwari;patch-777-10.txt;https://issues.apache.org/jira/secure/attachment/12419766/patch-777-10.txt","17/Sep/09 03:35;amareshwari;patch-777-11.txt;https://issues.apache.org/jira/secure/attachment/12419840/patch-777-11.txt","17/Sep/09 06:59;amareshwari;patch-777-12.txt;https://issues.apache.org/jira/secure/attachment/12419854/patch-777-12.txt","17/Sep/09 08:36;amareshwari;patch-777-13.txt;https://issues.apache.org/jira/secure/attachment/12419858/patch-777-13.txt","17/Sep/09 12:44;amareshwari;patch-777-14.txt;https://issues.apache.org/jira/secure/attachment/12419878/patch-777-14.txt","18/Sep/09 02:46;amareshwari;patch-777-15.txt;https://issues.apache.org/jira/secure/attachment/12419960/patch-777-15.txt","18/Sep/09 03:48;amareshwari;patch-777-16.txt;https://issues.apache.org/jira/secure/attachment/12419966/patch-777-16.txt","18/Sep/09 06:49;amareshwari;patch-777-17.txt;https://issues.apache.org/jira/secure/attachment/12419977/patch-777-17.txt","19/Aug/09 06:13;amareshwari;patch-777-2.txt;https://issues.apache.org/jira/secure/attachment/12416974/patch-777-2.txt","03/Sep/09 10:49;amareshwari;patch-777-3.txt;https://issues.apache.org/jira/secure/attachment/12418498/patch-777-3.txt","09/Sep/09 11:30;amareshwari;patch-777-4.txt;https://issues.apache.org/jira/secure/attachment/12419048/patch-777-4.txt","10/Sep/09 11:19;amareshwari;patch-777-5.txt;https://issues.apache.org/jira/secure/attachment/12419169/patch-777-5.txt","11/Sep/09 11:32;amareshwari;patch-777-6.txt;https://issues.apache.org/jira/secure/attachment/12419289/patch-777-6.txt","11/Sep/09 11:46;amareshwari;patch-777-7.txt;https://issues.apache.org/jira/secure/attachment/12419295/patch-777-7.txt","14/Sep/09 05:56;amareshwari;patch-777-8.txt;https://issues.apache.org/jira/secure/attachment/12419456/patch-777-8.txt","16/Sep/09 06:51;amareshwari;patch-777-9.txt;https://issues.apache.org/jira/secure/attachment/12419739/patch-777-9.txt","10/Aug/09 11:18;amareshwari;patch-777.txt;https://issues.apache.org/jira/secure/attachment/12416042/patch-777.txt",,,,,,,,,,,,,,,,19.0,,,,,,,,,,,,,,,,,,,,2009-07-21 18:16:51.29,,,false,,,,,,,,,,,,,,,,,,88766,Incompatible change,Reviewed,,,Fri Sep 18 18:42:20 UTC 2009,,,,,,,"0|i0jejr:",111287,Enhance the Context Objects api to add features to find and track jobs.,,,,,,,,,,,,,,,,,,,,"21/Jul/09 18:16;tlipcon;Introducing a factory governed by a conf parameter in here would be nice as well - since it's currently a static class it's very hard to interpose any custom code.","21/Jul/09 22:00;omalley;What is the use case that you are thinking of?","21/Jul/09 22:12;tlipcon;We have the need for a program to run an arbitrary client-provided jar and then monitor the jobs submitted by it. The easiest way to go about this is to interpose code in front of JobClient to catch the submission and hand off the job ID to the thread/process that needs to follow along. Without factory-izing JobClient, doing this is relatively tricky.","21/Jul/09 22:27;omalley;I can't believe I'm saying this, but rather than putting a plugin there, isn't it easier to just use aspectj to instrument the Job API?","21/Jul/09 22:30;tlipcon;I can't believe we did this, but that's exactly what we did ;-)","10/Aug/09 11:18;amareshwari;Attaching an early patch for review.

Patch does the following:
1. Deprecates RunningJob api. Job should be used instead. Made sure that job has all methods that RunningJob has.
2. Ports the existing JobClient functionality to new api.
This made org.apache.hadoop.mapred.HistoryViewer, org.apache.hadoop.mapred.LocalJobRunner, QueueAclsInfo and JobSubmissionProtocol classes public.
Modified JobSubmissionProtocol to use new api.

Owen, can you please look at the patch once and let me know if you want to add/remove/change any api?


","10/Aug/09 14:00;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12416042/patch-777.txt
  against trunk revision 802645.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 9 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 2272 javac compiler warnings (more than the trunk's current 2232 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/461/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/461/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/461/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/461/console

This message is automatically generated.","17/Aug/09 11:01;amareshwari;Patch updated with trunk","17/Aug/09 16:03;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12416762/patch-777-1.txt
  against trunk revision 804865.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 9 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 2272 javac compiler warnings (more than the trunk's current 2232 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/485/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/485/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/485/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/485/console

This message is automatically generated.","18/Aug/09 03:48;amareshwari;Test failure TestRecoveryManager is not related to the patch. It is due to MAPREDUCE-880","18/Aug/09 05:05;amareshwari;-1 javac. Looks spurious. I dont see any new javac warnings introduced in the patch.","19/Aug/09 06:02;amareshwari;Cancelling patch to incorporate offline comments from Amar
Comments include:
1. Introduce Counters.downgrade() instead of constructor 
2. 
{code}
+      org.apache.hadoop.mapreduce.JobClient.TaskStatusFilter newFilter = 
+        getNewFilter(filter);
+      printTaskEvents(events, newFilter, profiling, mapRanges, reduceRanges);
{code}
Use getNewFilter directly.

3. deprecate public methods in jobtracker, that got changed for new JobSubmissionProtocol
4. Move Counters(org.apache.hadoop.mapred.Counters counters) to a method in old api","19/Aug/09 06:13;amareshwari;Patch incorporating review comments except comment(4).
bq. Move Counters(org.apache.hadoop.mapred.Counters counters) to a method in old api
This needs CounterGroup constructor(s) to be made public. So, did not move this method to old api.","19/Aug/09 16:57;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12416974/patch-777-2.txt
  against trunk revision 805324.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 9 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 2272 javac compiler warnings (more than the trunk's current 2232 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/494/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/494/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/494/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/494/console

This message is automatically generated.","21/Aug/09 15:21;tomwhite;I think we can support the use case Todd mentioned above with a relatively small change (and without resorting to aspects). Change the JobClient constructors to static factory methods:

{code}
public JobClient() { ... }

public JobClient(Configuration conf) throws IOException { ... }
{code}

becomes

{code}
public static JobClient get() { ... }

public static JobClient get(Configuration conf) throws IOException { ... }
{code}

Then at a later point we can change the implementation of the static methods to return a custom implementation of JobClient, without having to change JobClient's API.","25/Aug/09 03:57;omalley;I'm not happy with this patch. I need to go through it in more depth, but:

1. The setters mostly look right, although some of them are missing the assertion that the job is in the setup phase.

2. The getters should move to JobContext.

3. I think JobClient is a bad name for the job browser. Something like JobBrowser is probably clearer.

","26/Aug/09 23:36;omalley;This is closer to what I had in mind. I think we need to take this chance to do a major clean up of the interface.","26/Aug/09 23:38;acmurthy;+1","27/Aug/09 00:39;philip;Overall, +1 on having this interface!  Some thoughts:

 * Can getReasonForBlackList return an enum?
 * Is there a reason why getJobs returns Job[] and not Collection<Job>?  
 * It seems like people may want to push filters down in getJobs.
 * Instead of get(Map,Reduce,SetupAndCleanup)TaskReports, should that just be a getTaskReport(TaskType)?  The number of task types is likely to increase.

-- Philip","28/Aug/09 11:05;amareshwari;Some questions/comments on proposed api:
1.
{code}
+
+  /* mapred.Queue.QueueState needs to extend this class */
+  public static enum QueueState {
+    STOPPED(""stopped""), RUNNING(""running"");
{code}
enum cannot be extended in java. Owen, do you mean I should wrap this in a class?

2.  bq.  public Job getJob(JobID job) throws IOException { return null; }
For returning Job handle from JobID, we should find a way to get configuration of the job, through JobSubmissionProtocol.

3. 
{code}
+  public QueueInfo getQueue(String name) throws IOException { return null; }
+  public Metrics getClusterStatus() throws IOException { return null; }
{code}

For these apis, JobSubmissionProtocol should be modified to return QueueInfo and Metrics, instead of bigger objects JobQueueInfo and ClusterStatus, right?
","28/Aug/09 18:11;acmurthy;bq. # Instead of get(Map,Reduce,SetupAndCleanup)TaskReports, should that just be a getTaskReport(TaskType)? The number of task types is likely to increase.

+1","31/Aug/09 04:42;sreekanth;With respect to
{noformat}
  public class QueueInfo {
    String getName() { return null; }
    String getSchedulingInfo() throws IOException { return null; }
    Job[] getJobs(int maxJobs) throws IOException { return null; }
    QueueState getState() throws IOException { return null; }
  }
{noformat}

Currently the class {{org.apache.hadoop.mapred.JobQueueInfo}} is client only view of the information pertaining to the queue and is not used in framework for any other purpose, why don't we reuse it instead of a creating a new class?

Also, in the framework the concept of queue was nothing but a tag associated with a Job and some schedulers need not honor the queue and can store the job in a single queue rather than in separate queue, are we planning to change that? 

Then, sending a list of jobs for all the client request might not be required as, there are currently two queue commands i.e. {{queue -list}} which prints out list of queues and associated scheduling information and {{queue -info <queuename> [-listjobs]}} the option of list jobs is optional in second case, by using the proposed api we might end up sending list of jobs all the time even tho' client does not request it.

Finally, MAPREDUCE-853 is introducing an hierarchy of queues and we should also try to handle those scenarios in the JIRA. ","03/Sep/09 10:49;amareshwari;Attaching an early patch for review.

Patch adds public classes org.apache.hadoop.mapreduce.Cluster, org.apache.hadoop.mapreduce.CLI . 
Moves org.apache.hadoop.mapred.JobSubmissionProtocol to public org.apache.hadoop.mapreduce.ClientProtocol.

Cluster maintains the life-cycle of RPCProxy. Instance of Cluster creates the RPCProxy. and Cluster.close() should be called for stopping the proxy.

Job is passed with handle to cluster. It uses cluster handle to submit job and qeury its status.
All RunningJob methods are added to org.apache.hadoop.mapreduce.Job . 
Moved the Job submission code to private class JobSubmitter.

org.apache.hadoop.mapreduce.CLI implements Tool and provides the functionality of *bin/hadoop job* option.

I'm still working on changing old client to use new code and remove duplication.","04/Sep/09 04:09;philip;Took a quick pass at your patch.  Some comments, mostly documentation-related.

bq. +  static Counters downgrade(org.apache.hadoop.mapreduce.Counters counters) {

You might have some JavaDoc for this method.  Also, variables would be clearer if everything were old_counter and new_counter, since it's hard to keep track what's what.

bq. ClientProtocol

Are we settled on the name ClientProtocol?  It's quite generic sounding, and, without the package, hard to decipher.  Since these protocols will be the names of the public-ish wire APIs, perhaps JobClientProtocol would be more descriptive?

bq. +public class CLI extends Configured implements Tool {

Some of Hadoop uses apache.commons.cli to parse command line arguments.  (And there's CLI2 too, referred to in Maven, though I don't see any usages of it.  You might consider using a command-line parsing library.

You might also consider splitting up the run() method into separate methods (even classes) for each piece of functionality.  This will make it much easier to test, and easier to parse, too.


bq. +public interface ClientProtocol extends VersionedProtocol {

In the javadoc here documenting the history of this protocol, you might mention the rename. 

bq. ""Changed protocol to use new api""

This is not very descriptive for someone unfamiliar with this ticket.


Cheers,

-- Philip","04/Sep/09 07:34;acmurthy;I think this is on the right track... I'm happy to see this shaping up well!

I'm a little unsure about JobSubmitter being a separate class, seems to me that since a Job can 'submit' itself (o.a.h.mapreduce.Job.submit) it shouldn't need another class (JobSubmitter) for that functionality. Maybe JobSubmitter (or JobSubmissionHelper) should have only static methods? Anyway, it's a minor issue. Thoughts?","04/Sep/09 07:45;acmurthy;In general, we should look at this as an opportunity to clean up the job-submission interface (currently JobClient) and the goal is not to be compatible on a feature-by-feature basis. I'll try and take a closer look at the interfaces added to org.apache.hadoop.mapreduce.Job soon, but I thought I should spell out the underlying vision.","09/Sep/09 11:30;amareshwari;Patch changing the old JobClient to use the code in mapreduce package. Now the old JobClient is just a wrapper for the public methods.
Patch also incorporates some of the comments from Philip.

bq. Are we settled on the name ClientProtocol? It's quite generic sounding, and, without the package, hard to decipher. Since these protocols will be the names of the public-ish wire APIs, perhaps JobClientProtocol would be more descriptive.
The protocol has more than Job. It has methods to access cluster also. 

bq. Some of Hadoop uses apache.commons.cli to parse command line arguments. (And there's CLI2 too, referred to in Maven, though I don't see any usages of it. You might consider using a command-line parsing library.
I also thought of this. But, this can be done in a seperate jira.

bq. I'm a little unsure about JobSubmitter being a separate class, seems to me that since a Job can 'submit' itself 
Since the submission code was huge, moved it to this private class.
Moved old submission code(writing old splits) also here, sothat the old JobClient doesn't need to know about any submission logic.","09/Sep/09 14:37;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419048/patch-777-4.txt
  against trunk revision 812546.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 36 new or modified tests.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    -1 javac.  The applied patch generated 2292 javac compiler warnings (more than the trunk's current 2236 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 221 release audit warnings (more than the trunk's current 220 warnings).

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/53/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/53/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/53/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/53/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/53/console

This message is automatically generated.","10/Sep/09 05:51;amareshwari;-1 overall. Is because of some hudson errors. Will re-submit the patch","10/Sep/09 11:19;amareshwari;Patch fixes a couple of bugs in LocalJobRunner and JobTracker.","10/Sep/09 13:57;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419169/patch-777-5.txt
  against trunk revision 813308.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 39 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 2292 javac compiler warnings (more than the trunk's current 2236 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/55/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/55/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/55/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/55/console

This message is automatically generated.","11/Sep/09 11:32;amareshwari;Making old job client set mapred.mapper.new-api and mapred.reducer.new-api to false, if they are not already set.
","11/Sep/09 11:46;amareshwari;Removing some debugging log messages from earlier patch.","11/Sep/09 20:04;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419295/patch-777-7.txt
  against trunk revision 813660.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 36 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 2292 javac compiler warnings (more than the trunk's current 2236 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/66/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/66/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/66/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/66/console

This message is automatically generated.","14/Sep/09 04:54;philip;I may be crazy to harm on this, but ""ClientProtocol"" still reads as very generic to me.  Perhaps JobTrackerClientProtocol, to at least indicate one of the components involved?

-- Philip","14/Sep/09 05:56;amareshwari;Patch updated with trunk","14/Sep/09 07:51;acmurthy;Review comments:

# As far as possible we shouldn't expose old interfaces through the new ones (e.g. ClusterStatus, JobStatus, TaskReport) etc. in ClientProtocol, Cluster etc. I'm still debating if we should deprecate JobStatus/TaskReport and replace them with newer ones in org.apache.hadoop.mapreduce and making the old ones derive from the new ones. Maybe it's beyond the scope of this jira
# Cluster shouldn't expose getClient() api, that is a hack. We should have everyone using public stable apis on Cluster - if necessary JobClient should construct old interface return values (ClusterStatus) from Cluster.
# ClientProtocol.getClusterStatus should return Cluster.Metrics
# Cluster shouldn't have getUGI interface
# Move Cluster.{QueueState|QueueInfo} to separate files
# JobClient should have @deprecated javadoc and it should point users to Job and Cluster
# Job has too many new constructors, we should minimize them as far as possible
# Job's constructor always makes 'new JobConf(jobConf)', that seems undesirable in several cases - Owen?

","14/Sep/09 18:26;omalley;{quote}As far as possible we shouldn't expose old interfaces through the new ones (e.g. ClusterStatus, JobStatus, TaskReport) etc. in ClientProtocol, Cluster etc. I'm still debating if we should deprecate JobStatus/TaskReport and replace them with newer ones in org.apache.hadoop.mapreduce and making the old ones derive from the new ones. Maybe it's beyond the scope of this jira
{quote}

We can't expose the old classes in the new API. In particular, it is critical that we can have clients just use the new API with no references to the mapred package. To do that, as Arun says, we need to move the functionality into the new API and have the old API extended the new classes. (And be deprecated) I think that has to be in the scope of this jira since this is the jira that is adding them to the new API.","14/Sep/09 18:28;omalley;{quote}
   8. Job's constructor always makes 'new JobConf(jobConf)', that seems undesirable in several cases - Owen?
{quote}

It should probably just say that the JobConf is cloned before changes are made.","15/Sep/09 04:12;acmurthy;Also, as a part of this jira we should drop JobProfile from ClientProtocol and just have ClientProtocol.getJobStatus() which has all the requisite info.","16/Sep/09 06:51;amareshwari;Patch with review comments incorporated","16/Sep/09 07:41;amareshwari;Uploaded Patch incorporates all the review comments.
Also makes methods in ClientProtocol throw InterruptedException","16/Sep/09 09:15;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419739/patch-777-9.txt
  against trunk revision 815628.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 39 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The patch appears to cause tar ant target to fail.

    -1 findbugs.  The patch appears to cause Findbugs to fail.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/85/testReport/
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/85/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/85/console

This message is automatically generated.","16/Sep/09 09:25;amareshwari;Uploaded Patch incorporates all the review comments.
Also makes methods in ClientProtocol throw InterruptedException","16/Sep/09 10:34;tomwhite;* The number of classes in the old org.apache.hadoop.mapred package is very large and daunting for users of MapReduce. We should only add classes to the new org.apache.hadoop.mapreduce if they are a part of the core public API for MapReduce. Internal classes with public visibility belong in another package. On this basis I would suggest moving (by analogy with HDFS packaging)
** CLI to org.apache.hadoop.mapreduce.tools
** ClientProtocol to org.apache.hadoop.mapreduce.protocol
* The Job constructors should be changed to be static factory methods to make Job submission more flexible in future (see https://issues.apache.org/jira/browse/MAPREDUCE-777?focusedCommentId=12746014&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12746014)
* Some of the value object classes have setters even though their state should only be read by user code. These are: JobStatus, QueueAclsInfo, QueueInfo, TaskCompletionEvent, TaskReport. These should be made immutable, or have package-private or protected setters.
* Cluster has a mixture of methods that return arrays and those that return collections. Can we change them all to be consistent (preferably collections)?
* Rename Cluster#getTTExpiryInterval() to the more readable getTaskTrackerExpiryInterval().
* ClusterMetrics#getDeccommisionTrackers() is misspelled and should be getDecommissionedTaskTrackers(). Similarly change the instance variable numDecommisionedTrackers to numDecommissionedTrackers (double 's'). In fact, the get*TaskTrackers() methods would be better called get*TaskTrackerCount() since they don't return tasktracker objects, but a count of those objects.
* CLI's usage string refers to JobClient.
* JobStatus's javadoc refers to JobProfile, which is in the mapred package so we probably don't want to refer to it.
* All public classes need javadoc to explain their role. ","16/Sep/09 10:38;amareshwari;Uploaded Patch incorporates all the review comments.
Also makes methods in ClientProtocol throw InterruptedException","16/Sep/09 10:40;amareshwari;Please ignore ""Uploaded patch....""  comment. That looks like a browser problem","16/Sep/09 13:23;amareshwari;Patch with Tom's comments incorporated.","16/Sep/09 14:05;tomwhite;Thanks Amareshwari.

ClusterMetrics#getDeccommisionedTaskTrackerCount() should be getDecommissionedTaskTrackerCount() (single 'c', double 's').","17/Sep/09 03:35;amareshwari;Sorry! missed that again. Corrected the spelling now.","17/Sep/09 05:44;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419840/patch-777-11.txt
  against trunk revision 815628.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 39 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 2289 javac compiler warnings (more than the trunk's current 2235 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/93/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/93/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/93/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/93/console

This message is automatically generated.","17/Sep/09 06:59;amareshwari;Patch updated with the trunk","17/Sep/09 07:46;acmurthy;This is close... 

# We should at least open a new jira for more tests: TestCluster, TestJob etc., I see you've converted TestJobClient
# o.a.h.mapreduce.JobStatus.{RUNNING|SUCCEEDED|...} etc. shud be an enum and it should have a 'int getValue()' which returns values compatible with o.a.h.mapred.JobStatus.{RUNNING|SUCCEEDED|...}
# Typo: arreyToBlackListInfo
# Job has a copy-paste javadoc which mentions NetworkedJob
","17/Sep/09 07:50;acmurthy;5.  Queue.getJobQueueInfo calls Enum.name() (state.name()), it should use Enum.toString() ?
","17/Sep/09 08:36;amareshwari;Patch updated with the comments","17/Sep/09 09:09;iyappans;+1 from QA

Tested all options of bin/hadoop job and checked for its correctness as well as check all options of bin/hadoop queueue
and checked for its correctness. 

1) JobClient <command> <args>
        [-submit <job-file>]
        [-status <job-id>]
        [-counter <job-id> <group-name> <counter-name>]
        [-kill <job-id>]
        [-set-priority <job-id> <priority>]. Valid values for priorities are: VERY_HIGH HIGH NORMAL LOW
VERY_LOW
        [-events <job-id> <from-event-#> <#-of-events>]
        [-history <jobOutputDir>]
        [-list [all]]
        [-list-active-trackers]
        [-list-blacklisted-trackers]
        [-list-attempt-ids <job-id> <task-type> <task-state>]

        [-kill-task <task-attempt-id>]
        [-fail-task <task-attempt-id>]

Generic options supported are
-conf <configuration file>     
-D <property=value>           
-fs <local|namenode:port>     
-jt <local|jobtracker:port>   
-files <comma separated list of files> 
-libjars <comma separated list of jars>  
-archives <comma separated list of archives> 


For scheduler, I started a capacity scheduler with linux task controller, with different queues and different
permissions to different users..

2) bin/hadoop queue <command> <args>
        [-list]
        [-info <job-queue-name> [-showJobs]]
        [-showacls]

Generic options supported are
-conf <configuration file>     specify an application configuration file
-D <property=value>            use value for given property
-fs <local|namenode:port>      specify a namenode
-jt <local|jobtracker:port>    specify a job tracker
-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster
-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.
-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute
machines.


Other test sceanrios  were, testing it with JT restart, running job, waiting job etc. The values can be seen by making
the variable mapred.job.tracker.retire.jobs to false.

Raised these generic bugs/improvements, which were already found in trunk before this patch.:

1) MAPREDUCE-983 bin/hadoop job -fs file:///sdsdad -list still works. It is not picking up the latest fs input 

2) MAPREDUCE-984 bin/hadoop job -kill command says"" job successfully killed"" even though job has retired 

3) MAPREDUCE-985 job -kill-task <task-id>] and -fail-task <task-id> are not task-ids they are attempt ids 

4) MAPREDUCE-994  bin/hadoop job -counter help options do not give information on permissible values.  
     
5)  MAPREDUCE-993  bin/hadoop job -events <jobid> <from-event-#> <#-of-events> help message is confusing  

6)  MAPREDUCE-992  bin/hadoop job -events < jobid> gives event links which does not work.  

","17/Sep/09 10:23;acmurthy;+1

I'd commit this once Hudson gives it the once-over.","17/Sep/09 11:11;tomwhite;More comments, mainly naming:

* There are occurrences of both ""blacklist"" and ""blackList"" in the public API (e.g. TaskTrackerInfo#getReasonForBlackList() and getBlacklistReport()). Either is correct since the word may be spelled as ""blacklist"" or ""black list"", but we need to be consistent throughout. 
* Cluster#getFs() would be better as getFileSystem() (particularly with the debate in HADOOP-6223). Also it would be good to have javadoc describing the fact it is returning the file system where job-specific files are placed.
* JobStatus#{setup,map,reduce,cleanup}Progress() would be better as getters to be consistent with the rest of the class.
* TaskCompletionEvent#getTaskAttemptID() should be getTaskAttemptId() to be consistent with getEventId().
* TaskCompletionEvent#setTaskID() should be setTaskAttemptId().
* TaskReport's method names should be made consistent with this convention too.","17/Sep/09 11:50;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419858/patch-777-13.txt
  against trunk revision 816147.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 39 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The patch appears to cause tar ant target to fail.

    -1 findbugs.  The patch appears to cause Findbugs to fail.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/43/testReport/
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/43/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/43/console

This message is automatically generated.","17/Sep/09 12:44;amareshwari;Patch incorporating comments.","17/Sep/09 23:21;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419878/patch-777-14.txt
  against trunk revision 816240.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 39 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The patch appears to cause tar ant target to fail.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/45/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/45/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/45/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/45/console

This message is automatically generated.","18/Sep/09 02:46;amareshwari;MAPREDUCE-907 broke contrib tests, since InterruptedException was not handled.
Patch throws out InterruptedException.","18/Sep/09 02:59;yhemanth;Amareshwari, the patch queue is getting longer. Can you please start a parallel run locally and post results ?","18/Sep/09 03:05;amareshwari;-1 javac. Is because of deprecated classes.

All contrib tests passed on my machine.","18/Sep/09 03:48;amareshwari;Found one more place where InterruptedException is not handled. Strangely, it is not resulting in compilation failure, but failing javac.

Running test-patch and ant test locally, will update the results.","18/Sep/09 04:14;amareshwari;test-patch result:
{noformat}
     [exec]
     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 39 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     -1 javac.  The applied patch generated 2289 javac compiler warnings (more than the trunk's current 2235 warnings).
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec]
     [exec]
{noformat}

-1 javac. Is because of deprecated classes","18/Sep/09 05:21;amareshwari;All core and contrib tests passed.","18/Sep/09 06:49;amareshwari;Patch with minor change. Instead of throwing InterruptedExcption from sqoop method, I catch the exception and throw IOException as suggested by Arun.","18/Sep/09 07:05;acmurthy;I just committed this. Thanks, Amareshwari!","18/Sep/09 08:59;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #49 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/49/])
    . Brand new apis to track and query jobs as a replacement for JobClient. Contributed by Amareshwari Sriramadasu.
","18/Sep/09 18:28;nidaley;So now everyone is going to get a -1 on javadoc?  What's being done about this?","18/Sep/09 18:42;nidaley;Committer and contributors need to look at failures more closely.  The reason the contrib got a -1 was because this patch broke the eclipse plugin build!  See http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/45/console

MAPREDUCE-1003 was filed and now fixed for this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support a hierarchy of queues in the capacity scheduler,MAPREDUCE-824,12432160,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,rksingh,yhemanth,yhemanth,04/Aug/09 13:38,18/Oct/11 07:01,12/Jan/21 09:52,27/Aug/09 07:50,,,,,,0.21.0,,,capacity-sched,,,,,,0,,,,,"Currently in Capacity Scheduler, cluster capacity is divided among the queues based on the queue capacity. These queues typically represent an organization and the capacity of the queue represents the capacity the organization is entitled to. Most organizations are large and need to divide their capacity among sub-organizations they have. Or they may want to divide the capacity based on a category or type of jobs they run. This JIRA covers the requirements and other details to provide the above feature.",,acmurthy,aroop,hong.tang,rksingh,theiger,vinodkv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Aug/09 11:51;rksingh;HADOOP-824-1.patch;https://issues.apache.org/jira/secure/attachment/12415733/HADOOP-824-1.patch","14/Aug/09 09:57;rksingh;HADOOP-824-2.patch;https://issues.apache.org/jira/secure/attachment/12416547/HADOOP-824-2.patch","14/Aug/09 10:06;rksingh;HADOOP-824-3.patch;https://issues.apache.org/jira/secure/attachment/12416548/HADOOP-824-3.patch","22/Aug/09 17:35;rksingh;HADOOP-824-4.patch;https://issues.apache.org/jira/secure/attachment/12417367/HADOOP-824-4.patch","22/Aug/09 18:37;rksingh;HADOOP-824-5.patch;https://issues.apache.org/jira/secure/attachment/12417369/HADOOP-824-5.patch","25/Aug/09 13:27;yhemanth;MAPREDUCE-824-6.patch;https://issues.apache.org/jira/secure/attachment/12417611/MAPREDUCE-824-6.patch","26/Aug/09 04:36;rksingh;MAPREDUCE-824-7.patch;https://issues.apache.org/jira/secure/attachment/12417701/MAPREDUCE-824-7.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,2009-08-06 11:51:19.166,,,false,,,,,,,,,,,,,,,,,,72504,Reviewed,,,,Tue Oct 18 07:01:31 UTC 2011,,,,,,,"0|i0jepz:",111315,Support hierarchical queues in the CapacityScheduler to allow for more predictable sharing of cluster resources.,,,,,,,,,,,,,,,,,,,,"04/Aug/09 13:49;yhemanth;Here is a proposal that came out after some discussions for consideration:

h3. User Scenario
Consider an organization ""Org1"". Suppose they have a queue which is allocated a capacity of 60% of the cluster resources. Org1 has the following types of jobs: 
- Production jobs 
- Research jobs - belonging to three projects - say Proj1, Proj2 and Proj3 
- Miscellaneous types of jobs. 

Org1 would like to have a greater control over how the 60% capacity is used effectively amongst these types of jobs. For e.g. they'd like the research jobs and miscellaneous jobs to get some minimum amount of capacity for their work, and a significant share of the capacity to be alloted to production jobs. Production jobs are submitted somewhat rarely to the cluster. Whenever they are submitted, they must get the first chance to run, followed by research jobs and then miscellaneous jobs. However, whenever there are no production jobs, the research jobs must get this unused capacity.

h3. A Sample Configuration 

The following is an illustrative expression of the features/changes we plan to implement in the capacity scheduler to solve the use cases mentioned in the User Scenario. The actual specification may be different from the one described here. However, effort will be made to keep it as close to this expression as possible. 
{noformat}
grid {
  Org1 min=60% {
    priority min=90% {
      production min=82%
      proj1 min=6% max=10%
      proj2 min=6%
      proj3 min=6%
    }
    miscellaneous min=10%
  }
  Org2 min=40%
}
{noformat}
h3. Description

The features introduced in the sample configuration are described below. 

h4. Sub-queues 

- Sub-queues are queues configured within queues. They provide a mechanism for administrators to link logically related queues (say on the basis of the organization they are set up for). 
- Using sub-queues, administrators would primarily be able to use capacity allocated to their organization more effectively than in the current model. 
- For instance, in the current implementation which has only a single level of queues, if there is unused capacity in a queue, it is divided among remaining queues in proportion to their capacities. This has the disadvantage that logically unrelated queues could stake a claim to this unused capacity even if the organization itself is being under-served. However, by defining a hierarchy, administrators can ensure that unused capacity can be first alloted to sub-queues within the organization at the same level. 
- Sub-queues can be nested. So there can be queues within a sub-queue. 
- The last queue in a hierarchy of queues is the only queue to which jobs can be submitted. In this JIRA, we'll call it leaf level queue. 

h4. Minimum Capacities 

- A minimum capacity of a sub-queue defines what percentage of the parent's capacity it is entitled to. 
- The scheduler will pick up a sub-queue to service if it's furthest away from meeting its minimum capacity. The current definition of being furthest away is a function of currently used capacity and minimum capacity. (used-capacity/minimum-capacity) 
- By default, a queue's capacity usage is elastic and will go beyond the minimum capacity if there is unused capacity. 
Leaf level queues that are fairly important, and their parent sub-queues recursively must be granted a high minimum percentage to ensure the scheduler chooses them first. 
- The minimum capacity of a queue can be zero. Setting to zero means that this queue will be serviced ONLY IF no other queue at the same level has anything more to run (irrespective of current usage). 
- If the minimum capacity for a queue is not specified, there are two choices: 
-- Defaults to zero 
-- Defaults to 100 - sum (minimum capacities for queues at the same level). 
--- If more than one queue has no minimum specified, this value will be equally split among all queues. 

h4. Maximum Capacity 

- A maximum capacity defines a limit beyond which a sub-queue cannot use the capacity of its parent queue. 
- This provides a means to limit how much excess capacity a sub-queue can use. By default, there is no limit. 
- A typical use case for using a maximum capacity limit could be to curtail certain jobs which are long running in nature from occupying more than a certain percentage of the cluster, which in the absence of pre-emption, could lead to capacity guarantees of other queues being affected. 
- Naturally, the maximum capacity of a queue can only be greater than or equal to its minimum capacity. 

h4. Scheduling among sub-queues 

When a tasktracker comes to get a task, the scheduler uses the following algorithm to assign a task to the tracker. 
- Sort all sub-queues at the first level according to a function of used-capacity and minimum-capacity. 
- Pick up the most needy queue, and see if it has work to do. 
-- If this is a leaf level queue with pending tasks, a task is picked up from the jobs in the queue as long as it is within any maximum limits for the queue. 
-- If this is not a leaf level queue, apply the algorithm recursively among sub-queues under this queue. 
- If no task is found, move on to the next queue in the list.","06/Aug/09 11:51;rksingh;Uploaded the patch:
Patch implements the model discussed above. This is first cut of the implementation.

Some Class level details:
======================
The hierarchial queue follows composite pattern.

Component
----------------
AbstractQueue :
------------------------
- This is abstraction for all the queues.
- implements default behavior for some of the abstraction.
- declares interfaces to access the children and its components.
- defines an interface for accessing a queue's parent in the recursive structure.

Composite
-----------------
ContainerQueue:

- This provides functionality mentioned as the part of Sub-Queues.
- consists of composition of AbstractQueue.
- provides functionality to manipulate children
- delegates respective children component calls.

Leaf
----------
JobQueue:

- This provides functionality similar to Leaf Queue mentioned in the document above.
- Jobs would be submitted only to this queue.
- Consists of list of jobs and acts on the JobInProgressListener events.


JobQueuesManager no more maintains list of runningJob and waitingJobs , they are maintained by JobQueue. It simply delegates
the call to JobQueue.


AbstractQueue have QueueSchedulingContext ,defined , it consists of all the queue related information required for scheduling decision.

Some refactoring has been done in terms of moving out some of the inner classes from CapacityTaskScheduler , esp the QueueSchedulingInfo and TaskSchedulingInfo.


I ll be uploading patch with testcases.","10/Aug/09 05:29;sreekanth;Started taking look at the patch, following are comments:

QueueSchedulingContext:
* Can a comment be added for the each of the fields of the QueueSchedulingContext, stating what each is meant to do?
AbstractQueue
* Change cummulative to cumulative.
* Childrens should be children
* Can we move the queue scheduling information string into QueueSchedulingContext? As we have for the TaskSchedulingContext?
* Can we take away state variables prevReduceClusterCapacity and prevMapClusterCapacity so there is no state maintained?

JobQueue:-
* Why are we having the jobStateChanged method in JobQueue, shouldnt it be in JobQueueManager which is JobInProgressListener and shouldn't it be handling the job state change event and handling and management of movement of jobs?

JobInitalizationPoller:
*Do we need to change the log statements? (Queue to AbstractQueue? Or Should it be JobQueue?)
* What would QueueManager.getQueues() give to {{JobInitializationPoller}} in {{CapacityScheduler.start()}}","10/Aug/09 05:34;sreekanth;To clarify the above comment was:

{quote}
What would QueueManager.getQueues() give to JobInitializationPoller in CapacityScheduler.start()
{quote}

The {{JobInitalizationPoller}} acts only on {{JobQueue}} not {{ContainerQueue}} which is configured by the {{QueueManager}}. 

I agree, the future patch would address configuration related changes, but atleast interim the patch should see to that {{JobInitalizationPoller}} would get only {{JobQueue}}.


","14/Aug/09 09:57;rksingh;This patch consists of the testcases for hierarchial queues 

Addressed the issues raised by sreekanth.

","14/Aug/09 10:06;rksingh;previous patch missed out the newly added testcases.
adding them","20/Aug/09 05:36;yhemanth;Some comments:
- AbstractQueue.updateContext can move into QueueSchedulingContext, as all the state it is operating on is in QSC. 
- Also prevMapClusterCapacity and prevReduceClusterCapacity can also be moved to the context. They can be private, and renamed to prev*Capacity, dropping the 'Cluster' because for container queues, they don't reflect the entire cluster capacity. Same naming change would apply to variables in QueueSchedulingContext (like setMapClusterCapacity, etc)
- AbstractQueue.getOrderedJobQueues, its not very clear that this is looking through the entire hierarchy. Also, it assumes that sorting is done before this. So, its not a very orthogonal API. Move this to the scheduler, and introduce a new API like AbstractQueue.getDescendentJobQueues().
- Override AbstractQueue.addChildren in JobQueue to throw an unsupported exception.
- Make AbstractQueue.getChildren package private and document it is for tests.
- I suggest we modify the algorithm in distributeUnConfiguredCapacity to follow this pattern to make it clearer:
{code}
for (Queue q : children) {
  if (q.capacity == -1) {
    unconfigured.add(q);
  }
}

// distribute capacity for all unconfigured queues.

for (Queue q : children) {
  q.distributeUnconfiguredCapacity();
}
{code}
- I would suggest we provide equals and hashCode in AbstractQueue to be based on the queue Name. toString in AbstractQueue should print the queue name.
- I didn't understand the need for setting the capacity in conf in distributeUnconfiguredCapacity. It seems like requiring the Configuration instance to be passed to distributeUnconfiguredCapacity is creating an undesirable dependency. Can you check if we can break this dependency.
- distributeUnConfiguredCapacity will throw a Divide by zero if there is no queues without configured capacity.
- We don't need to pass the supportsPriority variable separately to the JobQueue's constructor. Let's set that directly in the JobQueue.QueueSchedulingContext which we are already passing to JobQueue.
- In JobQueue, methods like addWaitingJob etc should be private. Also, I think some of the methods can be folded. For e.g. makeJobRunning just calls addRunningJob, so we can refactor to remove makeJobRunning and call addRunningJob directly.
- TaskData seems out of place in TaskSchedulingContext. The scheduling context contains state w.r.to scheduler. TaskData is a simple abstraction that returns a view of job information based on the task type. So, let's pull it out and call it TaskDataView which can be extended by MapTaskDataView and ReduceTaskDataView. There should be only one ‌instance of these per scheduler instance and they can be got from the scheduler itself.
- Rename TaskSchedulingContext.add to TSC.update.
- Can we pull out the whole hierarchy building logic into a separate class - like a QueueHierarchyBuilder ? It could be given the CapacitySchedulerConf and QueueManager and have an API like buildHierarchy - which would return the root of the queues. Capacity scheduler can thus be abstracted from how the hierarchy is created - it just gets the hierarchy from somewhere. For e.g. in tests, the hierarchy can be manually created and given to the given.
- Please remove mapScheduler.initialize() and reduceScheduler.initialize().
- tsi.getMaxCapacity() < tsi.getCapacity(): this check in areTasksInQueueOverLimit does not seem required. Because the check is already being done in tsi.getCapacity()
- totalCapacity modification in the loadContext is a no-op, because the changes will not be reflected in the caller method. Likewise the check for totalCapacity > 100.0 is a no-op in createHierarchy.
- The separator char for queues is chosen to be '.' in createHierarchy. It must be checked that this character doesn't appear anywhere else in the queue name.
- Call to root.sort() should be from TaskSchedulingMgr.assignTasks()
- JobQueuesManager.createQueue should be addQueue. Also, it can get the queue name from the job queue object directly, and doesn't need the extra parameter.
- JobQueueManager.getQueueNames can be getJobQueueNames.
","21/Aug/09 06:40;yhemanth;Looked at the test cases:
- Code seems duplicated between CapacitySchedulerUtils and CapacityTaskScheduler and TestContainerQueue.
- In some test cases, when we create a queue, it is already adding a child to the parent. So, why do we need additional calls to addChildren ?
- What's the difference between testConfiguredCapacity and testMinCapacity ?
- The test cases testing scheduling are nice. The comments are out of sync a bit, and will be hard to maintain. Instead I suggest that we assert what we are documenting in the tests itself, so that they themselves read as comments, and will also always be in sync.
- As discussed, getCapacity() should not return max capacity any time. It should always return the current capacity or limit, whichever is smaller. Otherwise, the sort order of queues would be affected. 
- areTasksInQueueOverLimit should be changed to something along these lines:
{code}
      if (tsi.getMaxTaskLimit() > 0) {
        if (tsi.getNumSlotsOccupied() >= tsi.getCapacity()) {
          return true;
        }
      } 
      
      if (tsi.getMaxCapacity() > 0) {
        if (tsi.getNumSlotsOccupied() >= tsi.getMaxCapacity()) {
          return true;
        }
      }
      return false;
{code}
- At the same time, testMaxCapacity should be removed. I would instead recommend a test case that sets a max capacity on a queue, and checks scheduling honors the decision.","22/Aug/09 17:35;rksingh;Implemented all the changes suggested above
The patch consist of everything except the new recommended testMaxCapacity","22/Aug/09 17:38;rksingh;The comment regarding algorithm for distribution of unconfigured capacity , is not implemented as , current implementation is more efficient.
Will be adding 1 more patch with test for max capacity.","22/Aug/09 18:37;rksingh;Attaching the new file with testMaxCapacity testcase","25/Aug/09 03:27;yhemanth;This is getting better. I do have some more feedback:

- updateStatsOnRunningJob, addRunningJob, removeRunningJob, removeWaitingJob - make private
- ASF licence header should be the first in the src file.
- Replace sortJobQueues with inline method.
- QueueHierarchyBuilder is creating a new instance of the CapacityTaskScheduler, which is unnecessary.
- static builder instance also seems unnecessary.
- In QueueHierarchyBuilder, when checking for separator char, IllegalArgumentException must show the queue name which failed the check.
- Discuss: Back dependency between QueueHierarchyBuilder and Scheduler - can this be avoided.
- AbstractQueue does not override equals, while hashcode is overridden. Also, the toString API was previously printing other information. I'd only asked the name of the queue to be prepended to it, not to remove the other information.
- It is a little confusing that the number of slots being asserted after task assignment does not include the currently scheduled task. Recommend to move the asserts before assignment.
- Root should always be set up only in a certain way. I would recommend, there's a single static instance of root, which is always got from the capacity scheduler, even in tests.
- In testMaxCapacity, rt.update in tests should send in the capacity of the clusters to be in sync.
- getTaskDataView() need not be in TaskSchedulingContext. Since it is static, it can be called directly from other classes like the scheduler, passing the type.
- AbstractQueue.addChildren should be addChild.

Some of the earlier comments are not taken:
- APIs in JobQueuesManager and JobQueue can be folded still.
- mapTSI and reduceTSI member variables of JobQueue are not needed.
- AbstractQueue.getChildren is still public
- getCapacity() should not return max capacity any time. It should always return the current capacity or limit, whichever is smaller.


","25/Aug/09 13:27;yhemanth;Attached patch incorporates all comments I raised, except the following:
- JobQueue.removeWaitingJob is used by JobInitializationPoller. Hence, retained the access level as package private.
- I also did not fold methods in JobQueue and JobQueuesManager since most seemed to be used at more than one place.

In addition to the comments, Rahul and I also found and fixed a bug in getMaxCapacity() and changed the default value of the same to -1 in the capacity-scheduler.xml.template.

Rahul, can you please take a look at the changes and make sure you are fine with them ?","25/Aug/09 13:27;yhemanth;Trying hudson.","26/Aug/09 04:35;rksingh;Some minor nitpickings:

 In QueueHierarchyBuilder variable scheduler is never used.
 In QueueHierarchyBuilder loadContext has parameter of QueueManager which is not used .changing the signature of loadContext to 
   only take queue name. same with  createHierarchy method.

Some of the testcases had some System.out.println which are debug statements removing those.
","26/Aug/09 04:44;rksingh;Uploaded the new patch with comments mentioned above .
","26/Aug/09 05:06;yhemanth;+1 for the changes. Thanks for catching the changes in QueueHierarchyBuilder.","26/Aug/09 06:49;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12417611/MAPREDUCE-824-6.patch
  against trunk revision 807640.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 16 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/518/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/518/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/518/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/518/console

This message is automatically generated.","26/Aug/09 15:38;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12417701/MAPREDUCE-824-7.patch
  against trunk revision 807954.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 16 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/520/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/520/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/520/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/520/console

This message is automatically generated.","27/Aug/09 07:50;yhemanth;I just committed this to trunk. Thanks, Rahul !","18/Oct/11 07:01;acmurthy;Editorial pass over 0.21/0.22/0.23 content for hadoop-0.23.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskTracker should handle disk failures,MAPREDUCE-2657,12497097,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,bharathm,bharathm,28/Jan/11 21:05,08/Sep/11 21:11,12/Jan/21 09:52,,0.20.203.0,,,,,,,,tasktracker,,,,,,0,,,,,Umbrella for TaskTracker disk failure handling jiras.,,cutting,davelatham,decster,devaraj,eli,hammer,lagomorph,ravidotg,raviteja,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-2656,MAPREDUCE-2651,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-09-08 21:11:53.538,,,false,,,,,,,,,,,,,,,,,,36992,,,,,Thu Sep 08 21:11:53 UTC 2011,,,,,,,"0|i02onj:",13595,,,,,,,,,,,,,,,,,,,,,"08/Sep/11 21:11;acmurthy;How does this relate to MAPREDUCE-2143?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Blacklisted TT reset time should be configurable,MAPREDUCE-2231,12494027,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,sinofool,sinofool,24/Dec/10 07:50,07/Sep/11 08:09,12/Jan/21 09:52,,,,,,,,,,jobtracker,,,,,,0,,,,,"A TaskTracker in the blacklist will be reset to healthy state after a day.
""after a day"" should be configurable.
",,atm,hammer,roelofs,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Dec/10 09:12;sinofool;MAPREDUCE-2231-v1.patch;https://issues.apache.org/jira/secure/attachment/12466928/MAPREDUCE-2231-v1.patch","24/Dec/10 17:58;sinofool;MAPREDUCE-2231-v2.patch;https://issues.apache.org/jira/secure/attachment/12466944/MAPREDUCE-2231-v2.patch","24/Dec/10 18:04;sinofool;MAPREDUCE-2231-v3.patch;https://issues.apache.org/jira/secure/attachment/12466945/MAPREDUCE-2231-v3.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2011-01-04 20:31:56.511,,,false,,,,,,,,,,,,,,,,,,62109,,,,,Wed Sep 07 08:09:07 UTC 2011,,,,,,,"0|i0e7rz:",81022,"Adds mapreduce.jobtracker.update.faulty.tracker.interval, which means how long a blacklisted tasktracker should back to normal.",,,,,,,,,,,,,,,,,,,,"28/Dec/10 03:30;sinofool;test-patch.sh passed locally.
Will someone fire a hudson build please?

+1 overall.  

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 system test framework.  The patch passed system test framework compile.


","04/Jan/11 20:31;roelofs;Note that heuristic blacklisting doesn't work very well in the first place.  See MAPREDUCE-1966 for more info.","26/Feb/11 20:33;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12466945/MAPREDUCE-2231-v3.patch
  against trunk revision 1074251.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/42//testReport/
Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/42//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/42//console

This message is automatically generated.","07/Sep/11 08:09;acmurthy;Sorry to come in late, the patch has gone stale. Can you please rebase? Thanks.

Given this is not an issue with MRv2 should we still commit this? I'm happy to, but not sure it's useful. Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add queue-level metrics 0.20-security branch,MAPREDUCE-2558,12509082,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,naisbitt,naisbitt,naisbitt,02/Jun/11 12:47,02/Sep/11 22:13,12/Jan/21 09:52,08/Jun/11 08:01,0.20.204.0,,,,,0.20.204.0,,,jobtracker,,,,,,0,,,,,We would like to record and present the jobtracker metrics on a per-queue basis.,,tgraves,vicaya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/11 02:52;naisbitt;MAPREDUCE-2558-0.20s.patch;https://issues.apache.org/jira/secure/attachment/12481312/MAPREDUCE-2558-0.20s.patch","08/Jun/11 00:39;naisbitt;MAPREDUCE-2558-testfixes.patch;https://issues.apache.org/jira/secure/attachment/12481762/MAPREDUCE-2558-testfixes.patch","02/Jun/11 15:15;naisbitt;queue-metrics-v2.patch;https://issues.apache.org/jira/secure/attachment/12481241/queue-metrics-v2.patch","02/Jun/11 20:39;naisbitt;queue-metrics-v3.patch;https://issues.apache.org/jira/secure/attachment/12481280/queue-metrics-v3.patch","02/Jun/11 12:51;naisbitt;queue-metrics.patch;https://issues.apache.org/jira/secure/attachment/12481226/queue-metrics.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,2011-06-02 16:18:38.55,,,false,,,,,,,,,,,,,,,,,,44115,Reviewed,,,,Fri Sep 02 22:13:24 UTC 2011,,,,,,,"0|i09dk7:",52630,,,,,,,,,,,,,,,,,,,,,"02/Jun/11 12:51;naisbitt;Initial patch draft.  I still need to add some tests, and I believe I am missing something with the metric registration/initialization.","02/Jun/11 15:15;naisbitt;The queue-specific metrics now appear in the jobtracker output.  I still need to add some tests","02/Jun/11 16:18;vicaya;Comments on patch v2:

The queue level instrumentation class configuration is not a requirement. If you don't want to test the mechanism I'd suggest that you remove the related code (you can still keep the instrumentation methods, it's the abstract class and class loading stuff that are unnecessary).

Looking up the queue from queue manager for metrics for every metrics update seems unnecessary as well. You can add a final Queue field in JIP and initialize it in the JIP ctor.

You need a unique name (which also maps to a MBean name) for every queue metrics so you don't have naming conflicts when you have multiple queues. An MBean/jconsole friendly QueueInstrumentation#create would look like this:
{code}
  return ms.register(""QueueMetrics,q=""+ queueName, ""Queue metrics"",
                   new QueueMetricsSource(queueName, conf));
{code}","02/Jun/11 17:30;naisbitt;With your first comment about the ""queue level instrumentation class configuration"", are you just saying that I should combine the QueueInstrumentation and QueueMetricSource classes, or are you just talking about removing the extra functionality for the createInstrumentation and get/setInstrumentationClass in the Queue class? (I'm not sure I understand what you mean there)","02/Jun/11 18:30;vicaya;Your first guess is what I meant :) i.e., you can combine QueueInstrumentation and QueueMetricsSource and call the resulting class QueueMetrics. The introduction of the QueueInstrumentation abstract class is only necessary when you have different implementations of queue metrics, which we don't (and probably won't) have.","02/Jun/11 18:37;naisbitt;Thanks.  That's what I thought.  I had originally created the abstract class thinking we may want to create a stub/mock queue metrics class for testing purposes.  I'm fine combining the classes and go ahead and do so - unless you think we may need them for testing.","02/Jun/11 18:47;vicaya;You can mock concrete classes the same way as long as the class and methods are not final:
{code}
QueueMetrics metrics = Mockito.mock(QueueMetrics.class);
{code}","02/Jun/11 20:39;naisbitt;Addressed Luke's comments.  I'm working on adding some tests now, but I thought it would be good to get the updates posted.","03/Jun/11 02:52;naisbitt;Addressed Luke's comments, and added tests","03/Jun/11 02:54;naisbitt;This patch is for the 20-security branch, so here are the results of the manual test-patch run:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec] 
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 Eclipse classpath. The patch causes the Eclipse classpath to differ from the contents of the lib directories.
     [exec] 


The javadoc and Eclipse classpath failures are pre-existing and unrelated to these changes.","03/Jun/11 03:03;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12481312/MAPREDUCE-2558-0.20s.patch
  against trunk revision 1130554.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/340//console

This message is automatically generated.","03/Jun/11 16:58;naisbitt;Again, this patch is for 0.20-security, so the test-patch failures from Hudson are not applicable.
Additionally, Luke said it looked good to him (by email).","03/Jun/11 22:55;cdouglas;+1

I committed this. Thanks, Jeff!","07/Jun/11 23:31;naisbitt;There are some tests that use invalid queues, and a NullPointerException is occurring instead of the expected IOException.  Another test uses an invalid queue for a job that it expects to pass (the invalid queue was previously ignored).","08/Jun/11 00:39;naisbitt;Added checks for invalid queues, and fixed test that was previously expecting an invalid queue to be ignored.","08/Jun/11 00:43;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12481762/MAPREDUCE-2558-testfixes.patch
  against trunk revision 1133175.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/361//console

This message is automatically generated.","08/Jun/11 08:01;mahadev;I just pushed the test fix. Ideally we should have opened another jira but  but its fine. thanks jeff!","02/Sep/11 22:13;omalley;Hadoop 0.20.204.0 was just released.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Combiner that aggregates all the mappers from a machine,MAPREDUCE-2812,12415698,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,marz,marz,26/Feb/09 19:06,11/Aug/11 18:11,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"From what I can tell, the Combiner just aggregates data from a single map task. It would be useful, especially during map-only jobs, to have a combiner that aggregates data from all the map tasks on a given machine. My use case for this is to vertically partition a set of records which start out in the same files. By doing this in a map-only task, way too many files are created (About 50 files are created per input split). By pumping all the data through a reducer, a lot of unnecessary overhead occurs. With the proposed feature, I would get 50*number of machines files rather than 50*number of input splits files for this use case.",,cutting,hammer,jghoman,lianhuiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,65413,,,,,2009-02-26 19:06:55.0,,,,,,,"0|i0e75b:",80920,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI interface for managing Jobtracker Queues and ACLs,MAPREDUCE-2810,12422838,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,rajive,rajive,15/Apr/09 01:54,11/Aug/11 18:08,12/Jan/21 09:52,,,,,,,,,,job submission,security,,,,,0,,,,,"As the number of users in a hadoop cluster increases, it gets difficult to manage Queues and ACLs in mapred-site.xml .  Its good to have a CLI  interface to update mapred-site.xml and validate the configuration. The CLI should provie

 - list of current ACLs
 - Update and refresh ACLs",,aaa,aw,rksingh,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-04-15 04:41:59.43,,,false,,,,,,,,,,,,,,,,,,65418,,,,,Wed Apr 15 04:41:59 UTC 2009,,,,,,,"0|i0e75r:",80922,,,,,,,,,,,,,,,,,,,,,"15/Apr/09 04:41;yhemanth;This issue is partly addressed in HADOOP-5396 (the part of refreshing ACLs).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Collect iops per task,MAPREDUCE-2102,12475715,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,acmurthy,acmurthy,03/Oct/10 16:27,05/Aug/11 14:01,12/Jan/21 09:52,,,,,,,,,,task,,,,,,0,,,,,We should enhance the MR framework to collect iops per task along with CPU/Memory (MAPREDUCE-220).,,aah,atm,dhruba,eli,hammer,henryr,johanoskarsson,lianhuiwang,philip,revans2,srivas,vicaya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-10-07 05:27:57.631,,,false,,,,,,,,,,,,,,,,,,65997,,,,,Fri Aug 05 14:01:12 UTC 2011,,,,,,,"0|i0e80f:",81060,,,,,,,,,,,,,,,,,,,,,"07/Oct/10 05:27;dhruba;The map task typically reads an HDFS file ad writes to local disk. So, this should encompass both reads and writes as well, isn't it?

also, maybe it is more interesting to collect bytes/sec  rather than IOPS. ","11/Oct/10 21:05;vicaya;Bytes read and written are already being collected. Additional io ops (made available via HADOOP-6859) help diagnose performance problems of certain applications.

I'm adding the support of file system counters in MAPREDUCE-901, which would subsume this jira.","17/Nov/10 18:20;acmurthy;This isn't about Hadoop filesystem counters - we could capture low-level OS metrics for iops, read/write bytes/sec, network iops etc.","05/Aug/11 14:01;revans2;I am not sure how to do this on other operating systems.  On Linux if Hadoop is running on a newer Kernel then it can use /proc/<PID>/io, but I believe that it has to be newer then 2.6.20.  Anything else would require a very ugly hack like LD_PRELOAD.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add Kerberos HTTP SPNEGO authentication support to Hadoop JT/NN/DN/TT web-consoles,MAPREDUCE-2287,12497019,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Not A Problem,tucu00,tucu00,tucu00,28/Jan/11 06:20,04/Aug/11 17:26,12/Jan/21 09:52,04/Aug/11 17:26,0.23.0,,,,,,,,security,,,,,,0,,,,,This JIRA is for the MAPRED portion of HADOOP-7119,,cdouglas,cutting,eli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/11 06:09;tucu00;ha-mapred-01.patch;https://issues.apache.org/jira/secure/attachment/12469795/ha-mapred-01.patch","31/Jan/11 13:17;tucu00;ha-mapred-02.patch;https://issues.apache.org/jira/secure/attachment/12469818/ha-mapred-02.patch","28/Jan/11 09:11;tucu00;ha-mapred.patch;https://issues.apache.org/jira/secure/attachment/12469647/ha-mapred.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2011-02-26 15:20:11.192,,,false,,,,,,,,,,,,,,,,,,66056,,,,,Thu Aug 04 17:26:07 UTC 2011,,,,,,,"0|i0jin3:",111950,,,,,,,,,,,,,,,,,,,,,"28/Jan/11 09:13;tucu00;refer to comment in HADOOP-7119","31/Jan/11 13:16;tucu00;wrong patch format","26/Feb/11 15:20;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12469818/ha-mapred-02.patch
  against trunk revision 1074251.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/35//testReport/
Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/35//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/35//console

This message is automatically generated.","04/Aug/11 17:26;tucu00;as per 'v3' HADOOP-7119 this patch is not required.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MapReduce Math Library,MAPREDUCE-2471,12506124,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,szetszwo,szetszwo,szetszwo,04/May/11 01:28,21/Jun/11 21:10,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"This is an umbrella JIRA for a MapReduce math library.

The core algorithms are _MapReduce-Sum_, _MapReduce-FFT_, _MapReduce-Multiplication_, etc.",,aah,atm,cdouglas,cutting,romainr,sandyr,shv,tlipcon,tomwhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/11 20:58;szetszwo;gmp_mult.c;https://issues.apache.org/jira/secure/attachment/12483343/gmp_mult.c","08/May/11 22:58;szetszwo;m2471_20110508.patch;https://issues.apache.org/jira/secure/attachment/12478553/m2471_20110508.patch","21/Jun/11 21:10;szetszwo;random_mult.c;https://issues.apache.org/jira/secure/attachment/12483346/random_mult.c",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2011-05-04 06:18:33.097,,,false,,,,,,,,,,,,,,,,,,70664,,,,,Tue Jun 21 20:58:22 UTC 2011,,,,,,,"0|i0e7hb:",80974,,,,,,,,,,,,,,,,,,,,,"04/May/11 01:34;szetszwo;I already have designed algorithms for computing summations, discrete Fourier transforms and multiplications on MapReduce; see [1].  Will post the implementation soon.

[1] *Schönhage-Strassen Algorithm with MapReduce for Multiplying Terabit Integers*
SNC 2011, to appear.
([Preprint PDF|http://www.cargo.wlu.ca/SNC2011/papers.html])","04/May/11 01:36;szetszwo;Sorry, the link to the preprint PDF should be this: http://people.apache.org/~szetszwo/ssmr20110430.pdf","04/May/11 06:18;tlipcon;Does this belong in MR proper? Seems like a great github project or incubator proposal.","04/May/11 17:41;szetszwo;Todd, this is a good question.  I will think about it.","08/May/11 22:58;szetszwo;m2471_20110508.patch: For review but it may not be committed.","21/Jun/11 20:58;szetszwo;gmp_mult.c: here is the c program.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DistributedCache for M-R chains,MAPREDUCE-2583,12509842,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,mmccuiston,mmccuiston,10/Jun/11 15:57,10/Jun/11 16:47,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"Currently the DistributedCache appears to be created at the granularity of a job.  In the case of a M-R chain, it is sometimes useful to share information out-of-band (as small files in hdfs) with each task in the chain.  For instance, the first M-R phase within a two-phase M-R chain might produce useful statistics that could be used to configure the second phase.",,atm,gates,lianhuiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-06-10 16:47:28.648,,,false,,,,,,,,,,,,,,,,,,71429,,,,,Fri Jun 10 16:47:28 UTC 2011,,,,,,,"0|i0e7cv:",80954,,,,,,,,,,,,,,,,,,,,,"10/Jun/11 16:47;revans2;I am not really sure what you are getting at here.  I have tried this sort of thing before by writing small statistics files as out-of-band files and reading them back in the next map/reduce as part of the distributed cache, but it did not turn out very well.  Even with the distributed cache if you are scaling up to 100s of mappers/reducers it will put a lot of load on the name node.  If it really is a requirement it is best to post process the files turning them into a single highly replicated files before passing them off to the next phase.

If you are turning this into a formal Map/Reduce feature then you probably want to do this compaction in the cleanup task, and have some sort of size limits on how much data can flow through this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Forward port MAPREDUCE-1966 Add task tracker graylisting,MAPREDUCE-2526,12507923,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,jeagles,jeagles,20/May/11 20:04,20/May/11 20:05,12/Jan/21 09:52,,0.23.0,,,,,,,,jobtracker,,,,,,0,,,,,"The current heuristic of rolling up fixed number of job failures per tracker isn't working well, we need better design/heuristics.",,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-1966,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,150246,,,,,2011-05-20 20:04:12.0,,,,,,,"0|i0e7fr:",80967,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MR-279: Metrics for shuffle,MAPREDUCE-2468,12506010,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,vicaya,vicaya,vicaya,03/May/11 01:58,03/May/11 22:53,12/Jan/21 09:52,03/May/11 22:53,,,,,,,,,mrv2,,,,,,0,,,,,Metrics for MR shuffle service.,,aah,cdouglas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/May/11 02:04;vicaya;mr-2468-shuffle-metrics-v1.patch;https://issues.apache.org/jira/secure/attachment/12478019/mr-2468-shuffle-metrics-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2011-05-03 22:53:09.283,,,false,,,,,,,,,,,,,,,,,,150210,Reviewed,,,,Tue May 03 22:53:09 UTC 2011,,,,,,,"0|i09dun:",52677,,,,,,,,,,,"mrv2, shuffle, metrics",,,,,,,,,,"03/May/11 22:53;cdouglas;+1

I committed this to the MR-279 branch. Thanks Luke!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fair Multiple Task Assignment Scheduler (Assigning multiple tasks per heart beat),MAPREDUCE-2261,12495485,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Invalid,,devaraj,devaraj,13/Jan/11 13:22,16/Mar/11 05:13,12/Jan/21 09:52,25/Feb/11 16:37,0.21.0,,,,,,,,,,,,,,0,,,,,"      Functionality wise the Fair Multiple Task Assignment Scheduler behaves the same way except the assignment of Tasks. Instead of assigning a single Task per heartbeat, it checks for all the jobs if any local or non-local Task that can be launched.

Fair Multiple Task Assignment Scheduler has the advantage of assigning multiple jobs per heart beat interval depending upon the slots available on the Task Tracker, by configuring the number of parallel tasks to be executed in a Task Tracker at any point of time. The advantages are as follows:

a) Parallel Execution allows tasks be to submitted and processed in parallel independent of the status of other tasks.
b) More number of tasks is assigned in a heartbeat interval and consequently multitasking capability increases.
c) With multi task assignment, Task Tracker efficiency is increased.
",,atm,eli,kam_iitkgp,lianhuiwang,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-01-13 18:45:47.396,,,false,,,,,,,,,,,,,,,,,,150096,,,,,Wed Mar 16 05:13:02 UTC 2011,,,,,,,"0|i0jilj:",111943,,,,,,,,,,,,,,,,,,,,,"13/Jan/11 18:45;tlipcon;How does this differ from the assignmultiple feature present in the fairscheduler in trunk?","14/Jan/11 12:58;devaraj;Hi Todd,

                   Existing fair scheduler assigns a single job per heart beat even more slots are available in the task tracker. Next job will be assigned to the same task tracker for the next heart beat only. This way we can assign multiple tasks but we cannot assign multiple tasks for the same heart beat.

   Fair Multi Task assignment scheduler assigns multiple jobs to the task tracker as in the below way.

	If number of jobs in the queue is less than the cluster capacity or equal, then it calculates the no of jobs can be assigned to the each task tracker (shared equally manner) and it will assign multiple jobs to the each task tracker for a heart beat.

	If number of jobs in the queue is more than the cluster capacity, then it assigns multiple jobs to the task tracker based on the task tracker capacity for a heart beat. This repeats for all the task trackers in the cluster.
","24/Feb/11 15:48;lianhuiwang;i agree with Devaraj K.
in the current version, the fair scheduler assign only one task int per hear beat.
in the class FairScheduler we can see the following:
assignTasks(TaskTracker tracker){
 for (Schedulable sched: scheds) {
   Task task = taskType == TaskType.MAP ? 
                    sched.assignTask(tts, currentTime, visitedForMap) : 
                    sched.assignTask(tts, currentTime, visitedForReduce);
   if (task != null) {
     tasks.add(task);
     break; // This break makes this loop assign only one task
   }
 }
}
if it assgin one task,the function will return.","24/Feb/11 18:01;tlipcon;I believe you're both looking at the fair scheduler code from 0.20. MAPREDUCE-706 rewrote a lot of the fair scheduler and includes this feature.","25/Feb/11 01:51;lianhuiwang;hi,Todd.
i see the code in the trunk version.
the trunk doesnot include the patch of MAPREDUCE-706?","25/Feb/11 02:01;lianhuiwang;MAPREDUCE-706 supports the FIFO pools instead of fair sharing.it only add the FIFO to the pools.
but is doesnot assign multiple tasks per heart beat.","25/Feb/11 07:05;tlipcon;MAPREDUCE-706 was a poorly named JIRA. It actually rewrote much of the fair scheduler, including adding this feature. In trunk, look at the usage of the mapAssignCap and reduceAssignCap variables. You'll see that assignTasks loops until the specified number of tasks have been assigned or the load manager indicates that no more can be assigned.

","25/Feb/11 12:17;lianhuiwang;Todd,thank you.i see the code.
outside of the for{},it has while{} that does the assign the capacity tasks.","16/Mar/11 05:13;devaraj;I have validated all features against trunk and all are available. Thanks Todd and Lianhui for giving me the required information. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Have a configurable metric reporting CPU/disk usage per user,MAPREDUCE-1808,12463937,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,alexvk,alexvk,alexvk,06/May/10 23:12,07/Mar/11 08:02,12/Jan/21 09:52,,,,,,,,,,tasktracker,,,,,,0,,,,,"Many organizations are looking at resource usage per department/group/user for diagnostic and resource allocation purposes.  It should be straightforward to implement a metric showing the simple resource usage like CPU time and disk I/O per user and aggregate them using Ganglia.

Eventually, we can create an API for pluggable metrics (there is one for Jobtracker and Tasktracker).

Let me know your thoughts.

Alex K
",,acmurthy,alexlod,aw,cdouglas,cutting,eli,farseeing,hammer,hong.tang,lianhuiwang,rajive,schen,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/May/10 17:16;alexvk;HADOOP-6755.patch;https://issues.apache.org/jira/secure/attachment/12444513/HADOOP-6755.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2010-05-07 02:09:41.597,,,false,,,,,,,,,,,,,,,,,,72445,,,,,Mon Mar 07 08:02:51 UTC 2011,,,,,,,"0|i0e8nj:",81164,,,,,,,,,,,monitoring,,,,,,,,,,"07/May/10 02:09;yhemanth;This seems to be very closely related to work being discussed in MAPREDUCE-220. Can you please coordinate ?","14/May/10 17:14;alexvk;I looked at MAPREDUCE-220.  I think the idea is a bit different here: to be able to monitor the usage per user in Ganglia or some other monitoring tool.  I am attaching a simple patch mostly for the demo purposes.

In general, I think there should be two systems: one monitoring, focusing on a few important metrics (cpu time, memory, disk usage per user), and more detailed per task and containing more metrics, which later can be picked up by some more detailed reporting/analysis system.

Alex K
","14/May/10 17:16;alexvk;The patch to report per-user user time and disk usage.","14/May/10 20:53;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12444513/HADOOP-6755.patch
  against trunk revision 944397.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/520/console

This message is automatically generated.","21/May/10 16:44;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12444513/HADOOP-6755.patch
  against trunk revision 946955.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 2 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/200/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/200/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/200/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/200/console

This message is automatically generated.","28/Feb/11 03:02;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12444513/HADOOP-6755.patch
  against trunk revision 1074251.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/79//console

This message is automatically generated.","07/Mar/11 08:02;tlipcon;Cancelling patch since it no longer applies.

Alex: it seems to me that this use case might be better suited by analyzing history logs, but I'm not entirely against it.

I noticed lots of watchers on the ticket - do others have some thoughts about this idea?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
want InputFormat and OutputFormat for zip archives,MAPREDUCE-228,12366825,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,cutting,cutting,09/Apr/07 18:01,20/Jan/11 15:32,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"An InputFormat and OutputFormat for zip-format archives would be useful.  It would provide a standard file format, friendly to foreign applications (e.g., streaming), that is also optimized for Hadoop.

The OutputFormat could generate archives containing a series of small (~1MB) files, each individually compressed.  The InputFormat could then generate a split for each file in the archive.  (Zip archives contain a directory at the end of the file that may be efficiently read when constructing splits.)  Input splits could thus be sent to a node where they are stored locally for the map phase, providing good performance.

This would thus permit applications to (1) use a standard file format; (2) keep data compressed; and (3) efficiently split input into chunks substantially smaller than HDFS blocks.  This is not available today.",,lianhuiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2007-08-31 20:21:32.784,,,false,,,,,,,,,,,,,,,,,,148613,,,,,Thu Jan 20 15:32:16 UTC 2011,,,,,,,"0|i0irvz:",107612,,,,,,,,,,,,,,,,,,,,,"31/Aug/07 20:21;arkady;It would be nice to automatically detect that the input is compressed and inserts this the corresponding conversion in from of actual application's InputFormat .
There should also be an option for Streaming to bypass the decompression.
","20/Jan/11 15:32;lianhuiwang;i agree with arkady borkovsky. let the FileInputFormat.getSplits(JobConf, int) to choose the compressed InputFormat accord to the filename. 
for example:LzoTextInputFormat or ZipTextInputFormat.
but now the FileInputFormat cannot support it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic add input for one job,MAPREDUCE-1434,12454877,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,shixing,shixing,01/Feb/10 06:19,20/Jan/11 03:25,12/Jan/21 09:52,,0.20.3,,,,,,,,,,,,,,0,,,,,"Always we should firstly upload the data to hdfs, then we can analize the data using hadoop mapreduce.

Sometimes, the upload process takes long time. So if we can add input during one job, the time can be saved.

WHAT?

Client:

a) hadoop job -add-input jobId inputFormat ...
Add the input to jobid

b) hadoop job -add-input done
Tell the JobTracker, the input has been prepared over.

c) hadoop job -add-input status jobid
Show how many input the jobid has.



HOWTO?

Mainly, I think we should do three things:

1. JobClinet: here JobClient should support add input to a job, indeed, JobClient generate the split, and submit to JobTracker.

2. JobTracker: JobTracker support addInput, and add the new tasks to the original mapTasks. Because the uploaded data will be 
processed quickly, so it also should update the scheduler to support pending a map task till Client tells the Job input done.

3. Reducer: the reducer should also update the mapNums, so it will shuffle right.

This is the rough idea, and I will update it .",,acmurthy,ashutoshc,baggioss,cdouglas,cutting,decster,fangy,forest520,gates,hammer,hanfoo_001,jly,jrideout,kimballa,liangly,lianhuiwang,luoli,omalley,philip,roelofs,sharadag,shixing,tlipcon,tomwhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/10 03:33;shixing;ASF.LICENSE.NOT.GRANTED--dynamic_input-v1.patch;https://issues.apache.org/jira/secure/attachment/12441910/ASF.LICENSE.NOT.GRANTED--dynamic_input-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2010-02-09 19:03:33.822,,,false,,,,,,,,,,,,,,,,,,149510,,,,,Sat Sep 11 12:58:51 UTC 2010,,,,,,,"0|i0e9av:",81269,,,,,,,,,,,,,,,,,,,,,"09/Feb/10 19:03;omalley;+1

It helps a more interesting use case where you have a pipeline of mapreduce jobs and don't want the 2nd set of maps to wait until the last reduce finishes. It would be great in job control could use this as an optimization.

You need to have a method where the application declares that all of the input has been added. To avoid having reduces holding slots that they can't use, I'd suggest that no reduces should be launched until the input is complete.

A timeout is also required so that if a user disappears the job is killed after N minutes of no new input and not having the input complete.","09/Feb/10 19:07;acmurthy;+1

I'm sure Pig/Hive would be substantial beneficiaries... their job pipelines would benefit a lot.","09/Feb/10 19:10;philip;One tricky bit is going to be figuring out how InputFormat.getSplits() is going to be updated when you add an input.  I think the feature is a great idea, but I don't necessarily see how to do it without making InputFormat smarter.","09/Feb/10 19:29;omalley;One approach might be to have a subclass of InputFormat, such as:

{code}
public abstract class IncrementalInputFormat extends InputFormat {
  InputSplit[] getNewInputSplits(JobContext context) throws IOException;
}
{code}

and such input formats return any ""new"" splits that they have found since the last time the method was called.","10/Feb/10 05:18;shixing;*1. TimeOut*
    The timeout is what I have forgotten. I think it can be set unlimited? or the reduce task is set unlimited.

*2. InputFormat*
    Now, I suppose that the inputs' input format added are  the same as that the running job's input format.

    Although, we can add different inputformat data for this job, but I didn't think this is what our purpose to add input. 
    If it  supports different inputformat, then we may be should use different mapper to support different inputformat.

*3. The issuse progress.*

   We achieve a demo to support the dynamic add input. And we didn't use the InputFormat to support the dynamic add input, we use the  interaction between JobClient and JobTracker(indeed JIP)

*submit a job*
    1) We submit a job that supports dynamic add input (mapred.dynamic.input=true)
    2) the JobTracker generate cleanup tip and setup tip use the maximumNo (Integer.Maximum, and Integer.Maximum - 1)
    3)  JobTracker pending the maxNo map(for maps are sorted by length), and set the 
{code}dynamicAddInputStatus=DYNAMIC_ADD_INPUT_RUNNING{code},
    when the dynamicAddInputStatus is not set DYNAMIC_ADD_INPUT_DONE, the maxNo map will always in pending phase. Here we should consider the reduce's run time with timeout.

*add input for a job*
    1) JobClient : addInput <jobId>, <inputDir>
        1.1) check whether the client can access the job and the job supports dynamic add input
        1.2) getSplits by conf.getInputFormat().getSplits(conf, conf.getNumMapTasks());  and write to the job's submitJobDir
        1.3) call the JobTracker  add input for the job
    2) JobTracker(indeed JIP)
        2.1) get the JIP by the jobid
        2.2) add maps for added split by new TIP
        2.3）schedule the new maps to run, without the maxNo map
    3) ReduceTask, it update the numMaps, when the shuffle will end when the shuffled mapOutput >= numMaps. We update the numMaps through getMapCompletionEvents.

*add input done*
    1) JobClient : inputDone <jobId>
    2) JobTracker update dynamicAddInputStatus
    {code}dynamicAddInputStatus=DYNAMIC_ADD_INPUT_DONE{code}
    then the maxNo map can be scheduled

*HowToUse it ?*

Client should judge whether it should add input to run or add input done, mainly by judge the new input size or files' num.

_do you have some suggestion?_","10/Feb/10 09:17;shixing;
checked this:

*_1. TimeOut_*
 _The timeout is what I have forgotten. I think it can be set unlimited? or the reduce task is set unlimited._


Indeed, there is still a map pending , so the reducer is always in shuffle phase. 

I have tested that In shuffle phase, the timeout doesn't have effect(why?), so we can ignore the time out.","11/Feb/10 18:11;kimballa;Owen,

The {{getNewInputSplits}} method proposed above requires the InputFormat to maintain state containing the previously-enumerated InputSplits. The proposed command-line tools suggest independent user-side processes performing the addition of files to the job, making this challenging. Given that splits are calculated on the client, but the ""true"" list of input splits is held by the JobTracker (or is/could the splits file be written to HDFS?), calculating just the delta might be challenging.

I think it might be more reasonable if one of the following things were true:
* The client code just calls {{getInputSplits()}} again. The same algorithm is run as in ""initial"" job submission, but the output list may be longer than the previous list returned by this method. The InputFormat is responsible for ensuring that it doesn't return any fewer splits than it did before (i.e., don't drop inputs)
* For that matter, if the input queue for a job is dynamic, I don't see why this same mechanism couldn't be used to drop splits that are, for whatever reason, irrelevant.
* {{getNewInputSplits()}} should have the signature: {{InputSplit [] getNewInputSplits(JobContext job, List<InputSplit> existingSplits) throws IOException, InterruptedException}}.

The latter case would present to the user a list of the existing inputs read from the existing 'splits' file for the job. That way state-tracking is unnecessary; you can just use (e.g.) a PathFilter to disregard things already in {{existingSplits}}.

A final proposition is that users must manually specify new paths (or other arbitrary arguments like database table names, URLs, etc) to include, in addition to the InputFormat. In which case, it might look more sane to have:
* {{getNewInputSplits()}} should have the signature: {{InputSplit [] getNewInputSplits(JobContext job, String... newSplitHints) throws IOException, InterruptedException}}.

The {{newSplitHints}} is effectively a user-specified argv; it can be decoded as a list of Paths, database tables, etc., and used appropriately by the InputFormat to generate new splits.

Other question: What are the semantics of a doubly-specified split? (Especially curious about the inexact match case, where the same file in HDFS is enumerated twice but the splits are at different offsets) Can/should the same file be processed twice in a job?

Finally: Why does a user-disconnect timeout kill the job? That's different than the usual case in MapReduce, where a user disconnect is not noticed by the server-side processes at all. I would think that after a user-disconnect timeout, that declares that all the input is added, and that the reduce phase can begin -- not that it should kill things. ","16/Apr/10 03:33;shixing;We can dynamic add input to a job by use cmd:
{noformat} 
hadoop job -D mapred.input.format.class=""YourInputFormatClass"" -input-add <jobid> <inputdir>
{noformat} 

and tell the master(jobtracker) that the input has been added done by:
{noformat} 
hadoop job -input-done <jobid>
{noformat} 
","10/Sep/10 03:22;roelofs;Is the patch here intended for review, or is it a work in progress that has stalled out?  (If the former, I guess it would be more obvious if the patch were officially ""submitted"" via the link at upper left.)","10/Sep/10 03:33;shixing;Yes it is the former, but I think the commiters should review it first? Or after I submit it ?","10/Sep/10 20:07;roelofs;I'm pretty sure ""Submit Patch"" is the preferred way to signal that a patch is ready for review; that will also trigger Hudson to perform automatic checks (or should, anyway--Hudson's been pretty flaky lately).

Before you do that, however, you should review the official Hadoop code conventions, which are the same as the Sun/Oracle ones (http://www.oracle.com/technetwork/java/codeconv-138413.html) except with half the indentation (4 -> 2, 8 -> 4, etc.).  I just skimmed over your patch and noticed a strange mix of 2-space, 3-space, 4-space, and 8-space indentation...that makes the code very hard to read.","11/Sep/10 12:58;luoli;Of course  we prefer that this patch been reviewed, actually this feature has been proved very helpful for us. Similar problems  people may encounter as us can be solved using this dynamic input job feature. ShiXing, maybe you could submit a new patch for this and make it's code conventions good. What do you think?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResourceAwareLoadManager to dynamically decide new tasks based on current CPU/memory load on TaskTracker(s),MAPREDUCE-961,12435061,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,schen,dhruba,dhruba,08/Sep/09 05:22,13/Jan/11 02:29,12/Jan/21 09:52,,0.22.0,,,,,,,,contrib/fair-share,,,,,,3,,,,,"Design and develop a ResouceAwareLoadManager for the FairShare scheduler that dynamically decides how many maps/reduces to run on a particular machine based on the CPU/Memory/diskIO/network usage in that machine.  The amount of resources currently used on each task tracker is being fed into the ResourceAwareLoadManager in real-time via an entity that is external to Hadoop.

",,aaa,aah,acmurthy,cdouglas,ddas,dhruba,ehf,eli,farseeing,hammer,he yongqiang,hong.tang,jdhok,jorda,matei,rschmidt,schen,tlipcon,vinodkv,zhong,zshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/09 18:53;schen;HIVE-961.patch;https://issues.apache.org/jira/secure/attachment/12421891/HIVE-961.patch","27/Oct/09 21:00;schen;MAPREDUCE-961-v2.patch;https://issues.apache.org/jira/secure/attachment/12423355/MAPREDUCE-961-v2.patch","09/Nov/09 20:36;schen;MAPREDUCE-961-v3.patch;https://issues.apache.org/jira/secure/attachment/12424390/MAPREDUCE-961-v3.patch","13/Nov/09 20:04;schen;MAPREDUCE-961-v4.patch;https://issues.apache.org/jira/secure/attachment/12424888/MAPREDUCE-961-v4.patch","23/Nov/09 19:44;schen;ResourceScheduling.pdf;https://issues.apache.org/jira/secure/attachment/12425873/ResourceScheduling.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,2009-09-08 06:08:46.985,,,false,,,,,,,,,,,,,,,,,,72456,,,,,Mon Nov 23 19:44:36 UTC 2009,,,,,,,"0|i0jfcn:",111417,,,,,,,,,,,fb,,,,,,,,,,"08/Sep/09 06:08;cdouglas;What does the ""fb"" tag denote?","08/Sep/09 06:14;dhruba;Just a unique string to help me query JIRAS I am very interested in.","08/Sep/09 06:22;cdouglas;Let's not play this game.","09/Sep/09 02:50;cdouglas;Dhruba and Nigel request that I restore the tag.","09/Sep/09 09:38;jdhok;I have two questions:
 1. Are you planning to use an existing monitoring system or create your own?
and
 2. Will this make mapred.tasktracker.{map|reduce}.tasks.maximum obsolete? What I mean to ask is since task assignment will be based on the state of the resources, will these settings still be necessary?","09/Sep/09 15:59;dhruba;> 1. Are you planning to use an existing monitoring system or create your own?

Most exiting monitoring systems cannot be used as it is because they do not report statistics based on hadoop jobs. The system has to report CPU/memory etc per hadoop job.

> 2. Will this make mapred.tasktracker.{map|reduce}.tasks.maximum obsolete? 
This is very likely to happen. I imagine that the scheduler can continue to schedule tasks (even though it exceeds may mapred.tasktracker.{map|reduce}.tasks.maximum because it can find that there is plenty of CPU left on a particular slave machine).","12/Oct/09 19:09;schen;1. This patch implements class org.apache.hadoop.mapred.MemBasedLoadManager which looks at the memory usage of the TaskTracker and the Job of the task to determine whether to load this task.
2. This patch also contains a resource utilization monitoring daemon which run on each of the TaskTrackers and a information collecting daemon called Collector which collects and aggregates the resource utilization information submitted from each monitoring daemons. 

Note that this is somewhat similar to org.apache.hadoop.mapred.TaskMemoryManagerThread but different. Because we are collecting information based both on TaskTracker and Jobs. TaskMemoryManagerThread does not know the memory usage based on Hadoop job. Also, we are reporting the information back to a master called Collector and resolve the memory issue in the jobtracker level where TaskMemoryManagerThread resolves this issue in the tasktracker level.","14/Oct/09 06:30;vinodkv;Hastily looked at the patch. Some comments overall.
 - We *already have a complete reporting framework in the form of HeartBeats* and don't think we need new setup of daemons yet.
   -- _InterTrackerProtocol_ can substitute _CollectorProtocol_
    -- Reporting can be done via _TaskTrackerStatus.ResourceStatus_ at the TaskTracker via HeartBeat
    -- Aggregate collection can be via the same _TaskTrackerStatus.ResourceStatus_ at the JobTracker via HeartBeat
    -- Getters for the utilization stats can be abstracted so schedulers can use them directly from the JT.

 - *The collecting framework of this patch can still be retained* - particularly aggregating stats over all the TaskTrackers and per job can be retained on the JobTracker side via a _Collector_ entity embedded in JobTracker.

 - *The TaskTracker knows well about jobs/tasks/sub-processes.* The current job-based aggregation of stats on the TaskTracker is weak because it identifies tasks corresponding to a job by grepping for job-ids in 'ps' . This is very error-prone. This complication arises from the fact that we are decoupling guaging from TaskTrackers into separate daemons. This can be made concrete by doing it in TaskTracker address space itself similar to what _TaskMemoryManager_ does. TaskTracker already maintains the state we need and knows which tasks belong to this job, and which processes belong to a task. One more reason strengthening the argument against a new framework.

 - *The MemBasedLoadManager will still work* but by obtaining information via TaskTrackerStatus and the _Collector_ embedded in JobTracker.

I am just trying to see if we can leverage already existing (and well tested) code.

The good news is that we can retrofit the current patch to do most of the above hardly losing any code. Particularly most of the guaging utilities are reusable without any changes. Thoughts?","14/Oct/09 06:52;dhruba;> The TaskTracker knows well about jobs/tasks/sub-processes

I like the idea of using ProcfsBasedProcessTree for finding total memory used by the subtree. Scott: is this possible to do?

I like the idea of integrating the Collector with the JobTracker in future. Let's see if we can configure it such that the Collector can run inside the JobTracker or outside it depending on the configuration specified by the adminstrator. I would seriously like to keep the option open where I have one Collector for multiple JobTrackers.. helps when I have too many map-reduce clusters floating around. do you think this is ok with you?

","14/Oct/09 20:45;schen;@Vinod:

Thank you for the suggestions. Combining the resource monitoring daemon in TaskTracker and the Collector in Jobtracker is a really good idea. 
I just repeat your points to see if I get them:
1. A lot of codes/logic can be reused such as the HeartBeats mechanism.
2. Information can be more cohesive (TaskTrackerStatus.ResourceStatus holds all the utilization information)
3. Monitoring daemon can access information of the TaskTracker (taskid, jobid...)
4. Collector can access information of the JobTracker (jobid, user, #map tasks, #reduce tasks...)

The reason why we built them as separate daemons is mainly because we want this to run on multiple map-reduce clusters as Dhruba mentioned.
Also, at this stage, it is easy to test these daemons without the dependency on JT or TT. We can easily change/restart these daemons without affecting the map-reduce cluster.

I will definitely study how to put these daemons inside TT and JT. I think one possibility is that we build them inside TT and JT but still provide the RPC interface in Collector.
If we need information on multiple clusters, we can go to the corresponding Collectors and get them via RPC.

@Dhruba:

Thanks. Reused the code in ProcfsBasedProcessTree is a good idea. But this class does not provide the CPU usage information.
I will see how to reuse this class to get both information.
","15/Oct/09 11:32;vinodkv;I think you got my point.

bq. The reason why we built them as separate daemons is mainly because we want this to run on multiple map-reduce clusters as Dhruba mentioned.
bq. will definitely study how to put these daemons inside TT and JT. I think one possibility is that we build them inside TT and JT but still provide the RPC interface in Collector.
bq. If we need information on multiple clusters, we can go to the corresponding Collectors and get them via RPC.

That sounds a slightly different use-case to me. Metrics api can be used for this.

Just curious: how do you intend to use it? Currently scheduler is very tightly coupled with a single cluster/JobTracker. Information exposed by multiple clusters is currently unusable by any single cluster unless you have some external components. If indeed you have external components outside of mapred, metrics api seems the correct tool. Thoughts?

bq. Also, at this stage, it is easy to test these daemons without the dependency on JT or TT. We can easily change/restart these daemons without affecting the map-reduce cluster.
Perhaps. But I think eventually we should move inside the framework. In any case, for a clean design, we can still factor them out to well defined classes and so. Once that is done, if ever we want to move them out into separate daemons, it won't be infinitely complex.","16/Oct/09 02:07;schen;>how do you intend to use it?
We are planing to use this on MAPREDUCE-1044. The idea is to use the information to decide whether to move a TT from one cluster to another. I will also survey the possibility of using metrics api. Thanks.

>Perhaps. But I think eventually we should move inside the framework. 
I agree. Moving this inside the framework is a better design. I will follow your suggestion to build this inside the framework. In the meantime, we will keep testing these independent daemons on our clusters.","16/Oct/09 03:12;vinodkv;bq. We are planing to use this on MAPREDUCE-1044. The idea is to use the information to decide whether to move a TT from one cluster to another. I will also survey the possibility of using metrics api. Thanks.
I see. Metrics do make sense there. Please continue if it really works for you.

Thank you both, @Dhruba and @Scott, for being accommodating! Thanks!","27/Oct/09 21:00;schen;After deploying on our cluster, I have fixed several minor things. Here's the new patch.
It seems this works well on our cluster. I will run more real-world tests on it.

I haven't integrated this in JT and TT. I will do that once I have time.

One more thought: we use PS and use grep to find job_id pattern and ppid = tasktracker pid.
This method has a benefit that it dose not depend on TT to find relevant processes.
With this approach, we are able to find some ""orphan"" jobs (TT does not track them but they are running) on our cluster. ","29/Oct/09 20:58;schen;I have submit a patch in MAPREDUCE-1167 to include RSS information in ProcfsBasedProcessTree.
The final goal is to gather CPU and memory information from ProcfsBasedProcessTree and send the information though heartbeat as Vinod suggested.","01/Nov/09 23:53;matei;Hi Scott and Dhruba,

I've looked at the patch a little bit and have a few comments:
# I agree with Dhruba that it would be good to have the option of running multiple Hadoop clusters in parallel. It's also good design to allow the metrics data to be consumed by multiple sources.
# In MemBasedLoadManager.canLaunchTask, you are returning true in some cases and saying that this is ""equivalent to the case of using only CapBasedLoadManager"". How is that happening? I think you would need to return super.canLaunchTask(...), not true. The Fair Scheduler itself doesn't look at slot counts.
# It might be useful to use the max map slots / max reduce slots settings as upper bounds on the total number of tasks on each node, to limit the number of processes launched. In this case an administrator could configure the slots higher (e.g. 20 map slots and 10 reduce slots), and the node utilization would be used to determine when fewer than this number of tasks should be launched. Otherwise, a job with very low-utilization tasks could cause hundreds of processes to be launched on each node.
# Have you thought in detail about how the MemBasedLoadManager will work when the scheduler tries to launch multiple tasks per heartbeat (part of MAPREDUCE-706)? I think there are two questions:
#* First, you will need to cap the number of tasks launched per heartbeat based on free memory on the node, so that we don't end up launching too many tasks and overcommitting memory. One way to do this might be to count tasks we schedule against the free memory on the node, and conservatively estimate them to each use 2 GB or something (admin-configurable).
#* Second, it's important to launch both reduces and maps if both types of tasks are available. The current multiple-task-per-heartbeat code in MAPREDUCE-706 (and in all the other schedulers as far as I know) will first try to launch map tasks until canLaunchTask(TaskType.MAP) returns false (or until there are no pending map tasks), and will the look for pending reduce tasks. With the current MemBasedLoadManager, this would starve reduces whenever there are pending maps. It would be better to alternate between the two task types if both are available.","02/Nov/09 23:29;schen;Hi Matei,

Thanks for the comment.

1. I agree. It is good to keep this option.
2. You are right about MemBasedLoadManger.canLaunchTask. I will make the necessary change.
3. I agree. There should still be max slots limit. But we can make it higher since now we have the resource monitoring.
4.1 There are some per task memory limit configuration (HADOOP-5881). But those are for virtual memory. We may need something similar but for physical memory.
4.2 I see. So we may need to count the previous continuously launched map tasks in canLauch task and return a false if there are too many map tasks launched in a row. Is this correct? 

You comments are very useful. I will make the necessary changes and upload a new patch soon. Let me know if you have other suggestions. Thanks!","03/Nov/09 01:00;matei;bq. I see. So we may need to count the previous continuously launched map tasks in canLauch task and return a false if there are too many map tasks launched in a row. Is this correct?

Yes, either that or to change the scheduler to alternate between looking for a map and looking for a reduce. Right now the logic in there is organized as:
{code}
for taskType in {MAP, REDUCE}:
  while true:
    if canLaunchTask(..., taskType):
     try to find a job with pending task
     if found a job:
       launch task
     else:
       break
{code}
It should become something like this:
{code}
while true:
  pick taskType to try next (if node has fewer maps than reduces, choose map; else choose reduce)
  if canLaunchTask(..., taskType):
   try to find a job with pending task
   if found a job:
     launch task
   else:
     break
{code}","03/Nov/09 01:01;matei;Actually I have a minor bug in the second code snippet... instead of picking one task type and breaking, you should sort the task types by whoever has the least number of active tasks on the node, and try both task types. For example, if there are fewer maps on the node than reduces, first look for a map, and then for a reduce. Only break if neither task type has tasks to launch.","03/Nov/09 02:49;schen;I see. Your pseudo code is very clear. Thanks.
I think this definitely makes more sense than first looking at all the map tasks and then looking at all the reduce tasks.
","09/Nov/09 20:36;schen;Lots of things changed in this patch. 
1. Make MemoryCalculatorPlugin.java collect available memory on the TT.
2. The available memory information is added in TaskTrackerStatus.ResourceStatus and transmit by heartbeat
3. MemBasedLoadManager launch the task if [Avalable memory - Max memory per task > Reserved Memory]
    where Memory per Task is from existing configuration and reserved memory needs to be configured.

I have also fixed the problem that Matei pointed out to return super.canLaunchTask().

@Matei: About the other suggestion about alternatively launching different types of task, I will open another JIRA and work on that. Because it is not part of MemBasedLoadManager. Thanks.","13/Nov/09 00:29;schen;I have filed the task scheduling order issue in MAPREDUCE-1198. I am working on it.","13/Nov/09 00:49;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12424390/MAPREDUCE-961-v3.patch
  against trunk revision 835237.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 18 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/240/console

This message is automatically generated.","13/Nov/09 19:08;schen;Fixed the failed test in TestTTMemoryReporting. This test will run on linux only. Previously I was running test on mac so I did not catch the this failure.","13/Nov/09 19:35;acmurthy;I'm looking at all the comments here and I cannot find a single, coherent, design for this feature - a major one. Can you please put up a design?

I'd first like to understand/debate the design before I look at the patch.
","13/Nov/09 20:39;schen;Thanks for the comment, Arun. I have changed the patch a lot following the suggestion froms Matei and Vinod. The last patch is total different from the first one. I am sorry about the confusion.

The following is the design
1. We obtain the available memory on the TT using MemoryCalculatorPlugin. Originally this class calculates only total memory only, we add a slight change so that it also computes the available memory.
2. The information is reported with TaskTrackerStatus.ResourceStatus back to JT.
3. In MemBasedLoadManager, we look at the available memory on TT, the maximum memory per task (from jobConf) and a configured reserved memory on TT. If (available memory - task memory > reserved memory), we return true which allows scheduler to lauch the task.

The initial idea also includes using the memory usage of a job collecting in the cluster. Right now we only use the value obtained in jobConf. After MAPREDUCE-220 is done, we can use the task memory estimated by the previous tasks.","13/Nov/09 22:12;schen;I will write an overall design document and post it here soon.","13/Nov/09 23:14;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12424888/MAPREDUCE-961-v4.patch
  against trunk revision 835968.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 9 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/242/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/242/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/242/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/242/console

This message is automatically generated.","16/Nov/09 20:17;schen;@Matei: I have implemented your pseudo code about the task type order and posted in MAPREDUCE-1198. Can you help me review it?","18/Nov/09 00:48;schen;I created a sub-task for this one for collecting the TaskTracker resource in MAPREDUCE-1218.
I will factor out the codes for TaskTracker resource collecting there and leave the scheduling related codes here. ","23/Nov/09 19:44;schen;@Arun: Here is the overall design document. I hope this provides a coherent big picture of the design. Let me know what you think.

Our goal is to collect resource information for tasks and tasktrackers and schedule tasks based on the overall information. But since MAPREDUCE-220 (per task information) is currently blocked by MAPREDUCE-901. We may first finish MAPREDUCE-1218 (per node information) and do some resource based scheduling based only on the per node resource information.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a low-level MapReduce API,MAPREDUCE-1452,12455257,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,,tomwhite,tomwhite,04/Feb/10 01:17,12/Jan/11 20:58,12/Jan/21 09:52,04/Feb/10 01:38,,,,,,0.22.0,,,,,,,,,0,,,,,"Add an API to MapReduce that operates at the raw bytes level. The existing (object-based) MapReduce APIs would be implemented on top of the raw API, and in future it will be easier to add new APIs (like MAPREDUCE-1183) and higher-level abstractions on MapReduce. ",,cdouglas,cutting,ddas,gates,hammer,jghoman,philip,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-1453,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-02-04 01:38:23.78,,,false,,,,,,,,,,,,,,,,,,149522,,,,,Thu Feb 04 01:38:23 UTC 2010,,,,,,,"0|i0jgo7:",111631,,,,,,,,,,,,,,,,,,,,,"04/Feb/10 01:38;acmurthy;Duplicate of MAPREDUCE-326",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow users to do speculative execution of a task manually,MAPREDUCE-1608,12459597,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,schen,schen,schen,19/Mar/10 00:26,17/Nov/10 17:39,12/Jan/21 09:52,,,,,,,,,,,,,,,,1,,,,,"Speculative execution improves the latency of the job. Sometimes the job has few very slow reducers. Spending a little more resource on speculative tasks can improve the latency a lot. It will be nice that the users can manually select one task and force the speculative execution on that task just like we can manually kill/fail task.

The proposal is add link says ""speculate"" in taskdetails.jsp page where we do ""kill/fail"".

Thoughts? ",,acmurthy,cdouglas,dhruba,hong.tang,lianhuiwang,philip,ravidotg,schen,sharadag,tlipcon,zhong,zshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-03-19 00:34:14.46,,,false,,,,,,,,,,,,,,,,,,149636,,,,,Wed Nov 17 17:39:37 UTC 2010,,,,,,,"0|i0e927:",81230,,,,,,,,,,,,,,,,,,,,,"19/Mar/10 00:34;tlipcon;I'm not entirely against a manual trigger, but this suggests that our current tuning knobs are insufficient. What are the cases when users would want to do this manually, and why don't our heuristics do it for them? I'm skeptical that humans can do a better job than the scheduler at deciding when speculation is necessary.","19/Mar/10 01:21;schen;Thanks, Tood. That's a good point. We should also work on making the tuning knobs better.

The reason why I am proposing this is that one of our user ask me if there is speculative execution in a single reducer case.
Our speculation policy is based on comparing a task to other tasks (or some average behavior).
So in this case the speculative execution will not be triggered. But the human knows that this task is slow.

The tuning knobs can be very good but it is hard to make it perfect.
It is good to have some control over the task when we need it (just like Kill/Fail task).","19/Mar/10 01:40;hong.tang;Interesting usage case. Are there other tasks running on the same node as the lone reduce task? If yes, are they slower than their peers and perhaps that would be an indication the node is ill-behaving and all tasks on that node should be speculatively executed?","19/Mar/10 05:38;acmurthy;This is an odd one.

Speculative execution is, in some sense, a *pure overhead*. Allowing users to trigger this without checks and balances has significant consequences... 

As Todd/Hong have mentioned we should consider making our heuristics better. I'm not sure this is a good idea.","19/Mar/10 05:42;tlipcon;I think Hong's idea is really clever, though. Do we have the right data structures in place to do it efficiently, already? Or would we have to add more per-node data similar to the faulty tracker stuff?","19/Mar/10 06:30;amar_kamat;bq. Interesting usage case. Are there other tasks running on the same node as the lone reduce task?
Speculation makes sense when you compare similar tasks as we can easily rule out the code/logic differences. 

The way I understand this is that you are trying to label/weigh the tasktracker based on how the currently running tasks are behaving on the given tracker. How about making a note of how many tasks (of a given type) on a given tracker got speculated and making scheduling decisions based on that. There might be cases where all the reducers get speculated on a given tracker. This should result into not scheduling reduces on such nodes and in future utilize it (i.e all the slots) completely for maps.

For example:
If a tracker T asks for new tasks of type X from job J, then schedule the task X of job J on T if 
(number of tasks speculated of T / number of tasks scheduled on T)  < C (where C is the threshold and default value = 0.8)

This condition says a simple thing, if the job cares about speculation, the there is no point in running a task of type X on tracker T as there is higher chance that it might get speculated. Thoughts?","11/Nov/10 00:43;akramer;As a user, I have found the ability to manually speculate tasks via the website incredibly useful--so useful that I'm starting to worry about RSI given that each speculation takes a click to the task page, a click to the task, a click on speculate, and a click on the confirm dialog box. These are frequently lost-task-tracker failures, and Hadoop currently just sets a timeout on them.

But how am I beating the current system? I'm comparing some tasks' performance to other tasks in the same job:

1) If there is only one task (either map or reduce) always speculate. Maybe turn this off for clusters that have very few slots, but in the case of >1000 slots or so, this is trivial and would basically prevent jobs taking literally twice as long.

2) Collect data on other tasks in the same job. If 99% of mappers went from 0% complete to >0% complete in 5 seconds and it's been 5 minutes while the last 5% of mappers change, speculate them. Ditto reducers. Unbalanced data may cause these problems, 

3) Collect data on delays. If a task doesn't improve its % complete in some timeframe determined by the other tasks for the same job, speculate the ""hung"" task.

...in other words, I agree that there is probably an easy way to model the failed tasks, but only from a modeling perspective. Getting the heuristics and models right and implementing them is probably much much more difficult than implemeting ""hadoop job -speculate-task task_identifier_here.""

But also, and implementing the latter is *necessary* to discover how and when the heuristics themselves are failing...giving users the ability to do this also gives admins the ability to see when users are doing this.","11/Nov/10 01:25;dhruba;Amar, thanks for ur comments. For interactive workloads, this feature has worked awesomely for us.

> As Todd/Hong have mentioned we should consider making our heuristics better. I'm not sure this is a good idea.

I think Arun is completely missing the point here. It is fine to make heuristics better and better, I have nothing against that. But when there is a manual-speculate-now button, it can override the system-specified speculation heuristics for that task. To go a ste further, the system can actually learn from the user-specified-speculation on how to automatically make the heuristics better, i.e. crowd-sourcing to the rescue!","11/Nov/10 18:45;schen;We can also add a cli option like
{code}
hadoop job -speculate-task <tid>
{code}

We have been using this feature for a while.
It is very helpful for our users. Sometimes this little feature can save lots of time. ","11/Nov/10 19:39;schen;We can make this feature configurable.
For example, we put
{code}
""mapreduce.cluster.speculation.controlable""
{code}
in MRConfig.java to allow the administrator to turn on/off this feature.

What do you think? ","11/Nov/10 22:17;hong.tang;bq. But when there is a manual-speculate-now button, it can override the system-specified speculation heuristics for that task. To go a ste further, the system can actually learn from the user-specified-speculation on how to automatically make the heuristics better, i.e. crowd-sourcing to the rescue!

I agree that knowledgeable users have more information about their programs and probably can speculate more accurately. Such info may also be used for the verification (future tuning) of speculative execution algorithms (e.g. through simulation). So here is my +1 on the idea.","16/Nov/10 18:07;acmurthy;How do you prevent devious/malicious users from speculating on all their tasks via a simple shell script?","16/Nov/10 18:35;hong.tang;bq. How do you prevent devious/malicious users from speculating on all their tasks via a simple shell script?
This is an orthogonal issue. An easy solution would be to set an upper limit of how many tasks that one can speculate manually, e.g. 1% of total {map|reduce} tasks.","16/Nov/10 22:04;schen;Hi Arun,
bq. How do you prevent devious/malicious users from speculating on all their tasks via a simple shell script?
Currently if a user wants to do that, the user can achieve this by setting
{code}
 mapreduce.job.speculative.slowtaskthreshold = 0
{code}
This way is even simpler.","17/Nov/10 17:35;acmurthy;Scott, thanks for bringing that to my attention, I wasn't aware of that. That is a highly dangerous knob - I'll open a jira to make it cluster-specific and not user-specific.","17/Nov/10 17:39;acmurthy;I meant 'job-specific', not 'user-specific'.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
herriot automation system test case for verification of bug fix to jobhistory,MAPREDUCE-1971,12470249,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,balajirg,balajirg,27/Jul/10 11:03,27/Sep/10 04:14,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"Run a few jobs and check the job history page .  Job history information should be displayed properly  . 
Analyze a running job . The values shown in the page should be correct . 
Concurrently access jobs in job history page . No exception should be thrown. 

In the developed herriot test case accesses the job tracker tracker directly, the jsp page access does the same.  	",herriot ,balajirg,garymurry,iyappans,vinaythota,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/10 11:04;balajirg;concurrent_exp_y20.patch;https://issues.apache.org/jira/secure/attachment/12450587/concurrent_exp_y20.patch","27/Jul/10 11:21;balajirg;concurrent_exp_y20_1.patch;https://issues.apache.org/jira/secure/attachment/12450589/concurrent_exp_y20_1.patch","02/Aug/10 07:40;balajirg;concurrent_y20_fix.patch;https://issues.apache.org/jira/secure/attachment/12451019/concurrent_y20_fix.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2010-07-27 11:14:50.882,,,false,,,,,,,,,,,,,,,,,,149898,,,,,Mon Aug 02 07:40:19 UTC 2010,,,,,,,"0|i0e89z:",81103,,,,,,,,,,,herriot,,,,,,,,,,"27/Jul/10 11:04;balajirg;First patch for y20. ","27/Jul/10 11:14;iyappans;Remove the extra comment ""//{"" in runSleepJob.
Code looks good otherwise.
","27/Jul/10 11:21;balajirg;Implemented iyappans comment. ","02/Aug/10 07:40;balajirg;This fixes the issue with running the test case in secured env. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use Jsch instead of Shell.java ,MAPREDUCE-1882,12467474,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,iyappans,balajirg,balajirg,21/Jun/10 11:34,27/Sep/10 04:14,12/Jan/21 09:52,,,,,,,,,,test,,,,,,0,,,,,"In herriot ( hadoop system test case dev) we often find that we are resorted to habit of ssh to remote node execute a shell command, and come out. It is wise to use Jsch instead of doing this through Shell.java ( hadoop code), since Jsch provides nice Java abstraction, the JIRA will only close after we import Jsch input hadoop build system and also fix all the existing test cases. ",herriot framework ,balajirg,cos,eli,iyappans,vinaythota,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/10 09:19;iyappans;1882-ydist-security-patch.txt;https://issues.apache.org/jira/secure/attachment/12450393/1882-ydist-security-patch.txt","28/Jul/10 11:04;iyappans;MAPREDUCE-1882.patch;https://issues.apache.org/jira/secure/attachment/12450686/MAPREDUCE-1882.patch","23/Jun/10 11:15;iyappans;RemoteExecution.patch;https://issues.apache.org/jira/secure/attachment/12447822/RemoteExecution.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2010-06-23 11:15:40.499,,,false,,,,,,,,,,,,,,,,,,149833,,,,,Fri Aug 27 04:09:26 UTC 2010,,,,,,,"0|i0e8jj:",81146,,,,,,,,,,,herriot,,,,,,,,,,"23/Jun/10 11:15;iyappans;Attachinga  patch for remote execute using Jsch.","25/Jun/10 08:11;balajirg;Looks good, one other helper that we can add is scp, since Jsch supports that also. ","29/Jun/10 20:47;cos;- The functionality seems to be common for MR and HDFS. Shall it be moved to Common instead?
- This 
{noformat}
+    jsch.setKnownHosts(""/homes/"" + user + ""/.ssh/known_hosts"");
{noformat}
is questionable. 
- This one won't work if RSA identities are in use:
{noformat}
+    jsch.addIdentity(""/homes/"" + user + ""/.ssh/id_dsa"");
{noformat}
- Where Jsch will be coming from? Ivy dependency resolution needs to be added as well. ","24/Jul/10 09:19;iyappans;one other helper that we can add is scp, since Jsch supports that also. 
- [Iyappan]  Yes, we can do it in further enchancements. Not in this Jira, since it is outside the scope.

The functionality seems to be common for MR and HDFS. Shall it be moved to Common instead? 

- [Iyappan] Ok.Since this is going to be used for testing only, I will put it under src/test/system/java/shared/org/apache/hadoop/common. 


This 
    jsch.setKnownHosts(""/homes/"" + user + ""/.ssh/known_hosts"");

is questionable. 

- [Iyappan] It is not adding anything new. It just picks up the existing ones. Known_hosts will have the already known machine names and their public keys. I dont see any issue here.

This one won't work if RSA identities are in use: 
+    jsch.addIdentity(""/homes/"" + user + ""/.ssh/id_dsa"");

- [Iyappan] I have added a wrapper function addressing that.Now user can specify dsa or rsa.

Where Jsch will be coming from? Ivy dependency resolution needs to be added as well
- [Iyappan] I have created  a seperate Jira for it HADOOP-6879.

","27/Jul/10 06:30;cos;Please don't forget to link blocker (or dependent) JIRAs","27/Jul/10 06:49;balajirg;+1","27/Jul/10 09:21;iyappans;Jsch in ivy","28/Jul/10 03:28;cos;And for the trunk?","28/Jul/10 11:04;iyappans;patch for trunk","30/Jul/10 01:36;cos;Patch looks good... However, this is common functionality and I assume HDFS test might need it as well? Shall it be moved to Common instead?","27/Aug/10 03:40;cos;Because this is the common functionality I'm moving the patch over to HADOOP-6879. This JIRA will closed and all its dependencies will be re-linked.","27/Aug/10 04:09;cos;I think once HADOOP-6879 is fixed this JIRA will only need to add Jsch artifact resolution to create working classpath for common remote execution functionality.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Per-node task limits in the fair scheduler,MAPREDUCE-704,12429500,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,matei,matei,03/Jul/09 18:04,08/Sep/10 20:27,12/Jan/21 09:52,,,,,,,,,,contrib/fair-share,,,,,,6,,,,,Some users would like to be able to limit the number of tasks from a job that they run on each node to better control cluster load. This JIRA will add this feature to the fair scheduler. MAPREDUCE-698 adds per-pool limits on number of running tasks as well.,,aaa,atm,davelatham,eli,hammer,jwarren,matei,octo47,peng.zhang,tomwhite,viper799,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-5170,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,148990,,,,,2009-07-03 18:04:18.0,,,,,,,"0|i0jean:",111246,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Per-pool task limits for the fair scheduler,MAPREDUCE-698,12429436,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,kevinpet,matei,matei,02/Jul/09 23:00,08/Sep/10 20:27,12/Jan/21 09:52,18/Dec/09 03:30,,,,,,0.21.0,,,contrib/fair-share,,,,,,1,,,,,The fair scheduler could use a way to cap the share of a given pool similar to MAPREDUCE-532.,,aaa,aw,dhruba,eli,hammer,kevinpet,matei,viper799,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-5170,,,,,,,,,,,,,,,,,,,,,"19/Oct/09 23:52;kevinpet;MAPREDUCE-698-prelim.patch;https://issues.apache.org/jira/secure/attachment/12422630/MAPREDUCE-698-prelim.patch","30/Nov/09 22:25;kevinpet;mapreduce-698-trunk-3.patch;https://issues.apache.org/jira/secure/attachment/12426456/mapreduce-698-trunk-3.patch","14/Dec/09 09:24;kevinpet;mapreduce-698-trunk-4.patch;https://issues.apache.org/jira/secure/attachment/12427904/mapreduce-698-trunk-4.patch","17/Dec/09 18:12;matei;mapreduce-698-trunk-5.patch;https://issues.apache.org/jira/secure/attachment/12428317/mapreduce-698-trunk-5.patch","24/Nov/09 10:30;kevinpet;mapreduce-698-trunk.patch;https://issues.apache.org/jira/secure/attachment/12425949/mapreduce-698-trunk.patch","23/Nov/09 09:46;kevinpet;mapreduce-698-trunk.patch;https://issues.apache.org/jira/secure/attachment/12425823/mapreduce-698-trunk.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,2009-10-19 23:52:21.684,,,false,,,,,,,,,,,,,,,,,,37340,Reviewed,,,,Tue Jan 05 15:58:42 UTC 2010,,,,,,,"0|i02t1z:",14308,Per-pool map and reduce caps for Fair Scheduler.,,,,,,,,,,,,,,,,,,,,"19/Oct/09 23:52;kevinpet;Attached patch works for me, but I think I need to make some changes for interaction with the multiple-assign before it's ready to commit. Patch is relative to 0.20 branch.

It allows two new fields in the pools definition, ""maxMaps"" and ""maxReduces"".

My need for this is interacting with outside services (in this case, an HBase cluster that's significantly smaller than my Hadoop cluster).","22/Oct/09 04:10;matei;Hi Kevin,

The patch looks reasonable for 0.20, though I haven't had a chance to look at it in detail yet. For trunk, MAPREDUCE-706 should have made your life easier; you should be able to just update the demand() method in PoolSchedulable to return the min of the pool's maxMaps (or maxReduces) and the sum of its jobs' demands. Let me know if you have trouble for that. In trunk there's also a fairly detailed design doc for the fair scheduler that should be helpful.

Matei","17/Nov/09 21:54;dhruba;Hi Kevin, will it be possible for you to upload a patch for trunk? We are definitely interested in trying it out. Thanks. ","17/Nov/09 21:54;dhruba;Hi Matie, if you could review the patch for 0.20, that will be great too.","23/Nov/09 09:46;kevinpet;Patch relative to head. Overrides demand() and assign(), unit test included.

I'm able to run the test I added locally, but I'm having trouble with some other test when I run the suite. I haven't tested this on a real cluster, just in unit tests.

Still need to add the display in scheduler servlet. It's sitting on my machine at work.","23/Nov/09 19:50;dhruba;Amazing Kevin, thanks a lot.  I will wait for your next version of the patch that includes the display in the UI.","24/Nov/09 10:30;kevinpet;Update patch to display caps in the scheduler UI.","24/Nov/09 11:45;dhruba;it would be nice if somebody can review this patch.","24/Nov/09 19:23;matei;The patch mostly looks good to me, but I have a few comments:
* There seem to be some extra newlines added in FairScheduler.java. You might want to just svn revert it.
* What is the purpose of the changes to build-contrib.xml? Are they just something you copied in from another build.xml file?
* In the unit test, it would be good to submit the jobs first (with an advanceTime between them) before doing any checkAssignment's, so that both jobs are initially available. Note that the scheduler will probably alternate between assigning tasks from the two jobs in this case, but it should still not let job 1 go over its max share.
* In the web UI, can you call the new ""Max"" column ""Max Share"" instead, so it is more consistent with the other column names?
* You can simplify Pool.numRunningTasks(TaskType type) to just return getSchedulable(type).getRunningTasks(). It might also be good to rename the method to getRunningTasks instead of nunRunningTasks so that it's more consistent with the rest of the code.
* Instead of making PoolSchedulable.getDemand() return maxTasks if demand > maxTasks, it would be better to change PoolSchedulable.updateDemand() to cap the demand at the end of the method, so that getDemand() just returns demand. Otherwise, it will be a little confusing to have a variable called demand in PoolSchedulable whose value is not the same as that returned by getDemand().","24/Nov/09 19:28;matei;Oops, I forgot to add one other important comment: The new config parameters should be documented in the fair scheduler's Forrest documentation in src/docs/src/documentation/content/xdocs/fair_scheduler.xml (in the Allocation File section). Also, the documentation should say what happens if the maxTasks of a pool is set lower than its minTasks: in this case, the maxTasks takes precedence. It might also be good to print a warning in PoolManager when loading a config file if we see a pool with maxTasks < minTasks. You should do this after you finish reading the a <pool> element.

One other very minor thing - there seem to be some tabs in the patch, replace them with spaces.

Thanks for taking the time to port this to trunk!","30/Nov/09 22:25;kevinpet;changes from previous patch:
- extra newline in fairscheduler.java removed
- removed ""single test"" changes from build-contrib.xml (they didn't accomplish what I wanted -- to run just a single test method)
- Regarding checkAssignment, I made the change you suggested, but I'm not sure I'm testing things in the best way. The only thing I'm concerned with is that it ends up scheduling the right number from each pool, the only way I was able to get it to actually assign the jobs was to use checkAssignment.
- in the UI, labels are ""Max Share""
- Removed Pool.numRunningTasks since it was only used from within PoolSchedulable, where this data is already available.
- Moved cap from getDemand() to updateDemand().
- Documentation updated
- Removed tabs.","01/Dec/09 12:45;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426456/mapreduce-698-trunk-3.patch
  against trunk revision 885530.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/156/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/156/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/156/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/156/console

This message is automatically generated.","02/Dec/09 05:59;matei;The patch looks good to me. The only other thing I'd ask for is some documentation of what happens if a pool's max share is set to lower than its min share. The best way to do this might be to log a warning using LOG.warn before the ""commit"" part of PoolManager.reloadAllocs and say that when a pool's max share is lower than its min share, the max share takes precedence. You could also display this warning on the web UI when pools with max share < min share exist, because admins are likely to be looking at the UI after they modify their config file.

The unit test failures seem to be unrelated to the patch - in particular, the one in the fair scheduler is due to MAPREDUCE-1245.","14/Dec/09 09:24;kevinpet;Added some warnings if it's configured with max < min, fixed layout in servlet to match current version.

Also, I believe the tests have been fixed in trunk, so this should pass release audit now.","14/Dec/09 09:25;kevinpet;cancel and resubmit to pick up newer patch.","14/Dec/09 09:25;kevinpet;Should be ready to go.","14/Dec/09 12:41;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427904/mapreduce-698-trunk-4.patch
  against trunk revision 889786.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/191/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/191/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/191/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/191/console

This message is automatically generated.","15/Dec/09 17:11;matei;Resubmitting patch to Hudson, since the last test run seemed to be very broken.

The changes look good as far as I can tell!","16/Dec/09 00:21;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427904/mapreduce-698-trunk-4.patch
  against trunk revision 890983.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/196/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/196/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/196/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/196/console

This message is automatically generated.","17/Dec/09 17:20;matei;I've looked at the patch more carefully, and it all looks good, except there seems to be a loop doing nothing in PoolManager:

{noformat}
+    for(String pool : poolNamesInAllocFile) {
+    }
{noformat}

I can remove this myself and commit the patch, unless there was a reason you had it there (and forgot to put in some code).","17/Dec/09 17:29;kevinpet;That was for checking if the allocations were consistent (min < max), I moved this into the loop where they read but missed this bit it looks like.","17/Dec/09 18:12;matei;Here's the patch with the for loop removed. I'm going to run it through Hudson for good measure, but it seems to be working fine from my point of view, and the test failures in the previous run were unrelated. I'll commit it unless Hudson complains.","18/Dec/09 03:04;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428317/mapreduce-698-trunk-5.patch
  against trunk revision 891920.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/214/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/214/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/214/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/214/console

This message is automatically generated.","18/Dec/09 03:30;matei;The test failures are not caused by this patch, so I've committed it. Thanks, Kevin!","05/Jan/10 15:58;hudson;Integrated in Hadoop-Mapreduce-trunk #196 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/196/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Start and Stop scripts for the RaidNode,MAPREDUCE-1673,12461132,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,rschmidt,rschmidt,rschmidt,05/Apr/10 12:29,24/Aug/10 21:21,12/Jan/21 09:52,21/Apr/10 00:20,0.22.0,,,,,0.21.0,,,contrib/raid,,,,,,0,,,,,We should have scripts that start and stop the RaidNode automatically. Something like start-raidnode.sh and stop-raidnode.sh,,dhruba,lianhuiwang,rschmidt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/10 10:40;rschmidt;ASF.LICENSE.NOT.GRANTED--MAPREDUCE-1673.2.patch;https://issues.apache.org/jira/secure/attachment/12441815/ASF.LICENSE.NOT.GRANTED--MAPREDUCE-1673.2.patch","06/Apr/10 02:38;rschmidt;MAPREDUCE-1673.1.patch;https://issues.apache.org/jira/secure/attachment/12440831/MAPREDUCE-1673.1.patch","05/Apr/10 12:32;rschmidt;MAPREDUCE-1673.patch;https://issues.apache.org/jira/secure/attachment/12440746/MAPREDUCE-1673.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2010-04-06 00:23:56.229,,,false,,,,,,,,,,,,,,,,,,149680,,,,,Wed Apr 21 00:20:43 UTC 2010,,,,,,,"0|i0jh87:",111721,,,,,,,,,,,,,,,,,,,,,"06/Apr/10 00:23;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12440746/MAPREDUCE-1673.patch
  against trunk revision 930423.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/354/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/354/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/354/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/354/console

This message is automatically generated.","06/Apr/10 02:38;rschmidt;Comments were not correct","06/Apr/10 10:37;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12440831/MAPREDUCE-1673.1.patch
  against trunk revision 930423.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/95/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/95/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/95/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/95/console

This message is automatically generated.","15/Apr/10 10:40;rschmidt;New patch!","15/Apr/10 13:18;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12441815/MAPREDUCE-1673.2.patch
  against trunk revision 933441.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/112/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/112/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/112/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/112/console

This message is automatically generated.","15/Apr/10 17:52;dhruba;+1. Looks good. These are startup/stop scripts and do not need any unit tests.","21/Apr/10 00:20;dhruba;I just committed this. Thanks Rodrigo.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
In JobTokenIdentifier change method getUsername to getUser which returns UGI,MAPREDUCE-1464,12455517,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,jnp,jnp,jnp,06/Feb/10 01:46,24/Aug/10 21:20,12/Jan/21 09:52,08/Feb/10 05:08,,,,,,0.21.0,,,,,,,,,0,,,,,The TokenIdentifier interface has changed in HADOOP-6510. This jira tracks corresponding change in MR. The only change is that in JobTokenIdentifier getUsername method will be changed to getUser that will return ugi.,,kzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/10 02:46;jnp;MR-1464.1.patch;https://issues.apache.org/jira/secure/attachment/12435053/MR-1464.1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2010-02-06 03:00:42.222,,,false,,,,,,,,,,,,,,,,,,149530,Reviewed,,,,Mon Feb 08 18:40:13 UTC 2010,,,,,,,"0|i0jgof:",111632,,,,,,,,,,,,,,,,,,,,,"06/Feb/10 03:00;kzhang;+1, pending hudson.","06/Feb/10 03:03;kzhang;Btw, it would be good if you add a JavaDoc saying this method may return null.","06/Feb/10 04:38;kzhang;I looked at the patch again and found that there is no null checking at the caller (per your HADOOP-6510.21.patch). I suggestion you return an empty ugi instead of null for getUser() method (do the same in HDFS-935) to reduce the chance of NullPointerException at runtime.","06/Feb/10 06:06;jnp;UserGroupInformation.createRemoteUser throws IllegalArgumentException if null or empty user is passed to it. Therefore UGI with empty user cannot be constructed. 

I need to revisit HADOOP-6510 patch for the possibility of null returned by getUser()","07/Feb/10 02:41;jnp;In HADOOP-6510 getUser() is used. It cannot return null if the user is successfully authorized. Any calls to getUser before the authorization is successful should check for null. Updated the patch in HADOOP-6510 to handle null returned by getUser before authorization is successful.","07/Feb/10 02:53;ddas;+1","07/Feb/10 02:57;jnp;tests, javac, javadoc and findbugs for this patch were run manually.","08/Feb/10 05:08;ddas;I just committed this. Thanks, Jitendra!","08/Feb/10 05:30;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #229 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/229/])
    . Makes a compatible change in JobTokenIdentifier to account for HADOOP-6510. Contributed by Jitendra Nath Pandey.
","08/Feb/10 18:40;kzhang;> UserGroupInformation.createRemoteUser throws IllegalArgumentException if null or empty user is passed to it.

I think createRemoteUser() should be changed to allow an empty ugi to be created. This will save the null checking at the caller and allow method chaining. Any code that uses ugi (like the authorize() method) should be able to deal with an empty ugi.

> In HADOOP-6510 getUser() is used. It cannot return null if the user is successfully authorized. Any calls to getUser before the authorization is successful should check for null.

I'm not aware that the SASL layer requires username (i.e., jobId) being non-empty. We're making a runtime assumption that jobId's won't be empty String or they won't authenticate successfully. In my view, the less such assumptions we make, the more robust the code is.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make changes to MapReduce for the new UserGroupInformation APIs (HADOOP-6299),MAPREDUCE-1385,12445847,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,ddas,ddas,ddas,18/Jan/10 18:50,24/Aug/10 21:20,12/Jan/21 09:52,27/Jan/10 08:35,,,,,,0.21.0,,,,,,,,,0,,,,,This is about moving the MapReduce code to use the new UserGroupInformation API as described in HADOOP-6299.,,aw,cdouglas,cutting,hammer,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/10 23:51;ddas;mr-6299.3.patch;https://issues.apache.org/jira/secure/attachment/12430816/mr-6299.3.patch","26/Jan/10 19:15;ddas;mr-6299.7.patch;https://issues.apache.org/jira/secure/attachment/12431449/mr-6299.7.patch","26/Jan/10 21:51;ddas;mr-6299.8.patch;https://issues.apache.org/jira/secure/attachment/12431472/mr-6299.8.patch","18/Jan/10 18:51;ddas;mr-6299.patch;https://issues.apache.org/jira/secure/attachment/12430663/mr-6299.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2010-01-25 21:55:37.659,,,false,,,,,,,,,,,,,,,,,,149480,Incompatible change,Reviewed,,,Thu Mar 04 18:24:17 UTC 2010,,,,,,,"0|i0jgif:",111605,,,,,,,,,,,,,,,,,,,,,"18/Jan/10 18:51;ddas;The patch for trunk","19/Jan/10 23:51;ddas;The earlier patch went out of sync. Updated patch.","25/Jan/10 21:55;omalley;Please add a call to UserGroupInformation.setConfiguration at the start of the JobTracker, and TaskTracker so that UGI uses the same configuration object. The Task should also use setConfiguration with the job's configuration.","25/Jan/10 22:35;omalley;Please use the long form of the user name rather than the short form in spots like:

{noformat}
-    JobInfo jobInfo = new JobInfo(jobId, new Text(ugi.getUserName()), 
+    JobInfo jobInfo = new JobInfo(jobId, new Text(ugi.getShortUserName()), 
{noformat}","26/Jan/10 00:25;omalley;The following code from Cluster.java and ClusterWithLinuxTaskController is a no-op:

{noformat}
UserGroupInformation.getCurrentUser().doAs(...)
{noformat}","26/Jan/10 19:15;ddas;Patch that addresses Owen's concerns. Running tests now. Running test-patch proved to be a real pain. Still trying to run it..","26/Jan/10 21:51;ddas;Tests pass with this patch.","27/Jan/10 03:37;ddas;Ok i could run findbugs on this patch and confirmed that it doesn't add any new warnings. I also ran javadoc and javac-warnings ones. No new warnings there too. I am sure I didn't introduce any new release audit warnings since there are no new files in the patch. Same for @author tags. It is almost impossible to run the stand-alone test-patch command, as it exists today, on this patch. ","27/Jan/10 07:46;omalley;+1
","27/Jan/10 08:35;omalley;I just committed this. Thanks, Devaraj!","30/Jan/10 16:26;hudson;Integrated in Hadoop-Mapreduce-trunk #221 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/221/])
    ","02/Feb/10 06:16;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #225 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/225/])
    ","03/Mar/10 08:06;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #255 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/255/])
    MAPREDUCE-1421. LinuxTaskController tests failing on trunk after the commit of . Contributed by Amareshwari Sriramadasu.
","04/Mar/10 18:24;hudson;Integrated in Hadoop-Mapreduce-trunk #248 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/248/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow storage and caching of delegation token.,MAPREDUCE-1383,12445703,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,boryas,boryas,boryas,15/Jan/10 23:41,24/Aug/10 21:20,12/Jan/21 09:52,29/Jan/10 19:56,,,,,,0.21.0,,,,,,,,,0,,,,,Client needs to obtain delegation tokens from all the NameNodes it is going to work with and pass it to the application.,,ddas,jghoman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-1405,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/10 19:39;boryas;MAPREDUCE-1383-1.patch;https://issues.apache.org/jira/secure/attachment/12430785/MAPREDUCE-1383-1.patch","29/Jan/10 01:10;boryas;MAPREDUCE-1383-10.patch;https://issues.apache.org/jira/secure/attachment/12431732/MAPREDUCE-1383-10.patch","29/Jan/10 18:38;boryas;MAPREDUCE-1383-11.patch;https://issues.apache.org/jira/secure/attachment/12431792/MAPREDUCE-1383-11.patch","21/Jan/10 23:23;boryas;MAPREDUCE-1383-2.patch;https://issues.apache.org/jira/secure/attachment/12431078/MAPREDUCE-1383-2.patch","26/Jan/10 01:08;boryas;MAPREDUCE-1383-5.patch;https://issues.apache.org/jira/secure/attachment/12431391/MAPREDUCE-1383-5.patch","27/Jan/10 01:57;boryas;MAPREDUCE-1383-6.patch;https://issues.apache.org/jira/secure/attachment/12431493/MAPREDUCE-1383-6.patch","28/Jan/10 17:51;boryas;MAPREDUCE-1383-9.patch;https://issues.apache.org/jira/secure/attachment/12431687/MAPREDUCE-1383-9.patch","29/Jan/10 07:22;boryas;MAPREDUCE-1383-BP20-3.patch;https://issues.apache.org/jira/secure/attachment/12431752/MAPREDUCE-1383-BP20-3.patch","29/Jan/10 18:56;boryas;MAPREDUCE-1383-BP20-4.patch;https://issues.apache.org/jira/secure/attachment/12431798/MAPREDUCE-1383-BP20-4.patch","01/Feb/10 19:19;boryas;MAPREDUCE-1383-BP20-5.patch;https://issues.apache.org/jira/secure/attachment/12434426/MAPREDUCE-1383-BP20-5.patch","01/Feb/10 19:51;boryas;MAPREDUCE-1383-BP20-6.patch;https://issues.apache.org/jira/secure/attachment/12434428/MAPREDUCE-1383-BP20-6.patch","01/Feb/10 23:42;boryas;MAPREDUCE-1383-BP20-7.patch;https://issues.apache.org/jira/secure/attachment/12434455/MAPREDUCE-1383-BP20-7.patch",,,,,,,,,,,,,,,,,,,,,,,12.0,,,,,,,,,,,,,,,,,,,,2010-01-22 07:49:29.771,,,false,,,,,,,,,,,,,,,,,,37180,,,,,Tue Feb 02 06:16:37 UTC 2010,,,,,,,"0|i02rmn:",14077,"mapreduce.job.hdfs-servers - declares hdfs servers to be used by the job, so client can pre-fetch delegation tokens for thsese servers (comma separated list of NameNodes).",,,,,,,,,,security,,,,,,,,,,"19/Jan/10 19:39;boryas;before submitting a job - contact all the namenodes (specified in the conf) to get their corresponding delegation token.","21/Jan/10 23:23;boryas;changed DfsClient to DistributedFileSystem
changed references thru tokenStorage to TokenCache.
","22/Jan/10 07:49;ddas;After a quick look at the patch, some comments:
1) The call to populateTokenCache should be done just before the submitJob call (so that we definitely have all the information to do with namenodes the job wishes to talk to at runtime, even those that are potentially added within calls to user's InputFormat.getSplits)
2) The configs mapreduce.job.hdfs-servers and mapreduce.job.kerberos.jtprinicipal could probably be defined in JobContext.java.
3) Also, though not directly related to this patch, but could you please move the JOB_TOKEN_FILE definition from JobContext.java to TokenStorage.java. Ditto for SecureShuffleUtils.JOB_TOKEN_FILENAME","26/Jan/10 01:08;boryas;preliminary patch","27/Jan/10 05:36;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431493/MAPREDUCE-1383-6.patch
  against trunk revision 903508.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The patch appears to cause tar ant target to fail.

    -1 findbugs.  The patch appears to cause Findbugs to fail.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/409/testReport/
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/409/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/409/console

This message is automatically generated.","27/Jan/10 08:47;ddas;[exec] compile-aspects:
     [exec]      [echo] 1.6
     [exec]      [echo] Start weaving aspects in place
     [exec]      [iajc] /grid/0/hudson/hudson-slave/workspace/Mapreduce-Patch-h6.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/mapreduce/JobSubmitter.java:39 [error] The import org.apache.hadoop.hdfs cannot be resolved
     [exec]      [iajc] import org.apache.hadoop.hdfs.DistributedFileSystem;
     [exec]      [iajc]        ^^^^^^^^^^^^^^^^^^^^^
     [exec]      [iajc] /grid/0/hudson/hudson-slave/workspace/Mapreduce-Patch-h6.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/mapreduce/JobSubmitter.java:40 [error] The import org.apache.hadoop.hdfs cannot be resolved
     [exec]      [iajc] import org.apache.hadoop.hdfs.security.token.DelegationTokenIdentifier;
     [exec]      [iajc]        ^^^^^^^^^^^^^^^^^^^^^
     [exec]      [iajc] /grid/0/hudson/hudson-slave/workspace/Mapreduce-Patch-h6.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/mapreduce/JobSubmitter.java:41 [error] The import org.apache.hadoop.hdfs cannot be resolved
     [exec]      [iajc] import org.apache.hadoop.hdfs.server.namenode.NameNode;
     [exec]      [iajc]        ^^^^^^^^^^^^^^^^^^^^^
     [exec]      [iajc] /grid/0/hudson/hudson-slave/workspace/Mapreduce-Patch-h6.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/mapreduce/security/TokenCache.java:32 [error] The import org.apache.hadoop.hdfs cannot be resolved
     [exec]      [iajc] import org.apache.hadoop.hdfs.DistributedFileSystem;
     [exec]      [iajc]        ^^^^^^^^^^^^^^^^^^^^^
     [exec]      [iajc] /grid/0/hudson/hudson-slave/workspace/Mapreduce-Patch-h6.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/mapreduce/security/TokenCache.java:33 [error] The import org.apache.hadoop.hdfs cannot be resolved
     [exec]      [iajc] import org.apache.hadoop.hdfs.security.token.DelegationTokenIdentifier;
     [exec]      [iajc]        ^^^^^^^^^^^^^^^^^^^^^
     [exec]      [iajc] /grid/0/hudson/hudson-slave/workspace/Mapreduce-Patch-h6.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/mapreduce/security/TokenCache.java:99 [error] DelegationTokenIdentifier cannot be resolved to a type
     [exec]      [iajc] public static Token<DelegationTokenIdentifier> getDelegationToken(String namenode) {
     [exec]      [iajc]                     ^^^^^^^^^^^^^^^^^^^^^^
     [exec]      [iajc] /grid/0/hudson/hudson-slave/workspace/Mapreduce-Patch-h6.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/mapreduce/security/TokenCache.java:181 [error] DistributedFileSystem cannot be resolved to a type
     [exec]      [iajc] if(fs instanceof DistributedFileSystem) {
     [exec]      [iajc]                  ^^^^^^^^^^^^^^
     [exec]      [iajc] /grid/0/hudson/hudson-slave/workspace/Mapreduce-Patch-h6.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/mapreduce/security/TokenCache.java:182 [error] DistributedFileSystem cannot be resolved to a type
     [exec]      [iajc] DistributedFileSystem dfs = (DistributedFileSystem)fs;
     [exec]      [iajc] ^^^^^^^^^^^^
     [exec]      [iajc] /grid/0/hudson/hudson-slave/workspace/Mapreduce-Patch-h6.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/mapreduce/security/TokenCache.java:182 [error] DistributedFileSystem cannot be resolved to a type
     [exec]      [iajc] DistributedFileSystem dfs = (DistributedFileSystem)fs;
     [exec]      [iajc]                              ^^^^^^^^^^^^
     [exec]      [iajc] /grid/0/hudson/hudson-slave/workspace/Mapreduce-Patch-h6.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/mapreduce/security/TokenCache.java:186 [error] DelegationTokenIdentifier cannot be resolved to a type
     [exec]      [iajc] Token<DelegationTokenIdentifier> token = 
     [exec]      [iajc]       ^^^^^^^^^^^^^^^^
     [exec]      [iajc] /grid/0/hudson/hudson-slave/workspace/Mapreduce-Patch-h6.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/mapreduce/security/TokenCache.java:187 [error] The method getDelegationToken(String) is undefined for the type TokenCache
     [exec]      [iajc] TokenCache.getDelegationToken(fs_uri); 
     [exec]      [iajc]            ^^^^^^^
     [exec]      [iajc] 
     [exec]      [iajc] 11 errors
     [exec] 
     [exec] BUILD FAILED

This is there is the log of the build. The reason for this is the hdfs jar is not present in the classpath when compile-aspects is running. Is there a way to avoid having to import hdfs.* in the patch.","28/Jan/10 02:22;ddas;Some comments:
1) Remove the LOG.info statements from TokenCache
2) Please add a method in TokenCache to load tokens from a file specified in the argument (rather than going indirectly through the conf). Then in Child.java, you can call that API directly rather than setting the file in conf.
3) Does it make sense to have a TokenCache.addPathsForGettingDelegationToken(Path[]) that's called from the places where you currently have TokenCache.obtainTokensForNamenodes. Then just before job submission you make one call to TokenCache.obtainTokensForNamenodes. That way we will be sure that there is not more than one per a namenode to get delegation tokens.
4) You define getDelegationTokens in TrackerDistributedCacheManager but don't invoke it. Also, you just need to pass the paths to the TokenCache.obtainTokensForNamenodes because that's already checking for duplicate entries.
We discussed offline that you need to fix the build so that the hdfs jars are in the classpath for the aspects compilation..","28/Jan/10 02:29;ddas;And yes, as per the offline discussion with Kan you need to have host:port instead of URIs for the service field of the delegation tokens.","28/Jan/10 17:51;boryas;addressed review comments.
changed ivy.xml in mamuk to include hdfs.jar
changed service field in the Delegation Token to ""inet_addr:port""","28/Jan/10 21:10;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431687/MAPREDUCE-1383-9.patch
  against trunk revision 903563.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/414/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/414/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/414/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/414/console

This message is automatically generated.","28/Jan/10 22:38;boryas;Two following tests are failing:
TestTaskTrackerBlacklisting
org.apache.hadoop.streaming.TestLoadTypedBytes

I ran them manually and they worked fine.
There is a jira for TestTaskTrackerBlacklisting intermittent failure (MAPREDUCE-1412).","29/Jan/10 01:10;boryas;make sure  DelegetionToken.setService() is ""/IP:PORT""","29/Jan/10 04:02;ddas;The ""/IP:PORT"" looks odd. Please change the service to be <IP-ADDRESS-OF-HOST>:PORT. Other than that patch looks fine.","29/Jan/10 06:10;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431732/MAPREDUCE-1383-10.patch
  against trunk revision 903563.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/415/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/415/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/415/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/415/console

This message is automatically generated.","29/Jan/10 07:22;boryas;Patch for earlier version of Hadoop. Not for commit here. ","29/Jan/10 18:38;boryas;bq.The ""/IP:PORT"" looks odd. Please change the service to be <IP-ADDRESS-OF-HOST>:PORT. 
After discussion with Kan, we aggreed on ""IP:PORT"". Kan will match in his implementation.

here is my diff:
< - sb.append(NetUtils.normalizeHostName(uri.getHost())).append("":"").append(port);
---
> +    sb.append(""/"").append(NetUtils.normalizeHostName(uri.getHost())).append("":"").append(port);
","29/Jan/10 18:56;boryas;same change (ip:port) for backport.","29/Jan/10 19:56;ddas;I just committed this. Thanks, Boris!","30/Jan/10 16:26;hudson;Integrated in Hadoop-Mapreduce-trunk #221 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/221/])
    . Automates fetching of delegation tokens in File*Formats Distributed Cache and Distcp. Also, provides a config  mapreduce.job.hdfs-servers that the jobs can populate with a comma separated list of namenodes. The job client automatically fetches delegation tokens from those namenodes. Contributed by Boris Shkolnik.
","01/Feb/10 07:27;ddas;The 0.20 patch has the TestTokenCache.java deleted. Please fix that and upload a new one. Other than that looks fine.","01/Feb/10 19:19;boryas;backport. fixed a typo  (append()=>sb.append) in TokenCache.java","01/Feb/10 19:51;boryas;merged backport","01/Feb/10 23:42;boryas;moved TestTokenCache.java from hadoop/security to hadoop/mapreduce/security","02/Feb/10 06:16;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #225 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/225/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
need security keys storage solution,MAPREDUCE-1338,12438613,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,boryas,boryas,boryas,20/Oct/09 19:55,24/Aug/10 21:19,12/Jan/21 09:52,21/Jan/10 21:32,,,,,,0.21.0,,,,,,,,,0,,,,,"set, get, store, load security keys

key alias - byte[]
key value - byte[]

store/load from DataInput/Output stream
",,aw,cos,cutting,ddas,ghelmling,hammer,jghoman,lianhuiwang,omalley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/09 22:03;boryas;HADOOP-6325.patch;https://issues.apache.org/jira/secure/attachment/12422734/HADOOP-6325.patch","20/Jan/10 21:26;boryas;MAPREDUCE-1338-10.patch;https://issues.apache.org/jira/secure/attachment/12430928/MAPREDUCE-1338-10.patch","11/Jan/10 22:55;boryas;MAPREDUCE-1338-2.patch;https://issues.apache.org/jira/secure/attachment/12429946/MAPREDUCE-1338-2.patch","13/Jan/10 01:40;boryas;MAPREDUCE-1338-4.patch;https://issues.apache.org/jira/secure/attachment/12430080/MAPREDUCE-1338-4.patch","15/Jan/10 18:16;boryas;MAPREDUCE-1338-6.patch;https://issues.apache.org/jira/secure/attachment/12430417/MAPREDUCE-1338-6.patch","16/Jan/10 00:35;boryas;MAPREDUCE-1338-7.patch;https://issues.apache.org/jira/secure/attachment/12430473/MAPREDUCE-1338-7.patch","19/Jan/10 01:10;boryas;MAPREDUCE-1338-8.patch;https://issues.apache.org/jira/secure/attachment/12430684/MAPREDUCE-1338-8.patch","19/Jan/10 19:41;boryas;MAPREDUCE-1338-9.patch;https://issues.apache.org/jira/secure/attachment/12430786/MAPREDUCE-1338-9.patch","21/Jan/10 19:05;boryas;MAPREDUCE-1338-BP20-2.patch;https://issues.apache.org/jira/secure/attachment/12431054/MAPREDUCE-1338-BP20-2.patch","22/Jan/10 23:23;boryas;MAPREDUCE-1338-BP20-3.patch;https://issues.apache.org/jira/secure/attachment/12431172/MAPREDUCE-1338-BP20-3.patch","29/Dec/09 01:22;boryas;MAPREDUCE-1338.patch;https://issues.apache.org/jira/secure/attachment/12429053/MAPREDUCE-1338.patch",,,,,,,,,,,,,,,,,,,,,,,,11.0,,,,,,,,,,,,,,,,,,,,2009-10-21 10:41:36.203,,,false,,,,,,,,,,,,,,,,,,37192,,,,,Tue Feb 02 06:16:44 UTC 2010,,,,,,,"0|i02rpj:",14090,"new command line argument:
 tokensFile - path to the file with clients secret keys in JSON format",,,,,,,,,,security,,,,,,,,,,"20/Oct/09 22:03;boryas;implementation + test","21/Oct/09 10:41;tomwhite;Could this use java.security.KeyStore? ","21/Oct/09 17:19;aw;Are these stores expected to pre-encrypted or do they do the encryption themselves?  I sort of echo what Tom says:  are we building something custom that we shouldn't be?","22/Oct/09 20:30;omalley;The intent is to have a key-value store for security credentials. It is the equivalent of a secure job configuration. It needs to be passable over RPC and therefore should be implemented as a writable.

I don't see a good way (or justification) to use the KeyStore.

They will only be stored in the JobTracker's system directory and so they don't need to be encrypted themselves.","22/Oct/09 20:40;omalley;I think this actually belongs over in map/reduce rather than in common.

The use case is: when the user submits a job, they need to include credentials. Currently this would go into the job conf, but that is visible from the web. We are better off factoring this out into a separate map. 

Note that the credentials will be disjoint for different HDFS clusters (or other filesystems). So it will look like:

""hdfs://nn1/"" -> binary blob1
""hdfs://nn2/"" -> binary blob2
""mapred.job.token"" -> binary blob3

This key storage should be included in the call to submitJob.

","24/Nov/09 21:24;boryas;Closing it. We used JobTokens class to store shuffle key.","24/Nov/09 21:32;ddas;I think we should leave this issue open. We will need a solution to support the use case of arbitrary user supplied security credentials...","16/Dec/09 01:24;boryas;{quote}
Note that the credentials will be disjoint for different HDFS clusters (or other filesystems). So it will look like:

""hdfs://nn1/"" -> binary blob1
""hdfs://nn2/"" -> binary blob2
""mapred.job.token"" -> binary blob3

This key storage should be included in the call to submitJob.
{quote}
If the keys are given on a command line - how can we pass it to the job. All the command lines arguments are passed thru config, and we want to avoid id.","28/Dec/09 22:21;boryas;Moved this issue from HADOOP-6325 to this one. It is mapreduce related.","29/Dec/09 01:22;boryas;preliminary patch,
test not complete,
debug info","11/Jan/10 22:55;boryas;Added TokenCache class.
It keeps static mapping between jobId and TokenStorage for this job.","12/Jan/10 07:44;ddas;Some comments after a quick look:
1) The json parser should be used from the jackson one
2) The loading of tokens in the child should happen more or less in the same way as it happens in the trunk (reading the filename via an env var, etc.)
3) The user API can be simplified. I would prefer something like TokenCache.getSecretKey(alias) rather than TokenCache.getTokenStorage(jobId).getSecretKey(alias)..","13/Jan/10 01:40;boryas;addressed comments by Devaraj.","13/Jan/10 03:04;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12430080/MAPREDUCE-1338-4.patch
  against trunk revision 898486.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/380/console

This message is automatically generated.","16/Jan/10 02:24;ddas;Looks good overall. Some comments:
1) Rename the shuffletoken to jobtoken (TokenStorage.setShuffle, etc.)
2) Rename loadJobToken to loadTokens
3) The javadoc at the beginning of the TokenCache file is misleading
3) Ideally the alias for the secrets in TokenStorage should be a Text object instead of String
4) Is there any reason why TestTokenCache is in mapred package whereas TestTokenStorage is in the security package?
5) I think there should be an API to get all the tokens (TokenCache.getAllTokens()). That will be helpful for HADOOP-6299.
6) Also, i think there should be an API to add tokens (TokenCache.addToken(Token)). That will be helpful for MAPREDUCE-1383.

One other thing that I realized while reviewing this patch was that the persistence of the token cache should happen in the submitJob method in the JobTracker, rather than in the Job's initialization, in order to better support job recovery. For example, we could have a case where the user submits a job, and then before the job initialization, the JobTracker gets restarted for some reason. If job recovery is enabled, the job submitted earlier will be considered for initialization at some point of time after the restart. But the job's tokencache wouldn't be there then.. But I am okay to handle this issue in a follow up jira.
","16/Jan/10 04:18;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12430473/MAPREDUCE-1338-7.patch
  against trunk revision 899844.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/392/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/392/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/392/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/392/console

This message is automatically generated.","19/Jan/10 01:10;boryas;addressed comments by Devaraj.
","19/Jan/10 04:36;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12430684/MAPREDUCE-1338-8.patch
  against trunk revision 900159.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 13 new or modified tests.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/394/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/394/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/394/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/394/console

This message is automatically generated.","19/Jan/10 19:41;boryas;fixed contrib test and warning","19/Jan/10 22:50;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12430786/MAPREDUCE-1338-9.patch
  against trunk revision 900815.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 13 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/277/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/277/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/277/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/277/console

This message is automatically generated.","20/Jan/10 08:41;ddas;Apologies for not being thorough enough earlier. But I think the API needs a little bit more of tweaking. 

I think in TokenCache, all APIs should be static (the addDelegationToken is not). 

The TokenStorage class should be annotated with InterfaceAudience Private. The TokenCache should be annotated with InterfaceStability Evolving. 

As far as possible, TokenCache should be the one that should be used by the framework and the applications. TokenStorage should be a helper class that is used internally by the TokenCache. In that sense, some of the APIs like TokenCache.getTokenStorage could be package private (so that you can still use it in your new tests to inspect the fields). TokenCache.setStorage should be annotated with InterfaceAudience Private (since you use it in LocalJobRunner, and hence the method cannot be made package private).

Can we move the loadJobToken calls to TokenStorage. The TokenCache internally checks for whether the tokenStorage field is null or not on every API invocation. If the field is null, it invokes the static method TokenStorage.loadJobToken, and gets an initialized TokenStorage object.

The setJobToken/getJobToken could be moved to TokenCache but annotated with InterfaceAudience Private.

Doing the above would make the TokenCache as the single point of entry to the token storage and make the APIs more clear IMO.

Does these make sense? Again, apologies for not commenting on these earlier.","20/Jan/10 21:26;boryas;addressed some issues brought up by Devaraj","21/Jan/10 03:53;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12430928/MAPREDUCE-1338-10.patch
  against trunk revision 901350.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 13 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/279/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/279/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/279/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/279/console

This message is automatically generated.","21/Jan/10 05:55;ddas;I guess I am okay with the current patch. I would have been happier if the TokenStorage was not exposed at all, but since the user facing API in TokenCache seems right, I am okay to have a follow up jira to factor out the TokenStorage class as per my earlier comment (""As far as possible...""). 
I intend to commit this patch soon.","21/Jan/10 19:04;boryas;Devaraj, please open a Jira on this issue, so we don't loos track of it.","21/Jan/10 19:05;boryas;attaching patch for backport","21/Jan/10 21:32;ddas;I just committed this. Thanks, Boris!","22/Jan/10 16:26;hudson;Integrated in Hadoop-Mapreduce-trunk #213 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/213/])
    . Introduces the notion of token cache using which tokens and secrets can be sent by the Job client to the JobTracker. Contributed by Boris Shkolnik.
","22/Jan/10 23:23;boryas;Patch for earlier version of Hadoop. Not for commit here. 

removed comments","02/Feb/10 06:16;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #225 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/225/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add counters for task time spent in GC,MAPREDUCE-1304,12443575,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,kimballa,tlipcon,tlipcon,16/Dec/09 21:43,24/Aug/10 21:19,12/Jan/21 09:52,25/Apr/10 02:40,,,,,,0.21.0,,,task,,,,,,0,,,,,"It's easy to grab the number of millis spent in GC (see JvmMetrics for example). Exposing these as task counters would be handy - occasionally I've seen user jobs where long GC pauses cause big ""unexplainable"" performance problems, and a large counter would make it obvious to the user what's going on.",,aah,acmurthy,cdouglas,hammer,johanoskarsson,schen,tlipcon,zhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-901,,,,,,,,,,,,,,,,,,,,,"27/Feb/10 02:08;kimballa;MAPREDUCE-1304.2.patch;https://issues.apache.org/jira/secure/attachment/12437301/MAPREDUCE-1304.2.patch","27/Mar/10 02:06;kimballa;MAPREDUCE-1304.3.patch;https://issues.apache.org/jira/secure/attachment/12439944/MAPREDUCE-1304.3.patch","27/Feb/10 00:06;kimballa;MAPREDUCE-1304.patch;https://issues.apache.org/jira/secure/attachment/12437279/MAPREDUCE-1304.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2009-12-17 04:54:24.102,,,false,,,,,,,,,,,,,,,,,,149424,Reviewed,,,,Sun Apr 25 02:40:11 UTC 2010,,,,,,,"0|i0jgav:",111571,,,,,,,,,,,,,,,,,,,,,"17/Dec/09 04:54;acmurthy;+1, this is useful information.","27/Feb/10 00:06;kimballa;Added a new TaskCounter, GC_TIME_MILLIS, which tracks this information. Patch also includes a testcase.","27/Feb/10 01:10;tlipcon;Two questions:

- I don't know the Task code quite well enough, but will this still be set in the case that the task fails?
- Is it worth putting this in a separate thread in the Child interface so it actually gets incremented as the task goes, with each umbilical heartbeat? It would be nice to see this on a per-task basis when a task appears to be ""stuck"".","27/Feb/10 01:34;kimballa;You're right that if a task throws an exception, I think that it will probably not set the counter. I could put the increment in a finally block and that would fix that issue.

Based on a quick look at the code in Counters and Counter, I think that there wouldn't be major issues with thread safety or performance (every operation on a Counter is already synchronized). An extra thread is pretty heavy-weight, though. I think something like this should actually go into the TaskReporter itself; it could just increment the gc counter itself right before sending a status every 3 seconds.

I ran a quick benchmark test which got a handle to the GarbageCollectorMXBean list and polled them; I put all of this in a loop and ran it a million times in about a half-second, so I don't think this would negatively impact performance.","27/Feb/10 01:36;tlipcon;bq. I think something like this should actually go into the TaskReporter itself

Seems reasonable enough to me.","27/Feb/10 02:08;kimballa;New patch moves this logic into TaskReporter so that it happens every status update interval.","02/Mar/10 15:12;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12437301/MAPREDUCE-1304.2.patch
  against trunk revision 916823.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/340/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/340/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/340/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/340/console

This message is automatically generated.","02/Mar/10 17:59;kimballa;Test failure is unrelated","22/Mar/10 22:46;cdouglas;Great idea. A few comments on the impl:
* This has no effect:
{noformat}
+    job.getConfiguration().set(""io.sort.record.pct"", ""0.50"");
{noformat}
* Moving the update of the GC counter to {{Task::updateCounters}} makes sense. To capture any {{FileSystem}} activity in the committer, the call to {{updateCounters}} in {{Task::done}} should be after the commit, anyway. This is a bug in the current code.
* Adding {{java.lang.management}} components shouldn't be widening the {{TaskReporter}} API and adding fields. There should be an {{\*Updater}} object tracking the particular statistic being tracked (as in the {{FileSystem}} counters) rather than letting the {{TaskReporter}} keep this state.
* Updating the GC counters must be synchronized or it risks reporting incorrect results (moving the update to {{Task::updateCounters}} would be sufficient).","27/Mar/10 02:06;kimballa;New patch incorporates CR comments.

I added a call to {{updateCounters()}} after the reporter thread is shut down, where the {{incrementGcCounter()}} call used to be.","27/Mar/10 10:15;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12439944/MAPREDUCE-1304.3.patch
  against trunk revision 928104.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/65/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/65/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/65/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/65/console

This message is automatically generated.","25/Apr/10 02:40;cdouglas;+1

I committed this. Thanks, Aaron!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We need a job trace manipulator to build gridmix runs.,MAPREDUCE-1295,12443321,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,dking,dking,dking,15/Dec/09 00:46,24/Aug/10 21:19,12/Jan/21 09:52,08/Jan/10 01:08,,,,,,0.21.0,,,tools/rumen,,,,,,0,,,,,"Rumen produces ""job traces"", which are JSON format files describing important aspects of all jobs that are run [successfully or not] on a hadoop map/reduce cluster.  There are two packages under development that will consume these trace files and produce actions in that cluster or another cluster: gridmix3 [see jira MAPREDUCE-1124 ] and Mumak [a simulator -- see MAPREDUCE-728 ].

It would be useful to be able to do two things with job traces, so we can run experiments using these two tools: change the duration, and change the density.  I would like to provide a ""folder"", a tool that can wrap a long-duration execution trace to redistribute its jobs over a shorter interval, and also change the density by duplicating or culling away jobs from the folded combined job trace.",,cdouglas,hammer,hong.tang,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/09 22:19;dking;mapreduce-1295--2009-12-17.patch;https://issues.apache.org/jira/secure/attachment/12428351/mapreduce-1295--2009-12-17.patch","22/Dec/09 01:56;dking;mapreduce-1295--2009-12-21.patch;https://issues.apache.org/jira/secure/attachment/12428686/mapreduce-1295--2009-12-21.patch","22/Dec/09 19:20;dking;mapreduce-1295--2009-12-22.patch;https://issues.apache.org/jira/secure/attachment/12428754/mapreduce-1295--2009-12-22.patch","24/Dec/09 01:39;dking;mapreduce-1295--2009-12-23.patch;https://issues.apache.org/jira/secure/attachment/12428888/mapreduce-1295--2009-12-23.patch","15/Dec/09 01:24;dking;mapreduce-1297--2009-12-14.patch;https://issues.apache.org/jira/secure/attachment/12427990/mapreduce-1297--2009-12-14.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,2009-12-16 01:07:41.61,,,false,,,,,,,,,,,,,,,,,,149416,Reviewed,,,,Sat Jan 09 16:15:39 UTC 2010,,,,,,,"0|i0jg9z:",111567,,,,,,,,,,,,,,,,,,,,,"15/Dec/09 18:04;dking;This is the folder.

The command line looks like this:

{noformat}
   Folder [ -output-duration duration ]           \
          [ -input-cycle duration     ]           \
          [ -concentration ratio      ]           \
          [ -seed seed                ]           \
          [ -temp temp-directory      ]           \
          [ -debug                    ]           \
          [ -skew-buffer-length n     ]           \
          [ -allow-missorting         ]           \
       input-path output-path
{noformat}

All paths and directories are general {{Path}}s.

{{-output-duration}} is the length of the range of the submit times of the output trace.

{{-input-cycle}} is the length of the input job submit time cycle.  For example, if {{-input-cycle}} is 2 hours, then 3PM and 1PM is treated alike.

{{-concentration}} is a double, a ratio of the density [number of jobs starting per hour] in the output over the input.  This can be less than or greater than 1.0.

{{-seed}} is a random number generator seed, used to create repeatable runs if desired.  If no {{-seed}} is provided, we state what the {{-seed}} should be if you want to repeat this run.

{{-temp}} is a temp directory, which must be able to hold about as much data as the input contains, compressed.  Trace data compresses at about 17:1.  This defaults to the directory of the output.  The temporary files are erased whether the job succeeds or fails, unless {{-debug}} is coded.

{{-debug}} induces the tool to produce a lot of debugging output, and causes the itnermediate files to be retained after a run.

{{-skew-buffer-length}} describes the length of the skew buffer, which defaults to 0.  The input to the folder should be sorted, and the job tracker log output is approximately sorted.  However, there are occasional small glitches in the job tracker logs, jobs that come out a few places earlier than they should to be ordered by submit time.  Code a {{-skew-buffer-length}} of {{i > 0}} to allow as many as {{i}} jobs to arrive earlier than they're supposed to and be buffered until they can be released.

{{-allow-missorting}} instructs the folder to just drop a job that arrives later than it should, if there's not enough room in the skew buffer for it.  If {{-allow-missorting}} is not coded, we abend the run instead.

If the run is successful, either because {{-allow-missorting}} is coded or {{-skew-buffer-length}} is big enough, the folder tells the user what the smallest {{-skew-buffer-length}} could have been for the run to succeed without omitting any jobs.","16/Dec/09 00:00;dking;I misnamed the patch.  

I will need to fix it anyway; I noticed that it doesn't print out the needed skew buffer size.  I will upload the new patch, with the correct name [and the then-correct date] after I see the hudson report and act on it if necessary.","16/Dec/09 01:07;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427990/mapreduce-1297--2009-12-14.patch
  against trunk revision 890983.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/197/console

This message is automatically generated.","16/Dec/09 04:43;cdouglas;The patch doesn't apply; could you update it?","17/Dec/09 22:19;dking;This patch applies on a direct download of Trunk, and replaces the previous patch","17/Dec/09 22:20;dking;This patch fixes an applicability issue.","18/Dec/09 09:35;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428351/mapreduce-1295--2009-12-17.patch
  against trunk revision 892117.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 2335 javac compiler warnings (more than the trunk's current 2331 warnings).

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/216/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/216/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/216/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/216/console

This message is automatically generated.","22/Dec/09 01:56;dking;This is the replacement for ...-17.patch","22/Dec/09 06:07;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428686/mapreduce-1295--2009-12-21.patch
  against trunk revision 893055.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 2331 javac compiler warnings (more than the trunk's current 2330 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/330/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/330/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/330/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/330/console

This message is automatically generated.","22/Dec/09 19:20;dking;Fprgot that {{RuntimeException}} implements {{Serializable}} and therefore any subclasses I define should have a {{serialVersionUID}}","23/Dec/09 02:06;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428754/mapreduce-1295--2009-12-22.patch
  against trunk revision 893055.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/335/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/335/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/335/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/335/console

This message is automatically generated.","23/Dec/09 02:59;cdouglas;The test failures are known (MAPREDUCE-1311, MAPREDUCE-1312).

Only a few minor nits:
* The two types of DeskewedJobTraceReader constructors could be combined by adding a private/protected cstr with a JobTraceReader formal
* Should the System.err messages in DJTR::nextJob be debug messages?
* The open/close idiom in {{run}} may be replaced by {{FileSystem::exists}}. Alternatively, require the user to provide a clean directory and use sequential segment numbering
* The first person is a little disorienting in the debug log messages- which could use log4j loggers- but whatever you prefer
* The {{deletees}} and similar accounting can be replace with {{FileSystem::deleteOnExit}}","24/Dec/09 00:06;dking;This is a comment on the previous comment timestamped 23/Dec/09 02:59AM.

   * I will make all the {{DeskewedJobTraceReader}} constructors take a {{JobTraceReader}} .  I see no reason why {{DeskewedJobTraceReader}} constructors shouldn't be public.
   * I will use {{FileSystem::exists}} .
      * I didn't notice that API when I wrote this code.  My bad.  
      * I don't want to require an empty directory because we use the output directory as the temp directory if none is supplied, and requiring an empty directory would make it impossible to do two runs into the same output directory.
   * I'll change the wording on some of the debug messages.  I'll consider going to LOG4J .
   * I stand by my decision to use {{deletees}} .  I don't like the {{deleteOnExit}} idiom because it makes it hard to use a tool as a callable component.

","24/Dec/09 01:39;dking;This addresses the issues raised earlier today.","24/Dec/09 04:37;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428888/mapreduce-1295--2009-12-23.patch
  against trunk revision 893469.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/242/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/242/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/242/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/242/console

This message is automatically generated.","08/Jan/10 01:08;cdouglas;+1

I committed this. Thanks, Dick!","09/Jan/10 16:15;hudson;Integrated in Hadoop-Mapreduce-trunk #200 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/200/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide documentation for Mark/Reset functionality,MAPREDUCE-1074,12424499,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,jothipn,jothipn,jothipn,04/May/09 05:43,24/Aug/10 21:18,12/Jan/21 09:52,09/Dec/09 09:14,,,,,,0.21.0,,,documentation,,,,,,0,,,,,HADOOP-5266 introduced support of Mark/Reset of Values Iterator. Documentation needs to be updated for the same.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/09 08:51;jothipn;mapred-1074.patch;https://issues.apache.org/jira/secure/attachment/12427146/mapred-1074.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2009-12-07 12:09:37.447,,,false,,,,,,,,,,,,,,,,,,149260,Reviewed,,,,Wed Dec 09 09:14:17 UTC 2009,,,,,,,"0|i0ebhz:",81625,,,,,,,,,,,,,,,,,,,,,"07/Dec/09 08:51;jothipn;Straight forward patch","07/Dec/09 12:09;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427146/mapred-1074.patch
  against trunk revision 887844.

    +1 @author.  The patch does not contain any @author tags.

    +0 tests included.  The patch appears to be a documentation patch that doesn't require tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/169/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/169/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/169/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/169/console

This message is automatically generated.","08/Dec/09 03:37;jothipn;The failed contrib test is due to MR-1124.
The failed core test, TestJobOutputCommitter is clearly unrelated as this patch is a documentation patch. It looks as if this test failed because of some configuration issues.","09/Dec/09 09:14;cdouglas;+1

I committed this. Thanks, Jothi!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modify JobHistory to use Avro for serialization instead of raw JSON,MAPREDUCE-980,12435680,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,cutting,jothipn,jothipn,15/Sep/09 05:09,24/Aug/10 21:17,12/Jan/21 09:52,18/Sep/09 22:24,,,,,,0.21.0,,,,,,,,,0,,,,,MAPREDUCE-157 modifies JobHistory to log events using Json Format.  This can be modified to use Avro instead. ,,cdouglas,cutting,eric14,guanying,hammer,hong.tang,philip,ravidotg,sharadag,snehal,tomwhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/09 18:55;cutting;MAPREDUCE-980.patch;https://issues.apache.org/jira/secure/attachment/12420063/MAPREDUCE-980.patch","18/Sep/09 18:47;cutting;MAPREDUCE-980.patch;https://issues.apache.org/jira/secure/attachment/12420061/MAPREDUCE-980.patch","18/Sep/09 18:25;cutting;MAPREDUCE-980.patch;https://issues.apache.org/jira/secure/attachment/12420060/MAPREDUCE-980.patch","18/Sep/09 16:47;cutting;MAPREDUCE-980.patch;https://issues.apache.org/jira/secure/attachment/12420047/MAPREDUCE-980.patch","18/Sep/09 15:32;cutting;MAPREDUCE-980.patch;https://issues.apache.org/jira/secure/attachment/12420034/MAPREDUCE-980.patch","18/Sep/09 14:04;cutting;MAPREDUCE-980.patch;https://issues.apache.org/jira/secure/attachment/12420023/MAPREDUCE-980.patch","17/Sep/09 18:43;cutting;MAPREDUCE-980.patch;https://issues.apache.org/jira/secure/attachment/12419913/MAPREDUCE-980.patch","16/Sep/09 23:12;cutting;MAPREDUCE-980.patch;https://issues.apache.org/jira/secure/attachment/12419829/MAPREDUCE-980.patch","16/Sep/09 22:49;cutting;MAPREDUCE-980.patch;https://issues.apache.org/jira/secure/attachment/12419827/MAPREDUCE-980.patch","16/Sep/09 00:02;cutting;MAPREDUCE-980.patch;https://issues.apache.org/jira/secure/attachment/12419709/MAPREDUCE-980.patch",,,,,,,,,,,,,,,,,,,,,,,,,10.0,,,,,,,,,,,,,,,,,,,,2009-09-16 00:02:01.165,,,false,,,,,,,,,,,,,,,,,,149185,,,,,Sat Sep 19 01:05:12 UTC 2009,,,,,,,"0|i0jffj:",111430,,,,,,,,,,,,,,,,,,,,,"15/Sep/09 05:11;jothipn;As per [this comment | https://issues.apache.org/jira/browse/MAPREDUCE-157?focusedCommentId=12745279&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12745279], I created this new Jira to follow the port to use Avro","16/Sep/09 00:02;cutting;Here's a first pass at this.  It must be applied after the MAPREDUCE-157 patch.  It compiles but has not yet been tested.","16/Sep/09 22:49;cutting;Here's a version that passes tests.

I will mark this as ""Patch Available"" as soon as MAPREDUCE-157 is committed so that Hudson can have a look at it.","16/Sep/09 23:12;cutting;Restore some dropped javadoc and muffed visibility.","17/Sep/09 06:32;sharadag;It would have been ideal if we could directly use Avro generated classes without wrapping them. Wrapper classes are not very maintainable because we need to modify at two places- schema definition and wrapper class. Any reason why we can't directly use generated classes ? From what I can think of  - this is done because we want all event classes to have a base interface, constructor and field getters. Having constructor and getters should be straight forward in code generator. For base class/interface, I think Avro can generate code from a template. SpecificRecordBase methods can directly go into the generated class (to work around multiple inheritance in java). Users can define the base class, interfaces or additional methods in the template which can be used to generate Avro specific class. I understand that this may not be doable at this point but something worth considering at some point to make Avro code generation feature more compelling.

","17/Sep/09 18:43;cutting;Proper 'svn diff' version of patch now that MAPREDUCE-157 is committed.","17/Sep/09 18:59;cutting;> Any reason why we can't directly use generated classes ?

You've already cited the biggest reason: the generated classes don't provide constructors or accessors.  Long-term, we could enhance Avro to generate these, but I'm not sure we'd want to directly use the generated classes even then.

The wrappers provide considerable utility, including:
 - Javadoc comments.  We could generate these perhaps from documentation in the schema.
 - Visibility: The wrappers only provide public getters, not setters.  We could perhaps add that to the schema and/or generator.
 - Type conversion:  In both the version included in MAPREDUCE-157 and this version there's a fair amount of field-specific type conversion.  For example, we don't directly serialize JobID instances, but rather use JobID's toString() and forName() methods to convert these to and from strings for serialization.  Similarly for counters, task ids, etc.  Ideally all of these would be naturally serializeable using Avro, but, until they are, the wrappers make it easy to incorporate things like these.
 - Compatibility: If we update the schema then Avro will handle reading old data, but, without the wrappers, we'd be unable to provide a back-compatible API for accessing the old data.  So if we remove a field from the schema, with the wrappers we're able to deprecate the accessor and implement it in terms of new/remaining fields so that applications don't have to be upgraded.

So I'm not entirely convinced that using wrappers for stuff like this is a bad pattern long term.
","17/Sep/09 20:17;philip;My experience with generated objects (from a couple of years using protocol buffers) is that one ends up wrapping them often (preferably with composition).  

The generated class is responsible for serialization and deserialization, and the wrapper class is responsible for added logic.  It's hard to make the generator do something reasonable for logic (or even inheritance) cross-language.  Having a wrapper also allows you to have two ways to use something, in two different contexts, where you might want different surrounding logic.  (So, if you had an Avro schema for an Event, the code that generates the Event might use one wrapper, and the code that consumes it might use the raw object, or have a different object.)","18/Sep/09 06:17;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419913/MAPREDUCE-980.patch
  against trunk revision 816454.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/101/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/101/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/101/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/101/console

This message is automatically generated.","18/Sep/09 14:04;cutting;Fixed a possible null pointer exception.","18/Sep/09 14:41;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12420023/MAPREDUCE-980.patch
  against trunk revision 816647.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The patch appears to cause tar ant target to fail.

    -1 findbugs.  The patch appears to cause Findbugs to fail.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/52/testReport/
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/52/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/52/console

This message is automatically generated.","18/Sep/09 16:14;cutting;Found a bunch more Ivy references to Avro 1.0 that need to be updated to 1.1.","18/Sep/09 16:47;cutting;Fixed all ivy.xml files to now refer to Avro 1.0 rather than 1.1.  Avro, Jackson and Paranamer versions are now specified in library.properties, so that this should not occur again.

'ant test-patch' reports:

{noformat}
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no new tests are needed for this patch.
     [exec]                         Also please list what manual steps were performed to verify this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{noformat}

No new tests are required, as MAPREDUCE-157 supplied sufficient tests.
","18/Sep/09 16:52;jothipn;MAPREDUCE-277 will conflict with this patch -- I added two more methods to the JobSubmittedEvent in that patch. Depending on which goes first, the other will have to merge.","18/Sep/09 16:54;jothipn;bq. I added two more methods to the JobSubmittedEvent

Sorry,  I meant two more arguments to the JobSubmittedEvent constructor.","18/Sep/09 17:53;cutting;> MAPREDUCE-277 will conflict with this patch

I'm happy to do the merge regardless of which is committed first.

Since MAPREDUCE-277 is a blocker, my preference would be to commit this issue first, then that one, since it is not subject to today's deadline.

I still need a committer to +1 this.
","18/Sep/09 17:59;jothipn;bq. my preference would be to commit this issue first, then that one, since it is not subject to today's deadline.

Sorry, just saw this. In the meanwhile, MAPREDUCE-277  got committed. ","18/Sep/09 18:11;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12420034/MAPREDUCE-980.patch
  against trunk revision 816664.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/53/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/53/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/53/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/53/console

This message is automatically generated.","18/Sep/09 18:22;cutting;Cancelling to resolve conflicts created by HADOOP-277.","18/Sep/09 18:25;cutting;Merged with MAPREDUCE-277.","18/Sep/09 18:37;cutting;'ant test-patch' on current patch:

{noformat}
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no new tests are needed for this patch.
     [exec]                         Also please list what manual steps were performed to verify this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{noformat}","18/Sep/09 18:41;jothipn;One a quick glance, minor nit -- Don't we need Javadocs for the newly introduced public constructors in Counter and CounterGroup?","18/Sep/09 18:47;cutting;> Don't we need Javadocs for the newly introduced public constructors in Counter and CounterGroup?

Added.
","18/Sep/09 18:55;cutting;Improved javadoc a bit.","18/Sep/09 18:58;jothipn;Just another clarification -- since the patch is using avro1.1, should we change the encoder to json instead of binary so that tools that scrape the logs instead of using EventReaders be supported? ","18/Sep/09 19:09;cutting;> should we change the encoder to json [ ... ]

That was my initial instinct too, but Eric and Owen both indicated to me that they preferred that we use binary.

Eric's comment is:

https://issues.apache.org/jira/browse/MAPREDUCE-157?focusedCommentId=12745279&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12745279

Owen has indicated this in offline discussions.  The idea is that one can easily use Avro to dump the binary as JSON, but that the binary is smaller and faster.

It's a trivial change to make if we prefer JSON instead of binary.","18/Sep/09 19:54;jothipn;I do not know if I am being naive here, but should the EventWriter.Version be something different than ""Avro-Binary""? Something that will help us keep track of schema evolutions, like ""1.0"" ? Or is the version used for a different purpose? ","18/Sep/09 20:14;cutting;> should the EventWriter.Version be something different than ""Avro-Binary""? Something that will help us keep track of schema evolutions

The entire schema is included in the file.  If the schema changes, Avro can still read old data.  We don't need to update the file version if we, e.g., add a field.  If we make such a fundamental change to the schema that Avro's automatic versioning cannot handle it, then we could change the version string to be ""Avro-Binary-v2"" or something.  Or we could examine the schema itself to determine which version it is.","18/Sep/09 20:17;jothipn;Got it. Thanks.","18/Sep/09 21:33;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12420061/MAPREDUCE-980.patch
  against trunk revision 816735.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/54/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/54/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/54/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/54/console

This message is automatically generated.","18/Sep/09 21:51;omalley;This looks like a good change. I love it when we get to rip out code.

+1","18/Sep/09 22:24;cutting;I just committed this.","19/Sep/09 00:31;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12420063/MAPREDUCE-980.patch
  against trunk revision 816782.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/116/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/116/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/116/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/116/console

This message is automatically generated.","19/Sep/09 01:05;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #58 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/58/])
    .  Modify JobHistory to use Avro for serialization.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileOutputCommitter should create a _DONE file for successful jobs,MAPREDUCE-948,12434773,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,amar_kamat,acmurthy,acmurthy,03/Sep/09 20:42,24/Aug/10 21:17,12/Jan/21 09:52,05/Oct/09 05:49,,,,,,0.21.0,,,client,,,,,,0,,,,,Oozie and other workflow systems could use a _DONE file (zero-length) to poll for job-completion to be used as input-availability triggers.,,nidaley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-10-01 16:58:49.408,,,false,,,,,,,,,,,,,,,,,,149160,,,,,Mon Oct 05 05:49:07 UTC 2009,,,,,,,"0|i0jf9z:",111405,,,,,,,,,,,,,,,,,,,,,"01/Oct/09 16:58;nidaley;Can you comment more on what this will do, how it will work?","05/Oct/09 05:49;amar_kamat;Incorporated in MAPREDUCE-947.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support a hierarchy of queues in the Map/Reduce framework,MAPREDUCE-853,12432937,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,,yhemanth,yhemanth,13/Aug/09 04:39,24/Aug/10 21:15,12/Jan/21 09:52,18/Sep/09 16:01,,,,,,0.21.0,,,jobtracker,,,,,,0,,,,,"In MAPREDUCE-824, we proposed introducing a hierarchy of queues in the capacity scheduler. Currently, the M/R framework provides the notion of job queues and handles some functionality related to queues in a scheduler-agnostic manner. This functionality includes:
- Managing the list of ACLs for queues
- Managing the run state of queues - running or stopped
- Displaying scheduling information about queues in the jobtracker web UI and job client CLI
- Displaying list of jobs in a queue in the jobtracker web UI and job client CLI
- Providing APIs for list queues and queue information in JobClient.

Since it would be beneficial to extend this functionality to hierarchical queues, this JIRA is proposing introducing the concept into the map/reduce framework as well. We could treat this as an umbrella JIRA and file additional tasks for each of the changes involved, sticking to the high level approach in this JIRA.",,aaa,acmurthy,cutting,gates,hammer,matei,theiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,149091,,,,,Fri Sep 18 16:01:56 UTC 2009,,,,,,,"0|i0jevj:",111340,,,,,,,,,,,,,,,,,,,,,"13/Aug/09 04:49;yhemanth;The motivation for hierarchical queues is discussed in the [proposal on MAPREDUCE-824|https://issues.apache.org/jira/browse/MAPREDUCE-824?focusedCommentId=12738975&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12738975], for those interested.

Briefly, hierarchical queues allow administrators to have a greater control over how capacity (or conceivably other policies) associated with a queue can be used. They also allow delegation of control. Large clusters could have root level queues set up for organizations, and then operators from the organizations could be given access to manage queues under the root level queues (which we are calling sub-queues in MAPREDUCE-824).","13/Aug/09 11:13;yhemanth;The basic proposal is to define the concept of sub-queues. Sub-queues are queues that are contained in other queues. Sub-queues can be nested. The last level of queues in the hierarchy, the leaf level queues, are called job queues, as they contain jobs. By that definition, all the queues that are defined in the present system are job queues.

An example organization could be:
{noformat}
grid {
  org1 {
    priority {
      production
      proj1
      proj2
      proj3
    }
    miscellaneous
  }
  org2
}
{noformat}

In this example, grid, org1, and priority are container queues. And production, proj1, proj2, proj3, miscellaneous and org2 are job queues. An example of how policies such as capacity can be assigned to this hierarchy and how it benefits them is described in MAPREDUCE-824.
","13/Aug/09 11:34;yhemanth;In summary, the following three areas would be impacted by this change:
- APIs in JobClient (public), JobSubmissionProtocol and QueueManager
- Configuration in mapred-queues.xml and mapred-site.xml
- UI - the hadoop queue family of commands in the CLI and the Scheduling information shown in the Jobtracker's web UI and Queue details pages.

It is a goal to be able to keep this feature backwards compatible. So sites not interested in hierarchical queues, but using the current single level of queues should be able to continue to function as before.

To keep details manageable and not confuse with the high level picture, I will add tasks under this JIRA for each of these areas.","18/Sep/09 16:01;yhemanth;All the three sub tasks associated to this JIRA are resolved. So, resolving the ticket.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MRUnit should support the new API,MAPREDUCE-800,12431298,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,kimballa,kimballa,kimballa,23/Jul/09 20:34,24/Aug/10 21:14,12/Jan/21 09:52,21/Aug/09 14:52,,,,,,0.21.0,,,contrib/mrunit,,,,,,0,,,,,MRUnit's TestDriver implementations use the old org.apache.hadoop.mapred-based classes. TestDrivers and associated mock object implementations are required for org.apache.hadoop.mapreduce-based code.,,aaa,hammer,johanoskarsson,metcalfc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/09 23:32;kimballa;MAPREDUCE-800.2.patch;https://issues.apache.org/jira/secure/attachment/12415679/MAPREDUCE-800.2.patch","23/Jul/09 20:36;kimballa;MAPREDUCE-800.patch;https://issues.apache.org/jira/secure/attachment/12414379/MAPREDUCE-800.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2009-08-03 09:12:44.748,,,false,,,,,,,,,,,,,,,,,,37417,Reviewed,,,,Fri Aug 21 14:52:09 UTC 2009,,,,,,,"0|i02tlz:",14398,Support new API in unit tests developed with MRUnit.,,,,,,,,,,,,,,,,,,,,"23/Jul/09 20:36;kimballa;Implementation of MRUnit over new API. New code is in the org.apache.hadoop.mrunit.mapreduce package, which contains many identically-named classes to those in the base org.apache.hadoop.mrunit package, similar to the mapred/mapreduce package style used in Hadoop MapReduce itself.

This adds mock implementations of Mapper.Context and Reducer.Context which are used as inputs to user-provided Mapper and Reducer classes. This takes advantage of the fact that even though Mapper.Context and Reducer.Context are not static classes, they make use of no state of their outer class Mapper or Reducer objects, merely the shared type signature.","03/Aug/09 09:12;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12414379/MAPREDUCE-800.patch
  against trunk revision 800232.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 14 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/433/console

This message is automatically generated.","05/Aug/09 23:32;kimballa;Attaching new patch resynced with trunk.","07/Aug/09 10:35;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12415679/MAPREDUCE-800.2.patch
  against trunk revision 801517.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 14 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/450/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/450/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/450/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/450/console

This message is automatically generated.","07/Aug/09 16:06;kimballa;failures are in streaming","21/Aug/09 14:52;tomwhite;+1

I've just committed this. Thanks Aaron!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MRUnit should be able to test a succession of MapReduce passes,MAPREDUCE-798,12431296,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,kimballa,kimballa,kimballa,23/Jul/09 20:32,24/Aug/10 21:14,12/Jan/21 09:52,24/Aug/09 12:01,,,,,,0.21.0,,,contrib/mrunit,,,,,,0,,,,,"MRUnit can currently test that the inputs to a given (mapper, reducer) ""job"" produce certain outputs at the end of the reducer. It would be good to support more end-to-end tests of a series of MapReduce jobs that form a longer pipeline surrounding some data.",,aaa,hammer,metcalfc,omalley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/09 23:26;kimballa;MAPREDUCE-798.2.patch;https://issues.apache.org/jira/secure/attachment/12415678/MAPREDUCE-798.2.patch","21/Aug/09 17:54;kimballa;MAPREDUCE-798.3.patch;https://issues.apache.org/jira/secure/attachment/12417289/MAPREDUCE-798.3.patch","23/Jul/09 20:32;kimballa;MAPREDUCE-798.patch;https://issues.apache.org/jira/secure/attachment/12414377/MAPREDUCE-798.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2009-07-23 21:38:10.606,,,false,,,,,,,,,,,,,,,,,,37416,Reviewed,,,,Mon Aug 24 12:01:57 UTC 2009,,,,,,,"0|i02tlr:",14397,Add PipelineMapReduceDriver to MRUnit to support testing a pipeline of MapReduce passes,,,,,,,,,,,,,,,,,,,,"23/Jul/09 20:32;kimballa;attaching implementation.","23/Jul/09 21:38;omalley;Wouldn't it be better to have it work with the ChainMapper?","23/Jul/09 21:45;kimballa;According to the comments in the ChainMapper source, it allows you to run jobs of the form {{M+RM*}}. This is designed to test pipelines of the form  {{(MR)+}}. Can you elaborate a bit more on how you think ChainMapper / ChainReducer fits here?","30/Jul/09 20:58;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12414377/MAPREDUCE-798.patch
  against trunk revision 799402.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 5 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/430/console

This message is automatically generated.","05/Aug/09 23:26;kimballa;Attaching new patch that resyncs with trunk.","07/Aug/09 08:02;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12415678/MAPREDUCE-798.2.patch
  against trunk revision 801517.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 5 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/449/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/449/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/449/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/449/console

This message is automatically generated.","07/Aug/09 16:08;kimballa;test failures are in streaming","21/Aug/09 14:58;tomwhite;If this is different from ChainMapper, it should probably not have ""Chain"" in its name. PipelineMapReduceDriver?","21/Aug/09 17:54;kimballa;+1 to PipelineMapReduceDriver. Attaching new patch with renaming suggestion incorporated.","22/Aug/09 02:40;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12417289/MAPREDUCE-798.3.patch
  against trunk revision 806747.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 5 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/503/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/503/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/503/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/503/console

This message is automatically generated.","22/Aug/09 02:42;kimballa;unrelated test failure.","24/Aug/09 12:01;tomwhite;I've just committed this. Thanks Aaron!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gridmix: Trace-based benchmark for Map/Reduce,MAPREDUCE-776,12430985,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,cdouglas,cdouglas,cdouglas,21/Jul/09 02:09,24/Aug/10 21:14,12/Jan/21 09:52,16/Sep/09 06:41,,,,,,0.21.0,,,benchmarks,,,,,,0,,,,,"Previous benchmarks ( HADOOP-2369 , HADOOP-3770 ), while informed by production jobs, were principally load generating tools used to validate stability and performance under saturation. The important dimensions of that load- submission order/rate, I/O profile, CPU usage, etc- only accidentally match that of the real load on the cluster. Given related work that characterizes production load ( MAPREDUCE-751 ), it would be worthwhile to use mined data to impose a corresponding load for tuning and guiding development of the framework.

The first version will focus on modeling task I/O, submission, and memory usage.",,aaa,acmurthy,aw,cutting,guanying,hammer,hong.tang,jothipn,matei,nidaley,sumanshg,tanjiaqi,tomwhite,yhemanth,zhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/09 21:02;cdouglas;M776-1.patch;https://issues.apache.org/jira/secure/attachment/12419402/M776-1.patch","15/Sep/09 03:47;cdouglas;M776-2.patch;https://issues.apache.org/jira/secure/attachment/12419603/M776-2.patch","15/Sep/09 08:17;cdouglas;M776-3.patch;https://issues.apache.org/jira/secure/attachment/12419612/M776-3.patch","15/Sep/09 18:31;cdouglas;M776-4.patch;https://issues.apache.org/jira/secure/attachment/12419661/M776-4.patch","15/Sep/09 23:44;cdouglas;M776-5.patch;https://issues.apache.org/jira/secure/attachment/12419708/M776-5.patch","22/Jul/09 10:26;cdouglas;MR776-0.patch;https://issues.apache.org/jira/secure/attachment/12414203/MR776-0.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,2009-09-15 06:29:00.546,,,false,,,,,,,,,,,,,,,,,,149037,Reviewed,,,,Thu Sep 17 06:14:12 UTC 2009,,,,,,,"0|i0jejj:",111286,,,,,,,,,,,,,,,,,,,,,"21/Jul/09 02:32;cdouglas;*Version 1*

The following is a list of features we plan to develop that distinguish it from its predecessors:

* Workload is generated from job history trace analyzer Rumen ( MAPREDUCE-751 ).
* Model the details of IO workload:
** Much larger working set.
** Input and output data volumes for every task.
** Input and output record counts for every task.
* Model the details of memory resource usage.
* Model the job submission rates

The main purpose of Gridmix is to evaluate MapReduce and HDFS performance. It will not, and is not intended to, capture improvements to layers on top of MapReduce. As its predecessors were, Gridmix will be principally a job submission client (though collecting high-level metrics- as was attempted in Gridmix2- would be a natural extension) will be developed in several stages. A usable and useful V1.0 of the submitting client must satisfy the following requirements in each functional categories listed below.

*Simplifying Assumptions*

The context for the following list is unpacked in subsequent sections. Briefly, the job properties that will not be closely reproduced:

* _CPU usage_. We have no data for per-task CPU usage, so we cannot attempt even an approximation (e.g. tasks should never be CPU bound, though this surely happens in practice).
* _Filesystem properties._ We will make no attempt to match block sizes, namespace hierarchies, or any property of input, intermediate, or output data other than the bytes/records consumed and emitted from a given task.
* _I/O rates._ Related to CPU usage, the rate at which a given tasks consumes/emits records is assumed to be 1) limited only by the speed of the reader/writer and 2) constant throughout the job.
* _Memory profile._ No information on tasks' memory use is available, other than the heap size requested. Given data on the frequency and size of allocations, a more thorough profile could be compiled and reproduced.
* _Skew._ We assume records read and output by a task follow observed averages. This is optimistic, as fewer buffers will require resizing, etc. Further, we assume that each map generates a proportional percentage of each reduce's input.
* _Job failure._ We assume that framework and hardware faults are the only cause of task failure, i.e. user code is correct.
* _Job independence._ The output or outcome of one job does not affect when or whether another job will run.

*Components*

* *Data Generation.* When run on a cluster without sufficient data- a common case for testing new versions of Hadoop- the submitting client should have a data generation phase to populate the cluster with a configurable volume of data.
** _Reasonable distribution._ We do not yet have sufficient data to generate a simulated block map. In the future, we may discover that this requires some processing of/from the Rumen trace to write data similar to that of the actual jobs. For now, we require only that the input data are distributed more-or-less evenly across the cluster, to avoid artifical hot-spots in the Gridmix data.
** _Content._ Since the map will employ a trivial binary reader, there are no constraints on the generated data. Future versions of Gridmix could produce compressible or compressed data- or even mock data to be consumed by a non-trival reader- but the only requirement we impose in V1.0 is that the generated data are random (i.e. not trivially compressible).
** _Namespace._ The generated data will make no attempt to emulate the block size, file hierarchy, or any other property of the data over which the user jobs originally ran. The generated data will be written into a flat namespace.
* *Generic MR job.* Gridmix must have a generic job to effect the workloads in the trace it receives from Rumen. For the first version, our focus will be on reproducing I/O traffic and memory.
** _Task configuration._ The number of maps and reduces must match the Rumen trace description.
** _Memory usage._ Lacking explicit data on actual memory usage in task JVMs, we assume that each task will consume and use most of the heap allocated to it. This is a pessimistic estimate- since it is expected that most users accept the default setting and use considerably less- but it will be predictable.
** _Map fidelity._ Each map will read the number of records and bytes specified in the Rumen trace. It is assumed that the map will output its records at an even rate, so records will be output at a rate from the map- per input record- that will effect the correct number of output records. Without data on the record read rate (and CPU usage), we cannot simulate effort expended per record, but the map itself will do some nominal work. We do not control block size, so if the data read in the original user job had a different block size, then the percentage of remote and local reads will likely be affected.
** _Shuffle fidelity._ We do not have sufficient data to describe skewed task output, e.g. a job with more than one reduce, such that at least one reduce receives most of its input from a single map task. Instead, we will assume that each map generates a percentage of bytes and records equal to that received by a given reduce. For example, if reduce r receives i% of the input records and j% of the bytes in the shuffle, then map m will output i% of its output records such that j% of its bytes go to reduce r. This will, with some error, match the input record and byte counts to each reduce. Note that combiners in users jobs will affect byte and record counts we consider, but no combiner will be run. Whether Rumen reports the shuffled bytes/records as the map output- or the bytes output from the map prior to the combiner- will not be distinguished by the Gridmix submission client.
** _Reduce fidelity._ Each reduce will- by definition- receive the set of bytes and records assigned to it from each map. As in the map, it will output at a constant rate, relative to the input records, such that it will output the correct number of records and bytes as described in the Rumen trace.
* *Driver Program.*
** _Input parameters._ The Gridmix client will accept a Rumen trace (either from a URI or stdin) of jobs, work directory, and (optional) the size of the data to be generated. One data generating task will be started per TT node; each will generate the same amount of data.
** _Submission._ Given that Gridmix is simulating a set of submitting clients, it must be possible to submit jobs concurrently in case split generation would otherwise cause a deadline to be missed. (Un)Fortunately, scalability limitations at the JobTracker impose an upper bound on the submission rate the client can effect. Errors in submission must be reported at the client.
** _Cancellation and shutdown._ It should be possible to cancel a run of Gridmix at any time without leaving jobs running on the cluster. Gridmix must make a best-effort to kill any running jobs it has started before exiting through a shutdown hook.
** _Monitoring._ Though V1.0 will not attempt to monitor job dependencies, and will merely submit each job at its appointed deadline, Gridmix must monitor the jobs it has submitted and perform success/failure callbacks for every completed job. For V1.0, it is assumed that Rumen will provide all necessary intelligence for processing the run at the JobTracker.

*Future Work*

There are a number of ideas and approaches we elect to defer until either the need is demonstrated or more time is available for implementation. Among them:

* _Job throttling._ Rather than attempt any sort of backoff at the client in response to JobTracker load, we take the position that a trace is an inviolate submission profile. This allows us to make comparisons between two runs of the same trace. However, if this is to replace GridMix and GridMix2 as the de facto load generator or if one wants to throttle submission on evidence that it interferes with some unrelated measurement, such throttling may become attractive.
* _Compression._ The compressibility of input, intermediate, and output data can be mined from task logs. It would be possible to generate data that would roughly matched the observed ""compressibility"" of user jobs, but this would require a targeted effort.
* _Designed jobs._ Given that MapReduce has a range of widely used libraries- whose development is a considerable portion of JIRA traffic- tracking improvements to this core functionality in proportion to their prevalence in user jobs would aid in identifying bottlenecks in the application layer. Support for specifying InputFormats, OutputFormats, and even commonly used Mappers and Reducers would augment the synthetic mix.
* _Failure._ Gridmix assumes that failures are caused by hardware or flaws in the framework, so it makes no attempt to inject failures into a particular run. Since this does not match some of the preliminary insights into the causes of task failure, it may be argued that the Rumen trace should include- and Gridmix should honor- application failures. Tracing where in the input the failure occurs- and which component fails- would be a way to characterize and measure approaches to mitigating resource waste. In particular, one may improve the proportion of ""useful"" work- and thus throughput- with a better strategy for punishing/throttling jobs that will ultimately fail.
* _Job dependencies._ Modeling dependencies between jobs would remove a lower bound on job throughput by starting dependent jobs immediately once their prerequisites have been met. In general, one can imagine a submission client that would accept a list of prerequisites of which a submission time is merely the most common.
* _Isolation._ We assume that Gridmix ""owns"" the cluster it's running on, i.e. the only jobs running on the cluster are those submitted by an instance of the client. Supporting load generation alongside user tasks would be another use for throttling job submission (and a possible alternative to very fine simulations of specific user tasks).","22/Jul/09 10:26;cdouglas;_Extremely_ preliminary shell of a nascent patch-in-progress using mocked Rumen traces for testing. Mostly experiments and scaffolding tagged with TODO items at this point, but the driver works and the I/O is correct through the shuffle within expected tolerances.","12/Sep/09 21:02;cdouglas;Updated patch. Processes Rumen job traces, satisfies the preceding save for the memory modeling.","15/Sep/09 03:47;cdouglas;Patch for review.","15/Sep/09 06:29;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419603/M776-2.patch
  against trunk revision 814749.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 8 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/77/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/77/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/77/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/77/console

This message is automatically generated.","15/Sep/09 08:17;cdouglas;Updated based on feedback from Hong","15/Sep/09 08:26;hong.tang;Patch looks good. +1.","15/Sep/09 17:09;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419612/M776-3.patch
  against trunk revision 815249.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/79/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/79/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/79/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/79/console

This message is automatically generated.","15/Sep/09 18:31;cdouglas;Fixed classpath issues on Hudson caused by build file modifications.","15/Sep/09 23:06;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419661/M776-4.patch
  against trunk revision 815249.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/81/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/81/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/81/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/81/console

This message is automatically generated.","15/Sep/09 23:44;cdouglas;Missed the Mini\*Cluster dependencies","16/Sep/09 05:17;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419708/M776-5.patch
  against trunk revision 815483.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/84/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/84/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/84/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/84/console

This message is automatically generated.","16/Sep/09 06:41;cdouglas;I committed this.","16/Sep/09 07:07;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #42 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/42/])
    . Add Gridmix, a benchmark processing Rumen traces to simulate
a measured mix of jobs on a cluster.
","16/Sep/09 15:29;hudson;Integrated in Hadoop-Mapreduce-trunk #84 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/84/])
    . Add Gridmix, a benchmark processing Rumen traces to simulate
a measured mix of jobs on a cluster.
","17/Sep/09 06:14;iyappans;Hi Douglas,

I have created  a Jira MAPREDUCE-988. 

ant package does not copy the 
hadoop-0.21.0-dev-capacity-scheduler.jar under HADOOP_HOME/build/hadoop-mapred-0.21.0-dev/contrib/capacity-scheduler/.

it was doing it till 15 of September. Since you have a  code change in build.xml. can you look at it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add input/output formatters for Vertica clustered ADBMS.,MAPREDUCE-775,12430970,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,otrajman,otrajman,otrajman,20/Jul/09 23:34,24/Aug/10 21:14,12/Jan/21 09:52,18/Sep/09 18:24,,,,,,0.21.0,,,contrib/vertica,,,,,,0,,,,,"Add native support for Vertica as an input or output format taking advantage of parallel read and write properties of the DBMS.
 
On the input side allow for parametrized queries (a la prepared statements) and create a split for each combination of parameters.  Also support the parameter list to be generated from a sql statement.  For example - return metrics for all dimensions that meet criteria X with one input split for each dimension.  Divide the read among any number of hosts in the Vertica cluster.
 
On the output side, support Vertica streaming load to any number of hosts in the Vertica cluster.  Output may be to a different cluster than input.
 
Also includes Input and Output formatters that support streaming interface.

Code has been tested and run on live systems under 19 and 20.  Patch for 21 with new API will be ready end of this week.  
",,aaa,acmurthy,anty,christophe@cloudera.com,cutting,dhruba,hammer,he yongqiang,kevinweil,kimballa,matei,omalley,philip,schubertzhang,tomwhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/09 12:14;otrajman;MAPREDUCE-775.2.patch;https://issues.apache.org/jira/secure/attachment/12419380/MAPREDUCE-775.2.patch","12/Sep/09 20:35;otrajman;MAPREDUCE-775.3.patch;https://issues.apache.org/jira/secure/attachment/12419401/MAPREDUCE-775.3.patch","13/Sep/09 14:41;otrajman;MAPREDUCE-775.4.patch;https://issues.apache.org/jira/secure/attachment/12419420/MAPREDUCE-775.4.patch","04/Aug/09 20:52;otrajman;MAPREDUCE-775.patch;https://issues.apache.org/jira/secure/attachment/12415514/MAPREDUCE-775.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2009-07-21 22:30:54.629,,,false,,,,,,,,,,,,,,,,,,149036,Reviewed,,,,Fri Sep 18 22:03:11 UTC 2009,,,,,,,"0|i0jejb:",111285,Add native and streaming support for Vertica as an input or output format taking advantage of parallel read and write properties of the DBMS.,,,,,,,,,,"vertica, db, formatter",,,,,,,,,,"21/Jul/09 22:30;omalley;I'd suggest putting this in a contrib module rather than the main source tree.","24/Jul/09 05:50;otrajman;src/contrib/vertica and  package org.apache.hadoop.vertica?","24/Jul/09 17:15;kimballa;+1.","25/Jul/09 12:45;omalley;+1","27/Jul/09 04:26;otrajman;Patch introduces Vertica optimized input and output formatters.  Includes unit tests (requires vertica jdbc drivers and database) and example as unit test.

We can provide software and license for anyone wanting to run the unit test.  All bindings are reflected so the driver is not required to compile.","03/Aug/09 11:44;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12414581/MAPREDUCE-775.patch
  against trunk revision 800232.

    -1 @author.  The patch appears to contain 1 @author tags which the Hadoop community has agreed to not allow in code contributions.

    +1 tests included.  The patch appears to include 15 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 204 release audit warnings (more than the trunk's current 203 warnings).

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/435/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/435/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/435/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/435/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/435/console

This message is automatically generated.","03/Aug/09 16:37;acmurthy;Most likely one of the newer files in the patch is missing the Apache License header...","03/Aug/09 19:33;otrajman;SQL test script missing the header, eclipse codegen put @author in and I fat fingered the AllTest.  Will fix/retest/repatch ASAP.","04/Aug/09 20:52;otrajman;Patch to address reported issues including missing license in sql init script and conditional unit tests that won't fail if there's no jdbc driver.","04/Aug/09 20:54;otrajman;take 2","24/Aug/09 21:27;otrajman;Fixing issues with new patch.  I seem to have replaced the original instead of adding a .N.patch - sorry for the confusion.","25/Aug/09 05:10;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12415514/MAPREDUCE-775.patch
  against trunk revision 807165.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 16 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/511/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/511/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/511/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/511/console

This message is automatically generated.","25/Aug/09 09:16;otrajman;Looks like test failures are existing and not related to the patch.  Can anyone more familiar with core validate?","08/Sep/09 23:39;kimballa;Omer, 

This looks like a great start. I've read through the patch and believe I understand most of how it works. I don't have any major architectural concerns, but there are a number of style issues that I think should be addressed before this is committed. All of these are outlined below. Comments are listed in the sequential order presented by your patch file.

(As for your question about test failures, build #511 has already been deleted by Hudson, so I can't check that.)

ivy.xml:
Do you actually depend on hsqldb?


Hadoop test classes typically go in the same package as that which they test (e.g., {{o.a.h.vertica}}), not a separate package like {{o.a.h.vertica.tests}}. This would save you a lot of imports in tests, and allows package-public things to be used for testing. (This applies to all your test classes)

AllTests.java
* The method name {{setUp()}} has special meaning in JUnit. Since your {{setup()}} method isn't a {{setUp()}}, can you change this to something less misleadingly-similar?
* In test description string in suite(), o.a.h.vertica, not o.a.h.sqoop.

* re your ""TODO: figure out jdbc jar packaging:"" Based on Hadoop source tree style, I recommend creating a {{src/contrib/vertica/lib}} directory, check any external jars you need in there, and modify {{src/contrib/vertica/build.xml}} to include the jars from that dir on the classpath for building, testing, etc. Since patches are text not binary, you should attach the jar to the JIRA issue separately. Note that adding a jar requires its license be A2-compatible. See 
[http://www.apache.org/legal/resolved.html#category-a] for a list of licenses which external dependencies may have applied to them.

TestExample.Reduce.setup(): I suggest that {{AllTests.setup();}} go in a static initializer block in {{TestExample}} rather than getting called in every {{Reduce.setup()}} call. Given that you actually require {{AllTests.setup()}} in virtually all your tests, I would suggest creating a {{VerticaTestCase}} class that subclasses {{TestCase}}, have this class call {{AllTests.setup()}} in a static initializer block, and then have all your {{Test\*}} classes subclass {{VerticaTestCase}} instead of {{TestCase}}. This way you won't worry about missing a call somewhere.

Also in this same method, why catch {{Exception e}} and print its stack trace? If {{Reduce.setup()}} fails for an exception, why shouldn't the whole test fail?

TestExample.Reduce.reduce(): Style nit: One-line {{if}} statements should still use curly-braces around the ""then"" clause. See [http://java.sun.com/docs/codeconv/html/CodeConventions.doc6.html#449]. Hadoop source should follow all Sun Java style conventions except for an indentation width of two spaces.

I don't think {{TestExample}}, etc, should have a {{run()}} method.

TestVertica.testVerticaRecord(): why are values of {{DATE}}, {{TIME}}, etc, commented out? Dead code should be removed, not commented out. Also, why catch the IOException and return? Why doesn't this method just throw IOException (and implicitly fail the test)?

In {{recordTest()}}, don't use ""assert values.equals(new_values)"", use JUnit: {{assertEquals(""failure message"", values, new_values);}}

Same with {{testVerticaSplit()}}, {{validateInput()}}, etc...


VerticaStreamingRecordWriter.java: Please use Java ""lowerCamelCase"" style for field and variable names, not ""under_scores"" (see {{writer_table}}, {{copy_stmt}}, etc. These should be {{writerTable}} and {{copyStmt}} respectively.)

In the constructor, the RuntimeException description {{""Vertica Formatter requies a the Vertica jdbc driver""}} contains a bunch of typos.

close() method: if statement should use curly-brace style described above.

write() method: Materializing {{record.toString()}} in {{LOG.debug()}} for every call to write is expensive. Consider wrapping this statement in a call to {{LOG.isDebugEnabled()}}.

VerticaConfiguration: comment above definition of {{DELIMITER}} has typos.


{{(input_query.charAt(input_query.length() - 1) == ';'}} ... perhaps {{inputQuery.endsWith(';');}} ?

{{getInputParameters()}} has meaningless javadoc attributes. See also getInputDelmiter() (which is a typo'd method name), setInputDelimiter(), etc... that all have empty {{@return}} attributes.

VerticaInputFormat: {{DateFormat}} is not thread-safe. {{datefmt}} should not be a static member.

This class also has a lot of {{under_score}} field and parameter names.

Can your javadoc comment for {{optimize()}} suggest when it is appropriate to call this, vs. when you would be better off not doing so? What's the heuristic a programmer should keep in mind?

This method also contains a lot of commented-out code. Please remove it entirely.

{{conn.wait(1000);}} should pull out {{1000}} into a static final constant, or even better, make it configurable.

VerticaRecordWriter.getValue() has hairy braces in an if..else statement. (You do this in {{write()}} as well.)

Also, what happens in the case where {{writer_table.split()}} returns a 0-length array? In this same method, can you please add a comment explaining why you're pulling {{rs.getString(4)}} and {{rs.getInt(5)}}? These seem arbitrary as-written.

VerticaInputSplit.executeQuery() has javadoc typos.

VerticaUtil uses tabs instead of spaces, and includes empty lines with leading whitespace. Various block statements and curly braces also require reformatting here, as well as {{variable_names}}.

VerticaRecord constructor has meaningless javadoc attributes.
Also, please obey 80-column limit in this class (as well as elsewhere).

in {{objectTypes()}}, why not use {{else if}} statements instead of just a series of {{if}} statements? You could then drop all the {{continue}} statements which make for awkward flow. Also, include a case at the end for unknown type where you throw an exception, rather than misalign the {{types}} ArrayList from the {{values}} ArrayList.

{{toSQLString()}}: Please do not start variable names with underscore. I suggest {{myDelimiter}} to differentiate it from {{delimiter}}.

Also, are fall-thrus in the case block intentional? If so, please mark this with a comment. 
","12/Sep/09 04:28;otrajman;Aaron - thank you for the excellent and thorough comments.  All accepted and fixed.","12/Sep/09 04:28;otrajman;Updated patch addressing Aaron's feedback.","12/Sep/09 04:46;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419368/MAPREDUCE-775.2.patch
  against trunk revision 813944.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 19 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/68/console

This message is automatically generated.","12/Sep/09 12:13;otrajman;whoops...wrong base dir.  replacing with same patch from up level up","12/Sep/09 12:14;otrajman;Still v2 patch but created from root dir","12/Sep/09 14:33;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419380/MAPREDUCE-775.2.patch
  against trunk revision 814122.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 19 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The patch appears to cause tar ant target to fail.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/69/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/69/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/69/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/69/console

This message is automatically generated.","12/Sep/09 20:34;otrajman;need to fix issue in ivy.xml","12/Sep/09 20:35;otrajman;close tag in ivy.xml","12/Sep/09 20:35;otrajman;crossing fingers","12/Sep/09 23:15;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419401/MAPREDUCE-775.3.patch
  against trunk revision 814122.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 19 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 221 release audit warnings (more than the trunk's current 220 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/70/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/70/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/70/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/70/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/70/console

This message is automatically generated.","13/Sep/09 14:41;otrajman;forgot apache license header in new fine","13/Sep/09 14:41;otrajman;Fourth time's a charm?","13/Sep/09 17:19;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419420/MAPREDUCE-775.4.patch
  against trunk revision 814122.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 19 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/71/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/71/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/71/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/71/console

This message is automatically generated.","14/Sep/09 16:50;kimballa;Thanks for making those changes. Looks good; +1.","17/Sep/09 21:55;dhruba;I will commit it later tonight if nobody gets to this one before that time.","18/Sep/09 18:24;ddas;I just committed this. Thanks, Omer!","18/Sep/09 22:03;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #57 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/57/])
    . Add native and streaming support for Vertica as an input or output format taking advantage of parallel read and write properties of the DBMS. Contributed by Omer Trajman.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configuration information should generate dump in a standard format.,MAPREDUCE-768,12430644,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,chaitk,rksingh,rksingh,16/Jul/09 08:25,24/Aug/10 21:14,12/Jan/21 09:52,25/Aug/09 14:12,,,,,,0.21.0,,,,,,,,,0,,,,, We need to generate the configuration dump in a standard format .,,acmurthy,chaitk,cutting,vinodkv,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/09 10:02;chaitk;MAPREDUCE-768-1.patch;https://issues.apache.org/jira/secure/attachment/12417247/MAPREDUCE-768-1.patch","21/Aug/09 11:00;chaitk;MAPREDUCE-768-2.patch;https://issues.apache.org/jira/secure/attachment/12417253/MAPREDUCE-768-2.patch","24/Aug/09 09:33;chaitk;MAPREDUCE-768-3.patch;https://issues.apache.org/jira/secure/attachment/12417458/MAPREDUCE-768-3.patch","25/Aug/09 04:40;chaitk;MAPREDUCE-768-4.patch;https://issues.apache.org/jira/secure/attachment/12417567/MAPREDUCE-768-4.patch","25/Aug/09 08:38;chaitk;MAPREDUCE-768-5.patch;https://issues.apache.org/jira/secure/attachment/12417585/MAPREDUCE-768-5.patch","25/Aug/09 07:52;chaitk;MAPREDUCE-768-5.patch;https://issues.apache.org/jira/secure/attachment/12417582/MAPREDUCE-768-5.patch","25/Aug/09 09:59;chaitk;MAPREDUCE-768-6.patch;https://issues.apache.org/jira/secure/attachment/12417593/MAPREDUCE-768-6.patch","25/Aug/09 10:52;chaitk;MAPREDUCE-768-7.patch;https://issues.apache.org/jira/secure/attachment/12417599/MAPREDUCE-768-7.patch","31/Aug/09 13:07;chaitk;MAPREDUCE-768-ydist-1.patch;https://issues.apache.org/jira/secure/attachment/12418142/MAPREDUCE-768-ydist-1.patch","25/Aug/09 13:51;chaitk;MAPREDUCE-768-ydist.patch;https://issues.apache.org/jira/secure/attachment/12417616/MAPREDUCE-768-ydist.patch","11/Aug/09 10:11;chaitk;MAPREDUCE-768.patch;https://issues.apache.org/jira/secure/attachment/12416184/MAPREDUCE-768.patch","25/Aug/09 10:50;chaitk;commands_manual.pdf;https://issues.apache.org/jira/secure/attachment/12417598/commands_manual.pdf","25/Aug/09 08:11;chaitk;jobtracker_configurationdump.txt;https://issues.apache.org/jira/secure/attachment/12417583/jobtracker_configurationdump.txt",,,,,,,,,,,,,,,,,,,,,,13.0,,,,,,,,,,,,,,,,,,,,2009-07-16 17:38:05.435,,,false,,,,,,,,,,,,,,,,,,37373,Reviewed,,,,Mon Aug 31 13:07:15 UTC 2009,,,,,,,"0|i02tbj:",14351,"Provides an ability to dump jobtracker configuration in JSON format to standard output and exits.
To dump, use hadoop jobtracker -dumpConfiguration
The format of the dump is {""properties"":[{""key"":<key>,""value"":<value>,""isFinal"":<true/false>,""resource"" : <resource>}] }",,,,,,,,,,,,,,,,,,,,"16/Jul/09 08:56;rksingh;Currently configuration keys can be  in following category based on what state of there values are being used

1.final (list of configuration keys  which cant be changed)
2.overridden(list of configuration keys which are overridden at runtime)
3.default (keys whose default values are being used)
4.All


So there is a requirement to generate configuration dump based on above categories , and in a standard format.


","16/Jul/09 17:38;acmurthy;Use case?","17/Jul/09 10:43;rksingh;If users give improper key they ran into issues and it takes long time to understand the behaviour
for example: instead of using ""a.b"" as key if  ""a.p"" is used , this lead to using ""a.b"" 's default value , and it takes significant time to find out whats happening. 

If there is a need for automation , there would be requirement that the dump should be in standard format.","17/Jul/09 11:05;rksingh;In addition to the comment above:

The new configuration dump would help user know what values they have set wrong or what values are being used .

","28/Jul/09 05:56;rksingh;Proposal:

- Generate a configuration dump  in JSON format.
- Dump consists of key,value and final flag.

Good to have :

- Information regarding the resource or a filename that  a given value came from
  or mark it unknown.","31/Jul/09 06:14;chaitk;*Proposal:*

- Create a class which can dump the configuration parameters in desired format (xml/JSON).
- Provide storage for the information regarding the resource, that sets the key most recently, in Configuration object as key to resource mapping. Storage of this information is done *only* when user wants to get the configuration dump.
- The pattern of dump would be key, value, final flag and resource that sets the key most recently.","10/Aug/09 06:36;chaitk;The following is the proposal for the structure of Json Object:

{ ""Properties"" :
 [ 

{""Key"" : <key>, ""Value"":<value>, ""isFinal"":<true/false>, ""Resource"" : <resource>}
 ]  
}

||JsonObject||
|Property[] properties|

||Property||
|Key|
|Value|
|isFinal|
|Resource|

The Object contains a list of properties with each element of the array being a property with attributes as *Key*,*Value*,*isFinal* and *Resource*. 
","10/Aug/09 19:41;cutting;What values can the ""Resource"" field take?  What fields of a property are required?  Should we permit a ""description"" field?","11/Aug/09 02:58;sreekanth;Resource field of the configuration property is the last resource from which the properties value has been loaded. The motivation of this field would be for administrators to know, for if they have accidentally overridden any property they didn't mean to.","11/Aug/09 06:47;rksingh;The current motivation is to allow administrator to have look at the configuration , as errors in configuration have evaded detection for long. The description field would add more verbosity to the information. 

The idea here is that adminitrators would use this json dump format and would write there own validators. Not sure if description would be of much use in those scenarios.","11/Aug/09 10:11;chaitk;uploaded patch which handles the output of various properties in json format to standard output stream.
It requires patch related to JIRA: HADOOP-6184","11/Aug/09 10:16;chaitk;The users can enable the output of configuration properties in json format by setting an environment variable. For example, in mapred-config.sh, one can keep a variable *mapred.jobtracker.dumpconfiguration* to true in order to get the json string of properties printed into the standard output stream.","11/Aug/09 10:32;chaitk;Example for above comment:
in mapred-config.sh:

export HADOOP_JOBTRACKER_OPTS=""-Dmapred.jobtracker.dumpconfiguration=true","21/Aug/09 10:02;chaitk;Uploading patch with the JobTracker.java and QueueManager.java modified in order to have provision to print the configuration properties in json format.","21/Aug/09 10:21;sreekanth;Took a look at the patch:

* Extract the printing of usage into a new method.
* Change the usage string to ""JobTracker [-dumpConfiguration]""
* Change the current if else condition in {{JobTracker}} to do the following:
{code}
if args.length == 0
  start jobtracker
else
  if args[1] == ""-dumpconfiguration""
     dump configuration
  else
     print usage
{code}","21/Aug/09 11:00;chaitk;uploading an updated patch with the above points considered. Since we are printing the usage only once, it is not being extracted into a different method.
The other two suggestions are implemented.","21/Aug/09 11:05;sreekanth;The changes in the patch look fine to me.
+1 to patch.","21/Aug/09 16:28;steve_l;in Configuration Management, getting a dump of resolved values is part of the ""preflight"" process; checking all is well. The more validation you can get off before you go, the better.

# Because Config can pull in JVM properties, you do need to do the expansion on the host that is using the configuration.
# It seems sensible to make this a general purpose Tools option,, print my config to stdout, so that anyone using any tool can see the values
# It's also handy to be able to ask a remote service endpoint for their config -any node, master or slave, should be able to serve up the config to someone it trusts. Which introduces one small problem -only users with admin rights should be allowed to see the configurations, in case they contain passwords or other sensitive topics.","24/Aug/09 09:10;yhemanth;bq. Because Config can pull in JVM properties, you do need to do the expansion on the host that is using the configuration.

The current scope of this JIRA is to do the dump on the host that is using the configuration. Hence, this is covered in HADOOP-6184.

bq. It seems sensible to make this a general purpose Tools option,, print my config to stdout, so that anyone using any tool can see the values
bq. It's also handy to be able to ask a remote service endpoint for their config -any node, master or slave, should be able to serve up the config to someone it trusts. Which introduces one small problem -only users with admin rights should be allowed to see the configurations, in case they contain passwords or other sensitive topics.

These two are good points and I think we should do them as incremental work. I recommend we think about it filing another JIRA for the same after this goes in.","24/Aug/09 09:32;yhemanth;I think we need a new patch, because the one on the jira currently is not applying.

But I briefly looked at the patch, and can think of a few minor comments:

- I think JobTracker.dumpConfiguration should not take JobConf as a parameter. It should create one inside the call.
- Similarly, QueueManager.dumpConfiguration should also not take a JobConf. Further, it should not load the default resources, because otherwise, the JobTracker's configuration would get dumped twice.
","24/Aug/09 09:33;chaitk;The patch is not compatible with the recent updates in mapreduce. Uploading patch with this issue resolved.","24/Aug/09 09:39;chaitk;Also, the above points mentioned by Hemanth are taken care of in the new patch uploaded.","25/Aug/09 03:23;yhemanth;Javadocs out of sync in both the APIs JobTracker.dumpConfiguration and QueueManager.dumpConfiguration. Other than that, +1.","25/Aug/09 04:40;chaitk;uploading patch with javadoc corrected.","25/Aug/09 07:52;chaitk;the JobTracker.dumpConfiguration has a System.out.println() which should actually be writer.write(""\n""). Uploading patch with this correction.","25/Aug/09 07:56;chaitk;ran test-patch and result is +1 for all except test cases. Since it is a new startup parameter that is being introduced for the jobtracker, it has been manually verified.

The result of test patch is :

+1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no new tests are needed for this patch.
     [exec]                         Also please list what manual steps were performed to verify this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec] ","25/Aug/09 08:11;chaitk;uploading the file that contains the configuration dump when -dumpConfiguration option is given.","25/Aug/09 08:38;chaitk;re-uploading the patch for hudson to pick it up.","25/Aug/09 09:23;yhemanth;+1 for code changes. Trying hudson.","25/Aug/09 09:27;chaitk;All the tests passed locally, i.e., for contrib and core. ","25/Aug/09 09:59;chaitk;The mapred-default.xml contains two properties related to queues, which are also present in QueueManager's xml file. Uploading patch with these properties removed from mapred-default.xml to prevent duplication.","25/Aug/09 10:50;chaitk;uploading the documentation in pdf format.","25/Aug/09 10:52;chaitk;Uploading the patch with changes needed for documentation done.","25/Aug/09 13:19;chaitk;tests and test-patch ran successfully.","25/Aug/09 13:51;chaitk;uploading patch for the internal Yahoo! distribution","25/Aug/09 14:02;yhemanth;Manual tests have been run to verify the command line works as expected. Regression testing has been done with respect to starting the JT without the Command line option and verifying it starts up as usual. Jobs submitted run fine.

On the basis of these tests, I will commit the patch to trunk.","25/Aug/09 14:12;yhemanth;I committed this to trunk. Thanks, Chaitanya !","31/Aug/09 13:07;chaitk;The previous patch for Yahoo! internal distribution seems to be incompatible with the changes made in mapred-default.xml. Uploading patch for Yahoo! internal distribution with this issue resolved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide summary information per job once a job is finished.,MAPREDUCE-740,12429851,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,acmurthy,hong.tang,hong.tang,08/Jul/09 20:36,24/Aug/10 21:14,12/Jan/21 09:52,19/Jul/09 05:06,,,,,,0.21.0,,,jobtracker,,,,,,0,,,,,"It would be nice if JobTracker can output a one line summary information per job once a job is finished. Otherwise, users or system administrators would end up scraping individual job history logs.",,aaa,acmurthy,hammer,jothipn,nidaley,omalley,vinodkv,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jul/09 23:48;acmurthy;MAPREDUCE-740_0_20090709.patch;https://issues.apache.org/jira/secure/attachment/12413173/MAPREDUCE-740_0_20090709.patch","14/Jul/09 07:25;acmurthy;MAPREDUCE-740_0_20090713.patch;https://issues.apache.org/jira/secure/attachment/12413394/MAPREDUCE-740_0_20090713.patch","14/Jul/09 07:25;acmurthy;MAPREDUCE-740_0_20090713_yhadoop20.patch;https://issues.apache.org/jira/secure/attachment/12413395/MAPREDUCE-740_0_20090713_yhadoop20.patch","17/Jul/09 07:14;acmurthy;MAPREDUCE-740_1_20090716.patch;https://issues.apache.org/jira/secure/attachment/12413781/MAPREDUCE-740_1_20090716.patch","17/Jul/09 07:14;acmurthy;MAPREDUCE-740_1_20090716_yhadoop20.patch;https://issues.apache.org/jira/secure/attachment/12413782/MAPREDUCE-740_1_20090716_yhadoop20.patch","19/Jul/09 05:00;acmurthy;MAPREDUCE-740_2_20090717.patch;https://issues.apache.org/jira/secure/attachment/12413940/MAPREDUCE-740_2_20090717.patch","19/Jul/09 05:00;acmurthy;MAPREDUCE-740_2_20090717_yhadoop20.patch;https://issues.apache.org/jira/secure/attachment/12413941/MAPREDUCE-740_2_20090717_yhadoop20.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,2009-07-09 07:41:24.442,,,false,,,,,,,,,,,,,,,,,,37262,,,,,Sat Jul 25 15:45:16 UTC 2009,,,,,,,"0|i02s7j:",14171,"Log a job-summary at the end of a job, while allowing it to be configured to use a custom appender if desired.",,,,,,,,,,,,,,,,,,,,"09/Jul/09 07:41;vinodkv;What kind of summary information are we targeting here? Any specific use cases?","09/Jul/09 20:19;hong.tang;@vinod

I do have a specific usage case where we want to keep track of the amount of resources being used by each job, each user, or each queue (for capacity scheduler). Granted, all these information is readily available in job history log. However, there are a few drawbacks by depending on job history logs: (1) we are interested in keeping a history of finished and possibly do group-by for user and queue. so scrapping individual history log is messy; (2) the added dependency to keep up with possible future changes to the history log format.

For starter, I think the summary should include the following information: 
	- job queuing/waiting time
	- job start time
	- job finish time
	- total maps/reduces
	- user id
	- job id (job-tracker ID + job sequence number)
	- map/reduce slot hours (need to apply multiplier for high ram tasks that take multiple slots per map/reduce task)
	- queue name
	- job status (success or failure)
	- cluster map/reduce slot capacity

The only thing that job history log does not provide currently is the slot hours for all maps and reduces belonging to the same job.","09/Jul/09 23:43;hong.tang;Additionally:
- We should summarize the information in one line in an easy-to-parse format, eg comma separated key=value list.
- We should also specify the number of map slots and reduce slots taken by each map task and reduce task.
- We may want to use a distinctive appender so that the administrator may choose to redirect the output of the summary info.
- The cluster wide capacity of map slots and reduce slots change over time. For now, let's simplify the definition as the map/reduce slot capacity by the time the job finishes.","10/Jul/09 00:09;rajive;bq. # job queuing/waiting time

can we report queued/submit time instead?

bq. # map/reduce slot hours (need to apply multiplier for high ram tasks that take multiple slots per map/reduce task)

This should be reported in sec to avoid rounding off for small jobs.

bq. # cluster map/reduce slot capacity

Why is this required in job accounting context? The metrics system reports mapred system information. 
","10/Jul/09 01:10;hong.tang;bq. can we report queued/submit time instead?
Submit time should be enough. Waiting time is just launch time - submit time.","10/Jul/09 23:48;acmurthy;Straight-forward patch which allows for a new (configurable) appender which can be used to direct job-summary (one line summary per job) to the desired location.","10/Jul/09 23:49;acmurthy;Example log:

{noformat}
09/07/10 16:39:39 INFO mapred.JobInProgress$JobSummary: jobId=job_200907101638_0001,submitTime=1247269137321,launchTime=1247269137920,finishTime=1247269179380,numMaps=10,numSlotsPerMap=1,numReduces=0,numSlotsPerReduce=1,user=arunc,queue=default,status=SUCCEEDED,mapSlotSeconds=39,reduceSlotsSeconds=0,clusterMapCapacity=4,clusterReduceCapacity=4
{noformat}
","11/Jul/09 01:03;hong.tang;+1.  Patch looks good.","11/Jul/09 01:04;rajive;+1 for the log format.","13/Jul/09 06:49;yhemanth;Arun, I remember an issue that job.startTime is not updated on restart correctly, but instead JobStatus.getStartTime is. Can you please check this ? (Maybe Amar will have the context)
Also, pulling JobSummary into a separate class will help unit testing it better. Would that work ?","14/Jul/09 07:24;amar_kamat;bq. I remember an issue that job.startTime is not updated on restart correctly, but instead JobStatus.getStartTime is. Can you please check this ? (Maybe Amar will have the context)
AFAIK JobInProgress.updateJobInfo() is called upon restart to change the start-time info. I think job-status still holds the old/incorrect start-time. job.startTime should be used. ","14/Jul/09 07:25;acmurthy;Updated patch for both trunk and yhadoop-20.","15/Jul/09 08:31;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12413395/MAPREDUCE-740_0_20090713_yhadoop20.patch
  against trunk revision 794101.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/391/console

This message is automatically generated.","15/Jul/09 09:22;yhemanth;Arun chatted offline with me. We decided it's ok to keep JobSummary as it is now. Also, the fix with respect to start time seems fine. I think my points have been addressed.","15/Jul/09 20:39;nidaley;-1.  No unit test or justification.

Should logJobSummary(...) have a null check on job so we don't get NPEs?  Ditto on meterTaskAttempt(..)? 
If you disagree on null check, can you document that input parameters must not be null OR document @throws NullPointerException if input parameter is null.","17/Jul/09 07:04;acmurthy;Cancelling patch to incorporate feedback from Nigel.","17/Jul/09 07:14;acmurthy;Updated patch to incorporate Nigel's feedback about javadocs. Also, I'm not adding any unit test since this patch only adds a an extra line of logging.","17/Jul/09 07:16;acmurthy;Manual testing performed: 
# I've run jobs which have SUCCEEDED, KILLED and FAILED 
# Checked with high-ram jobs to check metering is done correctly","17/Jul/09 18:32;dking;I expect to have a responsive patch to 751, which is the tool we're talking about, sometime 7/20 or 7/21.","19/Jul/09 05:00;acmurthy;Minor modifications necessitated by test cases, 'ant test-patch' and test cases pass.","19/Jul/09 05:06;acmurthy;I just committed this.","19/Jul/09 05:07;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12413941/MAPREDUCE-740_2_20090717_yhadoop20.patch
  against trunk revision 795470.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/411/console

This message is automatically generated.","25/Jul/09 15:45;hudson;Integrated in Hadoop-Mapreduce-trunk #29 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/29/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for FIFO pools in the fair scheduler,MAPREDUCE-706,12429517,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,matei,matei,matei,04/Jul/09 01:35,24/Aug/10 21:14,12/Jan/21 09:52,14/Aug/09 16:32,,,,,,0.21.0,,,contrib/fair-share,,,,,,0,,,,,"The fair scheduler should support making the internal scheduling algorithm for some pools be FIFO instead of fair sharing in order to work better for batch workloads. FIFO pools will behave exactly like the current default scheduler, sorting jobs by priority and then submission time. Pools will have their scheduling algorithm set through the pools config file, and it will be changeable at runtime.

To support this feature, I'm also changing the internal logic of the fair scheduler to no longer use deficits. Instead, for fair sharing, we will assign tasks to the job farthest below its share as a ratio of its share. This is easier to combine with other scheduling algorithms and leads to a more stable sharing situation, avoiding unfairness issues brought up in MAPREDUCE-543 and MAPREDUCE-544 that happen when some jobs have long tasks. The new preemption (MAPREDUCE-551) will ensure that critical jobs can gain their fair share within a bounded amount of time.",,aaa,aah,dhruba,eli,hammer,kimballa,lianhuiwang,matei,qwertymaniac,rksingh,schubertzhang,szetszwo,wenfengbx,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/09 01:43;matei;fsdesigndoc.pdf;https://issues.apache.org/jira/secure/attachment/12413903/fsdesigndoc.pdf","18/Jul/09 01:43;matei;fsdesigndoc.tex;https://issues.apache.org/jira/secure/attachment/12413904/fsdesigndoc.tex","14/Jul/09 23:43;matei;mapreduce-706.patch;https://issues.apache.org/jira/secure/attachment/12413498/mapreduce-706.patch","28/Jul/09 23:54;matei;mapreduce-706.v1.patch;https://issues.apache.org/jira/secure/attachment/12414820/mapreduce-706.v1.patch","30/Jul/09 22:15;matei;mapreduce-706.v2.patch;https://issues.apache.org/jira/secure/attachment/12415064/mapreduce-706.v2.patch","05/Aug/09 00:16;matei;mapreduce-706.v3.patch;https://issues.apache.org/jira/secure/attachment/12415552/mapreduce-706.v3.patch","07/Aug/09 00:47;matei;mapreduce-706.v4.patch;https://issues.apache.org/jira/secure/attachment/12415798/mapreduce-706.v4.patch","13/Aug/09 18:00;matei;mapreduce-706.v5.patch;https://issues.apache.org/jira/secure/attachment/12416467/mapreduce-706.v5.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,2009-07-30 08:12:11.747,,,false,,,,,,,,,,,,,,,,,,37361,,,,,Sat Aug 15 20:00:09 UTC 2009,,,,,,,"0|i02t7z:",14335,Support for FIFO pools added to the Fair Scheduler.,,,,,,,,,,,,,,,,,,,,"14/Jul/09 23:43;matei;Here's a patch adding this feature. The patch includes MAPREDUCE-548 because it depends on that issue.

This patch performs a fairly major restructuring of the fair scheduler to support different scheduling algorithms within each pool. However, the resulting code is simpler, more flexible and more efficient than the current fair scheduler. Many of the changes in the patch are also just changes to unit tests, which were necessary because deficits were removed (affecting behavior slightly in some tests) and the names of some internal data structures changed.

At a high level, the patch changes the scheduler to perform hierarchical scheduling, where a pool is chosen to assign a task to, and then the pool is asked to choose a task from among its jobs. The pool can choose to use either FIFO or fair sharing to do this.

To allow the same fair sharing code to be used across pools and across jobs, the patch introduces a new abstract class called Schedulable that can represent both a job (as JobSchedulable) and a pool (as PoolSchedulable). Schedulables can be asked to assign tasks and can be queried for metrics used in various scheduling algorithms, such as current number of running tasks, demand (number of tasks required), weight, fair share, priority, etc. PoolSchedulables aggregate the metrics from the jobs they contain. There are separate sets of Schedulables for maps and reduces, to let the same code be reused for both task types.

Apart from this large-scale change, the patch includes a few smaller but important changes that simplify the fair scheduler and let existing features to work with the new hierarchical model:
* Deficits are no longer used for fair sharing. Instead, we just assign tasks to the job/pool with the fewest running tasks (scaled by weight). The scheduler originally used deficits because they work well in CPU and packet scheduling and because they let jobs ""catch up"" after temporary unfairness, but we found that this can lead to bad behavior in a system with long tasks like Hadoop (MAPREDUCE-544). In addition, preemption (MAPREDUCE-551) can now be used to ensure jobs aren't starved of their share for too long a time. Deficits also don't make sense with FIFO, so it would have been difficult to use the existing code there. Therefore this patch removes deficits and moves to a simpler form of fair sharing. The new algorithm should also let small jobs start faster, without having to wait to accumulate a deficit.
* A new algorithm based on binary search is used for computing fair shares. Computing weighted fair shares in the presence of minimum shares and maximum shares (due to jobs whose demand is less than their share) is fairly tricky, and the previous algorithm ignlred the low-demand case and ran in quadratic time in the number of jobs. The new algorithm handles these cases, runs in linear time in the number of jobs, and is only about 60 lines of code.
* Preemption now happens at a pool level and not at a job level. Up until now the fair scheduler has contained only per-job data structures, so each job had a minimum share assigned (a fraction of its pool) and kept track of whether it was starved. This could lead to less than the right amount of tasks being preempted if, for example, jobs' min shares rounded down to less than the share of the pool. Per-job minimum shares also can't easily be assigned in FIFO pools. Instead, the new code keeps track of min shares and starvation at the level of pools. This leads to incompatible behavior with MAPREDUCE-551, but that code has not been in any release yet so it should be fine to change it here.

The new fair scheduler structure should make it straightforward to support features such as maximum shares for pools (MAPREDUCE-698), new scheduling algorithms within pools (e.g. shortest job first), and even sub-pools within pools (e.g. an organization pool can contain a sub-pool for each user). Over the next few days, I'll also post a design document detailing the new scheduler structure and algorithms.","18/Jul/09 01:43;matei;I've attached a design document explaining the new organization of the code and the fair scheduler design in general.","28/Jul/09 23:54;matei;Updated patch for current trunk.","30/Jul/09 08:12;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12414820/mapreduce-706.v1.patch
  against trunk revision 799126.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 9 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 210 release audit warnings (more than the trunk's current 203 warnings).

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/427/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/427/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/427/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/427/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/427/console

This message is automatically generated.","30/Jul/09 22:15;matei;I've fixed the release audit warnings by adding Apache license headers to the files in question. The contrib test failures are unrelated to this patch.","31/Jul/09 05:55;kimballa;Matei,

Great documentation -- that really helps! :) Also good that you added a lot of tests. +1 overall on this patch, subject to the following (relatively minor) questions and suggestions:


TestFairScheduler.obtainNewReduceTask():
Task task = new ReduceTask("""", attemptId, 0, maps.length, 1) <-- shouldn't this be ""reduces.length"" ?

TestFairScheduler.getLocalityLevel(): These locality level constants are used throughout the FairScheduler; they should be converted to an Enum. (Magic constants are evil.)

TestComputeFairShares.testEmptyList() -- should this call verifyShares() after computeFairShares() to assert that the list length is zero?

PoolManager.parseSchedulingMode(): why case sensitive 'fifo' and 'fair' ? maybe use toLower() ?

PoolSchedulable c'tor: scheduler.getClock().getTime() should be called only once to guarantee this.lastTimeAtMinShare == this.lastTimeAtHalfFairShare on start?

assignTask(): Is SchedulingMode guaranteed to never be extended by another internal algorithm? If not, turn ""else"" into ""else if"" and have an ""else throw InvalidArgumentException"" at the end of the case.

JobSchedulable.updateDemand(): why does this use System.currentTimeMillis() instead of getting the time from a Clock object?

Schedulable's class javadoc: typo ""algoirthms""

SchuldingAlgorithms.LOG: rather than use a string, use SchedulingAlgorithms.class.getName()

FairScheduler.UpdateThread.run(): why is preemptTasksIfNecessary() commented out? Needs a comment for rationale.

FairScheduler.assignTasks() -- Should convert System.out.println to log msg.

This method is also getting pretty long. Consider refactoring the inner loop into shorter methods if you need to add anything else to it in the future.

getAllowedLocalityLevel():
You have the comment:  // Job not in infos (shouldn't happen)- 
... So throw an exception if it does, or at least log this event with level ERROR, rather than returning an in-bounds value? When you get to switch(info.lastMapLocalityLevel), you'll naturally throw an NPE, so the caller should just deal with that and clean up its own mess.
","05/Aug/09 00:24;matei;Thanks for the review, Aaron! Here is a new patch taking into account your comments:

{quote}
TestFairScheduler.obtainNewReduceTask():
Task task = new ReduceTask("""", attemptId, 0, maps.length, 1) <-- shouldn't this be ""reduces.length"" ?
{quote}

This actually needs to be the number of maps according to the ReduceTask API; it lets it create a data structure for each map.

bq. TestFairScheduler.getLocalityLevel(): These locality level constants are used throughout the FairScheduler; they should be converted to an Enum. (Magic constants are evil.)

Fixed. I added a LocalityLevel enum with methods for getting the locality level of a given task and converting a locality level to a cache level cap for obtainNewMapTask.

bq. TestComputeFairShares.testEmptyList() - should this call verifyShares() after computeFairShares() to assert that the list length is zero?

Fixed.

bq. PoolManager.parseSchedulingMode(): why case sensitive 'fifo' and 'fair' ? maybe use toLower() ?

Fixed.

bq. PoolSchedulable c'tor: scheduler.getClock().getTime() should be called only once to guarantee this.lastTimeAtMinShare == this.lastTimeAtHalfFairShare on start?

Fixed.

bq. assignTask(): Is SchedulingMode guaranteed to never be extended by another internal algorithm? If not, turn ""else"" into ""else if"" and have an ""else throw InvalidArgumentException"" at the end of the case.

Fixed. Throws RuntimeException if there's another mode, which should cause the JobTracker to exit and make it obvious that there's something deeply wrong.

bq. JobSchedulable.updateDemand(): why does this use System.currentTimeMillis() instead of getting the time from a Clock object?

Fixed.

bq. Schedulable's class javadoc: typo ""algoirthms""

Fixed.

bq. SchuldingAlgorithms.LOG: rather than use a string, use SchedulingAlgorithms.class.getName()

Fixed. Also fixed this in PoolSchedulable.

bq. FairScheduler.UpdateThread.run(): why is preemptTasksIfNecessary() commented out? Needs a comment for rationale.

Oops! Fixed. I had commented that out for debugging.

bq. FairScheduler.assignTasks() - Should convert System.out.println to log msg.

Removed the println, it was also for debugging.

bq. This method is also getting pretty long. Consider refactoring the inner loop into shorter methods if you need to add anything else to it in the future.

I've refactored it to take out the delay scheduling code and the cap calculation code. It's now closer to fitting on one screenful.

{quote}
getAllowedLocalityLevel():
You have the comment: // Job not in infos (shouldn't happen)- 
... So throw an exception if it does, or at least log this e
{quote}

Fixed. I log an error so the scheduler doesn't crash if for some reason a bug does cause a job to go through there when it has no info set.

In addition to these fixes, I've added a bit more documentation on the delay scheduling methods in this version of the patch, and I've changed Schedulable.assignTasks to take the current time as a parameter so that we don't have multiple calls to System.currentTimeMillis on each heartbeat (in case that hurts performance).","05/Aug/09 00:41;kimballa;Sounds good! :)","06/Aug/09 08:30;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12415552/mapreduce-706.v3.patch
  against trunk revision 800693.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 9 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/443/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/443/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/443/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/443/console

This message is automatically generated.","06/Aug/09 20:19;matei;The test failures are in streaming and unrelated to this patch.","07/Aug/09 00:47;matei;I've made a few changes to the Web UI to show more / better info:
- Pools now have their scheduling mode and fair shares displayed.
- Jobs in a FIFO pool have their fair share displayed as NA instead of 0.

Attaching a patch with these changes.","08/Aug/09 06:26;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12415798/mapreduce-706.v4.patch
  against trunk revision 801959.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 9 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/456/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/456/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/456/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/456/console

This message is automatically generated.","12/Aug/09 15:39;matei;Contrib test failures are again unrelated.","12/Aug/09 16:51;tomwhite;+1

What testing did you carry out on this?

Agree the documentation is excellent. Can you add it to the source tree?

","13/Aug/09 17:56;matei;I tested the patch on a 50-node EC2 cluster. I tried each of the features individually (FIFO pools, delay scheduling, preemption, etc). I also checked that CPU utilization on the master was low (~1%) even when 50 jobs are simultaneously running.","13/Aug/09 18:00;matei;Here's a new patch that includes the design docs.","13/Aug/09 18:01;matei;I'm going to wait for Hudson to run on this and then commit it if it looks okay.","13/Aug/09 20:46;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12416467/mapreduce-706.v5.patch
  against trunk revision 803583.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 9 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 204 release audit warnings (more than the trunk's current 203 warnings).

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/472/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/472/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/472/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/472/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/472/console

This message is automatically generated.","14/Aug/09 16:32;matei;The test failures are again in unrelated tests, so I've committed this. Thanks for the reviews, Aaron and Tom!","14/Aug/09 22:06;szetszwo;The design doc is very nice (especially, it was typeset by tex)!

Some suggestions for future works:
- In hadoop, fs usually refers to FileSystem.  ""fsdesigndoc"" sounds like FileSystem design doc.  I think we should prevent overloading the term ""fs"".
- The tex file needs a license header.
- We do not have pdf files under ./src before.  ""fsdesigndoc.pdf"" is the first.  I think the correct approach is to generate the pdf file by ""ant docs"".  However, it may not be easy to do so.","15/Aug/09 16:37;matei;I've created MAPREDUCE-878 to rename the design doc and add the license header. With the PDF, I'm worried that many people won't have LaTeX installed, and it will be difficult to write an ant task to compile it, but if having a PDF in there is a problem, I can remove it. It's only 120 KB in size though.","15/Aug/09 20:00;szetszwo;> ... With the PDF, I'm worried that many people won't have LaTeX installed, and it will be difficult to write an ant task to compile it, ...
I agree.  Let's leave the pdf file there for the moment.  We may come up a solution later.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XML-based metrics as JSP servlet for JobTracker,MAPREDUCE-679,12429134,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,kimballa,kimballa,kimballa,30/Jun/09 00:00,24/Aug/10 21:14,12/Jan/21 09:52,18/Sep/09 22:09,,,,,,0.21.0,,,jobtracker,,,,,,0,,,,,"In HADOOP-4559, a general REST API for reporting metrics was proposed but work seems to have stalled. In the interim, we have a simple XML translation of the existing JobTracker status page which provides the same metrics (including the tables of running/completed/failed jobs) as the human-readable page. This is a relatively lightweight addition to provide some machine-understandable metrics reporting.",,aaa,acmurthy,amirhyoussefi,cdouglas,hammer,qwertymaniac,tlipcon,vinodkv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HDFS-453,MAPREDUCE-2818,,,,,,,,,,,,,,,,,,,,"03/Jul/09 23:22;kimballa;MAPREDUCE-679.2.patch;https://issues.apache.org/jira/secure/attachment/12412525/MAPREDUCE-679.2.patch","21/Aug/09 00:45;kimballa;MAPREDUCE-679.3.patch;https://issues.apache.org/jira/secure/attachment/12417204/MAPREDUCE-679.3.patch","28/Aug/09 23:51;kimballa;MAPREDUCE-679.4.patch;https://issues.apache.org/jira/secure/attachment/12418040/MAPREDUCE-679.4.patch","09/Sep/09 23:47;kimballa;MAPREDUCE-679.5.patch;https://issues.apache.org/jira/secure/attachment/12419112/MAPREDUCE-679.5.patch","17/Sep/09 18:10;kimballa;MAPREDUCE-679.6.patch;https://issues.apache.org/jira/secure/attachment/12419911/MAPREDUCE-679.6.patch","18/Sep/09 18:19;kimballa;MAPREDUCE-679.7.patch;https://issues.apache.org/jira/secure/attachment/12420056/MAPREDUCE-679.7.patch","30/Jun/09 00:00;kimballa;MAPREDUCE-679.patch;https://issues.apache.org/jira/secure/attachment/12412106/MAPREDUCE-679.patch","21/Aug/09 00:44;kimballa;example-jobtracker-completed-job.xml;https://issues.apache.org/jira/secure/attachment/12417202/example-jobtracker-completed-job.xml","21/Aug/09 00:45;kimballa;example-jobtracker-running-job.xml;https://issues.apache.org/jira/secure/attachment/12417203/example-jobtracker-running-job.xml",,,,,,,,,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,2009-07-02 16:27:07.907,,,false,,,,,,,,,,,,,,,,,,37283,Reviewed,,,,Tue Jan 12 23:20:32 UTC 2010,,,,,,,"0|i02sdj:",14198,Added XML-based JobTracker status JSP page for metrics reporting,,,,,,,,,,,,,,,,,,,,"30/Jun/09 00:00;kimballa;Initial version of this patch","02/Jul/09 16:27;steve_l;What is your test plan for this? I'd suggest something involving html unit while jobs are pushed up","03/Jul/09 22:20;kimballa;htmlunit isn't the right choice here since I'm not generating HTML. I'll add some tests that start a MiniMRCluster, perform an HTTP GET to retrieve the page, and check for well-formedness of XML results.","03/Jul/09 23:24;kimballa;New patch available. Per a code-review comment on HDFS-453, I've split the helper methods into a separate static class.  Also added a test case which starts a MiniMRCluster and retrieves the JSP page from the appropriate URL. It uses Java's built-in SAX parser to parse the page, assure it is well-formed, and that it contains exactly one {{<cluster>..</cluster>}} block which defines the page of data.","04/Jul/09 02:24;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12412525/MAPREDUCE-679.2.patch
  against trunk revision 790971.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/351/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/351/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/351/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/351/console

This message is automatically generated.","04/Jul/09 07:57;kimballa;Test failures are all unrelated.","13/Aug/09 16:02;steve_l;Same comments as for HDFS-453
# a JSPX page may make it easier for generating XML; especially for ensuring there isn't any whitespace ahead of the xml header declaration, or bad xml inside it.
# caching needs to be turned off here and in all JSP pages as per HDFS-91
# your {{generateJobTable}} code can/should iterate as {{for(JobInProgress job:jobs) instead of Object

Can you attach some example XML, I'd like to look at it a bit more too, see how well it would for for XSL/Xpath navigation","21/Aug/09 00:45;kimballa;Attaching example xml outputs","21/Aug/09 00:45;kimballa;Attaching new patch

* moves /jobtracker.xml.jsp to /jobtracker.jspx
* updates test to go along with it
* updates build.xml to fix bug in webapp compilation.","21/Aug/09 05:37;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12417204/MAPREDUCE-679.3.patch
  against trunk revision 806408.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 2 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/500/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/500/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/500/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/500/console

This message is automatically generated.","21/Aug/09 22:06;kimballa;2 findbugs warnings (dead stores) are in code auto-generated by JspC. I don't think there's anything I can do about that.

The failing unit tests don't seem related.

Adding this patch, now that it uses jspx, adds the following warning:
{code}
[jsp-compile] 09/08/21 14:30:00 WARN compiler.TldLocationsCache: Internal Error: File /WEB-INF/web.xml not found
{code}

This is confusing, since it seems like JspC auto-generates the correct web.xml anyway. Problematic?
","24/Aug/09 13:25;steve_l;TldLocations cache is some cache for globally defined taglibs 

http://tomcat.apache.org/tomcat-5.5-doc/jasper/docs/api/org/apache/jasper/compiler/TldLocationsCache.html

source is here:
http://svn.apache.org/repos/asf/tomcat/tc6.0.x/trunk/java/org/apache/jasper/compiler/TldLocationsCache.java

Looking at the source, the message comes from {{{processWebDotXml()}}}; it doesnt do any harm, except that it doesnt bother parsing any web.xml -defined content if web.xml is nowhere to be found. Its a warning, not an error. 

There is a servlet context property, org.apache.catalina.deploy.alt_dd, which can be used to identify an alternate deployment descriptor, but I have no idea how to set that from command line jspc.

Recommendation: ignore the warning.","24/Aug/09 18:32;kimballa;Good enough. Is this ready to be committed?","26/Aug/09 09:04;cdouglas;The code looks reasonable, save a really minor nit on semicolons:
{noformat}
+            JobTrackerJspHelper.generateJobTable(out, ""running"", tracker.runningJobs());;;;
{noformat}
and DecimalFormat is not threadsafe.

Adding a new metrics interface requires that we maintain it. Particularly if this is an ""interim solution,"" I hesitate to commit it when these metrics are already exported through existing interfaces. I'm not against adding this, but neither am I particularly enthusiastic about adding yet another metrics API. What is this in support of?","26/Aug/09 11:58;steve_l;I actually quite like XML interfaces you can GET as a way of seeing what's going on
* people can read them
* you can directly import them into any spreadsheet that can GET+Xpath a document, including google spreadsheets (if you were to expose your cluster to the world)
* code can get then XSLT them into useful forms
* if the error code is meaningful, you can have routers keep an eye on them. For that to work, you need the ability to set limits in the report (eg. have a query param that specifies the minimum #of nodes you want for the cluster to be considered functional)

And, unlike every other JSP page in the codebase, it has a test. This will reduce maintenance costs, as future patches are less likely to break things.","26/Aug/09 16:50;cdouglas;I'm not arguing against exporting XML APIs- we already do this for listing paths in the NameNode- but I'd like to have a use case before we commit to supporting an interface. The aforementioned is an example of an interim, XML interface: listing for HftpFileSystem, which was added for cross-version copying while we work on better protocols. Which is why I ask: what is this for? ""XML is widely supported and we have a unit test"" is answering a different question.","27/Aug/09 02:37;kimballa;We have clients who want to integrate some basic jobtracker/namenode monitoring and jobtracker history into external metrics-tracking applications. Scraping the HTML off the existing JSP's doesn't cut it -- XML representations of them are considerably more sane. Earlier versions of this patch are already in one-off deployments.

As for the ""interim"" nature of this -- Really, I think the interim is here to stay. HADOOP-4559 has not had any attention since early February, and that was just a one-off comment. No substantive dialogue there since Dec '08.
","28/Aug/09 23:08;cdouglas;Scraping HTML is obviously a poor choice, but there's already a metrics API and several implementations. This would exist outside of that framework and report (IIRC) a subset of the available metrics. Is there some aspect of that API that makes it unsuitable?","28/Aug/09 23:17;kimballa;Chris, to what API are you referring? No API reports job history in a universally-readable manner (e.g., REST + XML).","28/Aug/09 23:51;kimballa;Attaching new patch. Fixes formatting issue. Also makes JobTrackerJspHelper non-static to avoid thread-safety issue with DecimalFormat.","29/Aug/09 00:52;cdouglas;bq. Chris, to what API are you referring?

o.a.h.metrics and o.a.h.mapred.JobTrackerInstrumentation (JobTrackerMetricsInst). Ganglia, Chukwa, etc. push metrics through this interface. Adding a new interface isn't a small change. For example, the current patch calls {{JobTracker::runningJobs}} rather than {{JobTracker::getRunningJobs}}; the former is unsynchronized and accesses shared data structures. There have been similar issues caused by metrics frameworks attempting to pull information out of the JobTracker without an audit of the (regrettable) lock hierarchy. If this receives its data through the metrics API, then neither you nor maintainers need to consider how this servlet affects the shared JT data.

bq. No API reports job history in a universally-readable manner (e.g., REST + XML).

That's fair. Again, I like machine-readable formats and wouldn't object to this even if it were an interim solution, but I want to be clear about how it gets its data and what it supports, since we'll be committing ourselves to maintaining both.

Would JobHistory be a better home for this? Much of the data are not currently available in the web UI, let alone in a reasonable format.","29/Aug/09 11:56;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12418040/MAPREDUCE-679.4.patch
  against trunk revision 808730.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 2 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/538/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/538/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/538/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/538/console

This message is automatically generated.","30/Aug/09 19:22;cdouglas;I'd assumed that this was just pushing metrics, but looking more closely at the patch, it's writing per-job statistics. My comments about using the metrics APIs are invalid.

The unsynchronized calls into the JT should be changed, though it looks like the jobtracker jsp does the same thing. *sigh* This may also be affected by MAPREDUCE-870.","09/Sep/09 23:47;kimballa;As you note, this API exposes nothing that's not already part of jobtracker.jsp. I don't see this as a fundamentally new API, just a new presentation layer for existing data. The same exact hooks are used in both.

I'm attaching a new patch with a thread-safe call to getRunningJobs() instead of runningJobs(). I changed jobtracker.jsp to do the same; also added getCompletedJobs() and getFailedJobs() that use the same synchronzation discipline as getRunningJobs(). Finally, I added a ""retired jobs"" section that conforms to the API of MAPREDUCE-870.

Tested all of this locally by running jobs and checking the output XML for validation.","10/Sep/09 02:17;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419112/MAPREDUCE-679.5.patch
  against trunk revision 813140.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 2 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/21/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/21/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/21/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/21/console

This message is automatically generated.","10/Sep/09 17:49;kimballa;The new test failure is unrelated.","16/Sep/09 09:03;cdouglas;* This adds {{generateRetiredJobXml}} to JSPUtil to use the package-private ""mechanism"" for obtaining retired jobs from {{generateRetiredJobTable}}, right? I suppose it's unfair to hold this to a higher standard than the tracker page, but building a buffer of up to 100 jobs in XML before writing it out is not ideal. Could this be a little more sparing of memory by passing the Writer to the utility function?
* It may not matter given the preceding, but StringBuilder is preferred over StringBuffer in a single-threaded context
* This should use a real XML writer instead of writing the tags as string literals. xmlenc is used in some of the existing code. It might be better- since the data are all key=value pairs not nested structures- if these were expressed less verbosely. On reflection, wouldn't it make sense to just let each item in a {{<%s_jobs>}} be a {{job}} instead of a {{%s_job}}? e.g.
{noformat}
<running_jobs>
  <job jobid=""job_xxx_0003"" user=""fuser"" name=""word count"" map_complete=""8"" ...>
  <job jobid=""job_xxx_0005"" user=""faser"" name=""word count"" map_complete=""6"" ...>
</running_jobs>
{noformat}","16/Sep/09 12:23;steve_l;I'm not sure if Aaron could use Xmlenc here, as it is biased towards writing whole XML docs (with the XML header) rather than fragments. 

Given that XMLenc is no longer an active project, there is value in having our own classes to chuck out well-formed XML -and XHTML- though I'm not in a rush to implement this. I'd like to, though -something that escapes XML and makes it hard to embed bits of Javascript into your cluster status reports. ","16/Sep/09 20:48;cdouglas;bq. I'm not sure if Aaron could use Xmlenc here, as it is biased towards writing whole XML docs (with the XML header) rather than fragments.

I don't see what you mean. [XMLOutputter|http://xmlenc.sourceforge.net/javadoc/0.52/org/znerd/xmlenc/XMLOutputter.html] doesn't appear to require the declaration.

bq. Given that XMLenc is no longer an active project, there is value in having our own classes to chuck out well-formed XML and XHTML

It's a fast, simple XML writer; I'm pleased that it's not getting new features. That said, escaping user strings displayed in the web UI is an excellent idea.","17/Sep/09 17:34;kimballa;I don't see why using some additional XML writing library is necessary. I have already provided a unit test which ensures that the output of this page is well-formed XML. (For what it's worth, none of the other JSPs have any unit tests at all.) Perhaps there is a broader problem of refactoring the existing JSPs to ensure that all generated HTML/XML is well-formed, but that is outside the scope of this issue.

I'll get rid of the buffered job history and pass in a writer; expect a new patch for that soon. ","17/Sep/09 18:10;kimballa;Fix buffer issue in JSPUtil.java","17/Sep/09 20:47;cutting;Maybe we should file a separate issue to convert XML-generating JSPs to use an XML generator?

As a new feature, this patch has no potential for breaking anything existing and is hence seems low-risk and yet provides some valuable functionality that would be good to have in 0.21.","17/Sep/09 23:53;cdouglas;+0

The format is still overly verbose and should add key/value attributes to {{job}} tags rather than building nested structures, but if Aaron is unwilling to invest the time then the current patch is sufficient.","18/Sep/09 03:42;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419911/MAPREDUCE-679.6.patch
  against trunk revision 816240.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 2 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/100/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/100/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/100/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/100/console

This message is automatically generated.","18/Sep/09 03:57;kimballa;Unrelated test failure","18/Sep/09 18:19;kimballa;New patch sync'd with trunk","18/Sep/09 21:50;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12420056/MAPREDUCE-679.7.patch
  against trunk revision 816735.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 2 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/115/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/115/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/115/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/115/console

This message is automatically generated.","18/Sep/09 21:54;kimballa;unrelated test failure.","18/Sep/09 22:09;tomwhite;+1 

I've just committed this. Thanks Aaron!","19/Sep/09 01:05;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #58 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/58/])
    . XML-based metrics as JSP servlet for JobTracker. Contributed by Aaron Kimball.
","21/Sep/09 15:53;steve_l;It's worth noting this is the first JSP that has a test alongside. Aaron has now set the baseline for all JSP work going forward: tests are expected.","21/Sep/09 18:21;cos;Steve, while it's great to set such a baseline, we need to make sure that we aren't creating new testing infrastructure, incompatible with what we have so far. While JUnit framework isn't an ideal environment in any sense it gives a certain level of automation which is still useful for most of our activities. In my understanding this particular test is manual, right?","22/Sep/09 09:30;steve_l;Konstantin -no, it's not manual. 

A JUnit test case brings up a MiniMR cluster, constructs the URL {{""http://localhost:"" + infoPort + ""/jobtracker.jspx""}} and then does something very devious - hands off the URL to the XML parser and says ""parse this"". If the JSPX isnt there or wont run, the HTTP errors get picked up and reported. If the page is there but isn't valid XML, the parser will catch and report that. Very nice indeed. No need for even an extra JAR like HttpUnit or HtmlUnit. 

","22/Sep/09 10:31;vinodkv;I really like the fact that something real is finally done with respect to testing information presented via JSPs. Thanks Aaron!

But I still have one doubt - how do we keep the jsp and jspx pages in sync? I can think of ways in which we take the XML output as the standard and generate html from it (ala forrest). I can already see this kind of duplication across html/xml in this patch, generateRetiredJobTable() and generateRetiredJobXml() in JSPUtil.java for example. Thoughts about this?","22/Sep/09 17:17;kimballa;The ""most correct"" way to do this would be to integrate with some Java web framework's templating engine, which would, given a set of key/value data, either provide HTML or XML or JSON or a dozen other formats from the same source. But that is a big change and not something that's just going to come together overnight. Generating HTML from XML is a reasonable-sounding alternative.

For now, it's on us to add fields to the JSPX when doing so to the JSP, or vice versa. Suboptimal, for sure.
","22/Sep/09 17:27;tlipcon;bq. I can think of ways in which we take the XML output as the standard and generate html from it (ala forrest)

The standard way to do this is to use XSLT. Unfortunately, most everyone I've talked to who has used XSLT for generating web pages has decided it's a giant pain and wished they hadn't :)","24/Sep/09 08:33;steve_l;-1 to anything too fancy in the way of content generation
+1 to adding a couple of XSLs at the root of the webapps, so that XML status pages can be presented in a human readable form. ","12/Oct/09 15:12;nidaley;{quote}
Aaron Kimball added a comment - 18/Sep/09 02:54 PM
unrelated test failure.
{quote}

Aaron, the test failure seen by Hadoop QA was related to this patch.  Looks like you remove an unzip function in build.xml which has caused TestCopyFile to fail now for the past 2+ weeks.  MAPREDUCE-1029  has been opened to fix this. Please comment there on why it was removed and please be more careful analyzing failed test cases.","12/Oct/09 17:15;kimballa;Oops; I did not understand the linkage between these two components at all. Sorry about that. Commented on MAPREDUCE-1029.","12/Jan/10 23:20;amirhyoussefi;What's level of effort to back-port this to 0.20? Are there other JIRAs which this patch is dependent on? ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a new example MR that always fails,MAPREDUCE-567,12426309,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,philip,philip,philip,25/May/09 22:00,24/Aug/10 21:13,12/Jan/21 09:52,24/Jun/09 13:24,,,,,,0.21.0,,,examples,,,,,,0,,,,,"For testing how Hadoop behaves when jobs fail, it's nice to have an example job that simply fails in either the mappers or the reducers.",,aaa,hammer,nidaley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/09 22:03;philip;HADOOP-5910.patch;https://issues.apache.org/jira/secure/attachment/12408971/HADOOP-5910.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2009-05-31 09:56:15.119,,,false,,,,,,,,,,,,,,,,,,148897,Reviewed,,,,Tue Jul 07 17:34:22 UTC 2009,,,,,,,"0|i0ix6n:",108470,,,,,,,,,,,,,,,,,,,,,"25/May/09 22:03;philip;Attaching a simple job that fails its maps and/or reduces.  I used this while working on HADOOP-4041.","31/May/09 09:56;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12408971/HADOOP-5910.patch
  against trunk revision 780114.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/440/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/440/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/440/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/440/console

This message is automatically generated.","24/Jun/09 13:24;tomwhite;+1

I've just committed this. Thanks Philip!","07/Jul/09 17:34;hudson;Integrated in Hadoop-Mapreduce-trunk #15 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/15/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add preemption to the fair scheduler,MAPREDUCE-551,12408585,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,matei,matei,matei,15/Nov/08 23:51,24/Aug/10 21:13,12/Jan/21 09:52,27/Jun/09 03:46,,,,,,0.21.0,,,contrib/fair-share,,,,,,2,,,,,"Task preemption is necessary in a multi-user Hadoop cluster for two reasons: users might submit long-running tasks by mistake (e.g. an infinite loop in a map program), or tasks may be long due to having to process large amounts of data. The Fair Scheduler (HADOOP-3746) has a concept of guaranteed capacity for certain queues, as well as a goal of providing good performance for interactive jobs on average through fair sharing. Therefore, it will support preempting under two conditions:
1) A job isn't getting its _guaranteed_ share of the cluster for at least T1 seconds.
2) A job is getting significantly less than its _fair_ share for T2 seconds (e.g. less than half its share).

T1 will be chosen smaller than T2 (and will be configurable per queue) to meet guarantees quickly. T2 is meant as a last resort in case non-critical jobs in queues with no guaranteed capacity are being starved.

When deciding which tasks to kill to make room for the job, we will use the following heuristics:
- Look for tasks to kill only in jobs that have more than their fair share, ordering these by deficit (most overscheduled jobs first).
- For maps: kill tasks that have run for the least amount of time (limiting wasted time).
- For reduces: similar to maps, but give extra preference for reduces in the copy phase where there is not much map output per task (at Facebook, we have observed this to be the main time we need preemption - when a job has a long map phase and its reducers are mostly sitting idle and filling up slots).
",,aaa,aah,andyk,aw,bbansal,ddas,dhruba,jsensarma,kasha,kkambatl,matei,mingma,nidaley,qwertymaniac,rksingh,romainr,rschmidt,schen,tlipcon,vinodkv,yhemanth,zshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/09 01:14;tlipcon;fairshare-patches.tar.gz;https://issues.apache.org/jira/secure/attachment/12421791/fairshare-patches.tar.gz","08/Jan/09 06:42;matei;fs-preemption-v0.patch;https://issues.apache.org/jira/secure/attachment/12397380/fs-preemption-v0.patch","08/Feb/09 23:48;matei;hadoop-4665-v1.patch;https://issues.apache.org/jira/secure/attachment/12399788/hadoop-4665-v1.patch","09/Feb/09 00:01;matei;hadoop-4665-v1b.patch;https://issues.apache.org/jira/secure/attachment/12399789/hadoop-4665-v1b.patch","21/Feb/09 08:14;matei;hadoop-4665-v2.patch;https://issues.apache.org/jira/secure/attachment/12400651/hadoop-4665-v2.patch","19/Mar/09 22:35;matei;hadoop-4665-v3.patch;https://issues.apache.org/jira/secure/attachment/12402607/hadoop-4665-v3.patch","25/Mar/09 00:01;matei;hadoop-4665-v4.patch;https://issues.apache.org/jira/secure/attachment/12403578/hadoop-4665-v4.patch","07/May/09 05:26;matei;hadoop-4665-v5.patch;https://issues.apache.org/jira/secure/attachment/12407422/hadoop-4665-v5.patch","16/May/09 21:39;matei;hadoop-4665-v6.patch;https://issues.apache.org/jira/secure/attachment/12408325/hadoop-4665-v6.patch","02/Jun/09 05:40;matei;hadoop-4665-v7.patch;https://issues.apache.org/jira/secure/attachment/12409635/hadoop-4665-v7.patch","05/Jun/09 21:43;matei;hadoop-4665-v7b.patch;https://issues.apache.org/jira/secure/attachment/12410024/hadoop-4665-v7b.patch","19/Jun/09 01:24;matei;hadoop-4665-v7c.patch;https://issues.apache.org/jira/secure/attachment/12411175/hadoop-4665-v7c.patch","23/Jun/09 23:07;matei;hadoop-4665-v7d.patch;https://issues.apache.org/jira/secure/attachment/12411584/hadoop-4665-v7d.patch","24/Jun/09 21:50;matei;hadoop-4665-v7e.patch;https://issues.apache.org/jira/secure/attachment/12411713/hadoop-4665-v7e.patch","21/Jul/09 06:14;tlipcon;mapreduce-551-branch20.txt;https://issues.apache.org/jira/secure/attachment/12414075/mapreduce-551-branch20.txt",,,,,,,,,,,,,,,,,,,,15.0,,,,,,,,,,,,,,,,,,,,2009-01-16 19:23:38.331,,,false,,,,,,,,,,,,,,,,,,37377,Reviewed,,,,Sat Oct 10 01:14:04 UTC 2009,,,,,,,"0|i02tcf:",14355,Added support for preemption in the fair scheduler. The new configuration options for enabling this are described in the fair scheduler documentation.,,,,,,,,,,,,,,,,,,,,"08/Jan/09 06:42;matei;Here is an initial version of the patch for review. The main thing missing is unit tests.

The patch adds two things. First there's the preemption, which works as described in the issue - jobs may preempt others if either they aren't receiving their guaranteed share for some time, or they are at below half their fair share and negative deficit for some time. The times can be configured in the fair scheduler config file and thus modified at runtime, and the guaranteed share timeouts are per pool. On top of this, to aid with debugging and development of the fair scheduler in the future, there is a scheduler event log, which is disabled by default but creates some event logs in tab-separated format in $hadoop.log.dir/fairscheduler if you turn it on. These are meant to be nitty-gritty detailed logs with machine-parsable event types rather than the ""human-readable"" logs that go into the standard log4j log for the JobTracker. They are also potentially much larger on a large cluster, which is why they're off by default.

I'm running this through hudson to see whether there are complaints from findbugs, checkstyle, etc, but I will include some unit tests in the final patch.","08/Jan/09 06:57;matei;Yikes, forgot to add, this patch also includes HADOOP-4789 because it is dependent on it. That might make it more confusing to read than it needs to be.","08/Jan/09 07:06;matei;There is also one change here that is outside of the fair scheduler that I'd like feedback on. In order to allow the scheduler to kill tasks, I added a killTask method to the TaskTrackerManager interface. This means that you don't need to assume that the TaskTrackerManager is always a JobTracker, so you can write unit tests for it with a mock object. There was already a killJob in there. Is it okay to add killTask or is there a reason not to?","16/Jan/09 19:23;jsensarma;Dhruba and I looked at this together and got stuck on getAllowedLocalityLevel()

- why subtract nodeLocalWait from rackLocalWait 
- why getting config variables each time
- if we were not rack/node local last time - why don't we wait for locality next time? Seems like once we lose locality - we are anyway going to run up a deficit and then schedule a boatload of non-local tasks. seems like the whether we want to wait for locality or not should be based on how much deficit we are incurring and whether it's still warranted to wait for locality (as opposed to whether we were able to schedule the last task locally)

thoughts?","16/Jan/09 20:01;matei;Those are good points Joydeep. I will remove the getting conf variables every time, that was a mistake.

About subtracting the waits: This is just a question of how we interpret the parameters. Maybe we want nodeLocalWait and rackLocalWaits to be two different times that get added up for some ""total wait"". I originally meant for rackLocalWait to always be bigger than nodeLocalWait and thus capture the maximum delay. But since that is confusing and can lead to misconfiguration, I will make them add up as you said.

For your third point, the idea was as follows: When it still has a lot of maps left to launch, a job will almost always have node-local tasks, so waiting is fine. However, when there are only a few maps left, there will be fewer nodes on which it can launch node-local tasks, and these may be busy running long tasks or something. So, when it's waited for nodeLocalWait amount of time, the job will start being allowed to launch rack-local tasks instead. Once it has launched such a task, it is allowed to launch more rack-local tasks rather than having to begin the waiting all over again so that it doesn't drastically slow down the rate at which it can launch tasks if the nodes with node-local data still aren't becoming free. However, we remember the locality level of the last map launched, so if the job ever *does* manage to launch a node-local task again, we begin the wait period again. There's a similar story for going from rack-local to off-rack: the idea is that once you've had to wait so long as to launch an off-rack task, you probably have very few opportunities left for launching rack-local or node-local tasks, so you might as well be allowed to launch more off-rack tasks and finish the job rather than having your launch rate slowed to a trickle.

Now it's possible that just using shorter waits but requiring the wait to happen every time you need to launch a non-local task will work too. I don't know, but it seemed that in my gridmix tests the current implementation worked fine even for very small jobs (going from 2% to 75-80% node locality for 3-map tasks on a 100-node cluster).
","02/Feb/09 19:16;jsensarma;a couple of other points (let me get back on the comments above):

- default preemption timeout(s) for pools. right now unless we configure on a per pool basis - the default timeout is infinite.
- eventlog - this looks very expensive - there's a flush on every log() call. can we just take out this flush()?

","02/Feb/09 21:18;matei;We can definitely take out the flush if you are okay with possibly missing the end of the event log. Right now the event log was meant only as a debugging tool (which is why it's off by default), but maybe it's more useful to make it possible to always keep it on.

I will add a parameter for default min share preemption timeout as well.","08/Feb/09 23:48;matei;Here is an updated version of this patch. It includes some fixes from testing this at Facebook, as well as Joydeep's comments (change event log to use Log4j rolling file appender, and provide a way to set default preemption timeouts).","09/Feb/09 00:01;matei;Small update to change the fair scheduler log dir jobconf variable to a consistent format with the other event log related variables.","09/Feb/09 04:30;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12399789/hadoop-4665-v1b.patch
  against trunk revision 742171.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 9 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3813/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3813/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3813/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3813/console

This message is automatically generated.","14/Feb/09 09:02;dhruba;+1. code looks good.","14/Feb/09 15:06;yhemanth;Dhruba, we have been testing the FairScheduler of Hadoop 0.20 release quite heavily in the past couple of weeks as part of an evaluation we are doing. I cannot say how much committing this to Hadoop 0.20 could effect what we've tested. Is it really necessary for the 0.20 release ? Can we get it committed to trunk only instead ?

Also, I'd looked at this code cursorily. I think pre-emption is an optional feature here, right ?","14/Feb/09 20:14;matei;There's no reason to put it in 0.20 if 0.20 is in the testing stage right now. The issue was just opened against 0.20 because there was no 0.21 back then. It's true that the preemption is disabled by default but there's no need to complicate things.","16/Feb/09 07:18;yhemanth;Thanks, Matei. That would be very convenient. Dhruba, does this sound ok ? If you agree, we could set the fix in version to 0.21.","18/Feb/09 19:45;dhruba;We haven been running this code in production for a few weeks now. I would have ideally liked it to go into 0.20 (because 0.20 is not yet released), but if that is difficult that I am fine putting it in 0.21","19/Feb/09 03:44;yhemanth;Dhruba, we're planning on a release very soon based on 0.20. This would reset our testing cycle significantly. So, I would request you to move this to 0.21. Since you're already using this, I am assuming it will not impact you that much, I hope.","19/Feb/09 06:19;aaa;Hemanth,

  Facebook is not the only company that benefits from preemption, the 
community at large will benefit from this and the sooner that we get it 
out there the better. Also fact that Facebook has been running this for 
a number of weeks means that it will most likely not lead to any 
significant testing problems.

  Please consider fitting this in the 0.20 release, and let us know if 
there is anyway we can help to expedite.

Thanks,

-- amr
 
","19/Feb/09 09:02;yhemanth;Amr, that's a good point about the benefit to the community.

One middle ground option could be to make it part of Hadoop 0.20.1. so that the community doesn't wait until 0.21 (which could be way out right now).

One specific concern I had about this patch (from a cursory glance) was that it was locking the JobTracker, (the taskTrackerManager instance) in the update method which runs frequently. In general, this could interfere with regular processing in the jobtracker, like heartbeats (which are also synchronized on the same instance). We've in the past seen issues of this nature on large clusters. When the JT is locked up, tasktrackers could get lost and tasks could fail arbitrarily. HADOOP-5280 seems to be one specific instance of this (though we've not ascertained the reason JT got locked up). 

Does the middle ground option make sense ? It may help all of us with only a little compromise, no ?
","19/Feb/09 20:43;aaa; > Does the middle ground option make sense ? It may help all of us with 
only a little compromise, no ?

sure, thanks for being accommodating. What is rough eta for when 0.20.1 
would be out?

-- amr
","19/Feb/09 21:31;matei;That sounds good. I'm soon going to remove that lock by the way, and ensure that we only lock the JT when there are tasks to kill.","19/Feb/09 21:55;nidaley;We've got to stabilize 0.20.  We're currently working on fixing bugs.  Adding new features to contrib components at this point would not get my vote.  We're already 2.5 months past 0.20 core feature freeze.","21/Feb/09 08:14;matei;Here is an updated patch that removes the locking of the JobTracker in update(). Instead, we only lock the JT every 30 seconds when preemptTasksIfNecessary decides that it is time to check for tasks to preempt. This should improve JT scalability.","03/Mar/09 19:41;matei;Robert, this is not going into 0.20, is it? I thought Nigel said we should not do that.","19/Mar/09 22:35;matei;Updated the patch to work with the current trunk.","23/Mar/09 23:42;dhruba;We should not put it in the 0.20 release, especially because the branch is already cut.

It should be committed to trunk for now. If, at a later time (after making the 0.20 release), the community decides to pull it into the 0.20.1 release, then we can do that.

Paatch looks good. +1. Can you pl resubmit the patch so that HadoopQA gets to process it? Thanks.","23/Mar/09 23:56;matei;Passing patch through Hudson.","24/Mar/09 04:37;vinodkv;The latest patch hadoop-4665-v3.patch is not applying cleanly to trunk, perhaps because of HADOOP-4788.","24/Mar/09 19:42;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12402607/hadoop-4665-v3.patch
  against trunk revision 757958.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 12 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/128/console

This message is automatically generated.","25/Mar/09 00:01;matei;New patch that applies to trunk.","25/Mar/09 11:14;vinodkv;I've started looking at the patch. If I understand correctly the way preemption is done, the jobs are sorted in the order of deficits and tasks are taken away from the jobs at the bottom of the list (positive deficits) and given to the jobs with negative deficits at the top. By doing this, though we are catering to job's fair share, it doesn't look like we are serving a queue/pool 's guaranteed shares.

So, a queue/pool can be running only its min-maps/min-reduces, but still tasks can be preempted from jobs of this queue. Is that correct? If that's the case, then that doesn't sound correct to me.

Can you throw some light on this?","25/Mar/09 14:58;matei;The patch actually makes sure that no job is ever brought below its fair or min share. This happens in preemptTasks, where we compute tasksToLeave and make sure we leave the job with at least that many. Since each pool's min and fair share is distributed among the jobs in that pool, this will ensure that pool shares are also kept. So the deficit thing is just a global ordering to service the jobs that have been starving the most first, but in following it, we also ensure that we never bring jobs below their shares.

Actually when we remove deficits from the scheduler (which I already have working code for; it's just too big a change to push that, 4665 and 4667 in the same JIRA), the logic will be simpler. We'll just look for jobs that are far below their fair share (as a ratio of share) and preempt from jobs that are far above their fair share (again as a ratio). Then there won't be this confusion about whether we can have a service order based on deficits or not. This patch is just an attempt to get some of that code into the scheduler before pushing the big change.. If you'd prefer that I post a patch for the new non-deficit-based fair scheduler instead, I can do that too.","26/Mar/09 03:38;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12403578/hadoop-4665-v4.patch
  against trunk revision 758425.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 12 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/138/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/138/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/138/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/138/console

This message is automatically generated.","31/Mar/09 04:43;matei;The test failures seem to be unrelated to my patch - they happen in unmodified trunk too. One of them is HADOOP-5068. Is it okay for me to commit my patch?","31/Mar/09 05:04;vinodkv;Sorry for the late reply, was on a long weekend.

Your explanation w.r.t the min-shares of pools clarified my doubts. Thanks!

I have discussed the overall approach of this patch with Hemant, Sreekanth and Rahul offline. We concluded on one slight improvement - the preemption patch of capacity-scheduler treats the preemption timeouts to be a kind of SLA for the pool/queue and so leaves a couple of heartbeats for slots to become free after it preempts a task. Can we do something like that here? - Essentially, the proposal is to preempt a task when a job's fairshare/minshare are not met within PREEMPTION_TIMEOUT- 2/3 heartbeats.

Few other code comments I have:
 - Can we make PREEMPTION_INTERVAL configurable?
 - The check as to whether FairScheduler will do preemption(preemptionEnabled && !useFifo) is done deep inside - all the stats are calculated and then only preemption is skipped if not needed. Can we take this check, may be, to the beginning of preemptTasksIfNecessary() method or inside update() method itself.
 - The class FairSchedulerEventLog can just be package-private. So do all the methods inside - init, log, shutdown and isEnabled - they don't need to be public as of now.

Again, sorry for the late reply. Appreciate your patience.","01/Apr/09 13:32;vinodkv;Here's a more comprehensive review:

Major points:
 - As it's a new feature, I think preemption(mapred.fairscheduler.preemption) should be disabled(false) by default.
 - Can we do something like above stated proposal: to preempt a task when a job's fairshare/minshare are not met within PREEMPTION_TIMEOUT leaving 2/3 heartbeats for the task to actually get killed.
 - DUMP_INTERVAL and PREEMPTION_INTERVAL should be configurable. The variables themselves can be package-private instead of public.
 - The class FairSchedulerEventLog can just be package-private. So do all the methods inside - init, log, shutdown and isEnabled - they don't need to be public as of now.

 - FairScheduler.preemptTasksIfNecessary() method:
   -- This method does a Collections.reverse(jobs) after sorting the jobs. We can just traverse the list in reverse order to get the desired effect here.
   --  The check as to whether FairScheduler will do preemption(preemptionEnabled && !useFifo) is done deep inside - all the stats are calculated and then only preemption is skipped if not needed. Can we take this check, may be, to the beginning of preemptTasksIfNecessary() method or inside update() method itself.

 - FairScheduler.tasksToPreempt() method:
   -- The count tasksDueToFairShare seems to be calculated to see if full fair-share of slots are allotted or not instead of the advertised half of fairshare. I think this is a mistake as isStarvedForFairShare() is checking for half of fair-share. Or am I missing something?
   -- EventLogs in this method are a bit confusing if the job is short on both minshare and fairshare. In this case, it is roughly giving an impression that we are preempting twice. I think it would be clear to just log that the job is short by so many slots w.r.t minshare and w.r.t fairshare. And in the end, just before returning we can simply say the exact number of slots we are going to preempt.

 - FairScheduler.start() method:
   -- I think that the call loadMgr.setEventLog(eventLog) should be after eventLog is initialized with a new FairSchedulerEvenLog object.

 - TestFairScheduler.java:
  -- You have added setup and cleanup tasks creation code in initTasks() method. Is there any specific reason for doing this? In any case, much of it duplicates code from JobInProgress.initTasks(), if we really want, we can refactor this code into a new method say JobInProgress.createSpecialTasks().
  -- There are no new test-cases related to preemption. I think we should have one/some.

Minor nits:
 - Javadoc for getFairSharePreemptionTimeout() is incomplete. Also, if I am understanding correctly FairSharePreemptionTimeout is the same for all pools/jobs.
 - The xml tag for minSharePreemptionTimeouts is currently preemptionTimeout. It can better be minSharePreemptionTimeout.
 - update thread:  The log message ""Failed to update fair share calculations"" can better be ""Exception in Update thread"". This because,now , update thread does more than just updating calculations.","01/Apr/09 13:35;vinodkv;Some more miscellaneous points:
 - With the introduction of new timeouts, I think the importance of a template file for the allocations increases. I remember you saying something about it on some jira. Have you filed one?

 - This patch adds FairSchedulerEventLog for logging various events in the scheduler in machine-readable format. But there is no place from where utilities can determine the format of the log records: How should we track the event log records' format, add some schema file? Or alter the logs to be a list of key-value pairs similar to JobHistory instead of just values?

 - There is a lot of common code between {code}int org.apache.hadoop.mapred.FairScheduler.preemptTasks(JobInProgress job, TaskType type, int maxToPreempt){code} and {code}int org.apache.hadoop.mapred.CapacityTaskScheduler.MapSchedulingMgr.killTasksFromJob(JobInProgress job, int tasksToKill){code}. In fact most of it is the same. I think we should somehow try to refactor this common code. Don't know if we want to do it in this jira itself or not.","01/Apr/09 13:41;vinodkv;Oh, and Joydeep's previous comment isn't addressed yet.
bq. default preemption timeout(s) for pools. right now unless we configure on a per pool basis - the default timeout is infinite.
Because we already have a different plug for disabling preemption altogether, I think this default timeout as well as fairSharePreemptionTimeout should have some finite value.","01/Apr/09 18:19;matei;Hi Vinod,

A few comments:

* Is the 2-3 heartbeats for preemption really necessary? I imagine timeouts will be on the order of minutes, so a few seconds won't make a big difference. Although thinking of the timeout as an SLA is nice, it's also equally easy to think of it as ""this is when it can start killing tasks"". To me, putting this extra 2-3 heartbeats thing in seems like unnecessary complexity.
* The reason the preemption-enabled check is deep inside the method is to give you the ability to turn preemption off but see the SHOULD_PREEMPT log messages to figure out when your cluster *would* preempt tasks given certain settings. We wanted this at Facebook so that we can add some timeouts, count the SHOULD_PREEMPT messages over a week, and be sure that the settings chosen are good without actually losing a lot of tasks if there's a mistake. I think this is a good feature to keep for other people who are thinking of turning on preemption.
* There actually is a way to set a default preemption timeout as in Joydeep's comment - you can set defaultMinSharePreemptionTimeout in the XML file. The code for this is in PoolManager.
* The default settings of preemptionEnabled=true and no timeouts are to make preemption easy to configure gradually. We expect that most people will start out not wanting preemption, because it creates an extra worry of ""have we set it too low"". Then as people start running pools with ""production"" jobs (with min share set), they may want to enable preemption just for these jobs. They would be able to do that by just adding a preemptionTimeout entry to those pools in the config, and it would be active without needing to restart the JobTracker. Then if they see non-production jobs suffering, they could enable the fairSharePreemptionTimeout, again without requiring a cluster restart. The only reason to also provide a preemptionEnabled setting in the jobconf is for the testing purpose I mentioned above, where an organization switching over to preemption in production can figure out first whether it will kill too many tasks. Overall, my goal with all the fair scheduler config is to make it as easy as possible to use ""out of the box"". You don't need to define pools in advance, you don't need to define min shares or weights in advance, you don't need to decide when to use preemption in advance, etc, and the only setting you need in mapred-site.xml is the one that sets Hadoop to use the fair scheduler. Then as you decide you want the more advanced features, you enable them gradually. I actually think there are strong advantages to this over your proposal of having preemptionEnabled=false and having non-infinite default timeouts so again I'd like more motivation before making such a large code change. The other factor is that Facebook has been using the current version of the preemption code and found the current features useful.

I'll take a look at your other comments later this week. Regarding the code reuse in preemptTasks, it is indeed based on the one in the capacity scheduler, but I'd like to make refactoring that a separate issue from this JIRA. The right thing might be to have some of that functionality in TaskScheduler.","09/Apr/09 20:03;matei;Vinod, have you had a chance to look at my arguments above? I'd really like to get this patch committed soon because I have others that depend on it.","12/Apr/09 08:12;yhemanth;Matei, I've not been looking at this code much, but based on the discussion, I have only one comment: regarding the turning off of pre-emption. 

Your use case of organizations wanting to try out with pre-emption disabled, but still seeing when pre-emption would happen seems to me like a dry-run mode that you can see in utilities like an RPM update. As you've explained, looks like there are use cases for this.

From our work with the capacity scheduler, we've found there are environments where pre-emption is indeed not necessary. Even when it exists, it has proved to be a complex feature to reason about. From this perspective, it seems like it may make sense to provide an option to completely turn it off and have reasonable confidence that nothing related to pre-emption would be in effect, including any additional computation that it requires.

Hence, my suggestion is the following: have a flag to truly turn off pre-emption and have a variable that allows a dry-run of pre-emption when it is enabled. I believe this may not be a very difficult change ? (Indeed, I've been thinking of cases where a dry-run of the entire scheduling logic makes sense  - for e.g. to get a 'scheduling log' that can be replayed). 

The flip side of my proposal is an additional configuration option. But depending on what we think the right defaults are, we can still make the configuration easy for end users, no ? To that extent, your arguments about the proposed default values are fine with me.","15/Apr/09 04:20;vinodkv;Agree with Hemanth on having two knobs - one for completely disabling preemption and one for enabling a dry run.

Regarding leaving some heartbeats before killing tasks, I agree with your arguments and am fine with leaving that for now as the timeouts may indeed turn out to be in the order of minutes like you are saying. If it becomes a need later, we can do it in future in another JIRA issue.

I'm +1 to the rest of the arguments. So I'll be fine with the patch once the rest of the code comments are incorporated along with a knob for disabling preemption altogether.","05/May/09 06:11;cdouglas;The current patch no longer applies to trunk","07/May/09 05:26;matei;Here's a new patch. It incorporates Vinod's comments, except for changing the event log format to key-value pairs. The event log in its current state is meant to be used only for debugging and has a relatively simple tab-separated format, so I didn't want to complicate the API to it. We can have another JIRA for adding a parser class for it and turning it into more than a debug tool if there's demand for that. It also adds five unit tests for preemption and a default config file for the fair scheduler. The patch also makes the update interval configurable (since I was did that with the other two periodic check intervals).

Included in this patch is a fairly significant evolution of the fair scheduler unit testing framework, which adds tracking of tasks in FakeJobInProgress to allow for preemption to be tested meaningfully. The FakeJobInProgress and FakeTaskInProgress have some commonalities with the ones in the capacity scheduler, but unfortunately I wasn't able to use those directly because some of the classes used in the fair scheduler, such as Clock, don't have equivalents there. It would be nice to create a common testing framework for schedulers, but that should be a separate JIRA. I also think that the ultimate solution for that is not to make an elaborate FakeJobInProgress and related classes, but rather to make MiniMRCluster more user-friendly and switch all tests into it. We can also consider making the Clock class be used by all the MR code so that tests can run at an accelerated rate on these simulated clusters.

One other thing I will need to add in this patch is documentation for the preemption params. However, the other changes can be reviewed right now.","09/May/09 01:02;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12407422/hadoop-4665-v5.patch
  against trunk revision 772960.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 12 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/310/console

This message is automatically generated.","16/May/09 21:39;matei;Updated patch to keep up with trunk. Would appreciate a review.","02/Jun/09 05:40;matei;Attaching a new patch which includes updated documentation.","03/Jun/09 22:34;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12409635/hadoop-4665-v7.patch
  against trunk revision 781343.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 12 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.

    -1 release audit.  The applied patch generated 494 release audit warnings (more than the trunk's current 492 warnings).

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/457/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/457/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/457/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/457/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/457/console

This message is automatically generated.","04/Jun/09 16:19;matei;The test failure seems to be unrelated to my patch (HADOOP-5869).

I'm not sure what the release audit warning means. This is what it says:

{code}
5d4
<      [java]  !????? /home/hudson/hudson-slave/workspace/Hadoop-Patch-vesta.apache.org/trunk/build/hadoop-781343_HADOOP-4665_PATCH-12409635/conf/fair-scheduler.xml
300d298
<      [java]  !????? /home/hudson/hudson-slave/workspace/Hadoop-Patch-vesta.apache.org/trunk/build/hadoop-781343_HADOOP-4665_PATCH-12409635/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairSchedulerEventLog.java
{code}","04/Jun/09 16:28;tomwhite;These files are missing Apache license headers. Config files in Hadoop don't seem to include a header, so you can probably skip the one for fair-scheduler.xml.","05/Jun/09 21:43;matei;Thanks, that makes sense. I added the license to the .java file.","10/Jun/09 14:40;vinodkv;Matei, I have (re)started looking at the patch. The changes look good overall, except the following points:
 - preemptionInterval variable is initialized to 30000 whereas the default value is 15000. Shouldn't they be consistent?
- EagerTaskInitializationListener is not removed from the list of listeners in the terminate method 
- You seem to have missed one of my earlier points:
bq. The count tasksDueToFairShare seems to be calculated to see if full fair-share of slots are allotted or not instead of the advertised half of fairshare. I think this is a mistake as isStarvedForFairShare() is checking for half of fair-share. Or am I missing something?

The changes in test-cases and documentation seem to be huge. It'll take till tomorrow for me to complete the review of test-cases and documentation. Thanks for the patience.","11/Jun/09 07:19;matei;Hi Vinod,

Sorry for not addressing the half fair share point earlier, it looks like I forgot to post that. The difference between isStarvedForFairShare and tasksDueToFairShare is intentional. I want the threshold for triggering fair share preemption to be a lot lower than the fair share so that it doesn't trigger unless something is going horribly wrong in the cluster. The reason is that in standard use of the fair scheduler, we expect any critical (""production"") jobs to have min shares set, which are enforced much more precisely (and potentially with a smaller timeout). The fair share is for those jobs that are not critical and that we're okay with being a little unfair to if that reduces wasted work. So the service model is that we only do preemption if the job is being starved very badly. However, when we do preemption, we do bring you up to your full fair share, because at that point it's clear that you've been starved badly for a long time. Once you are at your full fair share, it will be easy for you to remain there as you'll be given chances to reuse those slots when your tasks finish. If some users request for a stricter enforcement of fair shares, we can make the ""half"" part configurable later, but we decided this model is a good way to prevent unnecessary preemption and exchange of slots back and forth between jobs, while also not being too unfair.

I'll make a patch with the other changes sometime in the next few days or maybe after I see some of your comments.

The changes in test cases and docs are indeed huge. The request was huge ;) (and important), and I took this opportunity to clean up the fair scheduler docs overall.","19/Jun/09 01:24;matei;I've attached a new patch taking into account Vinod's two comments and keeping up with trunk. Vinod, do you have a rough ETA on when you'll finish looking at this?","19/Jun/09 20:38;szetszwo;It would be great if we can have a unit test testing [the case|https://issues.apache.org/jira/browse/HADOOP-5701?focusedCommentId=12700284&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12700284] given in HADOOP-5701.  Not sure if you already have it.  I did not check the patch.","20/Jun/09 21:11;matei;Nicholas, that is exactly the case this patch fixes. The unit tests cover it.","23/Jun/09 11:52;vinodkv;Reviewed the latest changes and the test cases too, look good. Quickly looked at the documentation too, but couldn't run ant docs because of workspace problems. Hudson seems to have stuck again, please run ""ant test"" to make sure everything is fine and that the docs are building properly.

The changes are committable, but you need to make minor changes to the patch w.r.t directory structures so as to reflect the latest project split up. Can you please do so?","23/Jun/09 23:07;matei;Here's a patch against the mapreduce SVN. I still have to run all the tests, but if they work, can I commit this, Vinod?","24/Jun/09 02:32;matei;I ran ant test and all the tests passed except for TestReduceFetch, which is a known problem due to MAPREDUCE-433. I also ran ant docs and successfully built the documentation.","24/Jun/09 13:23;vinodkv;*Sigh*. I was hoping that you will commit the patch after generating the new patch. By the time you read this, MAPREDUCE-516 mostly will be committed :(. This patch will need an update after MAPREDUCE-516.","24/Jun/09 21:50;matei;I've updated the patch to work with the changes in MAPREDUCE-516. The updates were fairly minor. The docs and the the fair scheduler's tests still run successfully. I'm currently running the other tests through ant test. I'll commit it if those pass.","25/Jun/09 04:50;vinodkv;bq. I'll commit it if those pass.
+1. Please commit it before some other patch goes in and breaks this patch again :)","27/Jun/09 03:46;matei;Thanks Vinod. I've committed this. I had to do a little more merging due to a change in TaskTrackerManager and EagerTaskInitializationListener.","07/Jul/09 17:34;hudson;Integrated in Hadoop-Mapreduce-trunk #15 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/15/])
    ","21/Jul/09 06:14;tlipcon;Attaching a patch against branch-20 in case anyone finds this useful. (this should not be slated for inclusion in 0.20.1 since it is a new feature -- just attaching for those who would like to patch it in)","21/Jul/09 12:11;dhruba;Thanks Todd, we might give it a whirl with 0.20.","10/Oct/09 01:14;tlipcon;The branch-20 patch previously posted here had a backport error. There were also two other bugs (MAPREDUCE-1070 and MAPREDUCE-1089) found in fairshare since that patch. Here's a tarball with a working patch series from 0.20.1 release in case anyone is interested in applying on their own (this should not go into 0.20.2)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Global scheduling in the Fair Scheduler,MAPREDUCE-548,12408587,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,matei,matei,matei,16/Nov/08 00:09,24/Aug/10 21:13,12/Jan/21 09:52,14/Aug/09 16:33,,,,,,0.21.0,,,,,,,,,0,,,,,"The current schedulers in Hadoop all examine a single job on every heartbeat when choosing which tasks to assign, choosing the job based on FIFO or fair sharing. There are inherent limitations to this approach. For example, if the job at the front of the queue is small (e.g. 10 maps, in a cluster of 100 nodes), then on average it will launch only one local map on the first 10 heartbeats while it is at the head of the queue. This leads to very poor locality for small jobs. Instead, we need a more ""global"" view of scheduling that can look at multiple jobs. To resolve the locality problem, we will use the following algorithm:
- If the job at the head of the queue has no node-local task to launch, skip it and look through other jobs.
- If a job has waited at least T1 seconds while being skipped, also allow it to launch rack-local tasks.
- If a job has waited at least T2 > T1 seconds, also allow it to launch off-rack tasks.
This algorithm improves locality while bounding the delay that any job experiences in launching a task.

It turns out that whether waiting is useful depends on how many tasks are left in the job - the probability of getting a heartbeat from a node with a local task - and on whether the job is CPU or IO bound. Thus there may be logic for removing the wait on the last few tasks in the job.

As a related issue, once we allow global scheduling, we can launch multiple tasks per heartbeat, as in HADOOP-3136. The initial implementation of HADOOP-3136 adversely affected performance because it only launched multiple tasks from the same job, but with the wait rule above, we will only do this for jobs that are allowed to launch non-local tasks.",,aaa,aah,acmurthy,amirhyoussefi,andyk,cdouglas,ddas,dhruba,hammer,jsensarma,matei,mingma,omalley,rksingh,vivekr,yhemanth,zzningxp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/09 06:43;acmurthy;HADOOP-4667_api.patch;https://issues.apache.org/jira/secure/attachment/12398375/HADOOP-4667_api.patch","08/Jan/09 07:03;matei;fs-global-v0.patch;https://issues.apache.org/jira/secure/attachment/12397383/fs-global-v0.patch","05/Feb/09 23:05;matei;hadoop-4667-v1.patch;https://issues.apache.org/jira/secure/attachment/12399600/hadoop-4667-v1.patch","09/Feb/09 00:02;matei;hadoop-4667-v1b.patch;https://issues.apache.org/jira/secure/attachment/12399790/hadoop-4667-v1b.patch","21/Feb/09 08:25;matei;hadoop-4667-v2.patch;https://issues.apache.org/jira/secure/attachment/12400652/hadoop-4667-v2.patch","03/Jul/09 17:00;matei;mapreduce-548-v1.patch;https://issues.apache.org/jira/secure/attachment/12412509/mapreduce-548-v1.patch","08/Jul/09 23:53;matei;mapreduce-548-v2.patch;https://issues.apache.org/jira/secure/attachment/12412935/mapreduce-548-v2.patch","17/Jul/09 23:21;matei;mapreduce-548-v3.patch;https://issues.apache.org/jira/secure/attachment/12413886/mapreduce-548-v3.patch","22/Jul/09 20:23;matei;mapreduce-548-v4.patch;https://issues.apache.org/jira/secure/attachment/12414243/mapreduce-548-v4.patch","03/Jul/09 04:23;matei;mapreduce-548.patch;https://issues.apache.org/jira/secure/attachment/12412446/mapreduce-548.patch",,,,,,,,,,,,,,,,,,,,,,,,,10.0,,,,,,,,,,,,,,,,,,,,2009-01-14 18:34:05.57,,,false,,,,,,,,,,,,,,,,,,72469,,,,,Fri Aug 14 16:33:40 UTC 2009,,,,,,,"0|i0ivf3:",108184,,,,,,,,,,,,,,,,,,,,,"08/Jan/09 07:03;matei;Here's a preliminary version of this patch. It includes the patches for HADOOP-4789 and https://issues.apache.org/jira/browse/HADOOP-4665 because it depends on those. This may make it confusing to read but I will post simpler versions once those patches are in. The code of interest here is really just in assignTasks and getAllowedLocalityLevel in FairScheduler.java. In addition, doing this requires a change to the JobInProgress API to have an obtainNewMapTask version that takes a locality level (distance up the topology). This was already used internally in findMapTask but there is now a package-visible method that exposes it to the scheduler.","14/Jan/09 18:34;acmurthy;I'm interested in how you decide values for dataLocalWait and rackLocalWait...

OTOH, it seems we could use the notion of 'missed opportunities to schedule' rather in-lieu of dataLocalWait/rackLocalWait ... for e.g. we could just use the number of tasktrackers which were rejected by the Job since they didn't have the requisite data etc. Thoughts?","15/Jan/09 02:41;matei;How to set the waits is an interesting question. I set them to be times rather than number of tasktrackers so that they are very easy for an administrator to understand (if they want some kind of guarantee about response time) and so that you don't need to take into account number of nodes in your cluster to decide what is a reasonable number. However, how long to set them for depends on several factors:
* How much you weigh throughput vs response time. If you care only about throughput, it's generally better to wait longer.
* Percent of nodes that have local data for the job. If you are down to 1-2 map tasks left to launch, then maybe the expected wait time until you receive a local heartbeat is quite long and you might as well launch non-locally right away.
* Nature of tasks. If you have a task that is CPU-heavy, then there's little or no loss in response time from launching it non-locally.

Another thing we've thought about is what to do if there's a ""hotspot"" node that everyone wants to run on. In this case, setting the waits too high is a bad idea, because you'll end up with a lot of tasks waiting on the hotspot node and with other nodes being underutilized. One interesting question though is which tasks to launch on the hotspot node. If you have an IO-bound job where each task takes 20s to process a block, while another job is more CPU-heavy and takes 60s to process a block, then you want to run the IO-bound job locally and the CPU-bound job non-locally. The reason is that in the time it takes to run one task from the CPU-heavy job, you could've run 3 tasks from the IO-bound one and saved sending those 3 blocks across the network as well as saved response time.

I haven't included anything to deal with this case or with setting the waits in general in this patch, because we found that 10-15 second waits work well for dataLocalWait and then 20-25s work well for rackLocalWait. However, in a future patch, it might be worthwhile to look at some task statistics to determine IO rate for each job and identify the CPU-bound ones, then lower the waits on those ones so that they go to non-hotspot nodes.","16/Jan/09 00:41;acmurthy;bq. I set them to be times rather than number of tasktrackers so that they are very easy for an administrator to understand (if they want some kind of guarantee about response time) and so that you don't need to take into account number of nodes in your cluster to decide what is a reasonable number.

Hmm... sorry I should have been more clear. 

I propose we use a fraction of the cluster-size (i.e. total no. of tasktrackers in the sytem via ClusterStatus) rather than time. For e.g. we could say that dataLocalWait is equivalent to 100% of cluster size (which implies  1 round of heartbeats i.e. 5s for 500 nodes, 10s for 1000 nodes etc.) and 200% of cluster-size (i.e. 10s for 500, 20s for 1000). This will keep it relatively simple and tractable. Thoughts?","16/Jan/09 05:41;matei;That sounds like a good way to do it. What is the typical inter-heartbeat interval on a large cluster? And does it change if the nodes are busy vs if they're idle? I also imagine that we can just look at the heartbeat interval value (which the master knows) and add a fudge factor to that and set that as the wait time, thus avoiding having to count trackers.","20/Jan/09 18:41;acmurthy;bq. I also imagine that we can just look at the heartbeat interval value (which the master knows) and add a fudge factor to that and set that as the wait time, thus avoiding having to count trackers.

I'd argue it's easier and more direct to count trackers rather than try and convert it to a timestamp, at the very least it will save you a few million gettimeofday calls...","21/Jan/09 00:26;matei;That sounds like it should work and would be easier to code. The only caveat is that we have to be careful in the future if someone changes the heartbeat logic to e.g. reduce the heartbeat rate if the node is busy... hopefully this is not an issue since we just reviewed that.","21/Jan/09 00:36;matei;Also some notes on the implementation status.. I've been testing and improving a Hadoop 0.17 version of this patch extensively at Facebook. I'll post updates to the patches on here when that's more or less done and I have a chance to port them forward.","21/Jan/09 06:43;acmurthy;Here is a simple api proposal I've attached:

JobInProgress.addMapsSchedulingOpportunity() which is called at top of the scheduling loop before calling any of the obtain*MapTask methods.

A JobInProgress.getNumMissedMapsSchedulingOpportunities() method to get the no. of missed opportunities which can be used for global scheduling. We can use the same for HADOOP-4981 and other such areas...

Thoughts?","21/Jan/09 08:25;ddas;Arun (as Hemanth and I discussed offline with you), the direction of this seems good but I'd not set up this interface between the Scheduler and the JobInProgress. It seems more natural for the Scheduler to keep track of which jobs missed an opportunity in the current cycle and the corresponding overall counts (maybe through a utility class), rather than embedding that information within the job. Thoughts?","21/Jan/09 21:36;matei;I agree with Devaraj on this. The JobInProgress class should be agnostic to scheduling algorithm details - it's already doing way too much stuff. To keep track of per-job state, each scheduler can have an internal data structure. The fair scheduler has a JobInfo class for this already, and I believe the capacity scheduler has something similar too.

I think that ideally JobInProgress shouldn't be much more than a data structure letting you access useful info about the tasks in the job efficiently. The fact that it also does some scheduling (map locality and keeping track of speculative tasks) has led to a number of problems (schedulers scanning JIP fields directly to handle speculation and preemption, data structures not being maintained if speculation is off (HADOOP-4623), no way to know if a job can run a task on a node without calling obtainNew*Task and seeing if it returns null, a growing list of parameters to obtainNewMapTask, etc).","23/Jan/09 08:13;acmurthy;bq. The JobInProgress class should be agnostic to scheduling algorithm details - it's already doing way too much stuff. 

I agree, however that isn't something we can quickly fix. We already have a lot of scheduling information in the JobInProgress - locality-related caches etc. [1]

bq. To keep track of per-job state, each scheduler can have an internal data structure. The fair scheduler has a JobInfo class for this already, and I believe the capacity scheduler has something similar too. 

Given [1] this seems like we have the worst of both worlds - scheduling state in both JobInProgress and the schedulers, and worse - the exact same information maintained by _all_ schedulers. 

Hence my proposal to just keep it in JobInProgress for now... 

Meanwhile, I'm willing to hear a proposal for a Scheduler independent place to store job-related information, say JobInfo?
","25/Jan/09 08:53;matei;Having some kind of SchedulingInfo data structure associated with each job might be a good start towards separating scheduling stuff from non-scheduling stuff and ultimately having a common scheduler codebase. It could be a field within the JobInProgress, and maybe schedulers would be allowed to extend the base class to have their own info attributes. Is this the kind of thing you're proposing?

I'm still somewhat wary about making obtainNewMapTask sometimes not return tasks due to this scheduling opportunity stuff though. It seems like the more things we add to it, the harder it will be to break the cycle and switch to a saner API (hasMapTask and createMapTask for example). Furthermore, once a technique like this is in the JobInProgress class, it's hard to try out other methods for achieving the same goal. One of the nicer things about having the scheduler API is that although it makes the codebase more fragmented, it's enabled us to experiment with stuff like this. As a concrete example, once this basic patch is finished, I want to try a refinement for dealing with hotspot nodes that will launch IO-intensive tasks preferentially on those nodes to maximize the rate of local IO. Would it make sense to be working in JobInProgress for that? So I'd prefer if there was provide this functionality to all the schedulers without ingraining it in JobInProgress, or at least without blocking the road towards changes to this policy. Perhaps figuring out how to split up obtainNewMapTask into something generic that everyone can use (give me a task now) and something smarter (count attempts and handle the wait for me) and perhaps even a version that just says whether there is a task with the given locality level without initializing it would be possible without significant code changes. Does that make sense or do you think it's better to leave the API exactly the same and do this by default?","30/Jan/09 05:34;vinodkv;Matei, I just started looking at this JIRA and and your preliminary approach, but the first patch that you uploaded doesn't apply cleanly any longer on trunk. Agree that the approach is still being discussed upon, but can you please upload an updated patch so that it can help me in getting started? Thanks.","30/Jan/09 08:05;matei;I'll do this sometime in the next few days, since I have a couple of things on my queue before them. We have an updated patch for Hadoop 0.17 at Facebook, so there will be a number of improvements. If you want to try this patch in the meantime though, try reverting to revision 730768 (first week of January) and it should apply to that.","05/Feb/09 23:05;matei;Here is an updated version of this patch. It includes some bug fixes from testing and it is applies against the current trunk. Still haven't take into account some of Arun's comments though.","09/Feb/09 00:02;matei;Here is an update to this patch to take into account the changes I made to the preemption patch (HADOOP-4665).","09/Feb/09 00:03;matei;Some TODOs left:
- Switch from time-based waits to heartbeat counting
- Add a parameter to allow launching multiple tasks per heartbeat as in HADOOP-3136.","09/Feb/09 08:33;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12399790/hadoop-4667-v1b.patch
  against trunk revision 742171.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 9 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3814/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3814/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3814/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3814/console

This message is automatically generated.","21/Feb/09 08:25;matei;Updating patch to use latest changes to HADOOP-4665.","05/May/09 02:05;cdouglas;The patch no longer applies to trunk.","03/Jul/09 02:35;matei;I've been porting this JIRA to trunk now that HADOOP-4665 is in, and I tried making it use missed scheduling opportunities rather than time waitd in the process. However, I discovered a problem with that approach. Suppose that the cluster is full of long-running tasks except for 1 slot, and that our waiting job doesn't have local data on this slot. If we count missed scheduling opportunities and wait until we've seen as many as the total number of nodes, then we'll wait for (numNodes * heartbeatInterval) seconds, which is a very long time. On the other hand, setting the threshold to something smaller won't work in the case where the cluster is mostly idle. The problem is that the number of scheduling opportunities you get per second depends on the nature of tasks running in the cluster.

Therefore, I'm going to switch this patch back to counting time so that we have control over the amount of waiting. There will be a single call to System.currentTimeMillis at the start of the scheduler's assignTasks method. Owen, does one call per assignTasks have any performance impact in your experience? I imagine that even logging data through log4j causes gettimeofday to be invoked.","03/Jul/09 02:38;matei;(By the way, another solution I considered is to increment skip counts on every heartbeat, even if there are no free slots to assign tasks into during that heartbeat. But this seems silly - you have to iterate through the whole job list every time just to avoid one gettimeofday call.)","03/Jul/09 04:23;matei;Here is a new patch for this issue for trunk. It includes multiple unit tests both for delay scheduling and for assigning multiple tasks/heartbeat.","03/Jul/09 04:30;matei;A note on how the delay scheduling is activated in this patch. I chose to make the amount of time waited for a node-local task and then for a rack-local task to be 1.5x the current heartbeat interval each. This emulates the automatic scaling with heartbeat interval that was suggested. However, it is also possible to configure the delay manually through mapred.fairscheduler.locality.delay, and to disable it by setting this parameter to 0. A value of -1 for this parameter means ""compute the delay based on heartbeat interval"". That is the default.","03/Jul/09 08:23;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12412446/mapreduce-548.patch
  against trunk revision 790543.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/339/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/339/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/339/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/339/console

This message is automatically generated.","03/Jul/09 15:05;matei;Holy test failures, Batman. Sadly they seem to be unrelated to this patch (e.g. the streaming ones are from MAPREDUCE-699). Resubmitting patch because the capacity scheduler queue test was fixed this morning.","03/Jul/09 17:00;matei;Added a new unit test to verify that we balance load across nodes when the cluster is underloaded.","03/Jul/09 18:40;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12412446/mapreduce-548.patch
  against trunk revision 790918.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/347/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/347/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/347/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/347/console

This message is automatically generated.","03/Jul/09 22:45;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12412509/mapreduce-548-v1.patch
  against trunk revision 790971.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/348/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/348/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/348/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/348/console

This message is automatically generated.","06/Jul/09 15:25;matei;Test failures still appear unrelated.","08/Jul/09 23:53;matei;I updated the patch slightly to fix a deadlock that could occur with the auto-computing of the locality delay. Computing the next heartbeat interval requires a lock on the JobTracker. I was calling this while holding a lock on the FairScheduler, leading to a (FairScheduler -> JobTracker) locking order. However, assignTasks locks the JobTracker before the FairScheduler, so this could cause a deadlock. The solution was to move the auto-computing code outside the FairScheduler lock.

I also added a cap on the auto-computed locality delay of 15 seconds, because heartbeat intervals in large clusters can be 30-60 seconds. With this new cap there should still be no problem finding a rack-local slot if one exists.","10/Jul/09 03:38;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12412935/mapreduce-548-v2.patch
  against trunk revision 792704.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/370/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/370/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/370/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/370/console

This message is automatically generated.","17/Jul/09 11:02;tomwhite;I had a question about the latest patch:

* You have changed obtainNewLocalMapTask() and obtainNewNonLocalMapTask() in JobInProgress to delegate to obtainNewMapTask(). Is the check for {{!tasksInited.get()}} (from the former two) equivalent to {{status.getRunState() != JobStatus.RUNNING}} from the latter?","17/Jul/09 22:21;acmurthy;Matei, can you please ensure that the checks Tom pointed out are put back in?","17/Jul/09 23:23;matei;I've added those checks back. Arun, does this mean that checking for {{status.getRunState() != JobStatus.RUNNING}} instead of {{!tasksInited.get()}} in obtainNewMapTask is a bug? I removed the checks because I assumed that this one would also catch the tasks-not-inited case. I can also change the check in obtainNewMapTask to use {{!tasksInited.get()}} if necessary.","18/Jul/09 02:54;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12413886/mapreduce-548-v3.patch
  against trunk revision 794942.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/408/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/408/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/408/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/408/console

This message is automatically generated.","19/Jul/09 17:52;acmurthy;Yes, I think it's a good idea to check for tasksInited.get too... Devaraj?","20/Jul/09 05:46;ddas;I think what Matei did is okay. Basically, the tasksInited atomic boolean is used to take care of cases where job is failed/killed while initTasks is in progress (HADOOP-4236), and, for getting the setup/cleanup tasks without trying to lock the JobInProgress (HADOOP-5285). I don't think obtainNewMap* methods need to check the tasksInited flag. In those methods, the check for (jobStatus != JobStatus,RUNNING) should be sufficient since these methods lock the JobInProgress object, and, we invoke these methods only after initTasks has run. It will be nice if someone validates this..
But having the checks for tasksInited in those methods is harmless except for the extra check..","20/Jul/09 22:06;matei;I can add a check for both jobStatus != JobStatus.RUNNING and taskInited in my single obtainNewMapTask that the others are calling. In the pre-patch code, there was a lot of code duplication and some versions of obtainNewMapTask checked tasksInited while other checked jobStatus.","21/Jul/09 06:26;ddas;I think you can remove the check for taskInited. This will be true if the JobStatus is RUNNING.","22/Jul/09 20:23;matei;Alright, I've removed the check. Here's an updated patch for the current trunk.","28/Jul/09 05:02;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12414243/mapreduce-548-v4.patch
  against trunk revision 798239.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/426/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/426/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/426/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/426/console

This message is automatically generated.","14/Aug/09 16:33;matei;This feature has been committed as part of MAPREDUCE-706, so I'm closing this JIRA as a duplicate.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow admins of the Capacity Scheduler to set a hard-limit on the capacity of a queue,MAPREDUCE-532,12427077,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,rksingh,rajive,rajive,04/Jun/09 00:59,24/Aug/10 21:13,12/Jan/21 09:52,06/Jul/09 07:55,,,,,,0.21.0,,,capacity-sched,,,,,,0,,,,,"For jobs which call external services, (eg: distcp, crawlers) user/admin should be able to control max parallel tasks spawned. There should be a mechanism to cap the capacity available for a queue/job. ",,acmurthy,aw,ddas,ravidotg,vinodkv,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/09 15:41;rksingh;MAPREDUCE-532-1.patch;https://issues.apache.org/jira/secure/attachment/12412288/MAPREDUCE-532-1.patch","02/Jul/09 04:12;rksingh;MAPREDUCE-532-2.patch;https://issues.apache.org/jira/secure/attachment/12412354/MAPREDUCE-532-2.patch","06/Jul/09 08:51;yhemanth;MAPREDUCE-532-20.patch;https://issues.apache.org/jira/secure/attachment/12412592/MAPREDUCE-532-20.patch","02/Jul/09 07:59;rksingh;MAPREDUCE-532-3.patch;https://issues.apache.org/jira/secure/attachment/12412374/MAPREDUCE-532-3.patch","03/Jul/09 11:20;rksingh;MAPREDUCE-532-4.patch;https://issues.apache.org/jira/secure/attachment/12412479/MAPREDUCE-532-4.patch","04/Jul/09 12:33;rksingh;MAPREDUCE-532-5.patch;https://issues.apache.org/jira/secure/attachment/12412533/MAPREDUCE-532-5.patch","06/Jul/09 07:34;yhemanth;MAPREDUCE-532-6.patch;https://issues.apache.org/jira/secure/attachment/12412583/MAPREDUCE-532-6.patch","06/Jul/09 07:45;yhemanth;MAPREDUCE-532-7.patch;https://issues.apache.org/jira/secure/attachment/12412584/MAPREDUCE-532-7.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,2009-06-04 01:34:31.826,,,false,,,,,,,,,,,,,,,,,,37270,Reviewed,,,,Tue Jul 07 17:34:18 UTC 2009,,,,,,,"0|i02s9b:",14179,Provided ability in the capacity scheduler to limit the number of slots that can be concurrently used per queue at any given time.,,,,,,,,,,,,,,,,,,,,"04/Jun/09 01:34;ddas;Does HADOOP-5170 address the requirements?","04/Jun/09 02:33;rajive;
For job limits, Yes. Can we add similar limits per queue?

mapred.max.maps.per.queue and mapred.max.reduces.per.queue

","30/Jun/09 11:41;rksingh;Problem statement : There should be a option to control no of tasks
 run from the queue at any point of time.There should be mechanism to
 cap the capacity available for a queue/job.

 There are 2 approaches :

 1.Define a configuration variable ,
 ""mapred.capacity-scheduler.queue.<queue-name>.max.map.slots"". Likewise
 for reduces. This value is the maximum slots that can be used in a
 queue at any point of time. So for example assuming above config value
 is 100 , not more than 100 tasks would be in the queue at any point of
 time, assuming each task takes one slot. Typically the queue capacity
 should be equal to this limit. However, since the queue capacity is
 expressed as a percentage, it is likely to change, for e.g. if nodes
 go down or new ones are added. We were thinking of handling this
 discrepency by capping the queue capacity at the limit if required.
 So, if queue capacity is more than this limit, excess capacity will be
 used by the other queues. If queue capacity is less than the above
 limit , then the limit would be the queue capacity - as in the current
 implementation.

 2.Define a configuration variable ,
 ""mapred.capacity-scheduler.queue.<queue-name>.fixedCapacity"". This is
 a Boolean variable , once set to true , would make sure that queue
 capacity is the hard limit for the queue. So for example: cluster size
 is 200 , and queue capacity is 10% so hard limit is always 10% of the 
 cluster capacity. The problem with this approach is that the limit
 becomes dynamic , that is , if extra nodes are added to the cluster
 hard limit can actually increase . Given the use case this might not 
 be desirable.

 For the expressed use case, solution 1 seems more deterministic and 
 controlled. Does this work ?

 Note : All the calculations in Capacity scheduler are slot based ,so we
 have been using task and slot interchangeably. If the queue gets high
 RAM jobs, it might hit the limit earlier with fewer tasks. But this
 keeps the implementation simple and easy to follow.","30/Jun/09 19:51;rajive;I would prefer option 1. When we wan't to rate limit, absolute numbers is always good.","01/Jul/09 15:41;rksingh;This patch still need docs related changes.
","02/Jul/09 04:12;rksingh;removing System.out.println statement from the patch","02/Jul/09 07:59;rksingh;adding patch with doc changes and new change which solves the user limit problem","02/Jul/09 11:41;yhemanth;This is looking ok. Some comments (mostly minor):

- I would prefer the name LIMIT instead of 'CAP' everywhere.
- The formatting in CapacityTaskScheduler.start() where the new QueueSchedulingInfo is being created seems to be indented in too many lines. Can we fold them ?
- Methods introduced in TaskSchedulingInfo need not be public.
- areTasksInQueueOverCap - the getTSI call is repeated enough number of times to call once and cache.
- Since capacity is in terms of slots, I think we should compare against numSlotsOccupied as opposed to numRunningTasks. This also includes reserved tasktrackers in case we are dealing with high memory jobs.
- Just to be safe, I would recommend this check is for >=, rather than ==.
- Documentation of the maxTaskCap variable in capacity scheduler refers to 'map' slots, where it could be both.
- Currently we display the current # of slots in a queue in the UI. This could be lesser than the % of the cluster capacity configured if the limit parameter is defined and is lower. I think that might be confusing to the user.
- In the display, can we shorten the name, like maybe ""Map Tasks Limit"" instead of ""Maximum map tasks in a queue at a time :"". I also think it may be OK to not have a line separator for the limits, but club them with the Queue configuration section.

I am still to look at the test cases.","02/Jul/09 11:52;yhemanth;Some comments on test cases:

- Can we please include a short comment for the test cases describing what they do.
- We are using assert to check for some of the conditions. This does not work if the assert is not enabled. We should use assertNull instead.
- In all cases, I think it will be good to test the actual task returned, using the checkAssignment method. This keeps the logic of the test easy to understand.
- We also need a test case for user limits in the face of queue limits I suppose - to ensure the user limits are being computed based on the reduced queue limits.
- Also, we need a test case in the face of high RAM jobs - to make sure we count reservations  for hitting queue limits as well.","02/Jul/09 21:08;acmurthy;After discussions I'm changing the direction of this jira to simply allow for a configured hard-limit on the capacity of queues.

With this feature the consumers of the 'external service' will have to submit to a special queue for the service/resource, there-by limiting the fan-in for the service.","03/Jul/09 11:20;rksingh;Implemented the hemanth's changes and also added testcases for high ram jobs and user limit","04/Jul/09 06:01;yhemanth;This is looking good. I have few minor comments:

- Documentation of TaskSchedulingInfo.setCapacity needs to be updated. It is referring to setting the minimum value.
- TaskSchedulingInfo.toString() - The remaining count in case maximum task limit is less than capacity should be computed as maxTaskLimit-capacity.
- Also, we are not displaying the configured task limit anywhere.
- TaskSchedulingInfo.areTasksInQueueOverLimit - Documentation needs updating as we check for >= and also it is not running tasks, but occupied slots.
- In testHighMemoryBlockingWithMaxLimit, after the first map task is assigned, we check that no more map tasks are being assigned. This is correct. But just to be sure, we should also check that no reservations are created for the high ram job. One way to check that would be to make sure that checkOccupiedSlots does not change for map slots after the reduce of the second job is scheduled on tt1.
- testUserLimitsWithMaxLimits has some indentation errors.
- The 'delta' parameter introduced in checkOccupiedSlots is not very intuitive - can we do two things:
-- define an overloaded method, leaving the first one intact and use that for all the calls where 0,0 is passed as the last two parameters, 
-- and also document the parameters usage in the second method. This will make the number of changes very less in the test class.","04/Jul/09 12:33;rksingh;Applied the suggestion from hemanth","06/Jul/09 07:34;yhemanth;The attached patch (MAPREDUCE-532-6.patch) fixes a missed comment about not displaying the slot limit if configured. It also changes the display string about capacities a little bit.","06/Jul/09 07:36;yhemanth;Results of test-patch:

{noformat}
     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     -1 Eclipse classpath. The patch causes the Eclipse classpath to differ from the contents of the lib directories.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{noformat}

The -1 related to Eclipse classpath is because of a mismatch in ivy jars. I am told this does not need to be worried about.

The test changes only the capacity scheduler code base, and all unit tests of capacity scheduler pass.","06/Jul/09 07:45;yhemanth;The attached patch (MAPREDUCE-532-7.patch) corrects a missed Forrest documentation tag which was causing the docs build to fail. Verified docs are generated properly with this patch.","06/Jul/09 07:55;yhemanth;I just committed this to trunk. Thanks, Rahul !","06/Jul/09 08:51;yhemanth;Attached patch (MAPREDUCE-532-20.patch) is taken to apply against the Yahoo! Hadoop distribution at version 20. This is NOT to be committed externally.","07/Jul/09 17:34;hudson;Integrated in Hadoop-Mapreduce-trunk #15 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/15/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Collect information about number of tasks succeeded / total per time unit for a tasktracker. ,MAPREDUCE-467,12426555,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,sharadag,yhemanth,yhemanth,28/May/09 06:09,24/Aug/10 21:13,12/Jan/21 09:52,13/Jul/09 05:22,,,,,,0.21.0,,,,,,,,,1,,,,,"Collecting information of number of tasks succeeded / total per tasktracker and being able to see these counts per hour, day and since start time will help reason about things like the blacklisting strategy.",,aaa,amareshwari,ddas,iyappans,rksingh,sharadag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/09 10:29;sharadag;467_branch_0.20.patch;https://issues.apache.org/jira/secure/attachment/12413284/467_branch_0.20.patch","23/Jun/09 05:55;sharadag;467_v4.patch;https://issues.apache.org/jira/secure/attachment/12411499/467_v4.patch","29/Jun/09 07:27;sharadag;467_v5.patch;https://issues.apache.org/jira/secure/attachment/12412043/467_v5.patch","01/Jul/09 06:54;sharadag;467_v6.patch;https://issues.apache.org/jira/secure/attachment/12412242/467_v6.patch","09/Jul/09 11:25;sharadag;467_v7.patch;https://issues.apache.org/jira/secure/attachment/12413002/467_v7.patch","11/Jun/09 11:33;sharadag;5931_v1.patch;https://issues.apache.org/jira/secure/attachment/12410381/5931_v1.patch","16/Jun/09 05:39;sharadag;5931_v2.patch;https://issues.apache.org/jira/secure/attachment/12410757/5931_v2.patch","18/Jun/09 10:47;sharadag;5931_v3.patch;https://issues.apache.org/jira/secure/attachment/12411062/5931_v3.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,2009-06-04 10:26:20.448,,,false,,,,,,,,,,,,,,,,,,37258,Reviewed,,,,Mon Jul 13 18:37:46 UTC 2009,,,,,,,"0|i02s6v:",14168,Provide ability to collect statistics about tasks completed and succeeded for each tracker in time windows. The statistics is available on the jobtrackers' nodes UI page.,,,,,,,,,,,,,,,,,,,,"04/Jun/09 10:26;sharadag;To collect stats for last hour/day, we can have a moving window for that time period. A moving window can contain multiple time slots. The granularity of window movement/update is decided by the slot size. The slot size could be different for different time windows. For example, hour window could have 5 minutes, day window could have 1 hour update granularity. So in that case hour window would hold stats in 12 slots of 5 mins each. Likewise day window would hold stats in 24 slots of 1 hour each.

As the last slot time is crossed, a new slot would be added and the very first one would be knocked off. Hence moving the window by one slot.

A simple strategy could be to collect this information in TaskTracker and report that to JobTracker via TaskTrackerStatus. A subclass could be added to TaskTrackerStatus with fields, say:
tasksSinceStarted, tasksSuccededSinceStarted,
tasksSinceInLastHour, tasksSuccededInLastHour,
tasksSinceInLastDay, tasksSuccededInLastDay

To optimize on heartbeat size, we need not send the above fields with every heartbeat. This could be reported only at certain interval (typically the minimum slot size, 5 mins in above example).

An alternate way could be to compute all this in JobTracker. My vote goes for doing it in Tasktracker as this is mostly to do with individual Task tracker and doesn't need any global information.

Thoughts?
","04/Jun/09 10:28;sharadag;Correction: The fields names in last comment should read as:
tasksSinceStarted, tasksSuccededSinceStarted,
tasksInLastHour, tasksSuccededInLastHour,
tasksInLastDay, tasksSuccededInLastDay","04/Jun/09 15:27;yhemanth;I am assuming the moving window mechanism would be flexible enough to add new bucket sizes as required.

Regarding having the computation on the tasktracker, and reporting the status via status, one problem is that if we want to change the bucket size, it would involve a change in the status object.

Also, one requirement for this is to store this information on the JobTracker. Can you describe how this will be stored, mechanics with respect to lost tasktrackers etc  ? 

Will this information be available if the JobTracker restarts ?","11/Jun/09 00:51;sharadag;bq. I am assuming the moving window mechanism would be flexible enough to add new bucket sizes as required. 
Yes. I am planning to use and extend metric framework available in core, thru which custom window/bucket sizes can be defined.

bq. Regarding having the computation on the tasktracker, and reporting the status via status, one problem is that if we want to change the bucket size, it would involve a change in the status object.
To avoid that, instead of above fields, we can have say List<MetricInfo> metrics field in TaskTrackerStatus where MetricInfo could be:
class MetricInfo {
String name;
int tasks;
int tasksSucceeded;
}
Here name would be the name of the metrics. e.q. ""lasthour"", ""lastday"" etc. which could be configured in the metrics property file. 

bq. Also, one requirement for this is to store this information on the JobTracker. Can you describe how this will be stored, mechanics with respect to lost tasktrackers etc ?
Currently jobtracker doesn't store any information about lost tasktrackers. Storing info about lost trackers is not trivial and demands a separate jira issue. Consider the case of tracker getting lost and never coming back or coming back at different port. The jobtracker data structures need to be cleaned up for such trackers otherwise those data structures would be lying forever. 

bq. Will this information be available if the JobTracker restarts ?
Yes. Since this info is propagated from Tasktracker, it would be available after jobtracker restarts.","11/Jun/09 11:33;sharadag;This patch adds a MovingWindowContext class which captures the metrics in a moving time window. The window and bucket sizes can be configured using hadoop-metrics.properties
It is a very early patch. Testing is in progress. Not all fields captured.","16/Jun/09 05:39;sharadag;patch for review.","16/Jun/09 12:30;sharadag;Had an off line discussion with Devaraj/Eric, the concern raised is that metric context is an export interface and instead of using it, we should collect the metrics natively in hadoop. Administrators should not be able to remove this metric as it may in future used by Jobtracker to make decisions. Right?
Let me clarify a bit. Please note that only time windows are configured in the metric properties, and not the actual metric name which gets collected. Also a new context name is defined ""tasktracker"" (Refer hadoop-metrics.properties in patch) . So it does not come in between the existing metric contexts. Those can continue to be chukwa/ganglia etc.
If this doesn't sound like a good idea, I see few options:
1. Give a better name to the added context say ""core-mapred"", so that administrators don't override it. It would serve only to add/remove time windows.

2. Do not use Metrics api. Expose the time window configuration via mapred-site.xml.

3. Don't expose the configuration at all and have fixed windows, say ""last hour"" and ""last day"".

I went with extending the metrics API because I thought that it would help to collect any other existing metrics in time windows without making much change to the code. For example if we want to collect ""mapred"" metrics in time windows, then ""mapred"" context can point to the Composite context, which can be configured to use multiple contexts, one being time window context.

Thoughts?","18/Jun/09 10:22;sharadag;Had a discussion with Owen, following came up:
- Metric Api is an export interface, so we should not use it. We want to build the metrics natively in Hadoop so it should not be exposed via metrics config file.
- It is better to do the collection in jobtracker. The restart concern will go away as at some point we will have heartbeat transaction log. So recovery would be generic. Having it in jobtracker will give us more control to make scheduling decisions.
","18/Jun/09 10:47;sharadag;Attached patch collects the metrics in jobtracker. It doesn't use metric api. It defines a new class StatisticsCollector which keep statistics in time windows.
Stats are collected LAST_HOUR, LAST_DAY and SINCE_START. The stats are shown in jobtracker web ui on trackers list page.","19/Jun/09 00:23;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12411062/5931_v3.patch
  against trunk revision 785928.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/531/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/531/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/531/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/531/console

This message is automatically generated.","23/Jun/09 05:55;sharadag;Updated to trunk after the project split.
Also moved the time window list handling to StatisticsCollector from JobTrackerStatistics.","30/Jun/09 09:39;amareshwari;changes in JobInProgress and machines.jsp look good.
Comments in other code:
1. In JobTracker.ExpireTrackers,  statistics.taskTrackerRemoved(trackerName); should be called after the call to lostTaskTracker(current);
The same can be removed from lostTaskTracker(current).
2. Minor comment in JobTrackerStatistics and StatisticsCollector:  All binary operators except . should be separated from their operands by spaces. and 
A keyword followed by a parenthesis should be separated by a space. wrt http://java.sun.com/docs/codeconv/html/CodeConventions.doc7.html#475","01/Jul/09 06:54;sharadag;Updated the patch to trunk. Incorporated review comments.","01/Jul/09 09:16;amareshwari;Patch looks fine to me","02/Jul/09 05:44;sharadag;Retrying hudson","02/Jul/09 06:34;iyappans;Tested these scenarios  and found them to pass:

  a) Start a randomwriter job and check if all windows appear properly in the nodes section. 

Total Tasks last hour and succeeded task last hour
Total Tasks last day and succeeded task last day
Total Tasks since start and succeeded task since start

 b) Check after a job is run whether all tasks are captured properly in these windows. The number of tasks should be same. All windows needs to be populated.

 c) All windows needs to be refreshed after the time window given.

 d) Run simultaneous jobs and check if all windows are populated with proper values of tasks.

 e) Kill some tasks attempts and see if those numbers match.

 f) Run different kinds of jobs and see if tasks tracker is still able to get the number of tasks right. 

 g) Kill a job in the middle and see how tasks tracker numbers are populated in these windows.

 h) Check even after subsequent execution fo jobs and subsequent passing of time, still the tasks are able to be captured without any error.

 i) Restart job tracker and see if the tasks are captured properly.
","02/Jul/09 09:37;sharadag;ant test passed except TestJobInProgressListener, TestJobTrackerRestart and TestJobTrackerRestartWithLostTracker which are failing on trunk as well.
test patch passed as well:
+1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.","07/Jul/09 12:38;ddas;Looks fine to me. The UI can be improved. For example, the metrics could be printed out in a sorted fashion by number of succeeded tasks. The UI could also have percentage information instead of absolute numbers for the succeeded tasks' metrics.","07/Jul/09 15:51;sharadag;bq. the metrics could be printed out in a sorted fashion by number of succeeded tasks.
I agree that UI can be improved. Instead of doing piecemeal effort for providing sorting on succeeded tasks, I can imagine sorting capability on all the columns would be useful. Perhaps we can use some javascript API to do so which can provide sorting, pagination etc. This can be done in follow up jira.","09/Jul/09 10:48;iyappans;Additional testing done mainly on Jobtracker restart, TT restart and blacklisting. All testacses are found to pass:

1) If a tasktracker is globally blacklisted, that tasktracker should not  capture any more tasks. After coming out fo blacklisting after 24 hours, it should again start accepting tasks and increasing the task nubmers in its windows. : Pass

2) After restarting a blacklisted tasktracker, it shd be made healthy and continue to receive task numbers. - In 5 node cluster. :Pass

3) Killing a task tracker and also suspending a task tracker. - Killing a task tracker and restarting a tasktracker takes off all the information. Suspending and continuing with task tracker retains the information.In both scenarios they continue to receive task numbers after coming bacl to healthy mode. : Pass

3) JT is supended and brought back. - The task number windows are still retained.: Pass

4) Jt restart at differnt scenarios :
   a) When at least 3 tasks are waiting.   b) When file size of a  task is zero.   c) After some jobs are completed.    d) When it is two times restarted.   e) When a  job is 20% complete, when a job is 50% complete. - After job tracker restarts, all the information of the connected tasktracker, goes off. At this point the job.persist is true.

5) TT is suspended and brought back. Still  numbers shd be captured : pass

6) When different tasks are run at the same time. -Different task tracker windows is able to capture correct numbers. : pass

7) Kill a  task tracker and rejoin it. It shd work. - works, but previous task succeded info gone.

8) Run jobs with different priorities and then check if is captured properly. Also restart JT in this scenario and check if the job is restarted properly. : Pass

9) Changing job priority dynamically. How will it affect the task tracker's capture of tasks. -It captures it normally

10) In job level blacklisting scenario, tasks are continuign to be received.
","09/Jul/09 11:03;amareshwari;Sorry for the late comment,
update statistics should use tip.machineWhereTaskRan(taskid) insteadof status.getTaskTracker(). Then you may have to introduce a non-null check for the lost tracker case.","09/Jul/09 11:25;sharadag;Incorporated Amareshwari's comments.","10/Jul/09 10:37;iyappans;Tested some important scenarios and found them to pass:

1)  After restarting a blacklisted tasktracker, it shd be made healthy and continue to receive task numbers. - In 5 node cluster. :Pass

2) After task tracker is killed and goes out of node list, otehr nodes recive these tasks and execute them. Number of tasks match.

3) Some task attempts are killed. The numbers captured reflects teh failures properly.

4) Do a job restart. task trackers should start receiveing tasks again and reflect it in their windows..

5) For blacklisting scenarios, first  MAPREDUCE-746 needs to be fixed.

","10/Jul/09 10:57;amareshwari;+1 for the patch","13/Jul/09 05:22;sharadag;I just committed this!","13/Jul/09 10:29;sharadag;Patch for Yahoo's distribution for branch 20.","13/Jul/09 18:37;hudson;Integrated in Hadoop-Mapreduce-trunk #21 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/21/])
    . Provide ability to collect statistics about total tasks and succeeded tasks in different time windows.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide a node health check script and run it periodically to check the node health status,MAPREDUCE-211,12416755,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,sreekanth,aroop,aroop,12/Mar/09 15:18,24/Aug/10 21:13,12/Jan/21 09:52,30/Jun/09 14:01,,,,,,0.21.0,,,,,,,,,0,,,,,"Hadoop must have some mechanism to find the health status of a node . It should run the health check script periodically and if there is any errors, it should black list the node. This will be really helpful when we run static mapred clusters. Else we may have to run some scripts/daemons periodically to find the node status and take it offline manually.

",,acmurthy,atm,aw,cutting,ddas,eli,eric14,forest520,hammer,hong.tang,omalley,rajesh.balamohan,rajive,rksingh,sharadag,yhemanth,zhong,zzningxp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jun/09 05:48;yhemanth;MAPREDUCE-211-forrest.patch;https://issues.apache.org/jira/secure/attachment/12412132/MAPREDUCE-211-forrest.patch","18/Jun/09 09:58;sreekanth;active.png;https://issues.apache.org/jira/secure/attachment/12411054/active.png","18/Jun/09 09:58;sreekanth;blacklist1.png;https://issues.apache.org/jira/secure/attachment/12411055/blacklist1.png","25/Jun/09 08:30;sreekanth;blacklist2.png;https://issues.apache.org/jira/secure/attachment/12411764/blacklist2.png","18/Jun/09 09:58;sreekanth;cluster_setup.pdf;https://issues.apache.org/jira/secure/attachment/12411056/cluster_setup.pdf","29/May/09 03:48;sreekanth;hadoop-5478-1.patch;https://issues.apache.org/jira/secure/attachment/12409336/hadoop-5478-1.patch","12/Jun/09 05:16;sreekanth;hadoop-5478-2.patch;https://issues.apache.org/jira/secure/attachment/12410455/hadoop-5478-2.patch","15/Jun/09 11:37;sreekanth;hadoop-5478-3.patch;https://issues.apache.org/jira/secure/attachment/12410656/hadoop-5478-3.patch","16/Jun/09 10:33;sreekanth;hadoop-5478-4.patch;https://issues.apache.org/jira/secure/attachment/12410783/hadoop-5478-4.patch","16/Jun/09 13:20;sreekanth;hadoop-5478-5.patch;https://issues.apache.org/jira/secure/attachment/12410801/hadoop-5478-5.patch","18/Jun/09 10:01;sreekanth;hadoop-5478-6.patch;https://issues.apache.org/jira/secure/attachment/12411057/hadoop-5478-6.patch","25/Jun/09 08:31;sreekanth;mapred-211-common-3.patch;https://issues.apache.org/jira/secure/attachment/12411765/mapred-211-common-3.patch","23/Jun/09 09:16;sreekanth;mapred-211-core-1.patch;https://issues.apache.org/jira/secure/attachment/12411509/mapred-211-core-1.patch","30/Jun/09 12:36;sreekanth;mapred-211-internal.patch;https://issues.apache.org/jira/secure/attachment/12412161/mapred-211-internal.patch","23/Jun/09 09:16;sreekanth;mapred-211-mapred-1.patch;https://issues.apache.org/jira/secure/attachment/12411510/mapred-211-mapred-1.patch","23/Jun/09 10:03;sreekanth;mapred-211-mapred-2.patch;https://issues.apache.org/jira/secure/attachment/12411516/mapred-211-mapred-2.patch","25/Jun/09 08:31;sreekanth;mapred-211-mapred-3.patch;https://issues.apache.org/jira/secure/attachment/12411766/mapred-211-mapred-3.patch","26/Jun/09 15:11;sreekanth;mapred-211-mapred-4.patch;https://issues.apache.org/jira/secure/attachment/12411931/mapred-211-mapred-4.patch","29/Jun/09 11:27;sreekanth;mapred-211-mapred-5.patch;https://issues.apache.org/jira/secure/attachment/12412061/mapred-211-mapred-5.patch","30/Jun/09 03:51;sreekanth;mapred-211-mapred-7.patch;https://issues.apache.org/jira/secure/attachment/12412125/mapred-211-mapred-7.patch","30/Jun/09 06:26;sreekanth;mapred-211-mapred-8.patch;https://issues.apache.org/jira/secure/attachment/12412136/mapred-211-mapred-8.patch","30/Jun/09 09:49;sreekanth;mapred-211-mapred-9.patch;https://issues.apache.org/jira/secure/attachment/12412147/mapred-211-mapred-9.patch",,,,,,,,,,,,,22.0,,,,,,,,,,,,,,,,,,,,2009-03-14 15:33:39.38,,,false,,,,,,,,,,,,,,,,,,37271,Reviewed,,,,Tue Jul 07 17:34:21 UTC 2009,,,,,,,"0|i02s9j:",14180,Provides ability to run a health check script on the tasktracker nodes and blacklist nodes if they are unhealthy.,,,,,,,,,,,,,,,,,,,,"14/Mar/09 15:33;theiger; For consideration:

   * check_network_speed
    * check_filesystems
          o check for read-only filesystems 
    * check_ylock
          o required, if not installed some jobs which are using ycore libs will fail 
    * check_symlinks
          o checks for hadoop versions 
    * check_diskspace
          o checks if there is sufficient mapred tmp space
          o TT should handle this 
","14/Mar/09 22:03;aw;*sigh*

ylock is internal yahoo junk.

The real solution here is that there needs to be a script call out to provide user defined functionality.  Specific checks should not come out of the box by default.","17/Apr/09 21:20;aw;We were chatting a bit about this at staff today.  One of the topics that came up was what should happen if the TaskTracker is found to be in a bad state.  The general agreement was that it should be blacklisted with the option of coming back to life.   This helps prevents transient problems from becoming permanent problems.","12/May/09 10:58;yhemanth;We might start working on this soon. I thought it might be a good idea to share current thinking about the specs for this feature, and start a discussion.

In a brief discussion with the team, Eric and Owen, we came up with the following points:

- Provide an ability to the administrator to give a path to a script file that will be periodically run on the tasktracker. The interval of running can be configured.
- The tasktracker would run this in a separate thread and look for the exit code. If the script exits with a non-zero code, this will be reported to the JobTracker.
- Any output from the script (upon error) will be logged and if possible, also displayed on the web UI of the tasktracker.
- The jobtracker will blacklist this node when such a condition is reported.
- It will be a good idea to display the 'unhealthy' nodes on the UI.
- The tasktracker will need to continue to run the script so that if the condition is corrected, it will be reported again to the jobtracker for becoming available.
- A point came up about whether this mechanism can lead to the state of the node toggling. Maybe we can do some hysterisis, but as a follow-up for this jira.
- However, It may also be a good idea to show some stats like how oftern this node was blacklisted in the last 24 hours, and the current status by going to the tasktracker page on the web ui. This might help us decide if it's worthwhile introducing hysterisis.

Does this sound good ? Any other thoughts ?
","12/May/09 11:23;steve_l;-this fits in well with the ping/liveness stuff I've been doing. I already do this with things that GET the various JSP pages of a node, checking the health of the jetty endpoints. when those pages return a non-200 code, I return an error that includes the entire HTTP page sent back, as that is often useful. 

# It may be handy to have this stuff independent of the TT itself, so you can run a node-health checker on anything, and even if the TT refuses to play, you could do some checking of the node.
# Also, could it be a bit of JavaScript instead of a shell script?
# A scenario to worry about is what if something bad happens (e.g. a bit of NFS goes away) that causes all health checks in a big cluster to fail simultaneously. Would this overload the JT?
# Incidentally, I could imagine some scripts being slow, so I wouldnt have my ping code run the .sh every call, but instead run it on a regular frequency; a ping() would return the latest results. 
","12/May/09 15:15;steve_l;some related issues
* HADOOP-3323
* HADOOP-3767 
* HADOOP-3893
* HADOOP-5622","12/May/09 18:00;aw;

Torque specifically looks for a line that begins with ERROR on stdout, but reports the whole line in the node status.  So running pbsnodes will show the full status message and provides an easy way to audit all nodes on a giving torque server.  We really need the equivalent of dfsadmin -report for the JobTracker to provide this same level of output.

Additionally, torque ignores the exit status. In the vast majority of cases, the node is going to be good.  So the approach they take is that if a script has a syntax error (and would therefore have a 'fail' as an exit code), the node should be considered good anyway.","13/May/09 10:40;yhemanth;bq. this fits in well with the ping/liveness stuff I've been doing

This coupled with your comment on ping() returning the latest results seems to indicate that we have a thread that periodically executes and stores the results. In that sense, maybe we could build this solution now, and when HADOOP-3628 is committed to trunk, we could integrate the solution and results to be returned as part of ping(). Does that make sense ?

bq. It may be handy to have this stuff independent of the TT itself, so you can run a node-health checker on anything, and even if the TT refuses to play, you could do some checking of the node.

The health check script itself is definitely external and could be anything. All the TT would provide is the ability to run it periodically. So, I can imagine this being run standalone, or integrated with another daemon that provides a similar interface.

bq. Also, could it be a bit of JavaScript instead of a shell script?

Umm. Can we execute this from the TT directly ? AFAIK, this is not possible, right ? As of now, there is no plan to support anything other than a shell script.

bq. A scenario to worry about is what if something bad happens (e.g. a bit of NFS goes away) that causes all health checks in a big cluster to fail simultaneously. Would this overload the JT?

Since the plan is to send the information using the heartbeats itself, handling the load of requests should not be a problem. I am not sure how costly blacklist processing itself is on the JT, but hopefully not bad. We'll keep this in mind though.
","13/May/09 11:00;yhemanth;bq. We really need the equivalent of dfsadmin -report for the JobTracker to provide this same level of output.

Allen, I had command line reporting in mind when writing the specs. Worst case, if the effort seems large to cover we will do this as a follow-up JIRA, restricting the information to the web UI for now.

bq. Additionally, torque ignores the exit status

If this is acceptable to all, we can do the same as well.","13/May/09 14:29;steve_l;
Hemanth,

I could certainly merge this in with HADOOP-3628; I'm busy dealing with svn merge issues right now, and don't want this held up. I think it would be handy for me if we could run this on startup and then have ping query the latest state when checked.

>>     Also, could it be a bit of JavaScript instead of a shell script?

>Umm. Can we execute this from the TT directly ? AFAIK, this is not possible, right ? As of now, there is no plan to support anything other than a shell script.

you can run JS direct from a java6 jvm; the script engine is in the box. I've been wondering what it would take for JS support in MR jobs, but it could be handy for system health checks too, though native scripts give you the edge for low level system health. 


","14/May/09 06:52;eric14;looks good.

I think we should track some stats per node on the JT.  Just the total success & failures reported over the current and last hour and day long windows.  Showing this and current health and the error line (as allen suggests) on the JT console will let an operator quickly determine if any nodes are ill.

Do we currently track such success/failure ratios for tasks on a node?  That would also be great to display on the same console page.","15/May/09 11:43;ddas;TaskTrackers do report the number of failures, and it should be straightforward to maintain and report the number of successes as well.","29/May/09 03:48;sreekanth;Attaching first cut patch to address the issue: 

The patch does following:

* Patch requires two configuration items to be present in TaskTracker nodes, {{mapred.tasktracker.health_check_script}} and {{mapred.tasktracker.health_check_interval}} the {{mapred.tasktracker.health_check_script}} needs to be absolute path to script file. If the file does not exist when the TT starts up then the monitor is turned off.
* The monitor periodically runs the shell script. It ignores the exit code of the shell script, gets the output from the script, searches for a pattern ""ERROR"" in the output. 
* If ERROR is present in output, the monitor, sets health of the node as unhealthy and puts entire output as status to be set to JT.
* JT then depending on the value of the health of the node, decides to blacklist or white list the node.
* Attached test case which tests black listing and white listing as per output of the script.","03/Jun/09 04:34;omalley;One thing that bothers me is that it requires a fork and we've already seen that forks are expensive for   our servers. We'll also need a timer if the fork locks up to count as a failure.","05/Jun/09 17:28;hong.tang;- The health checking script may end up running more frequently than desired if the thread gets interrupted.
- The health checking script may also run at a lower frequency as desired because the code did not count the actual time spent on running the script. We should have a variable remember the last launch time, and in the beginning of the loop, get the current time, and either launch the script or sleep for the difference of the check-interval and the elapsed time since the last launch time.
- Like Owen said, we probably need to have a timeout when executing the script (and if it indeed happens, count it as failure).","07/Jun/09 13:52;steve_l;One thing to consider here is do you really want to run the health check in-VM, or do you want to bring up a second monitoring VM that can do the health checks, keep an eye on the VM running the node, and not be a threat to the stability of the main VM itself. ","08/Jun/09 04:39;hong.tang;+1. Might it be even easier by just hooking the script with cron?","08/Jun/09 09:40;steve_l;Cron? No, because you want to be able to force a health check with a ping() over the network, have a URL you can hit for the load balancer. ","12/Jun/09 05:16;sreekanth;Attaching a patch which address all other concern except forking a shell every delay interval.","12/Jun/09 06:02;omalley;I'd propose that we fork a small process off that runs the script for us every N minutes and reports status to the TT. That way we minimize the danger of causing the TT to fail. Furthermore, since it can have a tiny heap and few threads, the fork will be much cheaper. Thoughts?","12/Jun/09 07:30;yhemanth;To summarize some of the comments above, the issue we are discussing is whether the node health checker script should be launched as a separate process from the tasktracker (TT) itself, rather than as a thread in the TT, as done in the patch currently. There are some motivations for doing the same:

- A periodic process launch from a java service like the TT has caused problems in the past - for e.g. look at HADOOP-5059.
- Owen also mentioned instances where they'd seen the service itself lock up due to the process launch (and the underlying fork()/exec()) failing.

So, the proposal is to solve this problem by having the node health checker script as a separate process. This process can be configured with the following:
- Path to a script
- An interval
- TT's address for communication.

The process would periodically run the script (as done in the patch today) and report the status to the TT using RPC. To keep management simple, we can, in the first cut, launch this process from the TT itself and stop it when the TT is going down. In future, it should be possible to decouple this even more and have them run independently. The simplicity we buy in the first iteration is to not require administrators from worrying about managing this independently for the time being - until we gain some experience with how the health check script is running.

Does this sound fine ?
","12/Jun/09 07:48;hong.tang;@owen: Steve's comments seem to imply that we need the capability of launch the health checking script in response to external commands.

@Sreekanth: If I understand correctly, the new patch introduces another thread to check for command execution timeout. Is there a way to eliminate it (possibly by modifying shell execution API to accept timeouts)? ","12/Jun/09 08:55;vinodkv;bq. The process would periodically run the script (as done in the patch today) and report the status to the TT using RPC.
Can't the status reporting be simply done via stdout/stderr with START and END markers?","12/Jun/09 09:23;yhemanth;bq. Steve's comments seem to imply that we need the capability of launch the health checking script in response to external commands. 

Hong, while this is true, we can make it work as long as we are using an RPC to communicate with the health checking process. One command in the RPC is certainly the heartbeat that will both report the status as well as let the TT know the health checker is running. The other could be a 'run script now' command. This can be added as an extension on top of the basic framework - for e.g. when the ping is added to the TT. Would that work ?

bq. Can't the status reporting be simply done via stdout/stderr with START and END markers?

I suppose so. We've seen in the past a couple of issues if we're not careful with interacting with i/o streams of sub processes. Maybe a lot of these are fixed in Hadoop, and so not really an issue. But RPC seems simpler and cleaner. Would be glad to find what others think as well.","12/Jun/09 12:43;steve_l;I do a fair amount of health monitoring , and there is a lot to be said for something that runs is a separate process from any of the Hadoop services to do the checking.
# it could be its own service, fairly lightweight
# gives you the option of monitoring (and if need be killing) the TT process itself.
# stops you accidentally stamping on bits of the JVM. As an example, I'd deployed something that checked the health of bits of HDFS by checking that files where there, but that code closed the handle after use, killing any TT in the same process. ","15/Jun/09 09:26;sreekanth;Adding a little more to discussion, following is approach which I am taking to generate a new patch:

* Introduce a new health monitor service which is spawned off by task tracker when it starts.
* The service periodically reports the status of the node to the task tracker.
* The protocol is modeled out of {{TaskUmbricalProtocol}}
* The service would receive the host address and port as the command line arguments while starting up.
* The service then periodically sends the status update to task tracker based on the host and port specified to the service. 
* When TaskTracker is shutdown, the {{NodeHealthChecker}} would not be able to contact {{TaskTracker}} and would shut itself down.  The reason why this is done, is because task tracker's {{shutdown()}} or {{close()}} is not called when we do a {{stop-mapred.sh}} or task tracker can be killed with direct {{kill -9 ttpid}} in this case the TT might not inform all the clients which contact it to report services.
","15/Jun/09 11:37;sreekanth;Attaching a patch which does following:

* Added a new protocol which is used for node health reporting.
* Added a new Class which implements this Interface.
* The new class spawns off a new JVM, which does the health monitoring.
* Moved the timeout logic into {{Shell}}
* Retained the old test case which does
 1. Black listing due to node script reporting error 
2. White-listing of same tracker when script returns no error 
3. Blacklisting when the script times out.","16/Jun/09 08:59;yhemanth;To add to Sreekanth's comments:

- We are using a new port number for the TT to bind to for the health checker script to send updates. The other option was to use the same port as that used for the TaskUmbilicalProtocol. We thought the health service should not mix with child tasks reporting status and hence kept it different.

- The other important point is about how the health checker stops. Currently, the model is similar to how a child stops, in that if it can't report status to the TT, it kills itself. This is anyway required because it has to handle the case of the TT dying unexpectedly. However this is the extreme case. When the TT is stopped normally there are better options to stop the health check script. For e.g. we could add a shutdown hook to TT and send a signal to the health checker. We could make the health checker a separate daemon as well so that stop-mapred could stop it. Any of these options can be easily implemented as a follow-up once the basic structure is in place.

Please let us know if these points make sense.

","16/Jun/09 09:17;hong.tang;Both points make sense to me. One suggestion: when the health check fails to report status to TT, try to do a ""kill -9"" for the TT in case there might be a bug in the reporting sub-system that prevents health checker reporting states, but the other part of TT is still working fine. ","16/Jun/09 09:24;yhemanth;bq. when the health check fails to report status to TT, try to do a ""kill -9"" for the TT ...

Hong, is this to check if the TT is alive ? In which case, did you mean another signal, like -0 or kill -3. -9 is SIGKILL and would kill the TT. Also, in that case are you suggesting that we could keep the health checker around and continue trying to report after a while ?","16/Jun/09 10:33;sreekanth;Attaching patch merged with latest trunk.","16/Jun/09 10:57;steve_l;1. The timeouts in Shell would seem useful on their own; every shell operation ought to have timeouts for extra robustness.

2. This would fit fairly well under the HADOOP-3628 stuff, where the monitor would start and stop with the TT lifecycle; we'd have to think about how to integrate it with the ping operation -I think returning the most recent status would be good.


3. At some point in the future, it would be good for the policy of acting on TT failure to be moved out of the JT. In infrastructure where the response to failure is to terminate that (virtual) host and ask for a new one, you react very differently to failure. It's not something the JT needs to handle, other than pass up bad news.

4. I'm not sure about all the kill -9 and shutdown hook stuff, it's getting into fragile waters. Hard to test, hard to debug, creates complex situations especially  in test runs or stuff hosted in different runtimes

* this helper script stuff must be optional; I would turn it off on my systems as I test health in different ways.
* kill handlers are best designed to do very little and be robust against odd system states -and not assume any other parts of the cluster are live.

For the curious, the way SmartFrog  manages is its health is that every component tracks the last time it was asked by its parent for its health, if that time ever exceeds a (programmed) limit then it terminates itself. Every process pings the root component; its up to that to ping its children and act on failures -and to  recognise and act on timeouts. This works OK for single host work, in a cluster you don't want any SPOFs and tend to take an aggregate view : there has to be one Namenode, one JT, ""enough"" workers. I have a component to check the health of a file in the filesystem; every time it's health is checked, it looks for the file it was bound to, checks that it is present and within a specified size range. This is handy for checking that files you value are there, and that the FS is visible across the network (very important on virtual servers with odd networking). I dont have anything similar for checking that TT's are good, the best check would be test work.","16/Jun/09 12:33;yhemanth;I've started reviewing the patch. I think the last patch missed adding the new files. Please correct it.","16/Jun/09 13:20;sreekanth;Patch which includes the missed out file.","16/Jun/09 13:27;yhemanth;This is a reasonably complex patch, and in the current state, it has already come a long way. I do have the following comments:

Comments on the spec:

- It may be useful to allow arguments to be provided to the health check script.

Shell:

- It seems like it will be easier to use a Timer to do the interrupting rather than implementing the logic ourselves.
- The timeout thread seems to be being interrupted and joined twice.
- Do we need the setTimeoutInterval ? What happens if it is set after the command has started. I think it is better to have it set only once.

Configuration:

- Default value for mapred.healthChecker.interval can be a little larger, like 60000.
- Let's call mapred.healthChecker.failure_interval mapred.healthChecker.script.timeout. Likewise the variable in NodeHealthChecker failureintervalTime as scriptTimeout or some such name.

NodeHealthCheckerService:

- Instead of setting the health status in the TaskTracker, I think the variable can simply be stored as a local variable by NodeHealthCheckerService, and TT can call it using an appropriately synchronized accessor method.
- If the thread launching the health checker VM is a daemon, I don't think there's a need to join it in close.
- Does NodeHealthCheckerService need to be a public class ?
- Please give a name to the thread launching the health checker VM

NodeHealthChecker:

- Typo: HEALTH_CHECK_INTERVAL_PREOPERTY should be HEALTH_CHECK_INTERVAL_PROPERTY
- If there is an exception in executing the script - should we treat this as a failure ? As a follow-up, should we ignore the output in such a case, as it may not be meaningful to rely upon ?
- A related question is what to do if the output is null. Currently we don't seem to be reporting anything at all. This clearly seems like a bug. I think if it did not timeout, this should be treated as a success.
- Also, should we be more careful in where we check for ERROR. I think the output should begin with the string ERROR, rather than ERROR appearing anywhere in the string.
- I think we should check whether the script timed out before we check the output. It reads more logically that way.
- I would also suggest we move the code in the finally block to a separate method for making it more obvious. Among other things it would help us unit test this very important logic easily.
- I think the check for the health check script should be done in NodeHealthCheckerService. This will avoid us the cost of launching the VM and then failing. However, basic checks for the script can still be there in NodeHealthChecker so it can be used as an independent unit.
- Where do the log messages of the NodeHealthChecker go ? I think it should go to a separate log file.
Give a name to the Timer object created for running the health check script.

JobTracker:

- unBlacklistTracker may not blacklist a tracker if the reason for blacklisting was different. Shouldn't this be checked whenever a call is made, and the subsequent actions be done only if unBlacklistTracker returns a success.
- When tasktracker status is updated because the node becomes healthy, shouldn't isBlackListed be set to true ?

TaskTracker:
- shutdown() seems to have got an extraneous log message by mistake.

Test cases:
- In the testcase, can we also verify the healthReport value for unhealthy trackers ?
- I think we also need a testcase that combines task failures with health script failing and make sure we don't unblacklist a tracker if it was blacklisted due to another reason.

Documentation:
- New classes (like NodeHealthCheckerService) need Javadoc. Indeed, please document all new methods.
- We also need Forrest documentation on this super cool feature. *smile* 
","16/Jun/09 15:20;hong.tang;bq. Hong, is this to check if the TT is alive ? In which case, did you mean another signal, like -0 or kill -3. -9 is SIGKILL and would kill the TT. Also, in that case are you suggesting that we could keep the health checker around and continue trying to report after a while ?

@hemanth sorry for not being clear. I gave a bit more thoughts on the problem, and I think the following logic may be simpler and more robust (1,2 are the current logic, 3 is my suggestion) : 

(1) periodically launch the health checking script; 
(2) reporting status that back to TT (both good and bad); 
(3) if it fails to receive response from TT, wait for X seconds, do an extra kill (to ensure TT is dead), and quit itself. 

I scanned through the code, it seems that NodeHealthChecker.stop() would be a good place to perform step (3).","16/Jun/09 17:54;aw;bq.We are using a new port number for the TT to bind to for the health checker script to send updates. The other option was to use the same port as that used for the TaskUmbilicalProtocol. We thought the health service should not mix with child tasks reporting status and hence kept it different.

This is disappointing.  Hadoop has enough ports open that I think it qualifies as a cheese.","17/Jun/09 04:14;yhemanth;bq. This is disappointing. Hadoop has enough ports open that I think it qualifies as a cheese.

I was counting on you to object, Allen *smile*.

What do others feel ? I guess we could piggyback on the port used for the TaskUmbilicalProtocol - need Sreekanth to confirm this though. The main concern is if it will interfere with the processing of the tasks reporting their status to TT. To be clear though, the amount of work done in the RPC for the health reporter call is minimal - it just sets two variables.","17/Jun/09 04:22;yhemanth;bq. if it fails to receive response from TT, wait for X seconds, do an extra kill (to ensure TT is dead), and quit itself.

Hong, I am not certain about this. I am essentially viewing the TT as the master still, and the health monitor is just a helper service that monitors the health of the node, not the health of the TT itself. It seems wrong that this service could kill the master. I can conceive in future that we extend this to monitor the health of the TT also. And take corrective actions in case something is wrong with the TT. But I think that should be the topic of a different JIRA, or at a minimum an extension to this one. I would still like the scope of this to be restricted to providing a plug-in for checking the health of a node.



","17/Jun/09 05:00;omalley;I'd vote for using the same RPC port. I don't think it buys us enough to be worth the hassle of dealing with the extra port.","17/Jun/09 06:15;hong.tang;@hemanth The problem is that if there is something wrong that prevents the health checker from communicating to TT, health checker would quit voluntarily without TT's knowledge. So the value of this health checker service could be discounted. But on a second thought, I think maybe we should stop worrying about that for now and defer more complex logic to a later time.
","17/Jun/09 06:22;yhemanth;bq. The problem is that if there is something wrong that prevents the health checker from communicating to TT, health checker would quit voluntarily without TT's knowledge

That does sound like an issue. Maybe one simple solution is to send a timestamp with the TaskTrackerStatus report about when the health checker was last run. I am of course borrowing the idea from the information we have about when the last heartbeat was received from a TT. We could use that information to find out trackers that haven't updated their health for longer than a certain interval. Would that work ?","17/Jun/09 06:32;hong.tang;bq. Maybe one simple solution is to send a timestamp with the TaskTrackerStatus report about when the health checker was last run. I am of course borrowing the idea from the information we have about when the last heartbeat was received from a TT. We could use that information to find out trackers that haven't updated their health for longer than a certain interval. Would that work ?

I like it.","17/Jun/09 06:35;hong.tang;@owen Is there ever any chance the main RPC being overwhelmed and thus may not be able to respond promptly to health status information?","17/Jun/09 13:19;steve_l;# timestamps would help, would push more analysis up to whoever is asking the TT.
# If the main RPC is so owverwhelmed it can't answer a health query, that's a sign of a problem anyway. Your TT is no longer _live_

The health checking would fit in with the notion of a {{Ping}} operation, as raised in HADOOP-5622. Every service should have a way of saying ""are you up"". The failure to answer the query: trouble. If the call returns with an error: trouble. If the call returns saying it is well, then all you know is that the service thinks it is well, but still may not be capable of useful work.

What this operation does do is set more requirements on what gets returned -you probably want to return something machine readable, that can be extended by different services, depending on their view of the world. A hashtable containing writable stuff, perhaps. ","17/Jun/09 14:46;hong.tang;bq. The health checking would fit in with the notion of a Ping operation, as raised in HADOOP-5622. Every service should have a way of saying ""are you up"". The failure to answer the query: trouble. If the call returns with an error: trouble. If the call returns saying it is well, then all you know is that the service thinks it is well, but still may not be capable of useful work.

Though, at both times, the only one who knows about the trouble is the health checker and not the rest of the world. ","17/Jun/09 16:00;steve_l;> Though, at both times, the only one who knows about the trouble is the health checker and not the rest of the world.

why is why your management tools
# need to be HA toys themselves
# need to be able to ask the apps for their health
# may need to be able to do test jobs to probe system health
# may need the ability to react to failure according to the infrastructure in which HDFS is running, and your policy. 

If HDFS is running in anything that supports the EC2 APIs, if a TT is playing up I'd start by rebooting that node, if it still doesn't come up, decomission the namenode, terminate the VM and ask for a new one. That's a very different policy from a physical cluster, where you may want to blacklist the TT while its datanode services stays live. ","18/Jun/09 03:36;yhemanth;Folks, I still maintain that the focus of this jira is just checking health of the node as determined by an administrator supplied script. The last few comments are focusing more on health of a TT. For the purpose of making incremental progress, let us stick to the original scope and defer discussions of checking the health on the TT, and corrective actions there-of, to a separate jira.

So, to summarize, the health checker kills itself if it cannot communicate with the TT (similar to the child JVM). If this happens because the TT is down, well and good. The 'lost tasktracker' logic of the jobtracker would ensure this status is captured. If this happens because the TT was overwhelmed, well, maybe the TT is not 'healthy' any more. But the fact that we are reporting timestamps of the last health status gives the administrators an opportunity to know that something is amiss on this node, because it's health has not been updated for a while. Either way we can alert ourselves to problems. So, the purpose is still solved. Of course, there are better, more automated ways to do it. That would qualify for a next increment.

Hope this makes sense.","18/Jun/09 09:58;sreekanth;Attaching documentation pdf incorporating documentation change.

Attaching screenshot listing UI changes.","18/Jun/09 10:01;sreekanth;Attaching latest patch incorporating Hemanths comments:

Major Changes:

* Now {{NodeHealthChecker}} reports health status on same port which tasks use to report status.
* {{NodeHealthChecker}} can now be run in seperate VM or as thread, thread based start up can be used in {{MiniMRCluster}}
* Changed testcase to also test conditions with blacklisting across jobs and also verifying cluster capacity after we blacklist tracker.
* Also added new configuration entry which takes node health scripts arguments.
","18/Jun/09 10:40;hong.tang;+1 to hemanth's comments. ","18/Jun/09 15:41;aw;bq. But the fact that we are reporting timestamps of the last health status gives the administrators an opportunity to know that something is amiss on this node, because it's health has not been updated for a while.

Hmm.  What interface do admins have that make this obvious?  If a cluster has 2500 TTs, it isn't going to be obvious in a web UI that any given TT is sick.  ","18/Jun/09 16:14;yhemanth;bq. What interface do admins have that make this obvious? If a cluster has 2500 TTs, it isn't going to be obvious in a web UI that any given TT is sick.

Allen, just to be clear, nodes detected as unhealthy by the health check script are blacklisted as was discussed and as you commented in one of your earliest comments on this JIRA. Therefore the 'mapred job -list-blacklisted-trackers' will easily point out these.

We are only discussing a specific case where the health checker VM exits on a node, say because of some transient problem on the TT, *but* the TT is still up, and possibly unhealthy. It was only for detecting these TTs that I was talking about the timestamps. I agree with you that it will be difficult to look this information up on a web UI. But the fact that we are having this information centrally will help us build / enhance existing tools. 

I believe even with the current scope this feature is worth a try on live clusters. Given we have no way of detecting unhealthy nodes now, this is an improvement. If indeed we find many instances where the health checker is going down while TTs are up, we can easily provide additional tools based on the information currently being reported. Hope that makes sense.","21/Jun/09 15:10;yhemanth;Some comments on the second iteration. I still need to look at the forrest documentation, will do so after the code changes are fine:

Shell.java:
- Refactor the constuctors in ShellCommandExecutor to all reach one constructor that takes the timeout interval, instead of the current implementation which adds a timeout parameter to all the constructors.
- Also the static API should be overloaded to provide the timeout parameter.
- If the timer fires after the process exits but before the completed boolean is set to true, process.destroy is called twice and also the command is set as timed out. Maybe we should check if the process's exit value can be successfully retrieved to see if the process has terminated.
- I think we should set completed to true in the same place as before, which is after the error thread is joined. This will keep the behavior similar.

NodeHealthChecker:
- ignoreOutput is set to true if the script gets an exception. In this case, we are reporting the node as healthy, whereas in reality we don't know that. I don't think we should report true in such a case. Choices could be to not report anything, or to report it as unhealthy, with the exception string as the message - so it will show up to the administrators. Needs some discussion though.
- If there was a general exception in executing the script, ignoreOutput is set to false, and we are trying to read the output, which may not be valid.
- Regarding the error string, we should probably check what would be a right model for this. I actually meant that we check only if the entire output begins with ERROR. But what is currently implemented may also be right.
- Minor nit, let's read the output, only if the command has not timed out.
- No name given to the NodeHealthMonitorTimer.
- JobConf is being loaded multiple times in this class. This is a costly operation. I don't think it needs to be loaded that often. We can cache the value of JobConf.
- We are attempting to set the execute permissions on the script. That implicitly assumes the file will be owned by the user running the mapred script. This may not be right. I think it is easier to assume that the script has execute permissions set, and we'll fail if it doesn't. We could check this in shouldRun.
- NodeHealthMonitorTimer is not a timer, call it NodeHealthMonitorExecutor
- We don't need the timedout variable in the NodeHealthMonitorTimer instance. It can always be got from the shellcommandexecutor.
- I am thinking that 'lastReported' time can actually be filled in by the NodeHealthCheckerService itself.


JobTracker:
- It looks like the blacklisting and whitelisting of nodes is not exactly correct. The following invariants must hold true for a node.
-- A node can be blacklisted if it is unhealthy, or if the number of failures exceeds the configured limits. It can fail due to both reasons, in which case both of these must be captured in the fault info.
-- A node can be whitelisted iff it becomes healthy AND the number of failures falls below the limits. For e.g this means that shouldAssignTasksToTracker should check the health of the node also.
-- One of the two conditions alone can change - that is, an unhealthy node can become healthy, or number of failures alone could fall below the limits. In such cases, the node will still be blacklisted, but the reason for blacklisting would change.
- It seems like thinking about this problem in terms of a state transition will help coding.

TaskTracker:
- The update to the health monitor variables is done atomically, but the read is not so, because the read does not lock the tasktracker.
- Why do we need to stop the health checker service in start.

Documentation:
- Class documentation is missing in javadocs for the new classes, though methods are documented.

machines.jsp:
- Shouldn't the number of columns be 8/9 instead of 9/10.","21/Jun/09 15:56;hong.tang;""Node Healthy Status"" - can we show ""healthy/unhealthy"" instead of ""true/false""?","23/Jun/09 09:16;sreekanth;Attaching patches which incorporate Hemanths comments. Split the changes into two parts, commons and mapred.","23/Jun/09 10:03;sreekanth;Attaching latest patch, some debug statements accidentally gotten into the {{mapred-211-mapred-1.patch}}","24/Jun/09 05:35;eric14;-1

I think this patch has gotten too complex.  I don't follow the need for a sub-process to launch the scripts and attendant complexity.

1) The TT forks processing for a living.  The health check should be happening less frequently than task launch, so we should expect a simple fork to work.  If it does not, the TT is not healthy...

2) I see advantages to having an autonomous process doing the health checking.  There are pluses and minuses to that.  But a dedicated child of the TT has none of those advantages.

3) Complexity == bugs == more unhealthy TTs.  

We should strip this down to as simple a patch as possible.","24/Jun/09 06:27;yhemanth;Caught up with Owen and Sreekanth, and tried to drive to a consensus. We think Eric's arguments make sense. For the sake of closing on this patch, we are doing an about turn to the original model of having a simple thread that launches the health check script periodically. However, the change from the original patch will address Hong's and Owen's concerns about fork / script timing out, because the changes to the Shell class which introduce timeouts on launched processes will be retained. This will prevent the TT from holding up. Note there are test cases which actually test a timing out script.

At a point in future, I can imagine a completely separate service for doing the health checking (as was mentioned by Steve in one of the comments above). However, we'll get there when we get there. We will just start off with a simple in-VM process for now.","24/Jun/09 07:12;hong.tang;Going through the jira history again after I read eric and hemanth's comments, I began to realize that we are deviating from the original intention of having something simple to start with, and instead are adding bits and pieces that could/should go to future iterations or a completely different implementation (eg as a separate service). So +1 on the about turn. :)

@hemanth, just to be sure, we will still keep the time-stamp for the last successful launch of health check, right? ","25/Jun/09 08:30;sreekanth;Attaching UI from the latest patch:

* Made changes according to Hong's comment. Now showing ""healthy/UnHealthy"" instead of True/False
* Added a column called Seconds since node reported healthy.","25/Jun/09 08:31;sreekanth;Latest patch with following changes:

* Change from seperate VM to in VM of TaskTracker.
* Merged with latest trunk.","26/Jun/09 11:35;yhemanth;Some comments on this issue:

- NodeHealthCheckerService.initalize should be NodeHealthCheckerService.initialize (typo)
- reportHealthStatus can be simplified, currently it is proving to be somewhat difficult to follow the logic. One option could be to do the following:
{noformat}
 private Enum HealthCheckExitStatus {
   SUCCESS,
   TIMED_OUT,
   FAILED_WITH_EXIT_CODE,
   FAILED_WITH_EXCEPTION
 }
{noformat}
Fill this up where the script is run with the appropriate status. Then pass it to reportHealthStatus. Then we could just use the single Enum value and have a if..else if.. else block that reads better. Would that work ?
- Please give name to the Timer. (I've pointed this out in previous comments also)
- When the health checker is timed out, we should not be setting the timestamp.
- What do we show on the UI if the health checker is not configured on a TT. My suggestion would be to fill in timestamp as 0 in the TTstatus and if yes, then show status as N/A and no timestamp or message.
- We are not getting the values of the health status (status, message and time) atomically in Tasktracker. We should probably lock on the health status service object and get these values when filling up the task tracker status. (Raised this issue earlier)
- InterTrackerProtocol version should be changed (because TaskTrackerStatus structure has changed)
- TaskTrackerHealthStatus constructor has a constructor taking numberOfRestarts, which is not required.
- On JobTracker, it looks like currently we are storing all trackers - even healthy ones in the potentiallyFaultyTrackers data structure. This is unnecessary. If we fix this, we should also ensure that when a node's fault count falls to zero and is healthy, it is removed from this structure.
- The formatting in machines.jsp doesn't seem right. Can you please check it ?
","26/Jun/09 15:11;sreekanth;bq. NodeHealthCheckerService.initalize should be NodeHealthCheckerService.initialize (typo)
Done.
bq. reportHealthStatus can be simplified, currently it is proving to be somewhat difficult to follow the logic. One option could be to do the following: 
Done, introduced a new Enum {{HealthCheckExitStatus}} and doing the health report based on {{switch}} instead of nested {{if}}
bq. Please give name to the Timer. (I've pointed this out in previous comments also)
Done. Timer is named {{NodeHealthMonitor-Timer}}
bq. When the health checker is timed out, we should not be setting the timestamp.
Done.
bq. What do we show on the UI if the health checker is not configured on a TT. My suggestion would be to fill in timestamp as 0 in the TTstatus and if yes, then show status as N/A and no timestamp or message.
Done, when health checker is not configured, we are setting last updated time of node health to zero. Based on that setting the health status string as N/A.
bq. We are not getting the values of the health status (status, message and time) atomically in Tasktracker. We should probably lock on the health status service object and get these values when filling up the task tracker status. (Raised this issue earlier)
Done, introduced a method {{org.apache.hadoop.mapred.NodeHealthCheckerService.setHealthStatus(TaskTrackerHealthStatus)}} which is synchronized at object level all sets of status in {{NodeHealthChecker}} are synchronized.
bq. InterTrackerProtocol version should be changed (because TaskTrackerStatus structure has changed)
Done.
bq. TaskTrackerHealthStatus constructor has a constructor taking numberOfRestarts, which is not required.
Done.
bq. On JobTracker, it looks like currently we are storing all trackers - even healthy ones in the potentiallyFaultyTrackers data structure. This is unnecessary. If we fix this, we should also ensure that when a node's fault count falls to zero and is healthy, it is removed from this structure.
Done, we are creating the {{FaultInfo}} lazily, and removing it in {{unBlackList}} based on the fault count.
bq. The formatting in machines.jsp doesn't seem right. Can you please check it ?
Done, removed accidental tab characters.

","29/Jun/09 11:27;sreekanth;Attaching latest patch incorporating Hemanths offline comments:

* Fixing bug in the {{exceedsFaults()}} which previously didnt record if the tracker was blacklisted before.
* Refactoring the node health status changes into a single common method.","29/Jun/09 14:12;yhemanth;This is looking good now. The changes on the jobtracker are making more sense than before.

I have a few minor nits:

- The test methods added in JobTracker.java seem to be incorrectly indented.
- In the JSP, can you please check if the colspan value is correct ?
- Again in the JSP, we are computing the time since health reported success as Math.abs(time reported - current time). It can simply  be (current time - time reported)
- The test case's intent is perfect. Unfortunately, it can timeout if something goes wrong with the TTs. I would suggest we remove this problem.
- I have a few editorial comments on the forrest documentation. It is easier to correct the documentation and give you a patch for just that file, I'll do that rather than commenting on the JIRAs.","30/Jun/09 03:51;sreekanth;Incorporating Hemanth's Comments:

* Changed to the current number of columns in machines.jsp
* Changed indentation of {{JobTracker}} test methods.
* Removed old test case added two new test cases instead {{TestTaskTrackerBlacklisting}} and {{TestNodeHealthService}}
* {{TestNodeHealthService}} test functioning of {{NodeHealthService}} i.e on {{TaskTracker}} side
* {{TestTaskTrackerBlacklisting}} tests functionality of blacklisting on {{JobTracker}} end.","30/Jun/09 05:48;yhemanth;The recent changes also look fine. There is one small problem though that Sreekanth and I noticed while reviewing the tests. If the health check script is configured, but before it has run, we report a heartbeat, the default values of the node health are set to unhealthy. They should be healthy.

Also, I have attached here, a patch for the cluster_setup.xml file correcting documentation. Can you please use this text as is ?","30/Jun/09 06:26;sreekanth;Attaching patch with modification from Hemanth and setting initial node health value as true.

The patch would break the trunk compilation as it requires the changes which are present in latest trunk.","30/Jun/09 06:55;yhemanth;+1. I think this is good to go. Can you please run test and test-patch using the latest hadoop-common jar files, and upload results. I will commit it once done. Regarding the break in compilation, this is due to the dependent issue - HADOOP-6106. I updated [a comment there|https://issues.apache.org/jira/browse/HADOOP-6106?focusedCommentId=12725499&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12725499] about how we are planning to get the dependency into the HDFS and Map/Reduce sub projects to fix this.","30/Jun/09 09:49;sreekanth;Attaching latest patch fixing findbugs and release audit warnings.

Fixed also a bug found while TT reinit, the health checker null check was not done in {{TaskTracker.close()}}","30/Jun/09 09:50;sreekanth;output from ant test-patch

{noformat}
    [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 7 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec]
{noformat}","30/Jun/09 12:33;sreekanth;All tests passed locally except:


{{TestJobTrackerRestart}} - MAPREDUCE-683 also failing consistently on trunk
{{TestJobTrackerRestartWithLostTracker}} - MAPREDUCE-171 also failing on trunk.



","30/Jun/09 12:36;sreekanth;Internal Yahoo! patch for the issue.","30/Jun/09 13:36;sreekanth;All contrib test passed except {{TestQueueCapacities}} mentioned in  MAPREDUCE-522","30/Jun/09 14:01;yhemanth;Since the test cases were failing on trunk as well, I committed this. Thanks, Sreekanth !","30/Jun/09 18:33;aw;> Internal Yahoo! patch for the issue. 

Posting to the Internet sort of makes it not internal anymore. :(","01/Jul/09 04:00;yhemanth;Allen, this is as per the new rules to have all patches that we are running at Yahoo! to be publicly available following the announcement at the Hadoop Summit. But I agree, it is not internal anymore. I would just change the emoticon, because it seems like a good thing :-)","07/Jul/09 17:34;hudson;Integrated in Hadoop-Mapreduce-trunk #15 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/15/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Large-scale, automated test framwork for Map-Reduce",MAPREDUCE-1154,12439026,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,,acmurthy,acmurthy,25/Oct/09 06:47,27/Jul/10 22:45,12/Jan/21 09:52,27/Jul/10 22:45,,,,,,,,,test,,,,,,0,,,,,"HADOOP-6332 proposes a large-scale, automated, junit-based test-framework for Hadoop.

This jira is meant to track relevant work to Map-Reduce.",,chaitk,cutting,ddas,hammer,hong.tang,iyappans,jrideout,lianhuiwang,nidaley,philip,sharadag,sreekanth,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-6248,,,,,,,,,,,,,,,,,,,,,"25/Oct/09 07:15;acmurthy;testing.patch;https://issues.apache.org/jira/secure/attachment/12423139/testing.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2009-10-26 05:01:56.618,,,false,,,,,,,,,,,,,,,,,,149316,,,,,Tue Jul 27 22:45:16 UTC 2010,,,,,,,"0|i0jfxj:",111511,,,,,,,,,,,,,,,,,,,,,"25/Oct/09 07:03;acmurthy;Some Map-Reduce specific functionality to provide a flavour for the proposal:

{noformat}
  /**
   * Daemon/job artifacts.
   */
  public enum Artifact {
    LOGS,
    OUTPUTS
  }

  /**
   * Get tasktrackers on the given rack.
   * @param cluster map-reduce <code>Cluster</code>
   * @param rackId rack
   * @return tasktrackers on the given rack
   * @throws IOException
   * @throws InterruptedException
   */
  public static List<TaskTracker> getTaskTrackersOnRack(Cluster cluster, 
                                                        String rackId)
  throws IOException, InterruptedException;
  
  /**
   * Get tasks running on the given tasktracker.
   * @param cluster map-reduce <code>Cluster</code>
   * @param taskTracker task-tracker
   * @return tasks running on the given tasktracker
   * @throws IOException
   * @throws InterruptedException
   */
  public static List<TaskAttemptID> 
  getTasksRunningOnTaskTracker(Cluster cluster, TaskTracker taskTracker)
  throws IOException, InterruptedException;
  
  /**
   * Get tasktracker on the given host.
   * @param cluster map-reduce <code>Cluster</code>
   * @param hostName host
   * @return tasktracker on the given host
   * @throws IOException
   * @throws InterruptedException
   */
  public static TaskTracker getTaskTracker(Cluster cluster, String hostName) 
  throws IOException, InterruptedException;
  
  /**
   * Get tasktrackers on which tasks of the given job executed.
   * @param job map-reduce <code>Job</code>
   * @param taskAttemptIDs optional list of task-attempts whose tasktrackers
   *                       need to be fetched
   * @return tasktrackers on which tasks of the given job executed, optionally
   *                      for a specific set of task-attempts
   * @throws IOException
   * @throws InterruptedException
   */
  public static List<TaskTracker> 
  getTaskTrackersForJob(Job job, TaskAttemptID ... taskAttemptIDs) 
    throws IOException, InterruptedException;
  
  /**
   * Fetch artifacts of a given job (optionally a specific set of task-attempts of
   * the job).
   * @param job job whose logs are to be fetched
   * @param type artifact type
   * @param dir directory to place the fetched logs
   * @param taskAttemptIDs optional list of task-attempts of the job
   * @throws IOException
   */
  public static void fetchJobArtifacts(Job job, 
                                       Artifact type, Path dir, 
                                       TaskAttemptID ... taskAttemptIDs) 
  throws IOException;
  
  /**
   * Fetch job artifacts and check if they have the <code>pattern</code>.
   * @param job map-reduce job 
   * @param pattern pattern to check
   * @param fetch if <code>true</code> fetch the artifacts into 
   *              <code>dir</code>, else do not fetch
   * @param dir directory to place the fetched outputs
   * @param taskAttempts optional tasks of the job
   * @return <code>true</code> if the artifacts contain <code>pattern</code>,
   *         <code>false</code> otherwise
   * @throws IOException
   */
  public static boolean checkJobArtifacts(Job job, 
                                          Artifact type, String pattern, 
                                          boolean fetch, Path dir, 
                                          TaskAttemptID... taskAttempts)
  throws IOException;

  /**
   * Kill the given task-tracker
   * @param cluster map-reduce <code>Cluster</code>
   * @param taskTracker <code>TaskTracker</code> to be killed
   * @throws IOException
   * @throws InterruptedException 
   */
  public static void killTaskTracker(Cluster cluster, TaskTracker taskTracker) 
  throws IOException, InterruptedException;

  /**
   * Kill a given task-attempt running on the given tasktracker
   * @param cluster map-reduce <code>Cluster</code> 
   * @param taskTracker <code>TaskTracker</code> on which the task is running
   * @param taskAttemptId task-attempt to be killed 
   * @throws IOException
   * @throws InterruptedException
   */
  public static void killTask(Cluster cluster, TaskTracker taskTracker, 
                              TaskAttemptID taskAttemptId)
  throws IOException, InterruptedException;
  
{noformat}
","25/Oct/09 07:15;acmurthy;The attached patch has the skeleton of:

# Proposed utilities for Map-Reduce
# Newer apis we will need in the Map-Reduce framework to support the utilites:
 # enhancements to Job/Cluster for enhanced inspect-ability & control-ability
 # functionality to fetch logs for daemons/tasks etc., 
 # a new 'DataLoader' interface to provide input-data for large-scale map-reduce test-jobs. DataLoader has 3 basic implementations: DataGenerator (e.g. randomwriter), DistcpDataLoader, LocalDataLoader (fs -copyFromLocal).
# A sample test (ClusterTest.java) which exhibits how these things fit together.

PS: The changes to Job.java and Cluster.java are for the purposes of illustration only, eventually we will use aspects to weave them in for test-builds. We do not want these interfaces to be available on production clusters...","25/Oct/09 07:23;acmurthy;ClusterTest.java:

{noformat}
public class ClusterTest extends TestCase {

  Cluster cluster;
  Configuration conf;
  
  @Override
  protected void setUp() throws Exception {
    conf = new Configuration();
    ClusterTestUtils.setupCluster(conf);
    cluster = new Cluster(conf);
  }

  @Test
  public void testCluster() throws Exception {
    // Load data
    DataLoader dataLoader = new DistcpDataLoader();
    List<Path> inputs = new ArrayList<Path>();
    inputs.add(new Path(""in""));
    dataLoader.load(inputs, new Path(""out""));
    
    // Setup job
    Job job = Job.getInstance(cluster);
    job.setJobName(""cluster-test-mr-job"");

    // Run job
    long start = System.currentTimeMillis();
    job.submit();
    job.waitForCompletion(true);
    long end = System.currentTimeMillis();
    
    FileContext fc = FileContext.getLocalFSFileContext();

    // Fetch daemon logs and validate them
    Path daemonLogs = new Path(""daemon-logs"");
    assertFalse(""JobTracker logs contain FATAL warnings!"", 
                MRClusterTestUtils.checkDaemonLogs(
                    cluster, LogSource.JOBTRACKER, start, end, 
                    ""FATAL"", true, daemonLogs) == false);
    assertFalse(""TaskTracker logs contain FATAL warnings!"", 
                MRClusterTestUtils.checkDaemonLogs(
                    cluster, LogSource.TASKTRACKER, start, end, 
                    ""FATAL"", true, daemonLogs) == false);
    fc.delete(daemonLogs, true);
    
    // Fetch job logs, outputs etc. and validate them
    Path jobLogs = new Path(""daemon-logs"");
    Path jobOutputs = new Path(""daemon-logs"");
    TaskAttemptID m0 = null; // map 0
    TaskAttemptID m1 = null; // map 1
    TaskAttemptID r0 = null; // reduce 0
    assertFalse(""Task log contains FATAL warnings!"", 
                MRClusterTestUtils.checkJobArtifacts(job, 
                    Artifact.LOGS, ""FATAL"", 
                    true, jobLogs) == false);
    assertFalse(""Task log contains WARN warnings!"", 
                MRClusterTestUtils.checkJobArtifacts(
                    job, Artifact.LOGS, ""WARN"", 
                    false, jobLogs, m0, m1) == false);
    assertTrue(""Task outputs do not contain expected output!"", 
               MRClusterTestUtils.checkJobArtifacts(
                   job, Artifact.OUTPUTS, 
                   ""hello, world!"", true, jobOutputs, r0));
    fc.delete(jobLogs, true);
    fc.delete(jobOutputs, true);
  }
  
  @Override
  protected void tearDown() throws Exception {
    ClusterTestUtils.tearDownCluster(conf);
  }

}
{noformat}","26/Oct/09 05:01;philip;I'm vaguely uncomfortable with having a lot of code, even though it's test code, weaved in via AspectJ.  It seems like it will make it very easy to make changes that break the testing code (because the testing code is not visible to the regular tools, and is in an unexpected place).  I understand, of course, that the build system will check that the weaving can happen, but since these tests are inherently large-scale and not run at every Hudson (or are they?), it worries me a bit.

Has anyone done the ""reverse"" and ""unweaved"" functions from classes?  Seems like we could annotate functions with @RemoveInProduction, and then use some tool to forcibly remove methods from the resulting .class files.  Still opaque, but at least it's clear where the testing code is.

If you've already been working with this in AspectJ, I'm curious how the experience has been.

-- Philip","07/Dec/09 18:38;cos;bq. Has anyone done the ""reverse"" and ""unweaved"" functions from classes?
One way to do this is to create {{around}} advice for all calls of the method you are interested in. This advice will do nothing but return immediately.","02/Jun/10 05:07;cos;I'd suggest to close this JIRA because it has been largely covered by HADOOP-6332 and coming soon MAPREDUCE-1774","27/Jul/10 22:45;cos;This has been addressed as HADOOP-6332 and derived work.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
recovery after sybchronous Mapper failures on some records,MAPREDUCE-597,12375780,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,arkady,arkady,10/Aug/07 19:03,13/Jul/10 03:46,12/Jan/21 09:52,,,,,,,,,,contrib/streaming,,,,,,0,,,,,"Ii is sometimes hard or impossible to make sure that the Mapper reacts correctly to all the errors in the input data -- especially when reusing legacy or 3rd party code.
It would be nice if Streaming infrastructure had the following feature:
   * check the exit code of the mapper command; 
   * if the command has crashed 
      * log the record that was processed during the failure to the error log 
      * restart the command
      * feed it the remainder of the input 
This way most of the data gets processed.

This feature should be disabled by default -- the user should explicitly specify how many faults are allowed per task.
Once the number is exceeded, the whole job should fail without retries.

BTW: this functionality was described in the original MapReduce paper.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,148925,,,,,2007-08-10 19:03:19.0,,,,,,,"0|i0is87:",107667,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rumen needs a job trace sorter,MAPREDUCE-932,12434313,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,dking,dking,dking,28/Aug/09 18:31,02/Jul/10 07:15,12/Jan/21 09:52,,,,,,,,,,tools/rumen,,,,,,0,,,,,"Rumen reads job history logs and produces job traces.  The jobs in a job trace do not occur in any promised order.  Certain tools need the jobs to be ordered by job submission time.  We should include, in Rumen, a tool to sort job traces.",,cdouglas,ravidotg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-934,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/09 22:52;dking;MAPREDUCE-932--2009-09-18-PM.patch;https://issues.apache.org/jira/secure/attachment/12420096/MAPREDUCE-932--2009-09-18-PM.patch","18/Sep/09 17:29;dking;MAPREDUCE-932--2009-09-18.patch;https://issues.apache.org/jira/secure/attachment/12420051/MAPREDUCE-932--2009-09-18.patch","01/Sep/09 00:06;dking;patch-932--2009-08-31--1702.patch;https://issues.apache.org/jira/secure/attachment/12418194/patch-932--2009-08-31--1702.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2009-09-01 05:01:18.773,,,false,,,,,,,,,,,,,,,,,,149149,,,,,Mon Oct 19 00:24:35 UTC 2009,,,,,,,"0|i0jf7b:",111393,,,,,,,,,,,,,,,,,,,,,"01/Sep/09 00:06;dking;This patch will fail test-patch because the test cases lack an ASF license banner.

The test case and reference file are JSON files.  Although the Jackson JSON processor allows comments [which would allow for a human-readable banner], JSON standards do not have such a syntax, so there's no principled way to attach such a banner to every file including the test cases.","01/Sep/09 05:01;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12418194/patch-932--2009-08-31--1702.patch
  against trunk revision 808730.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 3 new Findbugs warnings.

    -1 release audit.  The applied patch generated 222 release audit warnings (more than the trunk's current 220 warnings).

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/30/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/30/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/30/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/30/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/30/console

This message is automatically generated.","01/Sep/09 13:00;dking;The findbugs warnings will be relatively routine to fix.

The release audit is two test cases which cannot reasonably get a banner ASF license comment.

The core tests that fail fail also in the base.","18/Sep/09 17:29;dking;The patch tester will complain that two files do not have an ASF license.  These files are JSON data files, and JSON does not provide a portable comment format.","18/Sep/09 21:33;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12420051/MAPREDUCE-932--2009-09-18.patch
  against trunk revision 816735.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    -1 release audit.  The applied patch generated 181 release audit warnings (more than the trunk's current 179 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/1/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/1/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/1/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/1/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/1/console

This message is automatically generated.","18/Sep/09 22:52;dking;Hudson's findbugs found a file opening leak that mine didn't.  I fixed it.","19/Oct/09 00:24;cdouglas;Could this compress the data for the testcase, as in MAPREDUCE-1077?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
User-configurable quote and delimiter characters for Sqoop records and record reparsing,MAPREDUCE-705,12429505,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,kimballa,kimballa,kimballa,03/Jul/09 19:09,02/Jul/10 06:31,12/Jan/21 09:52,22/Jul/09 14:14,,,,,,,,,,,,,,,0,,,,,"Sqoop needs a mechanism for users to govern how fields are quoted and what delimiter characters separate fields and records. With delimiters providing an unambiguous format, a parse method can reconstitute the generated record data object from a text-based representation of the same record.",,aaa,hammer,kimballa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/09 18:27;kimballa;MAPREDUCE-705.2.patch;https://issues.apache.org/jira/secure/attachment/12413046/MAPREDUCE-705.2.patch","15/Jul/09 19:10;kimballa;MAPREDUCE-705.3.patch;https://issues.apache.org/jira/secure/attachment/12413589/MAPREDUCE-705.3.patch","03/Jul/09 19:09;kimballa;MAPREDUCE-705.patch;https://issues.apache.org/jira/secure/attachment/12412517/MAPREDUCE-705.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2009-07-03 23:36:58.11,,,false,,,,,,,,,,,,,,,,,,37427,Reviewed,,,,Wed Jul 22 14:14:12 UTC 2009,,,,,,,"0|i02tof:",14409,Added user-configurable field/record delimiters and record parsing capabilities to Sqoop.,,,,,,,,,,,,,,,,,,,,"03/Jul/09 19:15;kimballa;
I have implemented delimiter control for Sqoop. Users can now customize which characters are used as delimiters between fields and records. Fields can also be enclosed in user-configurable quote characters, which can be made mandatory (all fields always quoted) or optional (fields only quoted when they contain a delimiter). An optional user-configurable escape character allows users to escape quotes (and more escape characters) inside fields.

This modifies the toString() method of generated classes to use the proper quote, delimiter, and escape characters for each record.

Sqoop-generated record types now also contain a parse() method which can read a record separated by the specified field delimiters and terminated by the specified record-termination delimiter, and can contain escapes and quotes as described above. The record will repopulate all its fields by reading the record. For example, suppose a table was created with:

{{CREATE TABLE mytable (foo INT, bar VARCHAR(32), baz DATE);}}

Importing this table from the database into HDFS via Sqoop would generate a class named {{mytable}} which can contain a single record consisting of {{foo}}, {{bar}}, and {{baz}} fields. The text file emitted in HDFS may contain lines like:

{{42,'hello','2009-07-04'}}

The {{mytable.parse()}} method will read a line of this form and populate its {{foo}}, {{bar}} and {{baz}} variables based on the three fields it can interpret.

The delimiters used when writing text to HDFS are controlled by these options:

{code}
Output line formatting options:
--fields-terminated-by (char)    Sets the field separator character
--lines-terminated-by (char)     Sets the end-of-line character
--optionally-enclosed-by (char)  Sets a field enclosing character
--enclosed-by (char)             Sets a required field enclosing char
--escaped-by (char)              Sets the escape character
--mysql-delimiters               Uses MySQL's default delimiter set
  fields: ,  lines: \n  escaped-by: \  optionally-enclosed-by: '
{code}

The {{(char)}} argument above can be specified either as a normal character (e.g., {{\-\-fields-terminated-by ,}}) or via an escape sequence. Arguments of the form {{\0xhhh}} will be interpreted as a hexidecimal representation of a character with hex number _hhh_. Arguments of the form {{\0ooo}} will be treated as an octal representation of a character represented by octal number _ooo_. The special escapes {{\n}}, {{\r}}, {{\""}}, and {{\b}} as well as two backslashes (which JIRA refuses to format...) act as they do inside Java strings. {{\0}} will be treated as {{NUL}}. This will insert {{NUL}} characters between fields or lines ((if used for {{\-\-fields-terminated-by}} or {{\-\-lines-terminated-by}}), or will disable enclosing/escaping if used for one of the {{\-\-enclosed-by}}, {{\-\-optionally-enclosed-by}}, or {{\-\-escaped-by}} arguments. Enclosing and escaping are disabled by default. For unambiguous parsing, both must be enabled, e.g., via {{\-\-mysql-delimiters}}.

Sqoop can also transcode one delimiter set into another. So if your data is currently enclosed in a certain set of delimiters, you can use Sqoop to help you convert it to a new set of delimiters.

The options described above will set the output delimiters used by {{toString()}}. If none of the following options are used, they also set the input delimiters used by {{parse()}}. But the input delimiters can be overridden on an individual basis with the options:

{code}
Input parsing options:
--input-fields-terminated-by (char)    Sets the input field separator
--input-lines-terminated-by (char)     Sets the input end-of-line char
--input-optionally-enclosed-by (char)  Sets a field enclosing character
--input-enclosed-by (char)             Sets a required field encloser
--input-escaped-by (char)              Sets the input escape character
{code}

If you use these options, then the {{parse()}} method generated for your type will use these delimiters, while the {{toString()}} method will use the delimiters set with the output options above. You can then use the generated class in a mapper which calls {{parse()}} on the input records of the old delimiter set, and emits {{Text}} objects generated with the output delimiter set from the record's {{toString()}} method.

The default delimiters are {{,}} for fields, {{\n}} for records, no quote character, and no escape character. Note that this can lead to ambiguous/unparsible records if you import database records containing commas or newlines in the field data.

This patch includes Hive support for user-defined delimiters. It will print a warning if you use {{\-\-escaped-by}}, {{\-\-enclosed-by}}, or {{\-\-optionally-enclosed-by}} since Hive does not know how to parse these. It will pass the field and record terminators through to Hive. If you do not set any delimiters and do use {{\-\-hive-import}}, the field delimiter will be set to {{^A}} and the record delimiter will be set to {{\n}} to be consistent with Hive's defaults.

This patch includes integrated {{mysqldump}} support for user-defined delimiters. If your delimiters exactly match the delimiters used by {{mysqldump}}, then Sqoop will use a fast-path that copies the data directly from {{mysqldump}}'s output into HDFS. Otherwise, Sqoop will parse {{mysqldump}}'s output into fields and transcode them into the user-specified delimiter set. This is about 50% slower (as measured on a 1.5 GB test dataset import on my machine). For convenience, the {{\-\-mysql-delimiters}} argument will set all the output delimiters to be consistent with {{mysqldump}}'s format.

The output of the {{\-\-mysql-delimiters}}-based test says:

{{INFO manager.LocalMySQLManager: Transferred 1.5747 GB in 124.7751 seconds (12.9231 MB/sec)}}

Whereas with the default delimiters, Sqoop reports:

{{INFO manager.LocalMySQLManager: Transferred 1.5328 GB in 181.3032 seconds (8.6571 MB/sec)}}

This patch is based after MAPREDUCE-685. That should be committed to trunk before this one.
This patch adds another file in {{testdata/hive/scripts}} so the number of release audit warnings is expected to increase by 1.
I have run all the Sqoop unit tests (including the several new tests added by this patch) and they pass on my machine. This includes the LocalMySQLTest which tests {{mysqldump}} compatibility, but is not run by Hudson. As noted above, I also run an at-scale test importing 1.5 GB of data from MySQL to HDFS.
","03/Jul/09 23:36;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12412517/MAPREDUCE-705.patch
  against trunk revision 790971.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 45 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/350/console

This message is automatically generated.","09/Jul/09 18:27;kimballa;Attaching rebased patch after MAPREDUCE-685 ","11/Jul/09 03:34;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12413046/MAPREDUCE-705.2.patch
  against trunk revision 793136.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 45 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 316 release audit warnings (more than the trunk's current 315 warnings).

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/377/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/377/artifact/trunk/current/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/377/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/377/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/377/console

This message is automatically generated.","13/Jul/09 22:10;kimballa;None of these are sqoop-related bugs.","15/Jul/09 19:10;kimballa;Attaching rebased patch after MAPREDUCE-710.","15/Jul/09 19:11;kimballa;Cycling patch status..","16/Jul/09 11:24;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12413589/MAPREDUCE-705.3.patch
  against trunk revision 794541.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 48 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 316 release audit warnings (more than the trunk's current 315 warnings).

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/399/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/399/artifact/trunk/current/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/399/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/399/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/399/console

This message is automatically generated.","16/Jul/09 16:19;kimballa;Still no sqoop-related bugs in the Hudson tests.","22/Jul/09 14:14;tomwhite;+1

I've just committed this. Thanks Aaron!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Export data to databases via Sqoop,MAPREDUCE-1168,12439457,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,kimballa,kimballa,kimballa,29/Oct/09 20:53,02/Jul/10 06:31,12/Jan/21 09:52,07/Dec/09 21:43,,,,,,,,,,,,,,,0,,,,,Sqoop can import from a database into HDFS. It's high time it works in reverse too.,,hammer,johanoskarsson,kimballa,qwertymaniac,tomwhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/09 23:12;kimballa;MAPREDUCE-1168.2.patch;https://issues.apache.org/jira/secure/attachment/12426306/MAPREDUCE-1168.2.patch","29/Oct/09 20:54;kimballa;MAPREDUCE-1168.patch;https://issues.apache.org/jira/secure/attachment/12423616/MAPREDUCE-1168.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2009-10-30 11:28:16.421,,,false,,,,,,,,,,,,,,,,,,37354,Reviewed,,,,Mon Dec 07 22:22:19 UTC 2009,,,,,,,"0|i02t6f:",14328,Add export capability to Sqoop,,,,,,,,,,,,,,,,,,,,"29/Oct/09 20:54;kimballa;This patch provides Sqoop with the ability to export tables from HDFS to an external RDBMS. Sqoop runs a MapReduce job over the contents of a directory (identified by {{\-\-export-dir}}), parsing the records contained within based on the auto-generated class definition for a table. DBOutputFormat is used to inject the records back into the database table (specified by {{\-\-table}}). The table must already exist in the target database.

Sqoop can auto-generate the appropriate ORM class for parsing the input files by examining the target table (much as is done during importing); the existing command-line options that govern delimiters are used to specify which delimiters are used in the files to be exported.

If an ORM class has already been generated for the table, this can now be specified with the {{\-\-jar-file}} and {{\-\-class-name}} options; code auto-generation is bypassed in this case. (This applies to imports as well.)

Export supports both delimited text files as well as SequenceFiles containing {{SqoopRecords}} as values (i.e., SequenceFiles created via a Sqoop import with {{\-\-as-sequencefile}}). Users do not need to identify the file type; it is automatically inferred. Gzipped text files will be handled transparantly.

Testing has been performed via unit tests (included) against HSQLDB with several column datatypes. I performed manual larger-scale testing by exporting 100MB and 500MB datasets containing 1- and 5 million rows respectively to tables in mysql.","30/Oct/09 11:28;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12423616/MAPREDUCE-1168.patch
  against trunk revision 831037.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 12 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/109/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/109/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/109/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/109/console

This message is automatically generated.","16/Nov/09 23:12;tomwhite;+1 This looks good. A few nits:

* ExportError. Should this be ExportException? Errors are generally only thrown by the JVM itself.
* ImportOptions. This is now being used for export options too, so should probably be renamed SqoopOptions.
* ExportJob#isSequenceFiles() could use magic numbers to detect the format. Could AutoInputFormat be used, or adapted?","27/Nov/09 23:12;kimballa;Thanks for reviewing, Tom.

I agree on the renames of ImportOptions -> SqoopOptions and ExportError -> ExportException. I had forgotten that exceptions with names ending in ""Error"" have an implied semantic meaning. For consistency, I've also renamed ImportError to ImportException. 

You'll need to do the following in the subversion tree after applying this patch:
{code}
svn mv --force src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/ImportOptions.java src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/SqoopOptions.java

svn mv --force src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ImportError.java src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ImportException.java

svn mv --force src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestImportOptions.java src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestSqoopOptions.java
{code}

AutoInputFormat is currently based on the old API; until this is ported, it's incompatible with the new API (which Sqoop tries to use as much as possible). Definitely worth keeping in mind for the future though, once streaming is moved to the new API. I've copied the magic-number test from AutoInputFormat into {{isSequenceFiles()}}. Note that this means that any text file beginning with the characters ""SEQ"" is going to be misinterpreted as a SequenceFile now. This case is hopefully rare.. we can address it when it comes up in practice?

This patch is also resync'd to trunk.

","28/Nov/09 02:43;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426306/MAPREDUCE-1168.2.patch
  against trunk revision 884832.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 52 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/276/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/276/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/276/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/276/console

This message is automatically generated.","28/Nov/09 05:50;kimballa;Unrelated contrib failure in fair scheduler.","07/Dec/09 21:43;tomwhite;I've just committed this. Thanks Aaron!","07/Dec/09 22:22;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #146 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/146/])
    . Export data to databases via Sqoop. Contributed by Aaron Kimball.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Postgresql support for Sqoop,MAPREDUCE-938,12434333,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,kimballa,kimballa,kimballa,28/Aug/09 23:30,02/Jul/10 06:31,12/Jan/21 09:52,08/Sep/09 12:54,,,,,,,,,,,,,,,0,,,,,Sqoop should be able to import from postgresql databases.,,aaa,hammer,martind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/09 23:31;kimballa;MAPREDUCE-938.patch;https://issues.apache.org/jira/secure/attachment/12418036/MAPREDUCE-938.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2009-08-29 08:48:48.582,,,false,,,,,,,,,,,,,,,,,,37406,Reviewed,,,,Tue Sep 08 17:20:51 UTC 2009,,,,,,,"0|i02tjj:",14387,Postgresql support for Sqoop,,,,,,,,,,,,,,,,,,,,"28/Aug/09 23:31;kimballa;Attaching patch which provides support and testcases for Postgresql imports via Sqoop. Provides both a JDBC implementation as well as {{\-\-direct}} mode support via a psql {{COPY TO STDOUT}} command.

Includes test cases enabled in ThirdPartyTests; I've run these successfully. Also tested via importing a large (multi-GB) dataset to HDFS.","29/Aug/09 08:48;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12418036/MAPREDUCE-938.patch
  against trunk revision 808730.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 11 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/537/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/537/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/537/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/537/console

This message is automatically generated.","29/Aug/09 18:04;kimballa;Core test failures are unrelated.","08/Sep/09 12:54;tomwhite;+1

I've just committed this. Thanks Aaron!","08/Sep/09 17:20;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #24 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/24/])
    . Postgresql support for Sqoop. Contributed by Aaron Kimball.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need sample map reduce program for arithmetic calculations from text files  and produce the results in hadoop 0.20.2,MAPREDUCE-1875,12467209,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Invalid,,chendhu,chendhu,17/Jun/10 10:21,17/Jun/10 14:58,12/Jan/21 09:52,17/Jun/10 14:58,0.20.2,,,,,0.20.2,,,,,,,,,0,0,,,,"Hi I am new to Hadoop Map-reduce programming. Can anyone provide me the sample map-reduce code for some basic arithmatic calculations. Like, read from the text /csv files and then do some addition/sub/multi/div opearations and produce the output.
Kindly anyone help me on this",Jdk 1.6 Hadoop 0.20.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-06-17 14:58:28.786,,,false,,,,,,,,,,,,,,,,,,149826,Incompatible change,Reviewed,,,Thu Jun 17 14:58:28 UTC 2010,,,,,,,"0|i0jhov:",111796,0,,,,,,,,,,0,,,,,,,,,,"17/Jun/10 14:58;acmurthy;This isn't a feature request. Please contact the mapreduce-dev@hadoop.apache.org for help.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
authorization checks for inter-server protocol (based on HADOOP-6600),MAPREDUCE-1539,12457534,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,boryas,boryas,boryas,26/Feb/10 08:08,14/May/10 02:15,12/Jan/21 09:52,14/May/10 02:15,,,,,,,,,,,,,,,0,,,,,authorization checks for inter-server protocol (based on HADOOP-6600),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/10 20:12;boryas;MAPREDUCE-1539-1.patch;https://issues.apache.org/jira/secure/attachment/12437238/MAPREDUCE-1539-1.patch","26/Feb/10 22:44;boryas;MAPREDUCE-1539-2.patch;https://issues.apache.org/jira/secure/attachment/12437262/MAPREDUCE-1539-2.patch","10/May/10 22:14;boryas;MAPREDUCE-1539-3.patch;https://issues.apache.org/jira/secure/attachment/12444149/MAPREDUCE-1539-3.patch","14/May/10 00:17;boryas;MAPREDUCE-1539-5.patch;https://issues.apache.org/jira/secure/attachment/12444448/MAPREDUCE-1539-5.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2010-05-12 01:04:27.461,,,false,,,,,,,,,,,,,,,,,,149586,Reviewed,,,,Fri May 14 02:15:22 UTC 2010,,,,,,,"0|i0jgw7:",111667,,,,,,,,,,,security,,,,,,,,,,"26/Feb/10 22:44;boryas;merged with trunk","10/May/10 22:14;boryas;merged with trunk","12/May/10 01:04;jnp;+1 pending hudson tests.","13/May/10 21:08;boryas;ran tests manually all passed.","14/May/10 02:15;shv;I just committed this. Thank you Boris.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add input and output formats for Avro value wrappers,MAPREDUCE-1634,12460284,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,,cutting,cutting,25/Mar/10 20:54,31/Mar/10 23:24,12/Jan/21 09:52,31/Mar/10 23:24,,,,,,,,,client,,,,,,0,,,,,HADOOP-6660 proposes adding an AvroValue wrapper for Avro data.  We should add InputFormat and OutputFormat implementations for Avro data files that use this.,,cdouglas,drew.farris,hammer,jrideout,tomwhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-6660,,,,,,,,,,,,,,,,,MAPREDUCE-815,,,,,,,,,,,,,,,,,,,,,"30/Mar/10 20:05;cutting;MAPREDUCE-1634.patch;https://issues.apache.org/jira/secure/attachment/12440270/MAPREDUCE-1634.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2010-03-31 20:41:34.8,,,false,,,,,,,,,,,,,,,,,,149653,,,,,Wed Mar 31 23:24:06 UTC 2010,,,,,,,"0|i0jh5j:",111709,,,,,,,,,,,,,,,,,,,,,"25/Mar/10 20:57;cutting;This issue is distinct from MAPREDUCE-815, which attempts to integrate Avro data without a wrapper.","30/Mar/10 20:05;cutting;Here's an early version of this patch.  It permits folks to write MapReduce programs that use Avro data files for input and output and Avro for intermediate data.  It still needs lots of documentation.

I'm now leaning towards adding this to Avro rather than MapReduce.  It tries to use only 0.19 APIs so it can work with any recent Hadoop release.","30/Mar/10 21:59;cutting;I propose to instead add this functionality to Avro.  I've filed AVRO-493 with that intent.  If there are no objections, I'll close this issue as a duplicate of that.","31/Mar/10 20:41;cdouglas;bq. I'm now leaning towards adding this to Avro rather than MapReduce. It tries to use only 0.19 APIs so it can work with any recent Hadoop release.

Would users working in Avro think to look in that project, rather than MapReduce, for these classes? Right now, Avro doesn't have any dependencies on MapReduce or pre-split Hadoop, right? Wouldn't this create a circular dependency? What are the advantages to adding this to Avro instead of MapReduce?","31/Mar/10 20:55;cutting;> Wouldn't this create a circular dependency?

The added Avro dependency on Hadoop is only at compile/test time.  It compiles against Hadoop 0.20.2, as retrieved from Maven.  Avro's POM does not require Hadoop.  The code should be compatible with Hadoop 0.18+ and perhaps even earlier, although I have not tested that yet.

> What are the advantages to adding this to Avro instead of MapReduce?

Primarily that it can be used with older Hadoop releases.

The Avro version of the patch is at AVRO-493.
","31/Mar/10 22:58;cdouglas;bq. The Avro version of the patch is at AVRO-493.

I read this; sorry if I missed answers to my questions, there.

So by adding these classes the Avro jar, one could run Avro through existing clusters. In that issue, it's mentioned that Amazon runs 0.18, most deployments run a version of Hadoop based on 0.20, etc. It's pragmatic to package these classes in Avro, but long-term... even in that light it makes sense to package this with Avro, I suppose. +1","31/Mar/10 23:24;cutting;> It's pragmatic to package these classes in Avro, but long-term... 

Yes, you're right, it's not black and white, but pragmatic.  In the past we've generally encouraged FileSystem and InputFormat implementations to be included in Hadoop so they're easier to maintain as Hadoop's APIs evolved.  However in this case we have a new InputFormat that we'd like to be able use with older releases of Hadoop.  And we now also expect the Hadoop APIs it uses to be stable for some time.  As Hadoop's APIs stablize we might see more such user-level ""library"" code maintained independently from Hadoop.

Closing this as a duplicate of AVRO-493.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Automate the local file permission checking when a job is running. ,MAPREDUCE-1655,12460777,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,,balajirg,balajirg,31/Mar/10 06:52,31/Mar/10 08:50,12/Jan/21 09:52,31/Mar/10 08:39,0.20.3,,,,,,,,test,,,,,,0,,,,,This test case will automate the file permission under mapred.local.dir when a job runs. ,,ravidotg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-6332,,,,,,"31/Mar/10 07:18;balajirg;patch_1655.txt;https://issues.apache.org/jira/secure/attachment/12440314/patch_1655.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,149667,,,,,Wed Mar 31 08:39:57 UTC 2010,,,,,,,"0|i0jh6f:",111713,,,,,,,,,,,,,,,,,,,,,"31/Mar/10 07:18;balajirg;This patch has the automated system test for local file permission check. ","31/Mar/10 08:39;balajirg;This is a duplicate of MAPREDUCE1616",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow JobTracker to pause task scheduling,MAPREDUCE-1227,12441331,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,,dms,dms,20/Nov/09 21:12,24/Mar/10 20:02,12/Jan/21 09:52,24/Mar/10 20:02,0.22.0,,,,,,,,,,,,,,0,,,,,"We want to have an ability to pause task scheduling in JobTracker.

The idea is: make job tracker still accept new jobs, but delay their running and do not schedule any new tasks from the currently running jobs.

It will help for example restarting the DFS cluster without affecting jobs: pause execution, restart the DFS, running tasks will fail, but will not be scheduled until the execution is resumed, so the job does not fail.

In general it should help fix non MR problems (DFS, network, etc.) while not failing running jobs and keep accepting new ones.

What do people think of the general idea?",,aah,acmurthy,aw,cdouglas,dhruba,eli,hammer,hong.tang,huwjedwards,johanoskarsson,matei,omalley,schen,tlipcon,zhong,zshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-11-20 21:23:30.183,,,false,,,,,,,,,,,,,,,,,,72502,,,,,Wed Mar 24 20:02:37 UTC 2010,,,,,,,"0|i0jg3r:",111539,,,,,,,,,,,,,,,,,,,,,"20/Nov/09 21:23;acmurthy;What about jobs which are already running?

Also, what you really want is the ability to run jobs from some queues (e.g. admin queue) and pause all the other queues. So, this feature isn't really going to buy you much. Ability to pause queues is a much more general solution - I believe the CapacityScheduler already implements this, hence this is more of a scheduler property.","20/Nov/09 22:24;dhruba;One of the reasons to pause running jobs and to not schedule new tasks is to enable a HDFS restarts without affecting currently-running and new-incoming jobs. The JT computes splits as soon as the job arrives. is there a way to postpone the calculation of splits of new-incoming jobs, otherwise newly submitted jobs might fail if HDFS is being restarted, isn't it?

","20/Nov/09 22:54;aw;Many tasks write to HDFS as well as read, so those would need to have some way to be paused.  This is particularly interesting when thinking about a streaming job.

But isn't the same functionality available if all of the task trackers are down?  [I've never tried it.]","20/Nov/09 22:55;aw;(and, you can get the ""don't shedule"" via task tracker decomm capability allegedly in 0.21)","20/Nov/09 23:02;dms;Well, the current thought is we can sacrifice those tasks that are currently running as long as they do not get rescheduled until the scheduling is resumed. This way the job itself wont fail, but the running tasks will of course have to fail.

When you achieve the 'don't schedule' goal with the task trackers decommissioning the task trackers will die as disallowed by the jobtracker, won't they, and thus will have to be restarted in the process of resuming, which is way more heavyweight than simply stopping giving them new tasks and then assigning those again.","20/Nov/09 23:55;acmurthy;JT does not compute input-splits, that computation is done on the client side before the job is submitted.","21/Nov/09 00:13;matei;There are other things that will need to be done to restart HDFS underneath MapReduce. For example, if I recall correctly, the JT can also write history logs to HDFS. To support this, we'll need to be able to recover writing to a file (or use append).","21/Nov/09 01:23;dms;So far there were few problems identified with this change.

Since the splits are being calculated on the client side before submission we need to see MAPREDUCE-207 go in before considering this change. Making it a blocking issue.

JobSubmitter also checks the outputspecs which requires hdfs access, there is an opinion that checkInputSplits should be reintroduced as optional check during job submission, we can make the output specs check as optional as well, this way the client will be able to submit jobs even when the HDFS is down.

JobHistory being written to the HDFS is a separate problem in a way, that can be solved on it's own making the JobHistory more reliable to HDFS problems.

Currently running tasks dying in case HDFS goes down is a problem, since some of those tasks may be the last task attempt before the job fail, so this pausing should also incorporate killing currently running tasks thus not increasing failure counts for the job.

Doing it on a scheduler level sounds like a good idea, but the killing of the currently running tasks should be there too.

Is there anything else I am missing? ","21/Nov/09 02:18;acmurthy;bq. Doing it on a scheduler level sounds like a good idea, but the killing of the currently running tasks should be there too.

I think we agree, should I close this as 'WONTFIX' ?","24/Mar/10 20:02;dms;Closing this one since we reached a conclusion long ago.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support multiple headless users to be able to submit job via gridmix v3,MAPREDUCE-1605,12459361,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,rksingh,rksingh,17/Mar/10 08:08,17/Mar/10 22:49,12/Jan/21 09:52,,,,,,,,,,contrib/gridmix,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-03-17 22:49:32.382,,,false,,,,,,,,,,,,,,,,,,149635,,,,,Wed Mar 17 22:49:32 UTC 2010,,,,,,,"0|i0e92f:",81231,,,,,,,,,,,,,,,,,,,,,"17/Mar/10 22:49;hong.tang;MAPREDUCE-1376 requires Gridmix3 being launched by a regular ""super user"" that has properly authenticated with Kerberos server. In this jira, we would like to allow Gridmix3 to be launched by a headless user authenticated with Kerberos through keytab.

The title is a bit misleading, what we want is to launch gridmix3 through a headless user, whether jobs are submitted as real user or headless users is probably irrelevant.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SequenceFile.Reader constructor leaking resources,MAPREDUCE-1504,12456705,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Fixed,,zshao,zshao,18/Feb/10 20:25,21/Feb/10 08:23,12/Jan/21 09:52,21/Feb/10 08:19,,,,,,,,,,,,,,,1,,,,,"When {{SequenceFile.Reader}} constructor throws an {{IOException}} (because the file does not conform to {{SequenceFile}} format), we will have such a problem.
The caller won't have a pointer to the reader because of the {{IOException}} thrown.

We should call {{in.close()}} inside the constructor to make sure that we don't leak resources (file descriptor and connection to the data node, etc).
",,bennies,dhruba,namit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-5476,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,149562,,,,,Sun Feb 21 08:19:14 UTC 2010,,,,,,,"0|i0jgrz:",111648,,,,,,,,,,,,,,,,,,,,,"21/Feb/10 08:19;zshao;Fixed in HADOOP-5476",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple Outputs doesn't work with new API in 0.20 branch,MAPREDUCE-1145,12438944,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,,jbooth,jbooth,23/Oct/09 16:46,19/Feb/10 18:06,12/Jan/21 09:52,03/Nov/09 07:15,0.20.1,0.20.2,,,,0.20.2,,,,,,,,,0,,,,,"I know this is working in the 0.21 branch but it's dependent on a ton of other refactorings and near-impossible to backport.  I hacked together a quick forwards-port in o.a.h.mapreduce.lib.output.MultipleOutputs.  Unit test attached, requires a one-liner change to FileOutputFormat.

Maybe 0.20.2?  ",,eli,gates,hammer,kimballa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/09 00:22;jbooth;MAPREDUCE-1145-branch-20.patch;https://issues.apache.org/jira/secure/attachment/12425412/MAPREDUCE-1145-branch-20.patch","19/Feb/10 18:06;hiral;updated-MAPREDUCE-1145-branch-20.patch;https://issues.apache.org/jira/secure/attachment/12436348/updated-MAPREDUCE-1145-branch-20.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2009-10-30 21:03:43.975,,,false,,,,,,,,,,,,,,,,,,149309,,,,,Fri Feb 19 18:06:32 UTC 2010,,,,,,,"0|i0jfw7:",111505,,,,,,,,,,,,,,,,,,,,,"30/Oct/09 21:03;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12423031/multiple-outputs.patch
  against trunk revision 831037.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/113/console

This message is automatically generated.","03/Nov/09 00:34;jbooth;Old patches were against our internal snapshot, which had different revision #s and borked the patch

This one's against hadoop/common/branches/0.20, should work, right?","03/Nov/09 07:15;cdouglas;{quote}
Old patches were against our internal snapshot, which had different revision #s and borked the patch

This one's against hadoop/common/branches/0.20, should work, right?
{quote}
Hudson applies the most recently attached file as a patch against trunk. It takes neither the fix version on JIRA nor any artifact in the patch (such as revision) into account when applying it and running tests.

With few exceptions, we don't backport features into old branches. Pushing new code into stable releases forces a cascade of new bug reports and unanticipated interactions that we cannot sustain in concert with development on trunk.","03/Nov/09 14:53;jbooth;Understood, I guess we can just leave this up if people want to take the responsibility for applying it themselves","19/Nov/09 00:22;jbooth;It was brought to my attention that the patch I posted before had the wrong unit test attached - found a few spare minutes and cleaned it up, in case any passersby want this capability.  Not reopening issue for 0.20 branch.","19/Feb/10 18:06;hiral;There is a bug where multiple outputs with different output key and value classes are not working.  All outputs have the same output key and value class.  Added patch to MultipleOutputs.java to fix this.

Here is the diff from Jay's patch:

313,314d312
< +import org.apache.hadoop.io.LongWritable;
< +import org.apache.hadoop.io.Text;
734c732
< +        outputFormat.getRecordWriter(new MOTaskAttemptContextWrapper(namedOutput,ctx));
---
> +        outputFormat.getRecordWriter(ctx);
876,906d873
< +
< +  private class MOTaskAttemptContextWrapper extends TaskAttemptContext {
< +
< +    private final Class<?+outputKeyClass;
< +    private final Class<?+outputValueClass;
< +
< +    public MOTaskAttemptContextWrapper(final String namedOutput,
< +                                       TaskAttemptContext ctx) {
< +      super(ctx.getConfiguration(), ctx.getTaskAttemptID());
< +      outputKeyClass=conf.getClass(MO_PREFIX + namedOutput +   KEY, LongWritable.class);
< +      outputValueClass=conf.getClass(MO_PREFIX + namedOutput +   VALUE, Text.class);
< +    }
< +
< +    /**
< +     * Get the key class for the job output data.
< +     * @return the key class for the job output data.
< +     */
< +    @Override
< +    public Class<?+getOutputKeyClass() {
< +      return outputKeyClass;  
< +    }
< +
< +    /**
< +     * Get the value class for job outputs.
< +     * @return the value class for job outputs.
< +     */
< +    @Override
< +    public Class<?+getOutputValueClass() {
< +      return outputValueClass;  
< +    }
< +  }",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
job.xml should add crc check in tasktracker and sub jvm.,MAPREDUCE-1254,12442027,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,buptzhugy,buptzhugy,01/Dec/09 03:07,10/Dec/09 20:45,12/Jan/21 09:52,,0.22.0,,,,,,,,task,tasktracker,,,,,0,,,,,"Currently job.xml in tasktracker and subjvm are write to local disk through ChecksumFilesystem, and already had crc checksum information, but load the job.xml file without crc check. It would cause the mapred job finished successful but with wrong data because of disk error.  Example: The tasktracker and sub task jvm would load the default configuration if it doesn't successfully load the job.xml which maybe replace the mapper with IdentityMapper. ",,hammer,hanfoo_001,tlipcon,zhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-12-04 06:55:47.403,,,false,,,,,,,,,,,,,,,,,,97395,,,,,Thu Dec 10 20:45:26 UTC 2009,,,,,,,"0|i0e9nb:",81325,,,,,,,,,,,,,,,,,,,,,"04/Dec/09 06:55;zshao;Can you explain why the sub task jvm will continue if it doesn't successfully load the job.xml?
Shouldn't it error out with an IOException?
","04/Dec/09 09:40;buptzhugy;Because the local inexpensive disks are not reliable, and we once found the non zero file became zero length, but the os kernel message has no warning, while some minutes later, the kernel message report the disk failtures. Durining that time,  the read operation return success without throw any IOException. 

In current implementation, it would throw IOException if the job.xml missing, but it couldn't detect the configuration file has corrupted or has being truncated.","05/Dec/09 00:20;zshao;Got it. It seems a good idea to read and check the checksum.
Will you upload a patch including a simple test case?
","05/Dec/09 00:24;tlipcon;Curious why the XML reading doesn't fail for an empty file. Emptiness is not valid XML, right?","10/Dec/09 10:38;buptzhugy;I just show the example that the inexpensive disk are not reliable, the kernel doesn't notice the hardware failture while it has being truncated.

1)job.xml in configuration are loaded asynchronous, and if it could  corrupted or missing before parse it, if it does happen, the corrupted data or default data would load without notice(that means some task run the right configuration, but some would run with wrong configurations);

2)the job.xml has so many important parameters, it need check before used;

3) if it doesn't crc check, why we generate the crc checksum file?  :)","10/Dec/09 20:45;tlipcon;bq. if it does happen, the corrupted data or default data would load without notice

This seems like a bug on its own (or a bug waiting to happen)

I'm not against the CRC (I think it's a good idea) but we should also fail a job if job.xml fails to parse as valid XML, I think.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shuffle should be refactored to a separate task by itself,MAPREDUCE-222,12368817,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,ddas,ddas,08/May/07 06:23,01/Dec/09 05:53,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"Currently, shuffle phase is part of the reduce task. The idea here is to move out the shuffle as a first-class task. This will improve the usage of the network since we will then be able to schedule shuffle tasks independently, and later on pin reduce tasks to those nodes. This will make most sense for apps where there are multiple waves of reduces (the second wave of reduces can directly start off doing the ""reducer"" phase).",,buptzhugy,cdouglas,guanying,jothipn,qwertymaniac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2008-03-02 05:35:32.061,,,false,,,,,,,,,,,,,,,,,,148609,,,,,Tue Dec 01 05:53:34 UTC 2009,,,,,,,"0|i0irzr:",107629,,,,,,,,,,,,,,,,,,,,,"02/Mar/08 05:35;amar_kamat;Does it makes sense to spawn a thread from the task tracker rather than a separate jvm? The reason being that the shuffle code is again a framework code. ","01/Dec/09 05:53;buptzhugy;I think it would be better if shuffle and sort phase  seperate from reduce task.

1) The reschduled reduce need shuffle and sort again if the former reduce task failed in current implentation. Example, the reduce shuffle and sort phase cost a lot of time if a reduce need fetch map midoutput  from 100k maps.

2) we could shuffle and sort while anothers job's or tasks' reducer running, which would maximize resource utilization. In current implentation, the reduce slots are comsumed if it is shuffle or waiting the map finished.

3) we could localized the reduce task on the tasktracker where it has shuffled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultipleInputs doesn't work with new API in 0.20 branch,MAPREDUCE-1170,12439471,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,,jbooth,jbooth,29/Oct/09 23:42,25/Nov/09 20:03,12/Jan/21 09:52,03/Nov/09 07:18,0.20.1,,,,,0.20.2,,,,,,,,,0,,,,,"This patch adds support for MultipleInputs (and KeyValueTextInputFormat) in o.a.h.mapreduce.lib.input, working with the new API.  Included passing unit test.  Include for 0.20.2?",,hammer,kimballa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/09 20:03;jbooth;MAPREDUCE-1170-branch-20.patch;https://issues.apache.org/jira/secure/attachment/12426139/MAPREDUCE-1170-branch-20.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2009-10-31 00:06:01.332,,,false,,,,,,,,,,,,,,,,,,149326,,,,,Wed Nov 25 20:03:21 UTC 2009,,,,,,,"0|i0jfyv:",111517,,,,,,,,,,,,,,,,,,,,,"29/Oct/09 23:44;jbooth;backported directly from 0.21 branch, only degradation is when splitting KeyValueTextInputFormat, it doesn't recognize Bzip2Codec as splittable, because that would have dragged in a bunch more classes","30/Oct/09 21:45;jbooth;Turns out the test only passes because it doesn't try to actually execute the job.  It just uses MultipleInputs to add the inputs, then checks that they were added to the appropriate structures in memory.

When you run an actual job using TextInputFormat, we get:

java.lang.ClassCastException: org.apache.hadoop.mapreduce.lib.input.TaggedInputSplit cannot be cast to org.apache.hadoop.mapreduce.lib.input.FileSplit
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:55)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:418)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:582)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:176)

This probably affects 0.21 as well, based on my brief reading of the code..  any suggestions?  Seems kinda hard to work around without changing the signature of InputSplit, which would be pretty disruptive.

One (very hacky) method that could be used would be to have LineRecordReader do something along the lines of 
if (split instanceof TaggedInputSplit) split = ((TaggedInputSplit)split).getInnerSplit()

Any other ideas?","31/Oct/09 00:06;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12423644/multipleInputs.patch
  against trunk revision 831037.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/114/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/114/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/114/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/114/console

This message is automatically generated.","02/Nov/09 02:33;jbooth;Cancelling patch until this fully works","02/Nov/09 23:03;jbooth;New patch fixes ClassCastException in LineRecordReader via 
<pre>
if (split instanceof TaggedInputSplit) fileSplit = (FileSplit) ((TaggedInputSplit) split).getInputSplit();
else fileSplit = (FileSplit) split;
</pre>

The old test just added the inputs and verified they were added, didn't actually run a job, so this error snuck through.

New test runs a job with MultipleInputs and 2 different mapper classes, ensuring that output is correct.  Passes.

The test fails on 0.21 branch though -- I'll make a separate JIRA and post a patch for that as well

","02/Nov/09 23:15;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12423858/MAPREDUCE-1170.patch
  against trunk revision 831816.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/219/console

This message is automatically generated.","03/Nov/09 00:35;jbooth;Old patch was against an internal snapshot, this one's against hadoop/common/branches/branch-0.20/, should work.","03/Nov/09 07:18;cdouglas;As in MAPREDUCE-1145, we cannot push new code into the 0.20 branch without testing and supporting it.","25/Nov/09 20:03;jbooth;Better patch including Amareshwari's more maintainable fix from MAPREDUCE-1178.

Not targeting for inclusion, just posting here in case any passersby want the patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
should dump stacks before timing out task,MAPREDUCE-202,12354060,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,omalley,cutting,cutting,26/Oct/06 17:33,09/Nov/09 04:36,12/Jan/21 09:52,09/Nov/09 04:36,,,,,,,,,,,,,,,0,,,,,"When a task process times out and is killed it is often difficult to determine why.  If its stack was dumped prior to killing it, then debugging would be vastly simplified.  Ideally the stack dump would be available through the web ui, but even the log would be sufficient.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-11-09 04:36:20.507,,,false,,,,,,,,,,,,,,,,,,148592,,,,,Mon Nov 09 04:36:20 UTC 2009,,,,,,,"0|i0iri7:",107550,,,,,,,,,,,,,,,,,,,,,"09/Nov/09 04:36;vinodkv;I believe this is a duplicate of and will be fixed in MAPREDUCE-1119.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide a way to open and read a side file using an existing InputFormat,MAPREDUCE-1130,12438761,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,pkamath,pkamath,21/Oct/09 22:22,21/Oct/09 22:22,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"In the Pig subproject there is a need to open a side file for implementing map side joins. In some cases, the entire file needs to be read as a side file and in some cases, there is a need to read a file beginning from a particular split to the last split. In order to use existing InputFormats to achieve this, the pig code would need to mimic hadoop in terms of calling InputFormat.getSplits and then for each split call  InputFormat.createRecordReader, RecordReader.initialize() and then call RecordReader.nextKey() repeatedly till we reach end of split - and then continue to the next split. It would be good if there are some utility methods in Hadoop to achieve this - to read the file partially to the end or entirely to the end.",,cutting,tomwhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,149297,,,,,2009-10-21 22:22:37.0,,,,,,,"0|i0e9tz:",81355,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mapred.map.tasks and mapred.reduce.tasks should be determined automatically by default,MAPREDUCE-200,12392374,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,cutting,cutting,26/Mar/08 18:31,29/Sep/09 20:05,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"The default values for mapred.map.tasks and mapred.reduce.tasks should be empty or -1, signalling that the framework should use an dynamically-determined default.  An appropriate default is perhaps the number of map and reduce slots in the cluster, since FileInputFormat interprets mapred.map.tasks as a minimum.

This would remove a common area of misconfiguration.",,cutting,hong.tang,johanoskarsson,omalley,tomwhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2008-04-11 09:58:46.817,,,false,,,,,,,,,,,,,,,,,,148590,,,,,Tue Sep 29 20:05:58 UTC 2009,,,,,,,"0|i0itbj:",107844,,,,,,,,,,,,,,,,,,,,,"11/Apr/08 09:58;omalley;+1 for using the cluster defaults for this. I might suggest using #map slots * 2.0 and # reduce slots * 0.99 since those seem to be a little better defaults that 1.0 for both maps and reduces.","29/Sep/09 20:05;aw;I know the bug is still open, etc, but has there been any movement on this?  Reducing the number of params to configure would be a good thing, and this one seems to be particularly good to eliminate.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DBInputformat not working with SQLServer,MAPREDUCE-894,12433555,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,popo6190,popo6190,popo6190,20/Aug/09 09:55,21/Sep/09 14:10,12/Jan/21 09:52,,0.21.0,,,,,,,,,,,,,,0,,,,,"org.apache.hadoop.mapreduce.lib.db.DBInputFormat
Microsoft SQLServer doesn't support LIMIT and OFFSET.

Fix:
Based on MAPREDUCE-716, I already implemented it.
By creating a new class org.apache.hadoop.mapreduce.lib.db.MsSqlDBRecordReader 
and modifying class org.apache.hadoop.mapreduce.lib.db.DBInputFormat 

Note: this fix is working only with SQLServer 2005 or higher.

",,aaa,enis,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/09 13:50;popo6190;MAPREDUCE-894.2.patch;https://issues.apache.org/jira/secure/attachment/12420189/MAPREDUCE-894.2.patch","21/Aug/09 04:07;popo6190;MAPREDUCE-894.patch;https://issues.apache.org/jira/secure/attachment/12417215/MAPREDUCE-894.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2009-08-25 11:52:55.114,,,false,,,,,,,,,,,,,,,,,,149123,,,,,Mon Sep 21 14:10:44 UTC 2009,,,,,,,"0|i0jf2f:",111371,,,,,,,,,,,,,,,,,,,,,"25/Aug/09 11:52;enis;Submitting for QA. ","26/Aug/09 00:58;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12417215/MAPREDUCE-894.patch
  against trunk revision 807640.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/517/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/517/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/517/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/517/console

This message is automatically generated.","08/Sep/09 21:15;kimballa;+1 - patch looks good subject to comments below.

Some small notes about the patch:

* In the case where orderBy is null/empty, your {{""SELECT OVER (ORDER BY "" +  allFieldNames + "" ASC)""}} will put the ASC after the final field name in the list. (e.g {{ORDER BY foo, bar, baz ASC}}) Is this correct SQL? Or is an ASC required after each comma-delimited field in the list?
* A nitpick: You use mixed tabs and spaces. Can you convert your new tabs to 2 spaces each?

Finally, since this depends on SQL Server, it doesn't include any new tests. Can you please describe what tests you ran to demonstrate correctness?
","21/Sep/09 13:50;popo6190;Change in this update: remove tab & replace with 2 space.","21/Sep/09 14:10;popo6190;""SELECT ROW_NUMBER() OVER (ORDER BY .... ASC)"" is a valid SQL statement.
ASC is just to make sure the order is ascending, and only need to put one after end of field list.

To test this new module, I populate some data in table and create a simple map-reduce application. This application will read the data from table using DBInputFormat, do a simple process and put the result in file. 
After that, checking the result file to see the result is correct.
In this case, I used SQL Server 2005 express edition.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
distcp can retry copying specified number of times in case of transient failures,MAPREDUCE-651,12428069,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,ravidotg,ravidotg,ravidotg,17/Jun/09 05:19,08/Sep/09 11:05,12/Jan/21 09:52,08/Sep/09 11:05,,,,,,,,,distcp,,,,,,0,,,,,"distcp can retry specified number of times copying if the mapreduce job fails with transient error.

Providing option -retries <num_tries> to discp would be useful for users who copy large amount of data and see transient errors.",,aaa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,87639,,,,,Tue Sep 08 11:05:16 UTC 2009,,,,,,,"0|i0ixlr:",108538,,,,,,,,,,,,,,,,,,,,,"17/Jun/09 05:54;ravidotg;Uploaded combined patch of HADOOP-5444(atomic copy support) and this issue to JIRA-5444, as there are some dependant code changes.","08/Sep/09 11:05;ravidotg;Resolving this JIRA as duplicate as this issue is tracked as part of the patch of MAPREDUCE-650 as there are clashing code changes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log job history events to a common dump file,MAPREDUCE-198,12425567,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Won't Fix,amar_kamat,amar_kamat,amar_kamat,15/May/09 08:15,20/Aug/09 01:56,12/Jan/21 09:52,20/Aug/09 01:56,,,,,,,,,,,,,,,0,,,,,As of today all the jobhistory events are logged to separate files. It would be nice to also dump all this info into a common file so that external tools (e.g Chukwa) can harvest history info. Job configuration should also be dumped. Whether to use a same log file for history dumps and configuration dumps should be configurable (by default everything goes to one file). ,,aaa,acmurthy,ddas,hammer,omalley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-05-15 08:33:47.993,,,false,,,,,,,,,,,,,,,,,,148588,,,,,Thu Aug 20 01:56:44 UTC 2009,,,,,,,"0|i0ix3z:",108458,,,,,,,,,,,,,,,,,,,,,"15/May/09 08:33;acmurthy;Is the proposal to add another 'log' statement in JobHistory with the lock on the JobTracker? If so, that is a slippery slope...","15/May/09 08:44;ddas;Ideally we should implement a queue where we dump the history data and a thread that processes that queue asynchronously. But that could be done in a later jira. This jira is meant to help Chukwa folks make better sense out of the history data.","15/May/09 09:00;acmurthy;Haven't we had problems with job-history being written to hdfs before? Will adding another log not exacerbate it? Hence, I'm trying to understand the proposed solution...","15/May/09 09:12;ddas;The most common history configuration is to write the files in the local disk and this jira is not changing that model.. So it will be yet another file in the local disk. Once we have fixes that will not lock the JT on history writes, creating such files on the hdfs will be not that big a deal (and long term that is the goal). ","15/May/09 10:08;ddas;I forgot to mention that this logging would use log4j. Sorry about that.","18/May/09 11:13;steve_l;Even if the stuff goes to the local filesystem today, is it not possible to run something after the work has completed (on the same machines as the log files) to push those logs into the DFS filesystem, and hence into something that can merge the logs off different machines into one continuous timeline (assuming such a timeline exists and can be determined)?

","20/Aug/09 01:56;eric14;We've abandon this idea.  We will not be working on it. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ability to pause/resume jobs,MAPREDUCE-227,12405788,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,Duplicate,amar_kamat,amar_kamat,amar_kamat,06/Oct/08 07:46,06/Aug/09 06:33,12/Jan/21 09:52,06/Aug/09 06:18,,,,,,,,,,,,,,,0,,,,,"Consider a case where the user job depends on some external entity/service like a database or a web service. If the service needs restart or encounters a failure, the user should be able to pause the job and resume only when the service is up. This will be better than re-executing the whole job. Hence there should be some way to pause/resume jobs (from web-ui/command line) etc.",,acmurthy,cutting,dhruba,hong.tang,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/08 20:12;amar_kamat;HADOOP-4350-v1.2.patch;https://issues.apache.org/jira/secure/attachment/12391669/HADOOP-4350-v1.2.patch","07/Oct/08 20:41;amar_kamat;HADOOP-4350-v1.3.patch;https://issues.apache.org/jira/secure/attachment/12391673/HADOOP-4350-v1.3.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2008-10-06 10:17:35.213,,,false,,,,,,,,,,,,,,,,,,148612,,,,,Thu Aug 06 06:33:16 UTC 2009,,,,,,,"0|i0iuvb:",108095,,,,,,,,,,,,,,,,,,,,,"06/Oct/08 07:49;amar_kamat;We can have two knobs
1) soft/cold pause : where the scheduled tasks are allowed to run to completion and no more tasks are scheduled from the job. Job is marked to {{PAUSED}}.
2) hard/hot pause : where the scheduled tasks are killed and no more tasks are scheduled from the job. Job is marked to {{PAUSED}}.","06/Oct/08 08:07;amar_kamat;The question one needs to answer here is for how long should we keep them in memory and how many of them?","06/Oct/08 10:17;vinodkv;HADOOP-3687 is a related issue.","06/Oct/08 10:49;amar_kamat;HADOOP-3687 will be useful to HADOOP-4350. When a _job- pause_ is issued, we should ideally pause the tasks also instead of killing or waiting for their completion. But with scheduler in the picture, there will be lot of jobs running simultaneously and hence killing the current wave of tasks for a job should not affect the job as such.","07/Oct/08 20:12;amar_kamat;Attaching a patch the gets the basic feature working.","07/Oct/08 20:41;amar_kamat;Fixed the race condition in {{TestJobPause}}.","07/Oct/08 21:17;omalley;I think this is not a good direction to go. It would be much better to use the scheduler to give priority to the jobs that need it. By pausing the jobs and tasks, you'll consume resources that will block effective work by other jobs.","08/Oct/08 10:06;ddas;I agree with Owen. Offline, Amar mentioned to me that the main intention behind this issue was to support namenode bounce (in which case the service  talked about in this jira would be the namenode service). I can see that point. However, the thing to note here is that in the case of namenode being unavailable, the JT itself won't be able to do anything useful (no new jobs can be launched, new task launches trying to use the dfs would die, etc). So if we just address the problem of JT pause (where we pause all jobs) as opposed to a single job pause it should be enough.","09/Oct/08 06:48;dhruba;We would definitely like to use the feature to pause all current jobs. Typically before the start of a scheduled HDFS maintainence window. This is, in some sense, similar to the ""safemode"" of HDFS.
","06/Aug/09 06:18;yhemanth;Duplicate of MAPREDUCE-828. The consensus on this JIRA towards the end was that a per job pause/resume functionality was not a desired direction, but there were use cases for pausing the jobtracker itself.

In order to start fresh, and not get confused by the initial comments in this issue, I am resolving this as a duplicate.","06/Aug/09 06:33;tucu00;Between AUG03 and AUG10 I'll be on vacations and I will not checking email.

I'll reply to your message at my return.

Alejandro
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for scheduling jobs based on memory requirements to the Fairscheduler,MAPREDUCE-550,12419366,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,yhemanth,yhemanth,25/Mar/09 04:23,20/Jun/09 08:01,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"In HADOOP-3759, we added the ability for users to specify jobs requesting for a certain amount of virtual memory. For e.g. users can say that their jobs require 2GB of memory to run. In HADOOP-4035, functionality was added to the capacity scheduler to schedule jobs based on this specified amount. This JIRA is to add similar support to the Fairshare scheduler.

The basic use case is that there are jobs that require a certain known amount of virtual memory, usually more than the JVM's heap size. This happens specifically for streaming jobs that can launch several processes from the child. Without being aware of these requirements, if tasks are scheduled on nodes just based on available slots, they have a potential of affecting the other processes running on the node, or if memory protection features are enabled (HADOOP-3581), they could result in the task being killed by the tasktracker.

The scheduler must take into account the requested amount of memory by the job, the amount of memory that can be committed to by a tracker, and schedule based on these inputs.",,robw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,148885,,,,,2009-03-25 04:23:13.0,,,,,,,"0|i0iwkn:",108371,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add a batch option to pipes launcher,MAPREDUCE-391,12377787,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,omalley,omalley,omalley,07/Sep/07 21:46,20/Jun/09 07:54,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,Users who are launching pipes jobs in a script would like to have a non-blocking batch option on the API.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,148759,,,,,2007-09-07 21:46:49.0,,,,,,,"0|i0isbj:",107682,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support detailed timing for MapReduce job,MAPREDUCE-226,12412950,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,hong.tang,hong.tang,21/Jan/09 22:52,20/Jun/09 07:51,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"It would be nice to break down the time each individual Map task or Reduce task spends on reading input, writing output, and executing the map() or reduce() calls. ",,hong.tang,macyang,ruikubo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,3644,,,,,2009-01-21 22:52:53.0,,,,,,,"0|i00sy7:",2622,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement a generic DFA ,MAPREDUCE-206,12369773,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,acmurthy,acmurthy,acmurthy,20/May/07 19:31,20/Jun/09 07:51,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"Owen alluded to it HADOOP-1183 and Devaraj talked about this in HADOOP-1337 ... I believe this will be a generally useful feature in other parts of hadoop too...

The proposal is to implement a generic state machine which can be configured as needed and could be used to track states/transitions of various entities. This will hopefully make code less complex (for e.g. in the shuffle) and more maintainable/understandable since the state transitions of entities will be visible in one place (for e.g. where it is configured) and not scattered across myriad sections.

The idea is quite simple:

class StateMachine {

  // Register a legal 'transition' from pre to post state on a given event and 
  // a user-provided hook/callback 
  void registerTransition(State pre, State post, Event event, Hook hook) 
  throws IllegalStateTransitionException;

  // Effect a transition from the state on the event, return the resulting state or 
  // throw an exception if the transition is illegal. 
  // cause/victim are user objects i.e. context/result of the transition.
  State doTransition(State state, Event event, Object cause, Object victim) 
  throws IllegalStateTransitionException;
}


-*-*-

Thoughts?",,romainr,sharadag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-278,,,,,,,,,,,,,,,,,,,,,"20/May/07 19:34;acmurthy;HADOOP-1395_1_20070521.patch;https://issues.apache.org/jira/secure/attachment/12357704/HADOOP-1395_1_20070521.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2007-05-21 13:02:02.377,,,false,,,,,,,,,,,,,,,,,,148596,,,,,Mon May 21 13:02:02 UTC 2007,,,,,,,"0|i0is1j:",107637,,,,,,,,,,,,,,,,,,,,,"20/May/07 19:34;acmurthy;Attached is a slightly better illustration of the simple 'state machine' via code. 

Appreciate any feedback/ideas...","20/May/07 19:56;acmurthy;I've attached a half-baked patch to illustrate how to use the *StateMachine* dfa here (https://issues.apache.org/jira/secure/attachment/12357705/mapred_as_dfa.patch) ... ","21/May/07 13:02;tomwhite;Are there any existing DFA frameworks that we could use, before we implement our own? E.g. http://www.brics.dk/automaton/, although this probably isn't suitable since it doesn't seem to be geared towards hooking into external events. Anyone know of such a library?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ability to configure gap/lag parameters for speculative execution for maps/reduces,MAPREDUCE-204,12387522,New Feature,Reopened,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,jsensarma,jsensarma,31/Jan/08 01:52,20/Jun/09 07:51,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"Motivation is obvious:

- sometimes reduces have side-effects - but maps don't (for example reduces compute something and write it somewhere)
- want to turn ON speculative for maps (because maps are usually way less expensive. map progress counters are reliable etc.) - but OFF for reduces (very expensive to run two reducers instead of one).

It's also likely that in future you may want different setting for speculative triggers for maps and reduces ... (but that's a different issue).

I will submit a patch (since we are putting this into effect on our cluster - and it seems simple enough :-)).",,omalley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2008-01-31 03:03:13.101,,,false,,,,,,,,,,,,,,,,,,148594,,,,,Fri Feb 01 18:30:18 UTC 2008,,,,,,,"0|i0it13:",107797,,,,,,,,,,,,,,,,,,,,,"31/Jan/08 03:03;omalley;This was fixed by HADOOP-2131.","01/Feb/08 09:22;jsensarma;thanks for pointing out the dup.

also looking for a way to configure these parameters - just going to repurpose this bug for that. the issue is that we want to run with very conservative settings for speculative execution - but we do want it on (to take care of those laggard tasks that never want to come out of initialization mode ..). i can think there is going to difference in how people with busy clusters want to tune spec. exec. vs. those that have idle clusters.

might become moot if spec. exec. is load aware and based on completion etc. - but until then - the fine grained control would be helpful.","01/Feb/08 09:23;jsensarma;repurposing","01/Feb/08 18:30;nidaley;Removing from 0.16.0 release as this is not a blocker",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"counters: want rates and averages, and phase-level sums",MAPREDUCE-203,12363521,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Major,,,cutting,cutting,23/Feb/07 20:23,20/Jun/09 07:51,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"It would be nice if the web ui displayed, for each counter, not just its total, but also its rate (counts/second).  Also, for job-level counts, per-task averages should be displayed, of both total and rate.  Ideally we could also include phase-level counts (map versus reduce).  Thus we could display, e.g., average map task input bytes/second for a job.
",,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2007-02-23 21:55:17.445,,,false,,,,,,,,,,,,,,,,,,148593,,,,,Fri Feb 23 22:06:43 UTC 2007,,,,,,,"0|i0irsf:",107596,,,,,,,,,,,,,,,,,,,,,"23/Feb/07 21:55;dbowen;
Should the rate (counts/second) be a running average, say the per-minute rate, updated every minute?  Or what?



","23/Feb/07 22:06;cutting;I was just thinking overall, i.e., for tasks the amount of time since start (if it's incomplete) or its total time (if it's complete), for phases, the total time in the phase, and for jobs, the total time in the job.   Instantaneous rates could be nice as a later feature.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocatedFileStatusFetcher to collect/publish IOStatistics,MAPREDUCE-7315,13348591,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Fixed,stevel@apache.org,stevel@apache.org,stevel@apache.org,31/Dec/20 12:00,31/Dec/20 16:03,12/Jan/21 09:52,31/Dec/20 16:02,3.3.0,,,,,3.4.0,,,client,,,,,,0,pull-request-available,,,,"
Part of HADOOP-16830: if a FileSystem's RemoteIterators implement IOStatisticsSource, then collect these and serve through the IOStatisticsSource API.

If the iterators don't (only S3A does, but ABFS will too), then nothing is collected",,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"steveloughran opened a new pull request #2579:
URL: https://github.com/apache/hadoop/pull/2579


   
   Part of HADOOP-16830 IOStatistics Support.
   
   Contributed by Steve Loughran.
   
   # Tests are in hadoop-aws so excluded here
   # full PR already reviewed by @bgaborg in #2553; this is the feature split up by module for better isolation/backport


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Dec/20 12:06;githubbot;600","hadoop-yetus commented on pull request #2579:
URL: https://github.com/apache/hadoop/pull/2579#issuecomment-752959475


   :broken_heart: **-1 overall**
   
   
   
   
   
   
   | Vote | Subsystem | Runtime |  Logfile | Comment |
   |:----:|----------:|--------:|:--------:|:-------:|
   | +0 :ok: |  reexec  |   0m 30s |  |  Docker mode activated.  |
   |||| _ Prechecks _ |
   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |
   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |
   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |
   |||| _ trunk Compile Tests _ |
   | +1 :green_heart: |  mvninstall  |  32m 59s |  |  trunk passed  |
   | +1 :green_heart: |  compile  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.9.1+1-Ubuntu-0ubuntu1.18.04  |
   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_275-8u275-b01-0ubuntu1~18.04-b01  |
   | +1 :green_heart: |  checkstyle  |   0m 29s |  |  trunk passed  |
   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |
   | +1 :green_heart: |  shadedclient  |  16m 16s |  |  branch has no errors when building and testing our client artifacts.  |
   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.9.1+1-Ubuntu-0ubuntu1.18.04  |
   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_275-8u275-b01-0ubuntu1~18.04-b01  |
   | +0 :ok: |  spotbugs  |   1m 16s |  |  Used deprecated FindBugs config; considering switching to SpotBugs.  |
   | +1 :green_heart: |  findbugs  |   1m 14s |  |  trunk passed  |
   |||| _ Patch Compile Tests _ |
   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |
   | +1 :green_heart: |  compile  |   0m 31s |  |  the patch passed with JDK Ubuntu-11.0.9.1+1-Ubuntu-0ubuntu1.18.04  |
   | +1 :green_heart: |  javac  |   0m 31s |  |  the patch passed  |
   | +1 :green_heart: |  compile  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_275-8u275-b01-0ubuntu1~18.04-b01  |
   | +1 :green_heart: |  javac  |   0m 27s |  |  the patch passed  |
   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |
   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |
   | +1 :green_heart: |  whitespace  |   0m  0s |  |  The patch has no whitespace issues.  |
   | +1 :green_heart: |  shadedclient  |  14m 58s |  |  patch has no errors when building and testing our client artifacts.  |
   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.9.1+1-Ubuntu-0ubuntu1.18.04  |
   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_275-8u275-b01-0ubuntu1~18.04-b01  |
   | +1 :green_heart: |  findbugs  |   1m 16s |  |  the patch passed  |
   |||| _ Other Tests _ |
   | +1 :green_heart: |  unit  |   6m 58s |  |  hadoop-mapreduce-client-core in the patch passed.  |
   | +1 :green_heart: |  asflicense  |   0m 33s |  |  The patch does not generate ASF License warnings.  |
   |  |   |  83m 15s |  |  |
   
   
   | Subsystem | Report/Notes |
   |----------:|:-------------|
   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2579/1/artifact/out/Dockerfile |
   | GITHUB PR | https://github.com/apache/hadoop/pull/2579 |
   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient findbugs checkstyle |
   | uname | Linux 8776cfff27cc 4.15.0-58-generic #64-Ubuntu SMP Tue Aug 6 11:12:41 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
   | Build tool | maven |
   | Personality | dev-support/bin/hadoop.sh |
   | git revision | trunk / 99d08a19ba5 |
   | Default Java | Private Build-1.8.0_275-8u275-b01-0ubuntu1~18.04-b01 |
   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.9.1+1-Ubuntu-0ubuntu1.18.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_275-8u275-b01-0ubuntu1~18.04-b01 |
   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2579/1/testReport/ |
   | Max. process+thread count | 1614 (vs. ulimit of 5500) |
   | modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core |
   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2579/1/console |
   | versions | git=2.17.1 maven=3.6.0 findbugs=4.0.6 |
   | Powered by | Apache Yetus 0.13.0-SNAPSHOT https://yetus.apache.org |
   
   
   This message was automatically generated.
   
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Dec/20 13:31;githubbot;600","steveloughran merged pull request #2579:
URL: https://github.com/apache/hadoop/pull/2579


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Dec/20 16:02;githubbot;600",,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2020-12-31 12:00:32.0,,,,,,,"0|z0lyd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add option to not kill already-done map tasks when node becomes unusable,MAPREDUCE-7168,13201762,New Feature,Patch Available,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,mkonst,mkonst,mkonst,01/Dec/18 02:22,16/Aug/19 01:31,12/Jan/21 09:52,,2.9.2,,,,,,,,mrv2,,,,,,1,,,,,"When a node becomes unusable, if there are still reduce tasks running, all completed map tasks that were run on that node are killed so that they can be re-run on a different node. This is because the node can no longer serve shuffle data, so the map task output cannot be fetched by the reducers.

If map tasks do not write their shuffle data locally, killing already-done map tasks will make the job lose map progress unnecessarily. This change prevents map progress from being lost when shuffle data is not written locally by providing a property mapreduce.map.rerun-if-node-unusable that can be set to false to prevent killing already-done map tasks.","Google Compute Engine (Dataproc), Java 8",medb,mkonst,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/18 02:48;mkonst;MAPREDUCE-7168.patch;https://issues.apache.org/jira/secure/attachment/12950263/MAPREDUCE-7168.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2018-12-01 04:03:58.145,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,,Fri Aug 16 01:31:11 UTC 2019,,,,,,,"0|s012e8:",9223372036854775807,,,,,,,,,,,,,2.9.2,,,,,,,,"01/Dec/18 04:03;hadoopqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 15s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 27s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 20m 15s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 59s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 46s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 10s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 13m  1s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 51s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 38s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 10s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  4s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 59s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 59s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 43s{color} | {color:orange} hadoop-mapreduce-project/hadoop-mapreduce-client: The patch generated 6 new + 667 unchanged - 0 fixed = 673 total (was 667) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  3s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 45s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 49s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 36s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  4m 18s{color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  9m 55s{color} | {color:green} hadoop-mapreduce-client-app in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 22s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 73m 33s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8f97d6f |
| JIRA Issue | MAPREDUCE-7168 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12950263/MAPREDUCE-7168.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  xml  |
| uname | Linux f2289db39a91 4.4.0-138-generic #164-Ubuntu SMP Tue Oct 2 17:16:02 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 99e201d |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_181 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7557/artifact/out/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7557/testReport/ |
| Max. process+thread count | 1605 (vs. ulimit of 10000) |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7557/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.

","16/Aug/19 01:31;hadoopqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 21s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 43s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 24m 44s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 50s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 42s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  2s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 34s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 38s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 45s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 12s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  0s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 34s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 41s{color} | {color:orange} hadoop-mapreduce-project/hadoop-mapreduce-client: The patch generated 6 new + 665 unchanged - 0 fixed = 671 total (was 665) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 56s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 16s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 48s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 39s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  5m 19s{color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 10m  6s{color} | {color:green} hadoop-mapreduce-client-app in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 30s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 80m  9s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=19.03.1 Server=19.03.1 Image:yetus/hadoop:bdbca0e |
| JIRA Issue | MAPREDUCE-7168 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12950263/MAPREDUCE-7168.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  xml  |
| uname | Linux 24fe5c50ec4d 4.4.0-157-generic #185-Ubuntu SMP Tue Jul 23 09:17:01 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 5882cf9 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_212 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7656/artifact/out/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7656/testReport/ |
| Max. process+thread count | 1637 (vs. ulimit of 10000) |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7656/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support raw binary data with Hadoop streaming,MAPREDUCE-5018,12633405,New Feature,Patch Available,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,stevenwillis,jayqhacker,jayqhacker,21/Feb/13 14:50,28/Feb/19 23:24,12/Jan/21 09:52,,1.1.2,,,,,,,,contrib/streaming,,,,,,2,BB2015-05-TBR,,,,"People often have a need to run older programs over many files, and turn to Hadoop streaming as a reliable, performant batch system.  There are good reasons for this:

1. Hadoop is convenient: they may already be using it for mapreduce jobs, and it is easy to spin up a cluster in the cloud.
2. It is reliable: HDFS replicates data and the scheduler retries failed jobs.
3. It is reasonably performant: it moves the code to the data, maintaining locality, and scales with the number of nodes.

Historically Hadoop is of course oriented toward processing key/value pairs, and so needs to interpret the data passing through it.  Unfortunately, this makes it difficult to use Hadoop streaming with programs that don't deal in key/value pairs, or with binary data in general.  For example, something as simple as running md5sum to verify the integrity of files will not give the correct result, due to Hadoop's interpretation of the data.  

There have been several attempts at binary serialization schemes for Hadoop streaming, such as TypedBytes (HADOOP-1722); however, these are still aimed at efficiently encoding key/value pairs, and not passing data through unmodified.  Even the ""RawBytes"" serialization scheme adds length fields to the data, rendering it not-so-raw.

I often have a need to run a Unix filter on files stored in HDFS; currently, the only way I can do this on the raw data is to copy the data out and run the filter on one machine, which is inconvenient, slow, and unreliable.  It would be very convenient to run the filter as a map-only job, allowing me to build on existing (well-tested!) building blocks in the Unix tradition instead of reimplementing them as mapreduce programs.

However, most existing tools don't know about file splits, and so want to process whole files; and of course many expect raw binary input and output.  The solution is to run a map-only job with an InputFormat and OutputFormat that just pass raw bytes and don't split.  It turns out to be a little more complicated with streaming; I have attached a patch with the simplest solution I could come up with.  I call the format ""JustBytes"" (as ""RawBytes"" was already taken), and it should be usable with most recent versions of Hadoop.
",,cutting,eli,jayqhacker,kkambatl,Naganarasimha,pfxuan,pratem,ravwojdyla,rgelhau,stevenwillis,Tagar,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-598,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-606,,,,,"14/May/14 17:32;stevenwillis;MAPREDUCE-5018-branch-1.1.patch;https://issues.apache.org/jira/secure/attachment/12644852/MAPREDUCE-5018-branch-1.1.patch","14/May/14 20:46;stevenwillis;MAPREDUCE-5018.patch;https://issues.apache.org/jira/secure/attachment/12644886/MAPREDUCE-5018.patch","21/Feb/13 15:45;jayqhacker;MAPREDUCE-5018.patch;https://issues.apache.org/jira/secure/attachment/12570317/MAPREDUCE-5018.patch","21/Feb/13 16:52;jayqhacker;justbytes.jar;https://issues.apache.org/jira/secure/attachment/12570327/justbytes.jar","21/Feb/13 16:52;jayqhacker;mapstream;https://issues.apache.org/jira/secure/attachment/12570328/mapstream",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,2013-02-21 15:55:26.454,,,false,,,,,,,,,,,,,,,,,,313900,,,,,Thu Feb 28 23:24:11 UTC 2019,,,,,,,"0|i1i633:",314245,"Add ""-io justbytes"" I/O format to allow raw binary streaming.",,,,,,,,,,,,1.1.2,,,,,,,,"21/Feb/13 15:42;jayqhacker;This patch adds a 'JustBytesWritable' and supporting InputFormat, OutputFormat, InputWriter, and OutputReader to support passing raw, unmodified, unaugmented bytes through Hadoop streaming.  The purpose is to be able to run arbitrary Unix filters on entire binary files stored in HDFS as map-only jobs, taking advantage of locality and reliability offered by Hadoop.

The code is very straightforward; most methods are only one line.

A few design notes:

1. Data is stored in a JustBytesWritable, which is the simplest possible Writable wrapper around a byte[].  It literally just reads until the buffer is full or EOF and remembers the number of bytes.

2. Data is read by JustBytesInputFormat in 64K chunks by default and stored in a JustBytesWritable key; the value is a NullWritable, but no value is ever read or written.  They key is used instead of the value to allow the possibility of using it in a reduce.

3. Input files are never split, as most programs are not able to handle splits.

4. Input files are not decompressed, as the purpose is to get raw data to a program, people may want to operate on compressed data (e.g., md5sum on archives), and as most tools do not expect automatic decompression, this is the ""least surprising"" option.  It's also trivial to throw a ""zcat"" in front of your filter.

5. Output is even simpler than input, and just writes the bytes of a JustBytesWritable key to the output stream.  Output is never compressed, for similar reasons as above.

6. The code uses the old mapred API, as that is what streaming uses.

Streaming inserts an InputWriter between the InputFormat and the map executable, and an OutputReader between the map executable and the OutputFormat; the JustBytes version simply pass the key bytes on through.

I've augmented IdentifierResolver to recognize ""-io justbytes"" on the command line and set the input/output classes appropriately.

I've included a shell script called ""mapstream"" to run streaming with all required command line parameters; it makes running a binary map-only job as easy as:

    mapstream indir command outdir

which runs ""command"" on every file in indir and writes the results to outdir.

I welcome feedback, especially if there is an even simpler way to do this.  I'm not hung up on the JustBytes name, I'd be happy to switch to a better one.  If people like the general approach, I will add unit tests and resubmit.  Also please let me know if I should break this into separate patches for common and mapreduce.","21/Feb/13 15:45;jayqhacker;justbytes patch submitted for code review.","21/Feb/13 15:55;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12570317/MAPREDUCE-5018.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3352//console

This message is automatically generated.","21/Feb/13 16:32;jayqhacker;I believe this is a more general, simpler, and more up-to-date approach to getting binary data in and out of Hadoop.","21/Feb/13 16:52;jayqhacker;I've attached a jar file with source and compiled binaries for people who want to try it out without recompiling Hadoop.  You can use the attached 'mapstream' shell script to run it easily.

For those interested in performance, the TL;DR is about 10X slower than native.  That's running 'cat' as the mapper on one file that fits in one block, compared to cat on a local ext4 filesystem on the same machine. If your files span multiple blocks, the non-local reads will be even slower.  That also doesn't include job overhead.  However, most mappers will be more CPU intensive, and the relative overhead of I/O diminishes; YMMV.","21/Feb/13 17:00;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12570328/mapstream
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3353//console

This message is automatically generated.","01/May/13 12:43;pratem;Hi Jay,
If its not splittable, how are you going to gain the benefit of using hadoop's infrastructure? The infrastructure would be busy making network IOs to build up the binary file from replicated sources and passing it a single mapper?

","10/May/13 21:26;jayqhacker;[~pratem], you're right, there are cases where it's not efficient.  Consider this though: if you have 100 TB of files in HDFS that you want to md5sum (or what have you), would you rather do an inefficient distributed md5sum on the cluster, or copy 100 TB out to a single machine and wait for a single md5sum?  Can you even fit that on one machine?

You still gain reliability: there are multiple copies of each file, and failed jobs get restarted.  It's also just convenient.

Here's the trick to make it efficient: use many files, and set the block size of individual files big enough to fit the whole file:

{{hadoop fs -D dfs.block.size=1073741824 -put ...}}

Then all reads are local, and you get all the performance Hadoop can give you.","16/May/13 03:48;pratem;Yes in that case its fine..We are creating a modified version of JustBytesInputFormat that does the splits as we could split our binary data with FixedLength Record sizes.Thanks for JustBytes!

One more query, at places our data contains \n and \r characters as part of the binary data and we dont want the stdin to interpret these characters, since its corrupts the data once it reaches the mapper.
Is there anything that can be done? I dont want to hexencode it before writing it to the stream to the mapper..","22/May/13 20:21;jayqhacker;You're welcome!  

It might be easier to just split your inputs yourself before putting them in HDFS (see {{split(1)}}), but perhaps your files are already in HDFS.

JustBytes shouldn't modify or interpret your data at all; it reads an entire file in binary, gives those exact bytes to your mapper, and writes out the exact bytes your mapper gives.  It does not know or care about newlines.  I would encourage you to run {{md5sum}} on your data outside HDFS and via {{mapstream}} to verify that it is not changing your data at all, and let me know if it is.","14/May/14 17:32;stevenwillis;A patch for the 1.1 branch","14/May/14 20:46;stevenwillis;New patch with tests","16/Jun/14 15:05;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12644886/MAPREDUCE-5018.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 2 warning messages.
        See https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4662//artifact/trunk/patchprocess/diffJavadocWarnings.txt for details.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-tools/hadoop-streaming.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4662//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4662//console

This message is automatically generated.","10/Mar/15 04:44;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org
  against trunk revision 47f7f18.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5260//console

This message is automatically generated.","28/Feb/19 21:07;Tagar;Any workaround for this .. would be great to use Hadoop Streaming facility for binary files.. ","28/Feb/19 23:24;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 23s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  6m 18s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 16m 23s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 15m 28s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m 55s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m  7s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 36s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m 51s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 30s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 27s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 14m 19s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 14m 19s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  3m  3s{color} | {color:orange} root: The patch generated 8 new + 2 unchanged - 0 fixed = 10 total (was 2) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m  0s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} shellcheck {color} | {color:green}  0m  0s{color} | {color:green} There were no new shellcheck issues. {color} |
| {color:green}+1{color} | {color:green} shelldocs {color} | {color:green}  0m 36s{color} | {color:green} There were no new shelldocs issues. {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 49s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  4m 14s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  1m  9s{color} | {color:red} hadoop-common-project_hadoop-common generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 10m 11s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  5m 32s{color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  6m 38s{color} | {color:green} hadoop-streaming in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 53s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}131m 24s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8f97d6f |
| JIRA Issue | MAPREDUCE-5018 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12644886/MAPREDUCE-5018.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  shellcheck  shelldocs  |
| uname | Linux 7b8f827152b6 4.4.0-138-generic #164-Ubuntu SMP Tue Oct 2 17:16:02 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 0d61fac |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_191 |
| shellcheck | v0.4.6 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7588/artifact/out/diff-checkstyle-root.txt |
| javadoc | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7588/artifact/out/diff-javadoc-javadoc-hadoop-common-project_hadoop-common.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7588/testReport/ |
| Max. process+thread count | 1608 (vs. ulimit of 10000) |
| modules | C: hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-tools/hadoop-streaming U: . |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7588/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MapReduce add NullInputFormat,MAPREDUCE-7163,13200597,New Feature,Patch Available,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,uranus,uranus,uranus,26/Nov/18 15:08,26/Nov/18 16:18,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"When job's inputformat set to NullInputFormat, we can use mapreduce as distributed shell. The following is an example, 
{code:java}
hadoop jar hadoop-streaming-xxx.jar \
-D mapreduce.job.name=distributed_shell \
-D mapreduce.job.maps=100 \
-D mapred.reduce.tasks=0 \
-inputformat org.apache.hadoop.mapred.NullInputFormat \
-input /user/test/in \
-output /user/test/out \
-mapper shell.sh
{code}",,uranus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/18 15:12;uranus;MAPREDUCE-7163.001.patch;https://issues.apache.org/jira/secure/attachment/12949510/MAPREDUCE-7163.001.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2018-11-26 16:18:22.03,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 26 16:18:22 UTC 2018,,,,,,,"0|s00v8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"26/Nov/18 16:18;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 40s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 20m 47s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 35s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 29s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 39s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 33s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 59s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 22s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 29s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 29s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 23s{color} | {color:orange} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core: The patch generated 7 new + 0 unchanged - 0 fixed = 7 total (was 0) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 33s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 34s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  1s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 17s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  4m 24s{color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 26s{color} | {color:red} The patch generated 1 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 57m 55s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8f97d6f |
| JIRA Issue | MAPREDUCE-7163 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12949510/MAPREDUCE-7163.001.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 5b886f41903b 4.4.0-138-generic #164-Ubuntu SMP Tue Oct 2 17:16:02 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / b098281 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_181 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7541/artifact/out/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7541/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7541/artifact/out/patch-asflicense-problems.txt |
| Max. process+thread count | 1602 (vs. ulimit of 10000) |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7541/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add peak memory usage counter for each task,MAPREDUCE-4710,12610614,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Duplicate,cindyli_2012,cindyli_2012,cindyli_2012,05/Oct/12 18:47,10/Jul/18 23:52,12/Jan/21 09:52,10/Jul/18 23:52,1.0.2,,,,,,,,task,,,,,,0,,,,,"Each task has counters PHYSICAL_MEMORY_BYTES and VIRTUAL_MEMORY_BYTES, which are snapshots of memory usage of that task. They are not sufficient for users to understand peak memory usage by that task, e.g. in order to diagnose task failures, tune job parameters or change application design. This new feature will add two more counters for each task: PHYSICAL_MEMORY_BYTES_MAX and VIRTUAL_MEMORY_BYTES_MAX. ",,acmurthy,aw,cindy2012,cindyli_2012,erik.fang,jira.shegalov,jrottinghuis,mayank_bansal,mingma,qwertymaniac,revans2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-6829,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/13 00:56;cindyli_2012;MAPREDUCE-4710-trunk.patch;https://issues.apache.org/jira/secure/attachment/12567473/MAPREDUCE-4710-trunk.patch","26/Nov/12 21:43;cindyli_2012;mapreduce-4710-v1.0.2.patch;https://issues.apache.org/jira/secure/attachment/12554918/mapreduce-4710-v1.0.2.patch","06/Oct/12 00:00;cindyli_2012;mapreduce-4710.patch;https://issues.apache.org/jira/secure/attachment/12548074/mapreduce-4710.patch","09/Dec/13 23:52;cindy2012;mapreduce4710-v3.patch;https://issues.apache.org/jira/secure/attachment/12617927/mapreduce4710-v3.patch","31/Dec/13 00:01;cindy2012;mapreduce4710-v6.patch;https://issues.apache.org/jira/secure/attachment/12620887/mapreduce4710-v6.patch","03/Oct/13 05:46;cindyli_2012;mapreduce4710.patch;https://issues.apache.org/jira/secure/attachment/12606531/mapreduce4710.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,2012-10-06 10:08:23.99,,,false,,,,,,,,,,,,,,,,,,244039,,,,,Fri Apr 29 23:25:00 UTC 2016,,,,,,,"0|i05fgn:",29602,,,,,,,,,,,,,,,,,,,,,"06/Oct/12 00:01;cindyli_2012;First draft patch attached. It contains change to the code and to the related unit test. ","06/Oct/12 10:08;qwertymaniac;Cindy,

Thanks! This would come useful.

The branch-1 is a maintenance branch that should not receive new features first and diverge from trunk. Please also provide a trunk patch that can go in first, so we maintain feature parity. Or if trunk does not require this, please explain.","09/Oct/12 05:25;acmurthy;Cindy - thanks for the patch. As Harsh explained you'll have to port this patch to trunk also. Please let me know if you need any help with that.

One comment on the patch - rather than track peak memory in Task.java I'd rather have a getPeakMemory interface defined in the ProcResourceValues structure so that the logic for this is all in one place. Makes sense? Thanks.","26/Nov/12 21:44;cindyli_2012;Thanks for the comments, Harsh and Arun!

I've added the change to interface for hadoop 1.0.2 version. Similar change will be applied to the trunk and patch for trunk will be updated soon. ","12/Dec/12 23:43;cindyli_2012;Arun, any comment for the interface change?","28/Jan/13 19:26;acmurthy;Cindi - can you please provide a patch from trunk too? Tx.","01/Feb/13 00:56;cindyli_2012;Added patch for trunk. ","10/Jul/13 22:57;acmurthy;[~cindyli_2012] I'm a little concerned about using statics without any synchronization at all - have you verified that there are no issues? Is there a better way around this than using statics?","03/Oct/13 05:46;cindyli_2012;Rebased patch to latest trunk.","25/Oct/13 17:45;cindyli_2012;Arun, can you please review the rebased patch? Thanks.","07/Nov/13 22:30;mingma;It doesn’t seem to be MR application specific, other YARN application might want this as well. Should it be done at NM level so that there are general container peak memory usage data?","09/Nov/13 05:25;jrottinghuis;Cindy, is your experience that your JVM is giving memory back to the OS to make PHYSICAL_MEMORY_BYTES_MAX inaccurate ?

What is the sampling rate to use to detect the max, or is sampling not needed here ?


","22/Nov/13 21:41;mayank_bansal;overall looks good couple of comments

1. Please create the patch with --no-prefix
2. Write some unit tests.

Thanks,
Mayank","09/Dec/13 23:53;cindy2012;submit the last one: mapreduce4710-v3.patch","10/Dec/13 01:25;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12617927/mapreduce4710-v3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-rumen:

                  org.apache.hadoop.mapreduce.security.TestJHSSecurity
                  org.apache.hadoop.mapreduce.v2.TestMRJobPeakMemory
                  org.apache.hadoop.mapred.TestJobCleanup

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4252//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4252//console

This message is automatically generated.","10/Dec/13 22:02;cindy2012;2 of the test failures are not related to my patch
fixed one assertion failure due to unit setting. added new path mapreduce4710-v4.patch. ","10/Dec/13 22:03;cindy2012;resubmit patch after fixing the assertion failure.","10/Dec/13 23:31;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12618111/mapreduce4710-v4.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-rumen:

                  org.apache.hadoop.mapreduce.security.TestJHSSecurity
                  org.apache.hadoop.mapreduce.v2.TestMRJobPeakMemory
                  org.apache.hadoop.mapred.TestJobCleanup

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4253//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4253//console

This message is automatically generated.","11/Dec/13 02:09;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12618144/mapreduce4710-v5.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-rumen:

                  org.apache.hadoop.mapreduce.security.TestJHSSecurity

                                      The following test timeouts occurred in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-rumen:

org.apache.hadoop.mapreduce.v2.TestMRJobPeakMemory

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4255//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4255//console

This message is automatically generated.","12/Dec/13 01:56;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12618338/mapreduce4710-v6.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4258//console

This message is automatically generated.","27/Dec/13 01:21;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12620559/mapreduce4710-v6.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-rumen:

                  org.apache.hadoop.mapreduce.security.TestJHSSecurity

                                      The following test timeouts occurred in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-rumen:

org.apache.hadoop.mapreduce.v2.TestMRJobPeakMemory

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4283//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4283//console

This message is automatically generated.","27/Dec/13 01:43;cindy2012;The failed test in test Report is not related to my patch. 
https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4283/testReport/","30/Dec/13 19:36;mayank_bansal;I think test which is been added is getting time out based on above report

Thanks,
Mayank","30/Dec/13 22:30;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12620868/mapreduce4710-v6.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The following test timeouts occurred in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-rumen:

org.apache.hadoop.mapreduce.v2.TestMRJobPeakMemory

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4290//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4290//console

This message is automatically generated.","31/Dec/13 01:44;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12620887/mapreduce4710-v6.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-rumen.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4291//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4291//console

This message is automatically generated.","07/Jan/14 23:04;mingma;A general question, should NM provide such data at the container level? It seems we need that information to support preemption and fairness anyway; NM needs to inform RM the actual resource utilization at container level; memory usage is one of the resource metrics. Currently ContainerStatus doesn't provide that level of details.
","10/Mar/15 06:55;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12620887/mapreduce4710-v6.patch
  against trunk revision 7711049.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5277//console

This message is automatically generated.","16/Jul/15 16:40;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12620887/mapreduce4710-v6.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 1ba2986 |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5891/console |


This message was automatically generated.","29/Apr/16 22:29;yufeigu;Hi [~cindy2012], are you still working on this?","29/Apr/16 23:25;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 0s {color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red} 0m 4s {color} | {color:red} MAPREDUCE-4710 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12620887/mapreduce4710-v6.patch |
| JIRA Issue | MAPREDUCE-4710 |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6471/console |
| Powered by | Apache Yetus 0.2.0   http://yetus.apache.org |


This message was automatically generated.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow raid policy to specify a parent policy,MAPREDUCE-1910,12468391,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Won't Fix,schen,schen,schen,01/Jul/10 23:58,01/Aug/17 17:12,12/Jan/21 09:52,01/Aug/17 17:12,0.22.0,,,,,,,,contrib/raid,,,,,,0,,,,,We encountered the problem that there are lots of redundancy in our raid.xml file. Most of the policy shares the same properties. It will be nice if a policy can inherit from a previously defined policy and get the default properties from it. This way it is easier to maintain the raid policies.,,aw,celinasam,dhruba,rschmidt,rvadali,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-08-01 17:12:57.616,,,false,,,,,,,,,,,,,,,,,,149854,,,,,Tue Aug 01 17:12:57 UTC 2017,,,,,,,"0|i0jhrr:",111809,,,,,,,,,,,,,,,,,,,,,"01/Aug/17 17:12;aw;MR RAID has been replaced by HDFS EC in modern versions of Hadoop.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adaptive Scheduler,MAPREDUCE-1380,12445613,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,jorda,jorda,jorda,15/Jan/10 09:25,07/Jan/17 01:59,12/Jan/21 09:52,,2.4.1,,,,,,,,,,,,,,0,,,,,"The Adaptive Scheduler is a pluggable Hadoop scheduler that automatically adjusts the amount of used resources depending on the performance of jobs and on user-defined high-level business goals.

Existing Hadoop schedulers are focused on managing large, static clusters in which nodes are added or removed manually. On the other hand, the goal of this scheduler is to improve the integration of Hadoop and the applications that run on top of it with environments that allow a more dynamic provisioning of resources.

The current implementation is quite straightforward. Users specify a deadline at job submission time, and the scheduler adjusts the resources to meet that deadline (at the moment, the scheduler can be configured to either minimize or maximize the amount of resources). If multiple jobs are run simultaneously, the scheduler prioritizes them by deadline. Note that the current approach to estimate the completion time of jobs is quite simplistic: it is based on the time it takes to finish each task, so it works well with regular jobs, but there is still room for improvement for unpredictable jobs.

The idea is to further integrate it with cloud-like and virtual environments (such as Amazon EC2, Emotive, etc.) so that if, for instance, a job isn't able to meet its deadline, the scheduler automatically requests more resources.",,aah,acmurthy,airbots,andyk,aw,cdouglas,charlescearl,devaraj,dhruba,dms,ehf,ericson,hammer,hong.tang,JackHoo,johanoskarsson,johnvijoe,jorda,junping_du,lianhuiwang,mahadev,matei,priyankajaiswal8,qwertymaniac,schen,sreekanth,tlipcon,tomwhite,tozka,varun_saxena,vicaya,vinodkv,xinxianyin,yash360@gmail.com,yhemanth,ywskycn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/14 08:39;jorda;MAPREDUCE-1380-branch-1.2.patch;https://issues.apache.org/jira/secure/attachment/12630631/MAPREDUCE-1380-branch-1.2.patch","04/Feb/10 08:48;jorda;MAPREDUCE-1380_0.1.patch;https://issues.apache.org/jira/secure/attachment/12434796/MAPREDUCE-1380_0.1.patch","16/Feb/11 17:39;jorda;MAPREDUCE-1380_1.1.patch;https://issues.apache.org/jira/secure/attachment/12471198/MAPREDUCE-1380_1.1.patch","16/Feb/11 17:40;jorda;MAPREDUCE-1380_1.1.pdf;https://issues.apache.org/jira/secure/attachment/12471199/MAPREDUCE-1380_1.1.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2010-01-15 15:55:37.926,,,false,,,,,,,,,,,,,,,,,,72470,,,,,Tue Jun 30 04:28:27 UTC 2015,,,,,,,"0|i0jghz:",111603,,,,,,,,,,,"scheduler, scheduling",,,,,,,,,,"15/Jan/10 09:26;jorda;This is still a work in progress, but I'll be submitting a patch and more details about the implementation soon. In the meantime, feel free to share your thoughts and suggestions.

Thanks.","15/Jan/10 15:55;acmurthy;bq. Note that the current approach to estimate the completion time of jobs is quite simplistic: it is based on the time it takes to finish each task, so it works well with regular jobs

Polo - Can you please expand on your definition of 'regular' jobs? Are these, for e.g. part of regular workflows? IAC, how do you propose to communicate this information to the AdaptiveScheduler?","17/Jan/10 22:33;jorda;{quote}
Can you please expand on your definition of 'regular' jobs? Are these, for e.g. part of regular workflows? IAC, how do you propose to communicate this information to the AdaptiveScheduler?
{quote}

Actually, ""regular"" isn't really appropriate here, thanks for pointing that out.

I actually meant uniform or homogeneous jobs, that is, jobs in which all the tasks take approximately the same amount of time to finish. It would be interesting to communicate some additional data, but so far it only uses standard information as provided by tasktrackers.","04/Feb/10 08:48;jorda;I'm attaching a patch with an initial version of the scheduler.

As I said, this is still a work in progress and I'll be posting new versions as they are ready. There is still some work left to make it useful for everyone and not just for our own needs, but I wanted to contribute it now since it may be of interest to other people.

(I'll also be posting a PDF with additional documentation later today.)","04/Feb/10 15:39;steve_l;Not sure I'd put the VM request policy in the scheduler. Better to give it some way of notifying _something_ that there isn't enough resources, include data on user and data, and give that other thing the ability to add machines if it so chooses. There may be other concerns like per-user quota, overall costs, etc, as well as the security issue of giving your scheduler the credentials to work with the infrastructure. ","04/Feb/10 16:26;jorda;{quote}
Not sure I'd put the VM request policy in the scheduler. Better to give it some way of notifying something that there isn't enough resources, include data on user and data, and give that other thing the ability to add machines if it so chooses. There may be other concerns like per-user quota, overall costs, etc, as well as the security issue of giving your scheduler the credentials to work with the infrastructure.
{quote}

Good point. The current description doesn't explain much, but that's exactly what we have in mind: a multi-tiered system in which the Hadoop scheduler just provides information to the resource manager/provider.","16/Feb/11 17:38;jorda;I'm sending a new version of the Adaptive Scheduler.

This new version is actually a new implementation with a different architecture roughly described in the attached PDF document. It supports the same features as the previous version, but at the same time provides new features and a framework for future improvements.

The new features are mostly focused on making the scheduler more aware of the resources and allowing a dynamic number of running tasks depending on the jobs and their need for resources (instead of a fixed number of slots).

It is still a work in progress and requires some additional tuning, but I thought it would be interesting to publish it as it is now given some of the ideas that have been proposed for Hadoop MapReduce NextGen (MAPREDUCE-279). The scheduler currently leverages job profiling information to ensure optimal cluster utilization, but our goal is to get rid of this kind of profiles and implement a more dynamic approach (e.g. using resource information data introduced by MAPREDUCE-1218).

I still don't know what's the status of the ""NextGen"" proposal and its implementation. But as soon as more details about NextGen are revealed we'll see whether it makes sense and it is worth/useful to adapt or use some of the ideas in the new Hadoop MapReduce architecture.
","16/Feb/11 17:39;jorda;Patch against trunk.","17/Oct/11 18:27;bigbang4u2;Where I can download this adaptive scheduler. I am not able to find, please help me","25/Mar/13 12:04;qwertymaniac;Unsure why this was resolved as fixed. Its not been committed anywhere, so reopening as unresolved.","26/Mar/13 09:52;priyankajaiswal8;Where can I download the whole adaptive scheduler. The patch given is showing some error.","07/Feb/14 20:08;airbots;This patch may need to be updated against Hadoop 1.x or 2.x","24/Feb/14 08:39;jorda;Attaching a more up-to-date version of the scheduler that should apply cleanly against 1.2.x.","24/Jul/14 07:45;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12630631/MAPREDUCE-1380-branch-1.2.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4765//console

This message is automatically generated.","02/May/15 23:32;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12630631/MAPREDUCE-1380-branch-1.2.patch |
| Optional Tests | shellcheck javadoc javac unit findbugs checkstyle |
| git revision | branch-1 / 5f5138e |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5607/console |


This message was automatically generated.","08/May/15 23:09;aw;Cancelling the patch.  Work on branch-1 has effectively stopped.  Unless there is some interesting in porting this work to branch-2, we should close this as won't fix.

Thanks.","19/May/15 01:17;ericson;I am a beginner of hadoop，I  want to solve this problem, but I have some questions: 
1.What is the specific meaning of the adaptive scheduler and the differences between the adaptive scheduler and capacity scheduler. 
2.According to my understanding, the adaptive scheduler is located in the package mapreduce, why it is not in yarn package.
3.While I have the code of hadoop 2.4.1, how can I alter them to add adaptive scheduler using the patch files above.
Please forgive my poor english, Would you please give me a hand？","19/May/15 01:17;ericson;I am a beginner of hadoop，I  want to solve this problem, but I have some questions: 
1.What is the specific meaning of the adaptive scheduler and the differences between the adaptive scheduler and capacity scheduler. 
2.According to my understanding, the adaptive scheduler is located in the package mapreduce, why it is not in yarn package.
3.While I have the code of hadoop 2.4.1, how can I alter them to add adaptive scheduler using the patch files above.
Please forgive my poor english, Would you please give me a hand？","31/May/15 08:00;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","31/May/15 08:01;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","31/May/15 08:01;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","31/May/15 08:01;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","31/May/15 08:01;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","31/May/15 08:01;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","31/May/15 08:01;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","31/May/15 08:01;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","31/May/15 08:01;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","31/May/15 08:01;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","31/May/15 08:01;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","31/May/15 08:01;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","31/May/15 08:01;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","31/May/15 08:01;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","31/May/15 08:01;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","31/May/15 08:01;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","31/May/15 08:01;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","31/May/15 08:01;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","31/May/15 08:01;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","31/May/15 08:01;JackHoo;I download the patch v1.2,then apply it to the hadoop-1.2.1.but it do not work on the cluster,when JobClient sumbit the job,then the task is pending all the time and never running ,what can I do ? 
I hope you can help me ,my email : 460759720@qq.com","02/Jun/15 05:46;jorda;It's not located in the yarn package because it predates yarn. This patch worked with older versions of Hadoop, but it won't work against newer versions, at least not as it is.","02/Jun/15 05:51;ericson;Is there any codes correspond to the yarn? I want to alter this scheduler to the yarn package. Would you please give me some suggestions?","02/Jun/15 06:05;jorda;This scheduler hasn't been ported to yarn. I haven't looked into yarn internals at all, but I'm assuming porting it is feasible. No one is working on that at the moment as far as I know.","30/Jun/15 04:28;vinodkv;Moving features/enhancements out of previously closed releases into the next minor release 2.8.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Online aggregation and continuous query support,MAPREDUCE-1211,12440518,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Won't Fix,,tcondie,tcondie,12/Nov/09 17:54,21/Oct/16 07:44,12/Jan/21 09:52,29/Jul/14 20:08,,,,,,,,,task,,,,,,0,,,,,"The purpose of this post is to propose a modified MapReduce architecture that allows data to be pipelined between operators. This extends the MapReduce programming model beyond batch processing, and can reduce completion times and improve system utilization for batch jobs as well. We have built a modified version of the Hadoop MapReduce framework that supports online aggregation, which allows users to see ""early returns"" from a job as it is being computed. Our Hadoop Online Prototype (HOP) also supports continuous queries, which enable MapReduce programs to be written for applications such as event monitoring and stream processing. HOP retains the fault tolerance properties of Hadoop, and can run unmodified user-defined MapReduce programs.

For more information on the HOP design, please see our technical report.
http://www.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-136.html

Further details are discussed in the following blog posts.
http://databeta.wordpress.com/2009/10/18/mapreduce-online/
http://radar.oreilly.com/2009/10/pipelining-and-real-time-analytics-with-mapreduce-online.html
http://dbmsmusings.blogspot.com/2009/10/analysis-of-mapreduce-online-paper.html

The HOP code has been published at the following location.
http://code.google.com/p/hop/",,acmurthy,alexb,ashutoshc,cdouglas,coderplay,cutting,dms,ekohlwey,eli,hammer,hong.tang,jghoman,johanoskarsson,kimballa,lianhuiwang,marz,matei,neilconway,oae,otis,philip,ravidotg,rxin,sztanko,tlipcon,tomwhite,zhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-11-12 18:33:58.186,,,false,,,,,,,,,,,,,,,,,,149356,,,,,Fri Oct 21 07:44:59 UTC 2016,,,,,,,"0|i0jg1z:",111531,,,,,,,,,,,,,,,,,,,,,"12/Nov/09 18:33;neilconway;BTW, there are a few different ways to slice this work:

(1) Basic pipelining support for moving data between tasks
(2) Pipelining data between jobs (do the HDFS write in the background)
(3) Leveraging #1 and #2 to do ""online aggregation"" (approximate answers as the query runs)
(4) Leveraging #1 and #2 to do ""stream processing"" (MR jobs that run continuously)

These are in increasing order of complexity/invasiveness. If there's any interest in merging this work, we might begin by just merging #1, which is relatively low-impact. Intra-job pipelining offers two benefits: (1) potentially reduced response times (better cluster utilization), by overlapping map computation with network I/O and reducer-side merging (2) more effective straggler handling (rather than starting a speculative task from scratch, we can use pipelining to ""checkpoint"" the partial work done by the straggler, and not bother re-sending that portion of the map output over the network again).","27/Oct/10 03:20;otis;Out of curiosity, are there any plans to either:
* get HOP or HOP-like functionality into Hadoop
* get HOP to the latest version of Hadoop
Thanks!
","12/Jul/13 02:43;appodictic;Why not close this? Seems zombified.","21/Oct/16 07:44;rxin;This seems useful.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Task Attempt State API to MapReduce Application Master REST API,MAPREDUCE-6284,12783503,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Fixed,ryu_kobayashi,ryu_kobayashi,ryu_kobayashi,20/Mar/15 06:13,30/Aug/16 01:18,12/Jan/21 09:52,08/May/15 07:02,,,,,,2.8.0,3.0.0-alpha1,,,,,,,,0,BB2015-05-TBR,,,,"It want to 'task attempt state' on the 'App state' similarly REST API.
GET http://<proxy http address:port>/proxy/<application _id>/ws/v1/mapreduce/jobs/<job_id>/tasks/<task_id>/attempts/<attempt_id>/state
PUT http://<proxy http address:port>/proxy/<application _id>/ws/v1/mapreduce/jobs/<job_id>/tasks/<task_id>/attempts/<attempt_id>/state ",,hudson,ozawa,ryu_kobayashi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/15 18:06;ozawa;MAPREDUCE-6284.1.patch;https://issues.apache.org/jira/secure/attachment/12726213/MAPREDUCE-6284.1.patch","20/Mar/15 06:15;ryu_kobayashi;MAPREDUCE-6284.1.patch;https://issues.apache.org/jira/secure/attachment/12705844/MAPREDUCE-6284.1.patch","22/Apr/15 11:37;ryu_kobayashi;MAPREDUCE-6284.2.patch;https://issues.apache.org/jira/secure/attachment/12727206/MAPREDUCE-6284.2.patch","01/May/15 04:57;ozawa;MAPREDUCE-6284.3.patch;https://issues.apache.org/jira/secure/attachment/12729716/MAPREDUCE-6284.3.patch","30/Apr/15 06:13;ryu_kobayashi;MAPREDUCE-6284.3.patch;https://issues.apache.org/jira/secure/attachment/12729410/MAPREDUCE-6284.3.patch","07/May/15 05:40;ryu_kobayashi;MAPREDUCE-6284.4.patch;https://issues.apache.org/jira/secure/attachment/12731084/MAPREDUCE-6284.4.patch","07/May/15 07:38;ryu_kobayashi;MAPREDUCE-6284.5.patch;https://issues.apache.org/jira/secure/attachment/12731115/MAPREDUCE-6284.5.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,2015-03-20 07:55:40.601,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri May 08 17:20:18 UTC 2015,,,,,,,"0|i270vr:",9223372036854775807,,,,,,,,,,,,,2.8.0,,,,,,,,"20/Mar/15 07:55;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12705844/MAPREDUCE-6284.1.patch
  against trunk revision 978ef11.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5317//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5317//artifact/patchprocess/patchReleaseAuditProblems.txt
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5317//artifact/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-web-proxy.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5317//console

This message is automatically generated.","17/Apr/15 18:06;ozawa;Attaching a patch for submitting again.","17/Apr/15 18:50;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12726213/MAPREDUCE-6284.1.patch
  against trunk revision c6b5203.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 1150 javac compiler warnings (more than the trunk's current 204 warnings).

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 43 warning messages.
        See https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5410//artifact/patchprocess/diffJavadocWarnings.txt for details.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager:

                  org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5410//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5410//artifact/patchprocess/patchReleaseAuditProblems.txt
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5410//artifact/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5410//console

This message is automatically generated.","22/Apr/15 11:37;ryu_kobayashi;fixed conflict.","22/Apr/15 12:40;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  17m 23s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 3 new or modified test files. |
| {color:red}-1{color} | whitespace |   0m  0s | The patch has 4  line(s) that end in whitespace. |
| {color:green}+1{color} | javac |   7m 31s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 29s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | site |   2m 57s | Site still builds. |
| {color:red}-1{color} | checkstyle |   5m 25s | The applied patch generated  5  additional checkstyle issues. |
| {color:green}+1{color} | install |   1m 34s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 31s | The patch built with eclipse:eclipse. |
| {color:red}-1{color} | findbugs |   2m 45s | The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings. |
| {color:red}-1{color} | mapreduce tests |  10m 21s | Tests failed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   1m 46s | Tests passed in hadoop-mapreduce-client-core. |
| {color:green}+1{color} | yarn tests |   0m 20s | Tests passed in hadoop-yarn-server-web-proxy. |
| | |  60m 26s | |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-yarn-server-web-proxy |
|  |  Found reliance on default encoding in org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(HttpServletRequest, HttpServletResponse, URI, Cookie, String, WebAppProxyServlet$HTTP):in org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(HttpServletRequest, HttpServletResponse, URI, Cookie, String, WebAppProxyServlet$HTTP): new java.io.InputStreamReader(InputStream)  At WebAppProxyServlet.java:[line 192] |
| Failed unit tests | hadoop.mapreduce.v2.app.webapp.TestAMWebServicesTasks |
|   | hadoop.mapreduce.v2.app.webapp.TestAppController |
|   | hadoop.mapreduce.jobhistory.TestEvents |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12727206/MAPREDUCE-6284.2.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle site |
| git revision | trunk / b08908a |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5430/artifact/patchprocess/whitespace.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5430/artifact/patchprocess/checkstyle-result-diff.txt |
| Findbugs warnings | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5430/artifact/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-web-proxy.html |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5430/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5430/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| hadoop-yarn-server-web-proxy test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5430/artifact/patchprocess/testrun_hadoop-yarn-server-web-proxy.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5430/testReport/ |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5430//console |


This message was automatically generated.","28/Apr/15 05:55;ozawa;[~ryu_kobayashi] thank you for updating. The patch introduces breaks of indentation. Could you check them? You can see our code conventions here: https://wiki.apache.org/hadoop/CodeReviewChecklist

For example, following indentation should be 4 spaces instead of 8 spaces:
{code}
   protected void doGet(HttpServletRequest req, HttpServletResponse resp) 
-  throws IOException{
+        throws ServletException, IOException {
{code}

Also, we should fix the warning by findbugs. It's caused since Charset is not given to create InputStreamReader. Please pass Charset.forName(""UTF-8"") like this:
{code}
      BufferedReader reader =
          new BufferedReader(new InputStreamReader(
              req.getInputStream(), Charset.forName(""UTF-8"")));
{code}

I'll take a look at the patch more deeper after addressing these comments.","28/Apr/15 07:03;ozawa;[~ryu_kobayashi] Checked your patch. Looks good to me overall. Please fix following minor nits:

1. Please use HttpServletResponse.SC_METHOD_NOT_ALLOWED instead of 405.
{code}
+      resp.setStatus(405);
{code}

2. We can use Precondition.checkArgument instead of checking null directly.

{code}
+    if (ta == null) {
+      throw new IllegalArgumentException(""ta cannot be null"");
+    }
{code}
","30/Apr/15 06:13;ryu_kobayashi;[~ozawa] I was fixed the things that you pointed out.","30/Apr/15 07:27;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  17m 52s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 3 new or modified test files. |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | javac |   7m 40s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 42s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | site |   2m 57s | Site still builds. |
| {color:red}-1{color} | checkstyle |   4m  0s | The applied patch generated  5  additional checkstyle issues. |
| {color:green}+1{color} | install |   1m 34s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   2m 46s | The patch does not introduce any new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | mapreduce tests |   9m 19s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   1m 37s | Tests passed in hadoop-mapreduce-client-core. |
| {color:green}+1{color} | yarn tests |   0m 20s | Tests passed in hadoop-yarn-server-web-proxy. |
| | |  58m 44s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12729410/MAPREDUCE-6284.3.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle site |
| git revision | trunk / aa22450 |
| checkstyle | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5476/artifact/patchprocess/checkstyle-result-diff.txt |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5476/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5476/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| hadoop-yarn-server-web-proxy test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5476/artifact/patchprocess/testrun_hadoop-yarn-server-web-proxy.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5476/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf903.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5476/console |


This message was automatically generated.","01/May/15 04:57;ozawa;Resubmitting a patch for the change of checkstyle.","01/May/15 08:16;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  18m 10s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 3 new or modified test files. |
| {color:green}+1{color} | javac |   7m 45s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 58s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | site |   2m 58s | Site still builds. |
| {color:red}-1{color} | checkstyle |   1m 39s | The applied patch generated  15 new checkstyle issues (total was 89, now 101). |
| {color:red}-1{color} | whitespace |   0m 15s | The patch has 1  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 34s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   2m 45s | The patch does not introduce any new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | mapreduce tests |   8m 55s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   1m 34s | Tests passed in hadoop-mapreduce-client-core. |
| {color:green}+1{color} | yarn tests |   0m 21s | Tests passed in hadoop-yarn-server-web-proxy. |
| | |  56m 51s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12729716/MAPREDUCE-6284.3.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle site |
| git revision | trunk / 1b3b9e5 |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5481/artifact/patchprocess/diffcheckstylehadoop-yarn-server-web-proxy.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5481/artifact/patchprocess/whitespace.txt |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5481/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5481/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| hadoop-yarn-server-web-proxy test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5481/artifact/patchprocess/testrun_hadoop-yarn-server-web-proxy.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5481/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf904.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5481/console |


This message was automatically generated.","07/May/15 06:46;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  17m 36s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 3 new or modified test files. |
| {color:green}+1{color} | javac |   7m 27s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 34s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | site |   2m 58s | Site still builds. |
| {color:red}-1{color} | checkstyle |   1m  8s | The applied patch generated  9 new checkstyle issues (total was 36, now 45). |
| {color:red}-1{color} | checkstyle |   1m 42s | The applied patch generated  7 new checkstyle issues (total was 14, now 21). |
| {color:red}-1{color} | whitespace |   0m  5s | The patch has 1  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 34s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   2m 42s | The patch does not introduce any new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | mapreduce tests |   9m 31s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   1m 33s | Tests passed in hadoop-mapreduce-client-core. |
| {color:green}+1{color} | yarn tests |   0m 21s | Tests passed in hadoop-yarn-server-web-proxy. |
| | |  56m  2s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12731084/MAPREDUCE-6284.4.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle site |
| git revision | trunk / 918af8e |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5657/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-app.txt https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5657/artifact/patchprocess/diffcheckstylehadoop-yarn-server-web-proxy.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5657/artifact/patchprocess/whitespace.txt |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5657/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5657/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| hadoop-yarn-server-web-proxy test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5657/artifact/patchprocess/testrun_hadoop-yarn-server-web-proxy.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5657/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf901.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5657/console |


This message was automatically generated.","07/May/15 08:37;hadoopqa;\\
\\
| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  17m 36s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 3 new or modified test files. |
| {color:green}+1{color} | javac |   7m 42s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 55s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | site |   2m 59s | Site still builds. |
| {color:green}+1{color} | checkstyle |   1m 25s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  5s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 35s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   2m 47s | The patch does not introduce any new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | mapreduce tests |   9m 58s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   1m 43s | Tests passed in hadoop-mapreduce-client-core. |
| {color:green}+1{color} | yarn tests |   0m 22s | Tests passed in hadoop-yarn-server-web-proxy. |
| | |  57m  6s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12731115/MAPREDUCE-6284.5.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle site |
| git revision | trunk / 449e442 |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5658/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5658/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| hadoop-yarn-server-web-proxy test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5658/artifact/patchprocess/testrun_hadoop-yarn-server-web-proxy.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5658/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf903.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5658/console |


This message was automatically generated.","07/May/15 08:48;ryu_kobayashi;[~ozawa] I fixed the checkstyle. and any more...","07/May/15 08:58;ozawa;OK, I'm checking...","08/May/15 06:51;ozawa;+1, checking this in.

Note that this feature is alpha lik [YARN's Application State Change API|http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html#Cluster_Application_State_API] as described in the documentation. 

","08/May/15 07:02;ozawa;Committed this to trunk and branch-2. Thanks [~ryu_kobayashi] for your contribution!","08/May/15 08:01;hudson;FAILURE: Integrated in Hadoop-trunk-Commit #7771 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7771/])
MAPREDUCE-6284. Add Task Attempt State API to MapReduce Application Master REST API. Contributed by Ryu Kobayashi. (ozawa: rev d18f10ad1b3e497fa1aaaeb85ba055f87d9849f7)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/client/MRClientService.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesAttempt.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/markdown/MapredAppMasterRest.md
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MockAppContext.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/JAXBContextResolver.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/dao/JobTaskAttemptState.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MockEventHandler.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AMWebServices.java
* hadoop-mapreduce-project/CHANGES.txt
","08/May/15 13:16;hudson;SUCCESS: Integrated in Hadoop-Yarn-trunk #921 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/921/])
MAPREDUCE-6284. Add Task Attempt State API to MapReduce Application Master REST API. Contributed by Ryu Kobayashi. (ozawa: rev d18f10ad1b3e497fa1aaaeb85ba055f87d9849f7)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesAttempt.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MockAppContext.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/dao/JobTaskAttemptState.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AMWebServices.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MockEventHandler.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/client/MRClientService.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/markdown/MapredAppMasterRest.md
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/JAXBContextResolver.java
","08/May/15 13:21;hudson;SUCCESS: Integrated in Hadoop-Yarn-trunk-Java8 #190 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/190/])
MAPREDUCE-6284. Add Task Attempt State API to MapReduce Application Master REST API. Contributed by Ryu Kobayashi. (ozawa: rev d18f10ad1b3e497fa1aaaeb85ba055f87d9849f7)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MockAppContext.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/client/MRClientService.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AMWebServices.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MockEventHandler.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/markdown/MapredAppMasterRest.md
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/dao/JobTaskAttemptState.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesAttempt.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/JAXBContextResolver.java
","08/May/15 15:17;hudson;FAILURE: Integrated in Hadoop-Hdfs-trunk #2119 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2119/])
MAPREDUCE-6284. Add Task Attempt State API to MapReduce Application Master REST API. Contributed by Ryu Kobayashi. (ozawa: rev d18f10ad1b3e497fa1aaaeb85ba055f87d9849f7)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MockAppContext.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AMWebServices.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MockEventHandler.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/markdown/MapredAppMasterRest.md
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesAttempt.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/client/MRClientService.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/JAXBContextResolver.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/dao/JobTaskAttemptState.java
* hadoop-mapreduce-project/CHANGES.txt
","08/May/15 16:08;hudson;FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #179 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/179/])
MAPREDUCE-6284. Add Task Attempt State API to MapReduce Application Master REST API. Contributed by Ryu Kobayashi. (ozawa: rev d18f10ad1b3e497fa1aaaeb85ba055f87d9849f7)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/dao/JobTaskAttemptState.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/client/MRClientService.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/markdown/MapredAppMasterRest.md
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MockAppContext.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MockEventHandler.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AMWebServices.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/JAXBContextResolver.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesAttempt.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java
","08/May/15 17:03;hudson;FAILURE: Integrated in Hadoop-Mapreduce-trunk #2137 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2137/])
MAPREDUCE-6284. Add Task Attempt State API to MapReduce Application Master REST API. Contributed by Ryu Kobayashi. (ozawa: rev d18f10ad1b3e497fa1aaaeb85ba055f87d9849f7)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MockAppContext.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/dao/JobTaskAttemptState.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesAttempt.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/markdown/MapredAppMasterRest.md
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/JAXBContextResolver.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/client/MRClientService.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MockEventHandler.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AMWebServices.java
","08/May/15 17:20;hudson;SUCCESS: Integrated in Hadoop-Mapreduce-trunk-Java8 #189 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/189/])
MAPREDUCE-6284. Add Task Attempt State API to MapReduce Application Master REST API. Contributed by Ryu Kobayashi. (ozawa: rev d18f10ad1b3e497fa1aaaeb85ba055f87d9849f7)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesAttempt.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/markdown/MapredAppMasterRest.md
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/dao/JobTaskAttemptState.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MockEventHandler.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MockAppContext.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/JAXBContextResolver.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AMWebServices.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/client/MRClientService.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add a ""Kill"" link to Task Attempts page",MAPREDUCE-6364,12829102,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Fixed,ryu_kobayashi,ryu_kobayashi,ryu_kobayashi,12/May/15 06:56,30/Aug/16 01:17,12/Jan/21 09:52,26/May/15 15:07,,,,,,2.8.0,3.0.0-alpha1,,applicationmaster,,,,,,0,,,,,"Add a ""Kill"" link to Task Attempts page, calling REST API by pushing the link.",,devaraj,hudson,ozawa,qwertymaniac,ryu_kobayashi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-6405,,,,,,,,"12/May/15 06:57;ryu_kobayashi;MAPREDUCE-6364-screenshot.png;https://issues.apache.org/jira/secure/attachment/12732161/MAPREDUCE-6364-screenshot.png","12/May/15 06:59;ryu_kobayashi;MAPREDUCE-6364.1.patch;https://issues.apache.org/jira/secure/attachment/12732163/MAPREDUCE-6364.1.patch","12/May/15 10:23;ryu_kobayashi;MAPREDUCE-6364.2.patch;https://issues.apache.org/jira/secure/attachment/12732208/MAPREDUCE-6364.2.patch","13/May/15 08:58;ryu_kobayashi;MAPREDUCE-6364.3.patch;https://issues.apache.org/jira/secure/attachment/12732513/MAPREDUCE-6364.3.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2015-05-12 07:49:58.737,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed May 27 15:28:49 UTC 2015,,,,,,,"0|i2eltb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"12/May/15 07:49;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  15m  0s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:red}-1{color} | tests included |   0m  0s | The patch doesn't appear to include any new or modified tests.  Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. |
| {color:green}+1{color} | javac |   7m 43s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 50s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   1m 13s | The applied patch generated  4 new checkstyle issues (total was 59, now 63). |
| {color:red}-1{color} | whitespace |   0m  0s | The patch has 1  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 36s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   2m 13s | The patch does not introduce any new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | mapreduce tests |   9m 17s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   1m 39s | Tests passed in hadoop-mapreduce-client-core. |
| | |  49m 31s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12732163/MAPREDUCE-6364.1.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 987abc9 |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5710/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-core.txt |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5710/artifact/patchprocess/whitespace.txt |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5710/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5710/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5710/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf903.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5710/console |


This message was automatically generated.","12/May/15 08:08;ryu_kobayashi;Fixed checkstyle and whitespace.","12/May/15 09:01;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  15m  6s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:red}-1{color} | tests included |   0m  0s | The patch doesn't appear to include any new or modified tests.  Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. |
| {color:green}+1{color} | javac |   7m 49s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |  10m  0s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 21s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   1m 12s | The applied patch generated  2 new checkstyle issues (total was 58, now 60). |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 36s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   2m 15s | The patch does not introduce any new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | mapreduce tests |  10m  1s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   1m 37s | Tests passed in hadoop-mapreduce-client-core. |
| | |  50m 32s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12732188/MAPREDUCE-6364.2.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 360dff5 |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5711/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-core.txt |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5711/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5711/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5711/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf904.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5711/console |


This message was automatically generated.","12/May/15 11:50;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 57s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:red}-1{color} | tests included |   0m  0s | The patch doesn't appear to include any new or modified tests.  Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. |
| {color:green}+1{color} | javac |   7m 44s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 46s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   1m 10s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  1s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 35s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   2m  9s | The patch does not introduce any new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | mapreduce tests |   9m 30s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   1m 38s | Tests passed in hadoop-mapreduce-client-core. |
| | |  49m 29s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12732208/MAPREDUCE-6364.2.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 360dff5 |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5713/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5713/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5713/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf904.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5713/console |


This message was automatically generated.","13/May/15 08:39;ozawa;Let me take a look.","13/May/15 08:47;ozawa;[~ryu_kobayashi], thank you for taking this issue. Please correct me if I'm wrong, but maybe you forgot replacing the last %s with attemptId?
{code}
+        stateURL =
+            String.format(""/proxy/%s/ws/v1/mapreduce/jobs/%s/tasks/%s/""
+                + ""attempts"", appID, jobID, taskID) + ""/%s/state"";
{code}

IMHO, enableUIActions is preferred.
{code}
+      this.isUIActions =
+          conf.getBoolean(MRConfig.MASTER_WEBAPP_UI_ACTIONS_ENABLED,
+              MRConfig.DEFAULT_MASTER_WEBAPP_UI_ACTIONS_ENABLED);
{code}","13/May/15 09:05;ryu_kobayashi;[~ozawa] Thanks for review. {{isUIActions}} was changed to {{enableUIActions}}. Last %s, it has been used in the following:

{code}
.append(String.format(stateURL, ta.getId()))
{code}

Because the variable name is easy to mistake it was changed from {{stateURL}} to {{stateURLFormat}}.","13/May/15 10:21;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 54s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:red}-1{color} | tests included |   0m  0s | The patch doesn't appear to include any new or modified tests.  Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. |
| {color:green}+1{color} | javac |   7m 38s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 47s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   1m 13s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  1s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 36s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 34s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   2m 12s | The patch does not introduce any new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | mapreduce tests |   9m 59s | Tests passed in hadoop-mapreduce-client-app. |
| {color:green}+1{color} | mapreduce tests |   1m 37s | Tests passed in hadoop-mapreduce-client-core. |
| | |  49m 59s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12732513/MAPREDUCE-6364.3.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 92c38e4 |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5725/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-core test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5725/artifact/patchprocess/testrun_hadoop-mapreduce-client-core.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5725/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf904.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5725/console |


This message was automatically generated.","26/May/15 14:55;ozawa;+1, checking this in.","26/May/15 15:07;ozawa;Committed this to trunk and branch-2. Thanks [~ryu_kobayashi] for your contribution.","26/May/15 15:13;hudson;FAILURE: Integrated in Hadoop-trunk-Commit #7902 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7902/])
MAPREDUCE-6364. Add a Kill link to Task Attempts page. Contributed by Ryu Kobayashi. (ozawa: rev 022f49d59e404678d0f322bb1b67208f3c17b7ef)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRConfig.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AttemptsPage.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/TaskPage.java
","27/May/15 00:14;hudson;SUCCESS: Integrated in Hadoop-Yarn-trunk-Java8 #209 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/209/])
MAPREDUCE-6364. Add a Kill link to Task Attempts page. Contributed by Ryu Kobayashi. (ozawa: rev 022f49d59e404678d0f322bb1b67208f3c17b7ef)
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/TaskPage.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AttemptsPage.java
","27/May/15 11:40;hudson;FAILURE: Integrated in Hadoop-Yarn-trunk #940 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/940/])
MAPREDUCE-6364. Add a Kill link to Task Attempts page. Contributed by Ryu Kobayashi. (ozawa: rev 022f49d59e404678d0f322bb1b67208f3c17b7ef)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/TaskPage.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AttemptsPage.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRConfig.java
","27/May/15 13:21;hudson;FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #208 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/208/])
MAPREDUCE-6364. Add a Kill link to Task Attempts page. Contributed by Ryu Kobayashi. (ozawa: rev 022f49d59e404678d0f322bb1b67208f3c17b7ef)
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRConfig.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/TaskPage.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AttemptsPage.java
","27/May/15 14:25;hudson;FAILURE: Integrated in Hadoop-Hdfs-trunk #2138 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2138/])
MAPREDUCE-6364. Add a Kill link to Task Attempts page. Contributed by Ryu Kobayashi. (ozawa: rev 022f49d59e404678d0f322bb1b67208f3c17b7ef)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AttemptsPage.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/TaskPage.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRConfig.java
","27/May/15 14:35;hudson;FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #198 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/198/])
MAPREDUCE-6364. Add a Kill link to Task Attempts page. Contributed by Ryu Kobayashi. (ozawa: rev 022f49d59e404678d0f322bb1b67208f3c17b7ef)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRConfig.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AttemptsPage.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/TaskPage.java
","27/May/15 15:28;hudson;FAILURE: Integrated in Hadoop-Mapreduce-trunk #2156 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2156/])
MAPREDUCE-6364. Add a Kill link to Task Attempts page. Contributed by Ryu Kobayashi. (ozawa: rev 022f49d59e404678d0f322bb1b67208f3c17b7ef)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/TaskPage.java
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AttemptsPage.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRConfig.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow specifing java home via job configuration,MAPREDUCE-6687,12962882,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Implemented,,He Tianyi,He Tianyi,27/Apr/16 05:03,21/Jul/16 01:36,12/Jan/21 09:52,21/Jul/16 01:36,,,,,,,,,applicationmaster,,,,,,0,,,,,"Suggest allowing user to use a preferred JVM implementation (or version) by specifying java home via JobConf, to launch Map/Reduce tasks. 

Especially useful for running A/B tests on real workload or benchmark between JVM implementations.",,He Tianyi,jlowe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-27 12:58:50.012,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 27 12:58:50 UTC 2016,,,,,,,"0|i2wst3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,"27/Apr/16 12:58;jlowe;This can already be accomplished by setting the appropriate value for JAVA_HOME in mapreduce.map.env and mapreduce.reduce.env or mapred.child.env for the tasks and yarn.app.mapreduce.am.env for the ApplicationMaster.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change mapreduce.jobhistory.jhist.format default from json to binary,MAPREDUCE-6613,12933226,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Fixed,rchiang,rchiang,rchiang,22/Jan/16 00:43,12/May/16 18:22,12/Jan/21 09:52,20/Feb/16 01:16,2.8.0,,,,,3.0.0-alpha1,,,,,,,,,0,,,,,"MAPREDUCE-6376 added a configuration setting to set up .jhist internal format:

mapreduce.jobhistory.jhist.format

Currently, the default is ""json"".  Changing the default to ""binary"" allows faster parsing, but with the downside of making the file not output friendly by using ""hadoop fs cat"".",,aw,daemon,hudson,jlowe,Naganarasimha,rkanter,zhz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/16 00:46;rchiang;MAPREDUCE-6613.001.patch;https://issues.apache.org/jira/secure/attachment/12783721/MAPREDUCE-6613.001.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2016-01-22 00:46:29.422,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,Incompatible change,Reviewed,,,Mon Feb 22 21:22:24 UTC 2016,,,,,,,"0|i2rtpb:",9223372036854775807,"Default of 'mapreduce.jobhistory.jhist.format' property changed from 'json' to 'binary'.  Creates smaller, binary Avro .jhist files for faster JHS performance.",,,,,,,,,,,,,,,,,,,,"22/Jan/16 00:46;aw;What does ""binary"" actually mean?  If it's protobuf, then it should really be ""protobuf"".  If it's avro, then it should really be ""avro"".","22/Jan/16 00:46;rchiang;Initial version.","22/Jan/16 00:55;rchiang;It's always Avro.  Your choices are Avro in json/text format or Avro in binary format.","22/Jan/16 00:57;aw;Ugh. That's even worse.  Our inability to actually say what things are is really terrible. :(","23/Jan/16 08:07;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 0s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red} 0m 0s {color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 48s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 10m 44s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 44s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 28s {color} | {color:green} trunk passed with JDK v1.7.0_91 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 27s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 19s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 37s {color} | {color:green} trunk passed {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red} 1m 37s {color} | {color:red} hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core in trunk has 2 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 12s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 12s {color} | {color:green} trunk passed with JDK v1.7.0_91 {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 23s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 1m 4s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 42s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 2m 42s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 28s {color} | {color:green} the patch passed with JDK v1.7.0_91 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 2m 28s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 26s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 14s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 29s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green} 0m 1s {color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 3m 17s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 9s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 7s {color} | {color:green} the patch passed with JDK v1.7.0_91 {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 3m 1s {color} | {color:green} hadoop-mapreduce-client-core in the patch passed with JDK v1.8.0_66. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 1m 3s {color} | {color:green} hadoop-mapreduce-client-common in the patch passed with JDK v1.8.0_66. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 2m 56s {color} | {color:green} hadoop-mapreduce-client-core in the patch passed with JDK v1.7.0_91. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 1m 1s {color} | {color:green} hadoop-mapreduce-client-common in the patch passed with JDK v1.7.0_91. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 24s {color} | {color:green} Patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 48m 55s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:0ca8df7 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12783721/MAPREDUCE-6613.001.patch |
| JIRA Issue | MAPREDUCE-6613 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux e4591860ed53 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 99829eb |
| Default Java | 1.7.0_91 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_66 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_91 |
| findbugs | v3.0.0 |
| findbugs | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6278/artifact/patchprocess/branch-findbugs-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core-warnings.html |
| JDK v1.7.0_91  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6278/testReport/ |
| modules | C:  hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core   hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common  U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Max memory used | 76MB |
| Powered by | Apache Yetus 0.2.0-SNAPSHOT   http://yetus.apache.org |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6278/console |


This message was automatically generated.

","26/Jan/16 20:47;rkanter;This seems like a good default to change:
- It has a ~2x speedup (see MAPREDUCE-6376)
- The parsing code already can handle either format
- I don't think we should worry about users catting the jhist file directly; that's not an official stable ""API"".  If someone really wants to do that, they could set the config back to ""json"".  Otherwise, defaulting to ""binary"" should help the most number of users.

[~jlowe], what do you think?","26/Jan/16 20:52;aw;bq. I don't think we should worry about users catting the jhist file directly

Umm, that's probably the #1 way people are doing post processing.","26/Jan/16 20:59;rchiang;[~aw], I had specifically put a target version of 3.0, since this is ""under the covers"" not backwards compatible.  Even in that case, would you consider that unacceptable?","26/Jan/16 21:00;aw;Breaking it in 3.0 (with a release note, of course) is exactly the right thing to do.","26/Jan/16 21:48;rkanter;That seems reasonable.  [~rchiang], can you write something in the release notes box?","26/Jan/16 21:53;rchiang;Done.","26/Jan/16 21:57;jlowe;I have no issues with this going into 3.0.  I agree with Allen that there are use cases today where people have built pipelines that consume the jhist files, so therefore it's risky to change the default in anything before 3.x.
","19/Feb/16 01:06;rkanter;+1 for 3.0.
Will commit tomorrow if nobody objects.","20/Feb/16 01:16;rkanter;Thanks everyone.  Committed to trunk!","20/Feb/16 01:24;hudson;FAILURE: Integrated in Hadoop-trunk-Commit #9333 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/9333/])
MAPREDUCE-6613. Change mapreduce.jobhistory.jhist.format default from (rkanter: rev 6eae4337d1929077ffa74734327775fb987ba910)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* hadoop-mapreduce-project/CHANGES.txt
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/jobhistory/JHAdminConfig.java
","22/Feb/16 21:22;rchiang;Thanks for the feedback everyone.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow external sorter plugin for MR,MAPREDUCE-2454,12505228,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Fixed,masokan,masokan,masokan,26/Apr/11 18:46,12/May/16 18:22,12/Jan/21 09:52,15/Dec/12 20:49,2.0.0-alpha,2.0.2-alpha,3.0.0-alpha1,,,2.0.3-alpha,,,,,,,,,0,features,performance,plugin,sort,Define interfaces and some abstract classes in the Hadoop framework to facilitate external sorter plugins both on the Map and Reduce sides.,,aah,acmurthy,ahmed.radwan,anty,ashutoshc,atm,avnerb,bharatjha,bikassaha,cdouglas,chaku88,cutting,ddas,decster,devaraj,dhruba,dzhang,eli,fengshen,gemini5201314,gortsleigh,hammer,jerrychenhf,jimhuang,jira.shegalov,junping_du,kasha,kkambatl,lakshman,lianhuiwang,mahadev,masokan,mayank_bansal,nemon,oae,omalley,ozawa,qwertymaniac,rajesh.balamohan,raviprak,sandyr,sms,srivas,sseth,sureshms,tgraves,tlipcon,tomwhite,tucu00,ujjwal.wadhawan,varun_saxena,vicaya,zzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-4049,MAPREDUCE-4039,MAPREDUCE-4891,,,,,,,,,,,,,,,,,,,"30/Aug/12 21:46;masokan;HadoopSortPlugin.pdf;https://issues.apache.org/jira/secure/attachment/12543168/HadoopSortPlugin.pdf","23/Jun/11 15:36;masokan;HadoopSortPlugin.pdf;https://issues.apache.org/jira/secure/attachment/12483610/HadoopSortPlugin.pdf","28/Apr/11 23:14;masokan;KeyValueIterator.java;https://issues.apache.org/jira/secure/attachment/12477711/KeyValueIterator.java","25/May/11 15:22;masokan;MR-2454-trunkPatchPreview.gz;https://issues.apache.org/jira/secure/attachment/12480420/MR-2454-trunkPatchPreview.gz","27/Apr/11 15:47;masokan;MapOutputSorter.java;https://issues.apache.org/jira/secure/attachment/12477557/MapOutputSorter.java","28/Apr/11 23:07;masokan;MapOutputSorterAbstract.java;https://issues.apache.org/jira/secure/attachment/12477710/MapOutputSorterAbstract.java","27/Apr/11 15:47;masokan;ReduceInputSorter.java;https://issues.apache.org/jira/secure/attachment/12477558/ReduceInputSorter.java","16/Nov/12 03:17;masokan;mapreduce-2454-modified-code.patch;https://issues.apache.org/jira/secure/attachment/12553727/mapreduce-2454-modified-code.patch","16/Nov/12 03:17;masokan;mapreduce-2454-modified-test.patch;https://issues.apache.org/jira/secure/attachment/12553728/mapreduce-2454-modified-test.patch","16/Nov/12 03:17;masokan;mapreduce-2454-new-test.patch;https://issues.apache.org/jira/secure/attachment/12553729/mapreduce-2454-new-test.patch","16/Nov/12 03:17;masokan;mapreduce-2454-protection-change.patch;https://issues.apache.org/jira/secure/attachment/12553730/mapreduce-2454-protection-change.patch","16/Nov/12 16:39;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12553789/mapreduce-2454.patch","16/Nov/12 03:18;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12553731/mapreduce-2454.patch","12/Nov/12 22:00;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12553200/mapreduce-2454.patch","12/Nov/12 18:44;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12553151/mapreduce-2454.patch","10/Nov/12 17:50;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12552985/mapreduce-2454.patch","09/Nov/12 19:06;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12552875/mapreduce-2454.patch","08/Nov/12 18:54;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12552689/mapreduce-2454.patch","07/Nov/12 19:59;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12552530/mapreduce-2454.patch","07/Nov/12 17:31;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12552499/mapreduce-2454.patch","13/Oct/12 23:50;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12549046/mapreduce-2454.patch","17/Sep/12 13:34;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12545413/mapreduce-2454.patch","05/Sep/12 17:44;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12543878/mapreduce-2454.patch","02/Sep/12 02:50;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12543461/mapreduce-2454.patch","01/Sep/12 23:22;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12543452/mapreduce-2454.patch","31/Aug/12 15:31;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12543287/mapreduce-2454.patch","31/Aug/12 01:54;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12543218/mapreduce-2454.patch","30/Aug/12 22:10;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12543174/mapreduce-2454.patch","28/Aug/12 22:02;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12542841/mapreduce-2454.patch","27/Aug/12 18:26;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12542643/mapreduce-2454.patch","31/Jul/12 18:01;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12538585/mapreduce-2454.patch","24/Jul/12 17:04;masokan;mapreduce-2454.patch;https://issues.apache.org/jira/secure/attachment/12537708/mapreduce-2454.patch","16/Aug/11 20:24;masokan;mr-2454-on-mr-279-build82.patch.gz;https://issues.apache.org/jira/secure/attachment/12490568/mr-2454-on-mr-279-build82.patch.gz",,33.0,,,,,,,,,,,,,,,,,,,,2011-04-26 20:42:40.386,,,false,,,,,,,,,,,,,,,,,,64875,,,,,Tue Sep 01 14:12:11 UTC 2015,,,,,,,"0|i00rov:",2418,MAPREDUCE-4807 Allow external implementations of the sort phase in a Map task,,,,,,,,,,,,,,,,,,,,"26/Apr/11 19:09;masokan;Sorry about that Chris.  Thanks for moving.

-- Asokan

On 04/26/2011 03:01 PM, Chris Douglas (JIRA) wrote:


     [ https://issues.apache.org/jira/browse/MAPREDUCE-2454?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Douglas moved HADOOP-7242 to MAPREDUCE-2454:
--------------------------------------------------

    Affects Version/s:     (was: 0.21.0)
                  Key: MAPREDUCE-2454  (was: HADOOP-7242)
              Project: Hadoop Map/Reduce  (was: Hadoop Common)



Allow external sorter plugin for MR
-----------------------------------

                Key: MAPREDUCE-2454
                URL: https://issues.apache.org/jira/browse/MAPREDUCE-2454
            Project: Hadoop Map/Reduce
         Issue Type: New Feature
           Reporter: Mariappan Asokan
           Priority: Minor

Define interfaces and some abstract classes in the Hadoop framework to facilitate external sorter plugins both on the Map and Reduce sides.



--
This message is automatically generated by JIRA.
For more information on JIRA, see: http://www.atlassian.com/software/jira


","26/Apr/11 20:42;omalley;You should also look at the work in MAPREDUCE-279. Once the MapReduce library is user code there are a lot more options available.","27/Apr/11 15:48;masokan;Hi Owen,
  Thank you very much for your suggestion.  Originally, I was experimenting with
my code on a Cloudera distribution which is based on Apache Hadoop 0.20.2.  I
added most of my code to the mapred package.  We did some extensive testing with
an external sorter plugin and found the results very encouraging.

It is really exciting to see where Hadoop is heading for the long term.  The
contribution we are making will be useful even when all the Task related classes
are visible as public and will live out of the core packages.

I am giving more details on the proposal below.  Please feel free to comment on.

The idea is to bypass the framework's sorting on both the Map and Reduce sides.
On the Map side, it is very easy.  Just define a public interface extending the
MapOutputCollector.  Please see the attached file MapOutputSorter.java.

An abstract class called MapOutputSorterAbstract(implementing MapOutputSorter)
will be provided which acts like a conduit to invoke methods in package
protected classes in the mapred package.  I guess once Hadoop evolves and pulls
out Task related classes from the core package, this abstract class may be
unnecessary but is harmless.  The abstract class exposes methods to send
progress message, to get a Counter object, to run the Combiner, to get a Map
output file to write to, and to get a Map index file to write to.  These methods
are very thin in the sense that they use simple delegation.

On the Reduce side, defining an external sorter interface is a bit tricky.
Please refer to the attached file ReduceInputSorter.java for details.

Again there will be an abstract base class ReduceInputSorterAbstract
(implementing ReduceInputSorter) which can be extended by users to implement
the external sorter in the reduce phase. This abstract class provides methods to
send progress message, to get Counter objects, and to update shuffle client
metrics.  I had to modify MapTask.java, ReduceTask.java, Shuffle.java,
Fetcher.java, and MapOutput.java to accommodate the external sorter.

If I can work with an Apache committer, I will be more than happy to discuss the
details of all code changes.  When I moved my code from Cloudera distribution to
Apache 0.21.0, I noticed some code refactoring that went in ReduceTask.java(for
good.)  I am still merging the changes and it may take a week or two to test it
in-house before the code can be tested formally and submitted for review.  As
far as packaging is concerned, I will try to define most of the classes in
mapreduce package rather than mapred package(as I did in the Cloudera
distribution.)

I appreciate an early feedback on this from everyone.

Thank you very much.

-- Asokan
","28/Apr/11 15:06;stevel@apache.org;# All new features should be on SVN-trunk, and with the plan to put MAPREDUCE-279 in there, I would strongly encourage you to get involved with the '279 project and make sure your needs are met there.

# It won't be enough to provide an interface, with one (private) implementation behind it. Nobody is going to support that. What would be better would be to have at least one implementation of the plugin in the main codebase -ideally it would be how the normal sorter would work- along with the tests to verify that everything works.

# The interfaces may want to extend {{Closeable}}, so they can be closed at the end of their work.","28/Apr/11 15:44;omalley;I suspect the right interfaces are a subset of the current mapreduce.RecordReader and RecordWriter. In particular, they would look like:

{code}
public abstract class RecordWriter {
  public abstract void write(Object key, Object value
                             ) throws IOException, InterruptedException;
  public abstract void close() throws IOException, InterruptedException;
}

public abstract class RecordReader {
  public abstract 
  boolean nextKeyValue() throws IOException, InterruptedException;

  public abstract
  Object getCurrentKey() throws IOException, InterruptedException;
  
  public abstract 
  Object getCurrentValue() throws IOException, InterruptedException;

  public abstract void close() throws IOException, InterruptedException;
}
{code}

Making the current shuffle code implement these classes would take work, but be doable.

I'll also take the chance to suggest Java's ServiceLoader library as the right way to configure the plugin.
","28/Apr/11 15:59;masokan;Hi Steve,
  Thanks for your comments.  I will definitely take a look at the MAPREDUCE-279 branch.  I will be providing a public implementation of the interface.  It will use GNU sort command as the external sorter.  Sure, I will extend my interfaces from Closeable.

-- Asokan
","28/Apr/11 23:11;masokan;Hi Owen,
  Thank you for your comments.  Here are my thoughts.

Map
---
On the Map side, the external sorter would also need the partition number, notjust the key and value.  I am not sure how RecordWriter can be used.  In the current MapTask, the sorting starts in MapOutputBuffer which implements
MapOutputCollector.  I thought it is natural for an external sorter to extendMapOutputCollector interface as well.  Perhaps, following Steve's suggestion we can rewrite MapOutputCollector as:

public interface MapOutputCollector<K, V> extends Closeable {
  public void collect(K key, V value, int partition)
    throws IOException, InterruptedException;
  public void flush() throws IOException, InterruptedException;
}

At present, ""extends Closeable"" is missing.

A digression
------------
If you treat the framework's sorter as a black box, it accepts a set of key and value pairs but produces raw key and value pairs.  There is an asymmetry.

An external sorter may not produce a RawKeyValueIterator(due to its own serialization mechanism - for example if records are piped to GNU sort, the key and value may be serialized with a TAB separator between them.)  If an
external sorter would like to use CombinerRunner classes defined in Task.java, it cannot do so without incurring an additional data move.  I was looking for an iterator that will return simple key and values.  I could not find any that is efficient.  The RecordReader looks appropriate functionally, but is not efficient when passed to ValuesIterator(defined in Task.java) which gets used as part of running a Combiner.  The key and values returned from RecordReader will have to be copied.  Any kind of such data move will affect the performance especially when dealing with huge volume of data.

I had to come up with a simple key, value iterator as below:

public interface KeyValueIterator<K, V> extends Closeable {
  /**
   * Get the current key.
   * 
   * @param key where the current key should be stored.  If this is null, a new
   * instance will be created and returned.
   * @param key current key
   */
  K getCurrentKey(K key) throws IOException, InterruptedException;
  
  /**
   * Get the current value.
   * 
   * @param value where the current value should be stored.  If this is null, a
   * new instance will be created and returned.
   * @return value current value.
   */
  V getCurrentValue(V value) throws IOException, InterruptedException;
  
  /**
   * Set up to get the current key and value (for getKey() and getValue()).
   * 
   * @return <code>true</code> if there exists a key/value, <code>false</code>
   * otherwise. 
   */
  boolean nextKeyValue() throws IOException, InterruptedException;
  
  /**
   * Get the Progress object; this has a float (0.0 - 1.0) indicating the bytes
   * processed by the iterator so far.
   * @return progress object.
   */
  Progress getProgress();
}

I was able to wrap the framework's RawKeyValueIterator inside an implementation of KeyValueIterator without additional data move.  This makes sure that anything outside the sorter sees only the simple key, value iterator.  The serialized representation stays internal to the sorter black box.  The external sorter is also happy as it does not incur any extra data move:-)

The abstract base class and MAPREDUCE-279
-----------------------------------------
As I mentioned in my previous post, I created an abstract base class called MapOutputSorterAbstract(I am attaching the source to Jira-2454)  in order to access package protected class methods.  I would appreciate if developers
familiar with MAPREDUCE-279 can take a look at the class and comment on whether the class can live completely outside the framework.  In MapTask.java, I needed to change the access of APPROX_HEADER_LENGTH to package public from private.

My specific questions are:

If TaskReporter, Counter, MapOutputFile be accessible as public classes, how can they be passed to external sorter from MapTask.java?

Will CombinerRunner as defined in Task.java be available?(I had to change the access to public.)

The classes SpillRecord and IndexRecord should also be made public.  Since IndexRecord is not an inner class of SpillRecord, I created another file IndexRecord.java and moved the code there.

Reduce
------
On the Reduce side, I was trying to come up with an interface that can be implemented by both the framework as well as an external sorter.  It was not easy to decouple shuffling and Merger since the shuffle is driving the Merge not
the other way around.  Since I wanted to reuse the framework's shuffle code, I ended up using a few ugly if's so that data is shuffled either to the framework's Merger or to an external sorter.

If the shuffle code can somehow be invoked from outside the core packages using public interfaces, the external sorter on the Reduce side can just implement a simple key, value iterator.  I think this might require some inversion of control and code rewrite in some sensitive areas.

On ServiceLoader:
-----------------
I thought of taking a simple approach: Users can configure a job with the name of the external sorter classes in configuration parameters like mapred.map.externalsort.class and mapred.reduce.externalsort.class and use
simple Java class loader to load the class.  This is very similar to configuring a Mapper class for example.  Am my missing something?  Is there a strong reason to use ServiceLoader?

Please give me your feedback.

If developers do not get the full picture of what I was playing with, I can try to make my changes locally on top of MAPREDUCE-279 branch and post a patch file.

Thanks everyone for your patience.

-- Asokan
","28/Apr/11 23:13;masokan;Sorry, the code was not formatted properly.  I have uploaded KeyValueIterator.java as well.
","01/May/11 00:49;masokan;I thought more on the implementation.  Here is what I came up with the steps
involved.  I can create one Jira per each step. If there is any objection from
anyone, I would like to hear about it before I jump in.  In the following, any
reference to 'framework code' implies current code in Map/Reduce in Hadoop
taken at the branch MAPREDUCE-279.
# Modify the framework code so that RawKeyValueIterator is visible only within
the sort/merge related code.  All others will see the new KeyValueIterator as
defined below:
{code:title=KeyValueIterator.java|borderStyle=solid}
public interface KeyValueIterator<K, V> extends Closeable {
  /**
   * Get the current key.
   * @param key where the current key should be stored.  If this is null, a new
   * instance will be created and returned.
   * @return current key
   * @exception IOException in case of error.
   * @exception InterruptedException in case of interruption.
   */
  K getCurrentKey(K key) throws IOException, InterruptedException;
  
  /**
   * Get the current key.
   * @return current key
   * @exception IOException in case of error.
   * @exception InterruptedException in case of interruption.
   */
  K getCurrentKey() throws IOException, InterruptedException;
  
  /**
   * Get the current value.
   * @param value where the current value should be stored.  If this is null, a
   * new instance will be created and returned.
   * @return current value.
   * @exception IOException in case of error.
   * @exception InterruptedException in case of interruption.
   */
  V getCurrentValue(V value) throws IOException, InterruptedException;
  
  /**
   * Get the current value.
   * @return current value.
   * @exception IOException in case of error.
   * @exception InterruptedException in case of interruption.
   */
  V getCurrentValue() throws IOException, InterruptedException;
  
  /**
   * Set up to get the current key and value (for getKey() and getValue()).
   * @return <code>true</code> if there exists a key/value, <code>false</code>
   * otherwise. 
   * @throws IOException in case of error.
   * @exception InterruptedException in case of interruption.
   */
  boolean nextKeyValue() throws IOException, InterruptedException;
  
  /**
   * Get the Progress object; this has a float (0.0 - 1.0) indicating the bytes
   * processed by the iterator so far.
   * @return progress object.
   */
  Progress getProgress();
}
{code}
This will enable any external sorter implementation to reuse existing code in
Task.java to run the combiner and the code in ReduceTask.java to run the
reducer.
# Modify the framework code so that an external sorter can be plugged on the
Map side.
# Modify the shuffle code on the Reduce side so that a shuffle can be started
by any code outside MR(perhaps in a separate thread.)  A callback interface
will be passed to shuffle.  Tentatively, the callback will look like this and
can be refined.
{code:title=ShuffleCallback.java|borderStyle=solid}
public interface ShuffleCallback<K, V> extends Closeable
  /**
   * To reserve space for the specified mapper output.
   * @param mapId mapper id.
   * @param requestedSize number of bytes in space to be reserved.
   * @param fetcher id of fetcher that will fetch the map output.
   * @exception IOException in case of error.
   */
  public MapOutput<K,V> reserve(TaskAttemptID mapId, long requestedSize,
                                int fetcher)
    throws IOException;

  /**
   * To shuffle the data from local mappers.
   * @param localMapFiles array of map output files to be sorted.
   * @return total number of bytes read from the mapper outputs.
   * @exception IOException if there is any IO error while reading.
   * @exception InterruptedException if there is an interruption.
   * @exception InterruptedException in case of interruption.
   * @exception ReduceInputSorterException any other exception that occurs while
   * sorting.
   */
  public long shuffle(Path localMapFiles[])
    throws IOException, InterruptedException, ReduceInputSorterException;

  /**
   * To shuffle the raw data coming from a non-local mapper.  Multiple threads
   * can call this method with input from different mappers.
   * @param inputFromMapper the raw input stream from the mapper. If map output
   * is compressed, the sorter is responsible for decompressing.
   * @param mapTaskId map task id corresponding to the stream.
   * @param compressedLength The size of the compressed data.
   * @return number of bytes read from the mapper stream.
   * @exception IOException if there is any IO error while reading.
   * @exception InterruptedException if there is an interruption.
   * @exception ReduceInputSorterException any other exception that occurs in
   * the sorter.
   */
  public long shuffle(InputStream inputFromMapper, String mapTaskId,
                      long compressedLength)
    throws IOException, InterruptedException, ReduceInputSorterException;

  /**
   * To commit shuffled data from a non-local mapper.  Usually, this method is
   * called right after shuffle() from the same thread once it is
   * decided to commit.
   * @param mapTaskId map task id corresponding to the shuffled data.
   * @exception IOException if there is any IO error while discarding.
   * @exception InterruptedException in case of interruption.
   * @exception ReduceInputSorterException any other exception that occurs in
   * the sorter.
   */
  public void commit(String mapTaskId)
    throws IOException, InterruptedException, ReduceInputSorterException;

  /**
   * To discard shuffled data from a non-local mapper. Usually, this method is
   * called right after shuffle() from the same thread once it is
   * decided to discard.
   * @param mapTaskId map task id corresponding to the shuffled data.
   * @exception IOException if there is any IO error while discarding.
   * @exception InterruptedException in case of interruption.
   * @exception ReduceInputSorterException any other exception that occurs in
   * the sorter.
   */
  public void discard(String mapTaskId)
    throws IOException, InterruptedException, ReduceInputSorterException;

  /**
   * To indicate end of input from all non-local mappers.  This should be called
   * after all non-local mapper outputs are committed.
   * @exception IOException if there is any IO error.
   * @exception InterruptedException if there is an interruption.
   * @exception ReduceInputSorterException any other exception that occurs in
   * the sorter.
   */
  public void close()
    throws IOException, InterruptedException, ReduceInputSorterException;
{code}
# Modify ReduceTask.java so that it will invoke the framework's merge or the
external sorter.
# The external sorter interface on the Map side will look like:
{code:title=MapOutputSorter.java|borderStyle=solid}
public interface MapOutputSorter<K, V> 
  extends MapOutputCollector<K, V> {
  /**
   * To initialize the sorter.
   * @param job job configuration.
   * @param mapTask map task invoking this sorter.
   * @param inputSplitSize size of input split processed by this sorter.
   * @param mapOutputFile map output file
   * @param reporter reporter to report sorter progress.
   * @exception IOException if there is any error during initialization.
   * @exception ClassNotFoundException if a class to be loaded is not found.
   * @exception UnsupportedOperationException thrown by the sorter if it
   * cannot support certain options in the job.  For example, a sorter may
   * support only a certain subset of key types.  The default sorter in the
   * framework will be used as a fallback when this exception is thrown.
   */
  public void initialize(JobConf job, MapTask mapTask, long inputSplitSize,
                         MapOutputFile mapOutputFile, TaskReporter reporter)
    throws IOException, ClassNotFoundException, UnsupportedOperationException;
}
{code}
The MapOutputCollector interface defined in MapTask.java will be made public
and will look like below:
{code:title=MapOutputCollector.java|borderStyle=solid}
public interface MapOutputCollector<K, V> extends Closeable{
  public void collect(K key, V value, int partition
                      ) throws IOException, InterruptedException;
  public void flush() throws IOException, InterruptedException, 
                             ClassNotFoundException;
  public void close() throws IOException, InterruptedException;
}
{code}
On the Reduce side, the external sorter interface will look like:
{code:title=ReduceInputSorter.java|borderStyle=solid}
public interface ReduceInputSorter<K, V> extends KeyValueIterator<K, V> {
  /**
   * Initialize the sorter.
   * @param job job configuration.
   * @param reduceTask reduce task invoking this sorter.
   * @param reporter reporter to report sorter progress.
   * @exception IOException if there is any error during initialization.
   * @exception ClassNotFoundException if a class to be loaded is not found.
   * @exception UnsupportedOperationException thrown by the sorter if it
   * cannot support certain options in the job.  For example, a sorter may
   * support only a certain subset of key types.  The default sorter in the
   * framework will be used as a fallback when this exception is thrown.
   */
  public void initialize(JobConf job, ReduceTask reduceTask,
                         TaskReporter reporter)
    throws IOException, ClassNotFoundException, UnsupportedOperationException;
{code}
# Some abstract base classes of the above two may be provided in the framework
to facilitate external sorter implementations.
# Provide a proof-of-concept implementation of an external sorter both on the
Map and Reduce sides using GNU sort command as the external sorter.
*All the changes mentioned above should not result in any performance
degradation of framework code when no external sorter is plugged in.*

","05/May/11 09:33;stevel@apache.org;Why shouldn't a sorter API also work with the existing sorter. If this is so, then the API and plugin code etc will get tested on every build, whereas a ""a proof-of-concept implementation of an external sorter both on the
Map and Reduce sides using GNU sort command as the external sorter."" isn't going to get tested very often, and the GPL license prevents the ASF from redistributing that sorter -ignoring all binary library issues-. ","05/May/11 09:45;stevel@apache.org;I'm also worried about the exception signatures of the interface. One of the hardest things to do in any Java plugin API is getting the list of possible exceptions throw right -if not you end up wrapping everything up. 

Presumably the list of exceptions thrown is derived from your (private) implementation. For example, the {{initialize()}} methods throw {{ IOException, ClassNotFoundException, UnsupportedOperationException }}.

A {{ClassNotFoundException}} ClassCastException implies the implementation is possibly playing with classloaders -which is a dangerous game and one in which most Java developers, myself included- shouldn't be doing. At the very least the exception list should include

{{ClassCastException}} -when the class loads but is the wrong type
{{InstantiationException}} you can't instantiate the class as its constructor isn't there
{{NoClassDefFoundError}} can't find the class though it used to be there
{{LinkageError}} etc. 

The other signatures may work, but the current initialize code is clearly biased towards one single implementation. We'd need more implementation, and some test cases that simulate other failures, to make sure whatever changes go into the MR engine actually handle them, presumably by catching and failing the job. I don't see any point in trying to fall back to another sorter as that would just hide a problem.","05/May/11 12:22;stevel@apache.org;oh, and the security exceptions that get raised if the class does have a constructor with the right signature but it's private needs to get caught and turned into something else","05/May/11 14:51;masokan;Hi Steve,
    Thank you very much for your comments.  I will try to make the sorting done on Map and Reduce side as pluggable.  The default implementation will be whatever is available in the framework.  It is easy to separate the sorting process on the Map side(currently all the code is in the class MapOutputBuffer which lives in MapTask.java.)  It is very hard to separate the merge on the Reduce side because of the way it is coded.  I am working to separate that as well.

Regarding GNU sort plugin, I am making the external sort command name configurable.  It can be POSIX sort command as well.  Since most Hadoop installations are Linux based, GNU sort is available as the POSIX sort implementation.  Other UNIX installations can use the POSIX sort command as an external sorter.  There is no GPL issue.  Perhaps, I can remove the word GNU and just call it UNIX.

Regarding class loader related exceptions: I will look at framework's code and see what it does when it loads a Mapper or Reducer class and follow the same since the scenario is very similar.  All issues you have raised w.r.t class loading are applicable there as well.

An explanation on UnsupportedOperationException:  If the external sorter uses a UNIX command like sort, it may not be able to handle a custom key type user has defined since the key comparator may be written in Java.  In such a case there will be message logged in syslog and the framework's sorter will be used.  I think this is fair enough.  Please let me know if you think otherwise.

When I am done with the implementation(on top of MAPREDUCE-279) and testing, I will post a patch file for review.  Would you be interested to work with me as a committer?

Thank you.","05/May/11 15:45;omalley;Actually, I think I made a mistake in pushing the objects into the interface, especially since I plan to change the serialization layer. I think it would be better to do:

{code:title=RawRecordWriter}
package org.apache.hadoop.mapreduce.task;

public abstract class RawRecordWriter implements Closeable {
  /**
   * Called once at start of processing
   */
  public abstract void initialize(TaskAttemptContext context
                                  ) throws IOException, InterruptedException;

  /**
   * Called once per a record. The key and value will be copied before write returns.
   */
  public abstract void write(int partition, ByteBuffer key, ByteBuffer value
                             ) throws IOException, InterruptedException;

  /**
   * Called once at task finish or failure.
   */
  public abstract void close() throws IOException;
}
{code}

For the Reduce side, we could just use the RawKeyValueIterator, but I suspect we'll be in 
better shape if we do something similar:

{code:title=RawRecordReader.java}
package org.apache.hadoop.mapreduce.task;

public abstract class RawRecordReader implements Closeable {
  /**
   * Called once at start of processing
   */
  public abstract void initialize(TaskAttemptContext context
                                  ) throws IOException, InterruptedException;

  /**
   * Advance to the next record. Returns false when there are no more records.
   */
  pubic abstract boolean next() throws IOException, InterruptedException;

  /**
   * Provides the ByteBuffer with the key. The ByteBuffer may be reused after each call to
   * next.
   */
  public abstract ByteBuffer getKey() throws IOException, InterruptedException;

  /**
   * Provides the ByteBuffer with the value. The ByteBuffer may be reused after each call to
   * next.
   */
  public abstract ByteBuffer getValue() throws IOException, InterruptedException;

  /**
   * Called once at task finish or failure.
   */
  public abstract void close() throws IOException;  
}
{code}

This has a couple of advantages:
* The plugin gets the TaskAttemptContext and the configuration.
* Serialization stays part of MapReduce instead of the sort library.","05/May/11 15:54;omalley;I should also comment that the initialize method would allow the plugin to start any additional
services that it needs.

MapOutputCollector's flush is a method that should be controlled by the sorter and not the framework.","05/May/11 15:56;stevel@apache.org;> Would you be interested to work with me as a committer?

I am too overloaded with non-Hadoop work I don't get the time to get my my own patches up to date with MAPREDUCE-279, let alone work on other things. Sorry

I recommend you get involved on the mapreduce-dev list, understand what's being discussed (if you subscribe to the mapreduce-issues list you'll get all JIRA changes), and so get involved in the bigger picture of where things are going -and get to know the people who do know their way round the codebase better than me.","05/May/11 21:52;masokan;Hi Owen,
  Thanks for your comments.  I like your suggestion on the signature of initialize() method and also not having a flush().  However, I prefer to pass the Key and Value as objects instead of serialized ByteArray for the following reasons:
* It is easier and more efficient when external program(like UNIX sort command) is invoked as a sorter.  The Key and Value types will be Text.  The bytes in the Text can be grabbed and passed to the program with a TAB between them.  There is no need to deserialize data passed in the ByteArray.  This is similar to what is happening with hadoop streaming when for example a Mapper is implemented by an external program.  Also, on the Map side the output of the mapper is key and value objects which can be directly passed to the sorter.  Thus there is no need for extra serializtion/deserialization.  Similar argument applies when output of the sorter is read on the Reduce side using RecordReader.
* The framework's serialization is in no way affected.  It is free to replace the serialization layer.  The external sorter can store the sorted output as simple UNIX text records in the final map output file since it will deal with the shuffled data on the Reduce side.
* For the RecordReader, I think it is better to change the signature of getKey() and getValue() as below:
{code:title=RecordReader}
Object getKey(Object key) // If key is null, it will be allocated first.
Object getValue(Object value) // If value is null, it will allocated first.
{code}
The reasons for these signatures are:
   ** The RecordReader will be used for running Combiner and Reducer.  This may involve saving the last seen key.  If the caller passes the key object, it can just save the object handle not the entire object since it owns the object.  If the callee is returning its own object, it is ephemeral and so the caller has to save it which results in extra copying.
   ** Creating an adapter to return key and value objects from their serialized counterparts(that is from RawKeyValueIterator) will not result in any extra data copying.  So the performance of the framework's sorter will not degrade.

Owen, do you have any suggestion on a committer with whom I can work on this?
Thanks.","06/May/11 20:30;omalley;The map output key and value types are controlled by the application, not the framework. A plugin that can only sort Text objects isn't general purpose enough. Even streaming created a lot of trouble for the users by requiring UTF-8 encoding of the data. 

The only acceptable solution would be to define this API and refactor the current code into a default plugin.

I hadn't thought enough about the combiner. It requires an inversion of control since the start of the combiner happens based on the spill.

{code:title=SortPlugin}
package org.apache.hadoop.mapreduce.task;

public abstract class SortPlugin {

  public interface CombinerCallback {
    /** Called once for each partition of the map output */
    void runCombiner(RawRecordReader reader,
                     RawRecordWriter writer
                    ) throws IOException, InterruptedException;
  }

  /** Called once in map task for collector to gather
      output coming from map. */
  public abstract RawRecordWriter createRawRecordWriter()
    throws IOException, InterruptedException;

  /** Called once in the map task, if there is a combiner. */
  public abstract void registerCombinerCallback(CombinerCallback callback)
    throws IOException, InterruptedException;

  /** Called once in the reduce task for iterator to provide
      input to the reduce. */ 
  public abstract RawRecordReader createRawRecordReader() 
    throws IOException, InterruptedException;
}
{code}","25/May/11 15:21;masokan;Thanks for all the comments I have received so far.

I am highlighting the changes and additions made.  Please look at the attached
patch file MR-2454-trunkPatchPreview.gz for details.  The patch file reflects
the changes made on the trunk revision.  I will post a patch file for MR-279
branch later once I have the build working.

I would like to receive feedback from all developers and especially the ones who
worked on the files:

Task.java, MapTask.java, and ReduceTask.java.

We will start testing the changes once I receive the feedback.

h4. OVERALL

* The interfaces SortPlugin, MapSortPlugin, and ReduceSortPlugin were added to
facilitate sort plugin implementations.

* The framework code was refactored to implement DefaultSortPlugin,
DefaultMapSortPlugin, and DefaultReduceSortPlugin.

* The shuffle code was decoupled from the framework's merge so that shuffle can
be used by other sort plugin implementations.

* An implementation of an external sort plugin was added under
contrib/sortplugin directory.  It uses Unix sort command to sort the keys.  This
is a work in progress.  Only the UnixMapSortPlugin is currently implemented.

* New interfaces SortinRecordWriter and SortoutRecordReader were introduced.
The sort plugins will provide implementations of these interfaces.

h5. Task.java

* Several useful static classes that were present in this file were taken out
and separate files containg corresponding public classes were created under
hadoop/mapreduce/task.

h5. MapTask.java

* The sort code in MapOutputBuffer class was taken out of this file and it will
live in DefaultMapSortPlugin.java under hadoop/mapreduce/task/map.

h5. ReduceTask.java

* Sort related code was taken out of this file and it will live in
DefaultReduceSortPlugin.java under hadoop/mapreduce/task/reduce.

* Helper methods to create ShuffleRunner and MergeManager instances were added.

h5. Shuffle.java
h5. MergeManager.java

* A new interface ShuffleRunner was introduced and Shuffle will implement this
interface.

* A new interface ShuffleCallback was introduced and will be implemented by
MergeManager.

* The Shuffle class will be dealing with shuffle only.  The MergeManager will no
longer be instantiated by Shuffle.

* An implementation of ShuffleCallback interface will be passed to the run()
method.

* MergeManager which implements ShuffleCallback will be instantiated by
DefaultReduceSortPlugin.

* Any other reduce sort plugin implementation will need to implement
ShuffleCallback interface outside the framework.

* Shuffle class will no longer be taking <K, V> as generic parameters.

h5. Fetcher.java
h5. ShuffleScheduler.java
h5. EventFetcher.java

* The classes in these files will no longer be taking <K, V> as generic
parameters.

* Fetcher will receive a ShuffleCallback object as opposed to a
MergeManager instance.

* Fetcher will delegate the responsibility of copying shuffled data to one of
concrete implementations of MapOutput.

h5. MapOutput.java

* The code in this file was refactored so that the class MapOutput will be
abstract with the concrete implementations OnDiskMapOutput and
InMemoryMapOutput created from the original MapOutput.java and Fetcher.java.
These concrete implementations will be used by MergeManager.

* MapOutput class will no longer be burdened with carrying unrelated information
(MEMORY, DISK, and WAIT.)

* Any other reduce sort plugin implementation will need to provide a concrete
implementation of MapOutput class outside the framework.
","08/Jun/11 14:04;masokan;Just a reminder.  I would like to receive any comments on the proposed patch to support a sort plugin so that I can continue with the Unix sort plugin implementation and the testing.
Thanks.
","23/Jun/11 15:35;masokan;Uploaded a short document(HadoopSortPlugin.pdf) on sort plugin.  Please provide your feedback.
","16/Aug/11 20:24;masokan;Attached a patch on top of MR-279 branch
","02/Jan/12 21:29;masokan;Attached a patch on top of the trunk.  All tests run by maven passed.

I would appreciate if a committer can take a look at the patch and help push it
into the trunk.

In the meantime, I am working on a NullSortPlugin implementation.  If there is
enough interest, I can add it to the patch later.  The idea of a NullSortPlugin
is to not sort the map output at all!  On the Reduce side, the shuffled records
will be passed to the Reducer without any merging.

The NullSortPlugin is useful to solve limit-N query problem that does not
require order-by(for a complete description of the problem, please refer to
HIVE-2004 and MAPREDUCE-1928.)  The idea is to stop an MR job that does simple
filtering(no sorting needed) after a certain number of records are selected.

The steps involved are as follows:

* Set the parameters mapred.map.max.attempts and mapred.reduce.max.attempts to
1, set mapred.reduce.slowstart.completed.maps to 0, and set the number of
reducers to 1 for an MR job.

* Implement a Mapper that does a condition based filtering.

* Implement a Reducer which would output only the first N records, print out a
diagnostic message in the log and throw an exception to stop the MR job.

The first step makes sure that the job is not restarted and the reducer is
started right away.  The third step limits the number of Map tasks started(which
is the desired goal.)  The diagnostic message in the log will be useful to find
out whether the job aborted or completed normally.","03/Jan/12 00:34;djain;Asokan,

Keep up the good work. Wish you happy new year.

Regards,
Deepak Jain
650-208-8790(c)

Sent from my iPhone


","04/Jun/12 22:03;masokan;The patch had been tested with the trunk revision.","04/Jun/12 22:59;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12530864/mapreduce-2454.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 3 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2437//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2437//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-core.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2437//console

This message is automatically generated.","06/Jun/12 22:24;masokan;Fixed findbugs and javadoc warnings.
","06/Jun/12 23:23;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12531175/mapreduce-2454.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.mapred.TestReduceFetchFromPartialMem

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2442//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2442//console

This message is automatically generated.","07/Jun/12 14:25;masokan;The failing test seems to be a flaky one.  Googling on {{org.apache.hadoop.mapred.TestReduceFetchFromPartialMem}} shows a lot of hits in mapreduce jira.  I will look at the test more closely to see whether it can be fixed.  I welcome input from other developers on this.  Meanwhile, I can retry the same patch file to see whether this failure goes away magically.
","12/Jun/12 04:26;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12531752/mapreduce-2454.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.io.file.tfile.TestTFileByteArrays
                  org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays
                  org.apache.hadoop.fs.viewfs.TestViewFsTrash
                  org.apache.hadoop.hdfs.server.namenode.TestProcessCorruptBlocks
                  org.apache.hadoop.hdfs.TestDatanodeBlockScanner
                  org.apache.hadoop.mapred.TestReduceFetchFromPartialMem

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2452//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2452//console

This message is automatically generated.","25/Jun/12 19:51;masokan;Trying one more time...
","25/Jun/12 22:22;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12533362/mapreduce-2454.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.io.file.tfile.TestTFileByteArrays
                  org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays
                  org.apache.hadoop.mapred.TestReduceFetch

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2507//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2507//console

This message is automatically generated.","13/Jul/12 13:50;acmurthy;Mariappan - Sorry it took me so long, and thanks for being super patient! I'm clearing my review backlog and I'll get to this very soon! Meanwhile, can you pls look at the test failures? Thanks again!","13/Jul/12 13:58;masokan;Thanks for your comments Arun.  I will start looking at the failing tests.

-- Asokan","18/Jul/12 15:23;masokan;Hi Arun,
  HADOOP-8537 is addressing the issue with TFile tests failing.  I will start looking at TestReduceFetch test.
","20/Jul/12 15:35;masokan;Hi Arun,
  I came across MAPREDUCE-1392 that mentions TestReduceFetch failures.  I was able to reproduce intermittent failures when I ran the test repeatedly on my laptop.  I may be wrong but it appears to be related to some timing issue.  I will investigate how the test can be stabilized so that it is rerunnable.  If I find anything or have a proposal to fix this issue, I will add my comments in MAPREDUCE-1392.
","23/Jul/12 03:20;masokan;I think I found the reason why TestReduceFetch test was failing.  The line:
{code}
    commitMemory -= size;
{code}
in the method unreserve() in MergeManager.java was missing in my patch.  I have corrected this problem and resubmitted the patch.
","23/Jul/12 05:52;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537540/mapreduce-2454.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2647//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2647//console

This message is automatically generated.","24/Jul/12 03:28;zzhang;Asokan,
Your patch is based on revision 1363960 which is in between the following two builds of the trunk. 
https://builds.apache.org/view/Hadoop/job/Hadoop-Mapreduce-trunk/1142/ (1363576)
https://builds.apache.org/view/Hadoop/job/Hadoop-Mapreduce-trunk/1143/ (1364020)

Both of the above builds failed with the following:
Running org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat
Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 8.385 sec <<< FAILURE!

If I am not mistaken, most likely the failure is not caused by your change. You just need to pick a clean base. What do you think?
","24/Jul/12 15:51;masokan;Hi Sean Zhang,
  You may be right.  I have been looking at this test and it does not seem to be related to my changes.  I will try with a clean base.  Thanks.
","24/Jul/12 17:05;masokan;Merged on top of trunk version 1361565.
","24/Jul/12 19:37;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537708/mapreduce-2454.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2652//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2652//console

This message is automatically generated.","25/Jul/12 02:20;masokan;MAPREDUCE-4470 addresses this test failure.  I am waiting for a fix.
","31/Jul/12 00:28;tucu00;Mariappan, the latest patch does not apply to trunk, would you post a new patch rebased to trunk? Thx","31/Jul/12 18:00;masokan;Hi Alejandro,
  Thanks for the reminder.  I picked up the changes in Fetcher.java and TestFetcher.java.  The test TestCombineFileInputFormat will still fail:(
","31/Jul/12 20:49;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12538585/mapreduce-2454.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2687//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2687//console

This message is automatically generated.","23/Aug/12 22:10;masokan;MAPREDUCE-4470 has been committed.  The test TestCombineFileInputFormat should no longer be failing.","27/Aug/12 19:28;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12542197/mapreduce-2454.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.hdfs.TestHftpDelegationToken

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2773//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2773//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2773//console

This message is automatically generated.","27/Aug/12 21:01;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12542643/mapreduce-2454.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 eclipse:eclipse.  The patch failed to build with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.hdfs.TestHftpDelegationToken

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2775//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2775//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2775//console

This message is automatically generated.","29/Aug/12 00:41;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12542841/mapreduce-2454.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.hdfs.TestHftpDelegationToken
                  org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2787//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2787//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2787//console

This message is automatically generated.","29/Aug/12 22:08;acmurthy;Asokan - I started looking at this patch. Can you pls provide a brief overview of your design, particularly the new interfaces you are adding and how they play? Is your old pdf still valid? Tx.","29/Aug/12 22:28;masokan;Hi Arun,
  Sure, I will expand the pdf documentation I posted long ago and upload an updated version shortly.

I also created a test which implements what I call as a NullSortPlugin.  I will copy this test as part of the patch.  This implementation is very simple and it basically demonstrates how sorting can be avoided to solve limit-N query problem described in HIVE-2004 and MAPREDUCE-1928.","30/Aug/12 21:46;masokan;Hi Arun,
  I have attached the latest patch that includes a test.  I have also uploaded the latest pdf document on the plugin in order to make the code review somewhat easier.  If you want more details in the document, please let me know.

It appears that some tests unrelated to this Jira have been failing.  I am not able to pick up a clean trunk to work with.

Thanks for your patience.

-- Asokan
","30/Aug/12 21:51;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12543168/HadoopSortPlugin.pdf
  against trunk revision .

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2794//console

This message is automatically generated.","31/Aug/12 00:32;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12543174/mapreduce-2454.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 5 new or modified test files.

    -1 javac.  The applied patch generated 2066 javac compiler warnings (more than the trunk's current 2059 warnings).

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.hdfs.TestHftpDelegationToken

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2795//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2795//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2795//console

This message is automatically generated.","31/Aug/12 04:34;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12543218/mapreduce-2454.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 5 new or modified test files.

    -1 javac.  The applied patch generated 2060 javac compiler warnings (more than the trunk's current 2059 warnings).

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.hdfs.server.namenode.TestStorageRestore
                  org.apache.hadoop.hdfs.TestHftpDelegationToken

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2798//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2798//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2798//console

This message is automatically generated.","31/Aug/12 15:31;masokan;Fixed the new test to get rid of compiler warnings.","31/Aug/12 18:27;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12543287/mapreduce-2454.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 5 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.hdfs.TestHftpDelegationToken
                  org.apache.hadoop.hdfs.TestDatanodeBlockScanner

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2799//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2799//console

This message is automatically generated.","01/Sep/12 23:22;masokan;Got rid of Javadoc warning.
","02/Sep/12 02:05;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12543452/mapreduce-2454.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 5 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.hdfs.server.namenode.TestListCorruptFileBlocks

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2810//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2810//console

This message is automatically generated.","02/Sep/12 02:50;masokan;Rebasing to a previous version of trunk.
","02/Sep/12 05:21;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12543461/mapreduce-2454.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 5 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks
                  org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2811//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2811//console

This message is automatically generated.","05/Sep/12 17:44;masokan;Made some minor change to create single instances of DefaultMapSortPlugin and DefaultReduceSortPlugin.
","05/Sep/12 20:12;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12543878/mapreduce-2454.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 5 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2814//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2814//console

This message is automatically generated.","05/Sep/12 21:00;masokan;Hi Arun,
  The failing test TestBlocksWithNotEnoughRacks seems to be unrelated to this patch.  MiniDFSCluster is failing during shutdown.  Can you please review the patch while I look at the cause of failure?

Thanks.

-- Asokan
","17/Sep/12 13:34;masokan;The failing test is passing when run on my machine.  Submitted the patch with some minor updates.
","17/Sep/12 16:01;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12545413/mapreduce-2454.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 5 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2856//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2856//console

This message is automatically generated.","17/Sep/12 16:54;masokan;Hi Arun,
  Can you please review the patch?  If you need any additional information about the patch itself, please let me know.

Thanks.
-- Asokan
","23/Sep/12 13:09;masokan;Posted a patch for MAPREDUCE-4482(backport of MAPREDUCE-2454 to Hadoop 1.2) and a revised design document there.","24/Sep/12 18:22;masokan;I would like to receive feedback from Arun, Alejandro, Sean, Deepak, or other developers, and committers who are watching this Jira so that it can be committed.
Thanks.

-- Asokan","09/Oct/12 15:58;zzhang;This is going to enable a new set of potential for Hadoop. I hope this can be committed soon. ","10/Oct/12 17:56;tucu00;Initial feedback on the patch (I'll do a more detailed review):

* Nice work
* patch needs rebase, TestReduceTask.java has been moved to hadoop-mapreduce-client-jobclient/
* remove introduced & unused imports through out the patch
* reformat lines with over 80 chars through out the patch

I'm not trilled on how we are mixing mapred and mapreduce classes in the APIs of pluggable sort. But given how the current MR stuff implementation is done, I don't think it is possible to avoid that without a mayor cleanup/refactoring of much bigger scope.

One thing would be quite useful, and I'd say a pre-requisite before committing it, is a performance comparison of terasort with and without the patch; we shouldn't be introducing a sensible performance penalty.","13/Oct/12 23:50;masokan;Hi Alejandro,
  Thanks for your feedback.  I have rebased to the recent trunk version.  Moved TestReduceTask.java to hadoop-mapreduce-client-jobclient.  Removed unused imports and reformatted the lines to 80 chars.

Preliminary runs of terasort benchmark on a smaller cluster did not show any performance degradation.  I am trying to get access to a 10 node cluster to run the terasort benchmarks with and without my patch.  Please let me know whether that is good enough for comparing.  Once I finish running the benchmark, I will post the results.

Thanks once again for reviewing the patch.

-- Asokan","14/Oct/12 02:29;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12549046/mapreduce-2454.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 5 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2928//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2928//console

This message is automatically generated.","14/Oct/12 18:23;acmurthy;Mariappan, I've started taking a look - there is a *lot* to digest here. I apologize this has taken so long, but I've personally started on this many, many times and then got distracted since there is so much here for me to review. I'm sure it's the same for several other committers - it's not an excuse, but unfortunately you are asking all reviewers here for significant investment of their time... 

This is *much* more so because you are fiddling with some of the most core pieces of the MR framework (i.e. sorting and shuffling).

Also, a meta point - it's also easier to work in an open-src community by starting with small and building up some credibility quickly by fixing bugs, tests etc. This was people are more comfortable when you make bigger changes.

I'm not alone sharing this, see a recent interview with Todd:
{quote}
What is your advice for someone who is interested in participating in any open source project for the first time?

Walk before you run. One mistake I’ve seen new contributors make is that they try to start off with a huge chunk of work at the core of the system. Instead, learn your way around the source code by doing small improvements, bug fixes, etc. Then, when you want to propose a larger change, the rest of the community will feel more comfortable accepting it. One great way to build karma in the community is to look at recently failing unit tests, file bugs, and fix them up.
{quote}

In general, it would be better if you could break this into a series of smaller patches and do this work in a development branch. This will make it easier to review.

I understand this is frustrating, I apologize - but this is unfortunately a *lot* of work and highly risky one to boot.

----

Having said this - how do we proceed?

Let's start a discussion on mr-dev@ on design review, a dev branch where we make a series of small changes and then proceed there-on. Thoughts?

Again, apologies.","14/Oct/12 22:30;masokan;Hi Arun,
  Thanks for your feedback.  Though I have confidence in my contribution(I have been running Terasort without any problems for data sizes of 2 TB on a small cluster), I understand your concerns on the size of the patch.  I can think of the following sub-steps each addressed in a different Jira:

* Refactor {{Task.java}} so that the classes {{ValuesIterator}}, {{CombinerRunner}}, and {{CombineValuesIterator}}, {{CombineOutputCollector}} can be taken out to separate
files.

* Refactor {{MapOutput.java}} to create {{InMemoryMapOutput}} and {{OnDiskMapOutput}} classes.

* Refactor {{Shuffle.java}} and {{MergeManager.java}} to decouple shuffle and merge.  This should also allow one to make shuffle pluggable.  There will be a small change to {{ReduceTask.java}} as part of this decoupling since {{ReduceTask}} will instantiate both {{Shuffle}} and {{MergeManager}} objects.

* Refactor {{MapTask.java}} so that the code related to sort on the map side is moved to a new file {{MapSort.java}}.  Introduce {{SortinRecordWriter}} and {{SortoutRecordReader}} classes as part of this refactoring.

* Refactor {{ReduceTask.java}} so that merge related code is moved to a new file {{ReduceSort.java}}.

* Define corresponding interfaces for {{MapSort}} and {{ReduceSort}} classes and make these implementations pluggable.

How does the above sequence of changes sound to you?  I can raise separate Jiras for each one.  We can keep these changes in a separate branch before moving to the trunk if you wish.

If you have other suggestions, please let me know.

Thanks again.

-- Asokan
","17/Oct/12 03:21;masokan; Hi Arun,
  I stepped back and looked at some of the Jiras that were developed under branches(like MAPREDUCE-279, HDFS-1623, etc.)  They contributed major enhancements to Hadoop.  MAPREDUCE-2454 on the other hand has just refactored existing code to make some classes more modular and pluggable.

There is no significant new functionality added.  There is no change in the way shuffling is done.  The communication between shuffle and merge is kept intact.  The MAPREDUCE-318 refactoring in fact helped a lot.  There is only a minor rearranging of the code in MAPREDUCE-2454.

I followed your advice diligently to work on the trunk first and I picked up suggestions from some other developers who are watching this Jira.  During our brief meetings, the suggestions you gave were very valuable and I followed them.

I have been testing the changes for the last year or so and I have been keeping the Jira up to the latest trunk.  I have not hit any issues in my testing w.r.t both functionally and performance.

I understand that your time is very precious.  I already updated the design document for you to make it easier for code review.  If you can suggest anything that will expedite the committing, please let me know.

Thanks.

-- Asokan","18/Oct/12 23:55;acmurthy;Asokan, sorry I've been busy with stuff - thanks for understanding.

I've spent sometime thinking about this - and I feel we can do something far simpler to address Syncsort's goal of plugging in your proprietary sort while mitigating risk to MR itself.

How about this: I feel we could accomplish both goals by something very simple... by making MapOutputBuffer pluggable by introducing a MapOutputCollector interface. That's about it. This way, you can supply a custom MapOutputBuffer which plugs in your sort for your customers while we can just keep our current implementation. 

Hopefully, that makes sense. What else would you need?

I'm basically trying to vastly minimize the APIs we spread out, this way when we want to change our sort implementation for MAPREDUCE-4039 or Sailfish etc. we have *much* more leeway, at the same time we don't affect you at all.

Thoughts? ","07/Nov/12 17:31;masokan;Hi Arun,
  Thank you very much for allotting some time to have a conversation with you during Strata 2012.  Here is the list of items we discussed and how I followed up in the new patch.

* With YARN, different MR data processing engines can co-exist in addition to the sort/merge done after map and before reduce.  Keeping this in mind, I am calling the sort plugin interface on the map side as {{PostMapProcessor.}}
Similarly, the merge done on the reduce side will be abstracted as {{PreReduceProcessor.}}
* The {{PostMapProcessor}} can simply extend the existing {{MapOutputCollector}} with an {{initialize()}} method.  The current {{MapOutputBuffer}}
in MapTask.java will implement this interface as the default implementation.
* On the reduce side, my suggestion is to define {{PreReduceProcessor}} based on methods already available in {{MergeManager}} class.  With minimal changes, this will allow {{MergeManager}} to implement {PreReduceProcessor.}}
* There is a concern about exposing some APIs as public.  Since the revised patch is much smaller than the one submitted before(one fourth of the original patch size), the chance of breaking anything is minimized.  Also, I feel that only a handful of developers will write plugins.  I have marked all the exposed APIs with proper annotations that APIs are not stable and there is a risk using them.  The plugin developers should keep up with the changes in the exposed APIs.  The core Hadoop developers need not worry about maintaining backward compatibility.

The revised patch can be easily integrated with shuffle plugin.

I repeatedly ran terasort benchmark on a  cluster with 55 nodes.  The performance difference with and without the patch was egligible(plus or minus 1%.)

I would like to receive feedback from you and other developers who are watching this Jira.  In the meantime, I am creating a new test to test the plugin.

Thanks.
-- Asokan
","07/Nov/12 17:54;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12552499/mapreduce-2454.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 2026 javac compiler warnings (more than the trunk's current 2025 warnings).

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2992//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2992//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2992//console

This message is automatically generated.","07/Nov/12 19:59;masokan;Fixed the compiler warning.
","07/Nov/12 20:14;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12552530/mapreduce-2454.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2994//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2994//console

This message is automatically generated.","07/Nov/12 22:27;tucu00;Asokan,

I like the new approach, it is much simpler. 

Patch looks good, some minor comments:

* Do we need 2 different interfaces MapOutputCollector and PostMapProcessor? Couldn't we collapse both on PostMapProcessor?

* If keeping 2 different interfaces, make MapOutputCollector an outer interface

* MapTask class should be annotated as public/unstable

* MRJobConfig.java line 496 has a false space diff

* PreReduceProcessor should be annotated as public/unstable

","08/Nov/12 18:54;masokan;Hi Alejandro,
  Thanks for the feedback.  I changed {{MapOutputCollector}} to {{PostMapProcessor}} so that there is only one interface.  I also made the other changes you suggested.  I am uploading the new patch.  Please take a look at it.

-- Asokan","08/Nov/12 19:14;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12552689/mapreduce-2454.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2999//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2999//console

This message is automatically generated.","09/Nov/12 01:22;tucu00;Asokan, if I understood you correctly you were working in a new testcase. This is not in the latest patch, correct? When you upload the patch with the new testcase please fix the following nit:

*Fetcher.java has 2 unused imports*

import java.io.InputStream;
import java.io.OutputStream;

Then, IMO we are good to go.","09/Nov/12 19:06;masokan;Hi Alejandro,
  Thanks for catching the unused imports.  I updated Fetcher.java.  I have also added a test in the latest patch.

-- Asokan
","09/Nov/12 20:12;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12552875/mapreduce-2454.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.mapreduce.TestJobMonitorAndPrint
                  org.apache.hadoop.mapred.TestClusterMRNotification

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3005//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3005//console

This message is automatically generated.","09/Nov/12 23:33;masokan;Hi Alejandro,
  I ran the tests on my box.  The failing tests are failing without my patch.  The failure does not seem to be related to my patch.

-- Asokan
","10/Nov/12 17:50;masokan;Hi Alejandro,
  I made the following changes:
* Removed some unused parameters from {{Shuffle}} constructor.
* Made some methods public in {{CombinerRunner}} which is defined in Task.java so that {{CombinerRunner}} can be reused from plug-in implementations.

Please provide your feedback.

Thanks.
-- Asokan","10/Nov/12 18:46;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12552985/mapreduce-2454.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.mapreduce.TestJobMonitorAndPrint

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3008//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3008//console

This message is automatically generated.","12/Nov/12 18:44;masokan;Hi Alejandro,
  I made minor changes in annotations.  Also, I am summarizing the visibility changes in the following table(Hope it is formatted correctly.  If not I will attach a separate file to this Jira):

||Class||Protection||Annotation||Reason||
|{{org.apache.hadoop.mapred.MapTask}}|Changed from package protected to public|Annotated as Public, Unstable|To be able to access inner interface {{PostMapProcessor}} and inner class {{MapOutputBuffer}} from external plug-ins.|
|{{org.apache.hadoop.mapred.MapTask.MapOutputCollector}}|Name changed to PostMapProcessor and protection changed from package protected to public|Annotated as Public, Unstable|To be able to implement external plug-ins.|
|{{org.apache.hadoop.mapred.MapTask.MapOutputBuffer}}|Changed to a static inner class and protection changed from package protected to public|Annotated as Public, Unstable|To be able to use the default implementation of {{PostMapProcessor}}.|
|{{org.apache.hadoop.mapred.SpillRecord}}|Changed from package protected to public|Annotated as Public, Unstable|To be able to write the intermediate output from {{PostMapProcessor}} plug-ins.|
|{{org.apache.hadoop.mapred.Task.CombinerRunner}}|Changed from protected to public|Changed from Private, Unstable to Public, Unstable|For code reuse to run the {{Combiner}} from external {{PostMapProcessor}} plug-ins.|
|{{org.apache.hadoop.mapred.Task.TaskReporter}}|Changed from protected to public|Changed from Private, Unstable to Public, Unstable|Has to be passed to {{CombinerRunner}} from external {{PostMapProcessor}} plug-ins.|
|{{org.apache.hadoop.mapreduce.task.reduce.ExceptionReporter}}|Changed from package protected to public|Annotated as Public, Unstable|To report exception to {{Shuffle}} from {{PreReduceProcessor.}}|
|{{org.apache.hadoop.mapreduce.task.reduce.MapHost}}|Changed from package protected to public|Annotated as Public, Unstable|Passed to {{shuffle()}} method in {{MapOutput.}}|
|{{org.apache.hadoop.mapreduce.task.reduce.MapOutput}}|Changed from package protected to public|Annotated as Public, Unstable|Returned from {{reserve()}} method in {{PreReduceProcessor.}}|
|{{org.apache.hadoop.mapreduce.task.reduce.MergeManager}}|No change|Changed from Private, Unstable to Public, Unstable|To reuse the default implementation of {{PreReduceProcessor.}}|
|{{org.apache.hadoop.mapreduce.task.reduce.Shuffle}}|No change|Changed from Private, Unstable to Public, Unstable|Passed to external {{PreReduceProcessor}} plug-ins.|
|{{org.apache.hadoop.mapreduce.task.reduce.ShuffleClientMetrics}}|Changed from package protected to public|Annotated as Public, Unstable|Passed to {{shuffle()}} method in external {{MapOutput.}}|

","12/Nov/12 19:46;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12553151/mapreduce-2454.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3017//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3017//console

This message is automatically generated.","12/Nov/12 21:42;tucu00;Asokan,

Thanks for patience working out the design and implementation of this JIRA.

The latest patch looks good and it addresses the concerns voiced regarding big changes that could destabilize the MR framework.

There are few minor NITs that should be addressed in the patch (at the end of this comment).

+1 after these NITs are addressed and jenkins test-patch OKs the new patch.

Patch NITs:

* PreReduceProcessor.java: unused import: Shuffle
* ReduceTasks.java: line 357, PreReduceProcessor merger = (PreReduceProcessor) ReflectionUtils.newInstance(..., no need for the ""(PreReduceProcessor)"" casting.
* Shuffle.java, unused imports: FileSystem, LocalDirAllocator, CompressionCodec, MapOutputFile, RawKeyValueIterator, Reducer, CombineOutputCollector
* TestLimitNQuery.java, unused imports: FileInputStream, FileNotFoundException, FileStatus, JobCounter, TaskAttemptID, ReflectionUtils
","12/Nov/12 22:00;masokan;Hi Alejandro,
  Thanks for the pointing out the nits.  I fixed all of them.  I am uploading a new patch.

-- Asokan","12/Nov/12 22:59;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12553200/mapreduce-2454.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3019//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3019//console

This message is automatically generated.","13/Nov/12 01:03;tucu00;+1","15/Nov/12 14:35;tucu00;I'm planning to commit this patch around mid day PST today.","15/Nov/12 14:57;acmurthy;Asokan - thanks again for being patient and working with me through this, and also for spending time with me in person on the design. 
I think this is very close! Thanks again!

Also, Alejandro, thanks for taking a look.

Some comments:
# All the apis should be marked 'LimitedPrivate' (not Public), 'Unstable' to make it clear that this is only for implementers.
# I'd rather keep the names as MapOutputBuffer or MapOutputSortingOutput rather than PostMapProcessor to make it clear that this is the sort buffer. Similarly, we should just call it ReduceInputMerger or some such?
# We shouldn't need to make TaskReporter public since we already have a public Reporter api, correct?
# Perhaps one of my biggest concerns is about MapOutput, I don't understand why it has a 'shuffle' method and shuffles the output itself - it is merely meant to be an abstraction of the output - can you pls help me understand this?
# Since you've already made SpillRecord (java) public, we don't have to move out IndexRecord outside?
# In general, it would be useful if you could not make formatting changes in a large patch to keep it smaller - I see it's gone from 60K or so 100+K again! :)

Thanks again.","16/Nov/12 03:16;masokan;Hi Arun,
  Thanks for your comments.  I have addressed each one of them below in the same order:

* I have modified all the annotations to LimitedPrivate instead of Public.
* Kept the name and moved {{MapOutputCollector}} to a separate file.  Also, renamed {{PreReduceProcessor}} to {{ReduceInputMerger.}}
* The class {{CombinerRunner}} expects a {{Task.TaskReporter}} object to be passed to the {{create()}} method.  {{CombinerRunner}} will be used by plugin implementations to run the {{Combiner.}}  Also, {{MapOutputBuffer}} expects {{TaskReporter}} not just {{Reporter.}}
* Currently, {{MapOutput}} has {{commit()}} and {{abort()}} methods for the shuffled data.  It is natural to have {{shuffle()}} in there too.  Besides, {{shuffle()}} will become polymorphic so that {{OnDiskMapOutput}}, {{InMemoryMapOutput}}, or plugin implementations can implement {{shuffle()}} differently.  Currently, there is an if-then check in Fetcher.java(to decide whether to shuffle to disk or memory) which I thought was not very clean.
* We need to make {{IndexRecord}} public.  Once we do that, Java compiler does not like two public classes at the same level in a single source.  The other possibility is to make {{IndexRecord}} as a static inner class of {{SpillRecord.}}  I tried to do that, but I got compilation errors from four other source and test files since I have to add an import statement widening the scope of the changes.
* The test I am contributing to the patch is making it look bigger.  The test has the implementation of a plugin that avoids sorting.  I am planning to contribute a more robust implementation of such a plugin separately for MAPREDUCE-4039 if no one else volunteers:)  As part of that, I can modify this test to make use of that plugin so that it will become much smaller.
To give you an idea of the breakdown of the patch, in addition to the overall patch file I am attaching multiple patch files each with the following items addressed:

** patch with only access protection and annotation changes(mapreduce-2454-protection-change.patch)
** patch with only refactored and modified code(mapreduce-2454-modified-code.patch)
** patch with only modified test(mapreduce-2454-modified-test.patch)
** patch with only new test added(mapreduce-2454-new-test.patch)

Once again thank you very much for allotting some time to look at the patch.

-- Asokan
","16/Nov/12 04:15;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12553731/mapreduce-2454.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.mapreduce.v2.TestMRJobsWithHistoryService

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3036//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3036//console

This message is automatically generated.","16/Nov/12 16:39;masokan;The failing test does not seem to be related to my patch.

I made one last annotation change(Task.TaskReporter was still marked as Public) and I am uploading a new patch file.
","16/Nov/12 17:35;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12553789/mapreduce-2454.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.mapreduce.v2.TestMRJobsWithHistoryService

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3038//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3038//console

This message is automatically generated.","16/Nov/12 17:39;masokan;This test failure is not related to my patch.

-- Asokan
","16/Nov/12 17:55;tucu00;+1. Asokan, thanks for following up and addressing Arun's comments. I think the patch is ready to be committed. I'll be committing later today.","16/Nov/12 18:20;acmurthy;Asokan, does the test fail without your patch?

Tucu - pls hold on, let me do a final review since I've spent a lot of time on this codeline for a long time now. Thanks.","16/Nov/12 18:35;acmurthy;Ok, I'm glad I caught this...

One issue is that you don't want to pass a shuffle down to the merger, it's the other way around - you want to pass a merger to the shuffle.

Also, I'm not wild about making the change to MapOutput to shuffle itself, like I said - it was initially designed as merely a 'struct'.

----

I've said this before on this jira, I'd really appreciate if you could break this apart into smaller chunks - it makes a reviewer's job much easier... for e.g. I missed the shuffle/merger change since it's a largish patch.

Can you pls create some natural sub-tasks:
# Move MapOutputCollector out as an interface into a new class outside of MapTask
# Introduce a new ReduceInputMerger or some such interface which is sufficient for your purposes
# Then mark all interfaces you need as 'LimitedPrivate' 

I appreciate your patience, thanks again.","16/Nov/12 18:39;masokan;Hi Arun,
   Yes, the test fails without my patch.  I confirmed it on my box.

Thanks.

-- Asokan","16/Nov/12 18:43;acmurthy;Again, I apologize for being very careful on this one... as you can imagine, this is a fairly core piece of MR and hence my careful consideration. Also, you could really help me out by breaking this apart into smaller chunks I can review/commit fast in an iterative manner. Thanks.","16/Nov/12 21:49;masokan;Hi Arun,
  Thanks for your comments.  Regarding your comment on passing shuffle to merge: I did this with the following rationale:

* Conceptually, merge can take its input from different types of sources; today we have two types: one is Shuffle and the other is from local files for a local job.  Tomorrow, we may add a hybrid of Shuffle and local map output files(this will involve adding another method in the ReduceInputMerger) to avoid shuffling local map outputs for optimizing the performance.  This new approach which decouples Shuffle and Merge is more flexible.
* In current implementation, Shuffle which is supposed to transfer bytes from map outputs to the merger, is also returning a RawKeyValueIterator which in turn implies it is doing more than transferring bytes.

-- Asokan
","17/Nov/12 05:51;acmurthy;It's conceivable that we may take several directions, including push shuffle etc., which will require a sorter and not a merger. 

IAC, it's too early to call - hence I'd prefer we keep current semantics since they don't make a big difference either way, correct? 

Thanks.","18/Nov/12 12:46;masokan;Hi Arun,
  I would like to make the following points:

* We talked about different processing that can happen before the {{Reducer.}}  Currently, we have a *merge*.  It can be a *sort* as you mentioned or a simple *copy* as well.  The *copy* case arises when one wants to avoid sorting that happens in the MR data flow.  It would enable hash based aggregation or join in the {{Reducer.}}
* Regardless of the processing done or whether shuffle is push or pull based, the processing should be in control of driving the processing not the shuffle.  This is not obvious for a *sort* or *merge*.  For a *copy*, it makes a big difference.
* For a *copy*, we want the {{Reducer}} to receive the <key, value> pairs as soon as data is shuffled(unlike *sort* or *merge* which has to wait until the last <key, value> pair is seen before outputting the first <key, value> pair.)  There is no need to spill data to disk on the reduce side.
* With the current arrangement where shuffle assumes that the processing(*merge*) can return a {{RawKeyValueIterator}} only at the end of shuffling, it is impossible to support *copy*.  There is inherent deadlock because *copy* wants to return the <key, value> pairs right away whereas shuffle
thinks that it can happen only at the end.
* The change I made is very simple.  It does not alter any semantics and it allows the processing to be a *copy* without any deadlock.  In fact, the test I created as part of this Jira does a simple *copy* before the {{Reducer.}}

I hope I clarified the reason for the change.

Thanks.

-- Asokan
","19/Nov/12 01:56;acmurthy;bq. With the current arrangement where shuffle assumes that the processing(merge) can return a RawKeyValueIterator only at the end of shuffling

That is not true at all. 

It's trivial to return an iterator from a copy-only shuffle which is backed by a blocking shuffle which waits till *any* (not *all*) key/value pairs have been shuffled over the network.

So, I don't think the change is necessary.","19/Nov/12 01:58;acmurthy;IAC, as I've asked many times before, can you pls upload the smaller patches to sub-tasks so we can keep discussions focussed on a single change rather than debate cross-cutting changes? This way I can review/commit simpler changes faster to progress rapidly? Thanks.","19/Nov/12 05:53;tucu00;Arun, 

Thanks for taking the time to help me review this JIRA, I really appreciate that. I also agree this JIRA is in the core of MR and merits for extra attention it got.

The current approach, which has been done following your recommendations, has been already thoroughly reviewed by you and myself. Plus the current size of the patch is approx 100K, with 33K of it in testcases, less than 15K of new code and the rest a straight forward refactoring of existing code. 

All the feedback you've provided has been integrated in the patch. 

There is only one discussion left, that you brought up, regarding 'passing a shuffle down to the merger'.

Because of this, I don't think it is necessary to break up this JIRA in sub JIRAs at this point.

Once we decide on this pending issue I want to commit this patch. Any further improvement/change/fix can be done incrementally with follow up JIRAs in the same way we work in the rest of Hadoop. 

Cheers
","19/Nov/12 18:32;tucu00;Now following up with Arun's concern on 'passing a shuffle down to the merger', after spending some extra time looking at the code with and without the patch.

I agree with Asokan's arguments on why the shuffle ought to be passed to the merger as in the latest patch. 

It is a clear separation of concerns, the shuffle only shuffles data without having to be aware of how that data is handled afterwards.

The change does not change the end behavior of the shuffle-merge phase, thus it does not break any existing MR application. Nor it can break any existing Hadoop plugin (as all this was hardcoded and it could not be replaced).

Also, the change does not preclude in the future implementing things like a push shuffle.

Regarding Arun's suggestion:

bq. It's trivial to return an iterator from a copy-only shuffle which is backed by a blocking shuffle which waits till any (not all) key/value pairs have been shuffled over the network.

This would require changes in the shuffle, which could significantly increase the scope of work of this JIRA. On the other hand, the latest patch does not modify the Shuffle.

My take here is along the lines of Arun's comment:

bq. I've spent sometime thinking about this - and I feel we can do something far simpler to address Syncsort's goal of plugging in your proprietary sort while mitigating risk to MR itself....How about this: I feel we could accomplish both goals by something very simple..

Echoing Arun, we are mitigating risk while enabling the desired functionality.





","19/Nov/12 19:27;acmurthy;{quote}
I agree with Asokan's arguments on why the shuffle ought to be passed to the merger as in the latest patch. 
{quote}

-1

As I've noted before we are making changes without actual necessity, while hurting future functionality such as reduce-side sort etc. by elevating 'merge' to be a primary operation. I don't see the need for this, I haven't seen an actual argument why it's required.

IAC, I'm frankly getting tired of arguing details on a 100k patch - please break this up so that I can review/commit them and we can actually have focussed discussions. Thanks.

","19/Nov/12 19:30;acmurthy;Ok, I've created the subtasks. Asokan - please attach the patches you already have over there.","19/Nov/12 23:03;tucu00;Asokan, in order to get things moving, would you mind uploading the broken down patches to the sub JIRAs?. I don't see any contention with the visibility and MAP side sub JIRAs, so those could go in right the way. Thx.","20/Nov/12 14:17;tucu00;Arun, just for the record, as I've told you over the phone I disagree with the reasons of your -1. But I'm Moving to subtasks to get this going.","20/Nov/12 15:36;masokan;Hi Arun & Alejandro,
  I will follow your suggestions and create the sub-patches.  I will post them as soon as I have them ready and tested.

Thanks.
-- Asokan
","20/Nov/12 15:54;masokan;Hi Arun,
  I just have one question for you.  You stated the following:
{quote}
It's trivial to return an iterator from a copy-only shuffle which is backed by a blocking shuffle which waits till any (not all) key/value pairs have been shuffled over the network.

So, I don't think the change is necessary.
{quote}
I am curious to know what you meant by this.  Can you please elaborate?

Thanks.
-- Asokan","20/Nov/12 17:33;acmurthy;Alejandro - you, or Asokan, haven't given a single reason *why* the change is required... I've asked for this multiple times. All I've heard is you disagree with my opinion, particularly after I've given you ideas to accomplish some fairly abstract goals in the future within the context of existing functionality. With due respect, it seems like we are falling into the trap of ignoring opinions from people who have spent a lot of time & effort maintaing the codebase without motivating changes sufficiently, especially when people have little context in the codebase they are suggesting major modifications in.  

IAC, let's continue this discussion on MAPREDUCE-4808.","24/Nov/12 10:24;lakshman;Asokan & Alejandro, iiuc the objective/goal of this feature is to make the Shuffler & Merger pluggable. So that, mapreduce user can plugin better algorithms.

We are in the process of implementing the ""Network Levitated Merge"" algorithm [ http://pasl.eng.auburn.edu/pubs/sc11-netlev.pdf ]. With this, we wanted to avoid the disk usage on reducer side completely and directly shuffle the map outputs to the reducer.

IMO, with MAPREDUCE-2454 and other sub-tasks as well, we may *not be able to plugin* the above algorithm.

Request you to provide your suggestions on this.","24/Nov/12 17:05;masokan;Hi Laxman,
  The way the Shuffle plugin is defined, it can complement the Merger plugin or it can act alone in order to generate the final merged output.

In the former case, the shuffle plugin can implement accelerated shuffling in software or in hardware+software combination.  You can still use a Merger plugin to do the merge.

In the latter case(this is what is referred as ""Network Levitated Merge"" in the paper you mention), the Shuffle plugin itself can do the shuffle+merge.  In this case, the Shuffle plugin can choose to ignore the Merger plugin.

In essence, with individual units pluggable you get more choices and not restrictions.

-- Asokan","24/Nov/12 19:01;lakshman;Asokan, thank you very much for your quick response and detailed clarification.
I will give a try with the patches available here. I will get back to you soon on this.","13/Dec/12 23:06;tucu00;I've asked Asokan for a preliminary patch with all the subtasks for MAPREDUCE-2454 and run gridmix both on trunk and on the patch in a 35 machines cluster. The trace had ~1000 jobs. I've done 2 runs with trunk and 2 runs with the patch.

TRUNK: 

Time spent in simulation: 43mins, 31sec
Time spent in simulation: 41mins, 28sec

MAPREDUCE-2454: 

Time spent in simulation: 42mins, 48sec
Time spent in simulation: 40mins, 8sec
","15/Dec/12 17:52;acmurthy;Thanks for the due diligence Tucu.

With MAPREDUCE-4807 and MAPREDUCE-4809 we have accomplished the original goals of this jira i.e. make the sorter pluggable.

With that let's close this my merging to trunk and unlink the other enhancements from this task, they are independent of the original goals of this and can be tackled as such.

Asokan - thanks for being patient and working with us. I'm glad that at the end we are able to support your original goals of allowing Syncsort to plugin the custom sort, while, simultaneously, keeping our long-term maintenance costs low. Thanks again.","15/Dec/12 20:30;tucu00;Great. I've just committed MAPREDUCE-4809 & MAPREDUCE-4807 to trunk. Thanks.","15/Dec/12 20:49;acmurthy;Thanks Tucu. Closing this.","17/Dec/12 23:14;masokan;Hi Alejandro & Arun,
  Thanks to both of you for committing this.

-- Asokan","19/Dec/12 02:25;jerrychenhf;Hi Asokan,
Thank you for your work on this. I am following the whole thread and one of the thing that I feel interested in is pluggable sort in reduce side. While this was discussed and was in the intial patches. It seems to me that it is not in the final patch as I didn't see any interfaces mentioned above added or modified in reduce side. 

As my understanding is that the current sort in map reduce is participating by both Map and Reduce. Does replacing only map side make any sense? 

- Haifeng","19/Dec/12 15:10;masokan;Hi Jerry,
  Thanks for your interest.  I will be continuing the work in MAPREDUCE-4808 to make the reduce side sort pluggable.  I already posted a patch there.  Please take a look at the interfaces defined in the patch.  If you have any comments, please post them in MAPREDUCE-4808.

-- Asokan
","24/Dec/12 02:55;jerrychenhf;Hi Asokan,
Thanks for your clarification and I realized now that there is MAPREDUCE-4808 there. ","14/Aug/15 12:05;bharatjha;Hello Mariappan Sir,
     I want to apply your mapreduce-2454.patch on hadoop and i want to check the performance. On which hadoop I should try on it. and what are the steps for it?


Thanks","18/Aug/15 14:30;masokan;Hi Bharat,
   This patch went into Hadoop release 2.0.3.  If you want to implement a plugin yourself, please first download the source of any version of Hadoop above 2.0.3.  You can start with the sample implementations in the tests that were created as part of the patch.

-- Asokan","28/Aug/15 11:02;bharatjha;Hello Mariappan Sir,

Thanks for the reply

  I installed hadoop-2.0.2 and hadoop-2.0.3 on my single system. I was checking the performance of hadoop-2.0.3  compared to hadoop-2.0.2.

As per tersort report hadoop-2.0.3 takes less cpu time 2000ms to 8000ms(on 10gb of data) than hadoop-2.0.2.  will be the performance of hadoop-2.0.3 will increase on the cluster compared to hadoop-2.0.2.

I am learning to configure multinode cluster .
","01/Sep/15 14:12;masokan;Hi Bharat,
   This patch only makes the sort pluggable.  It does not result in any performance improvement.  There could be other reasons why you may be noticing a performance difference.

-- Asokan"
Adding new target to build.xml to run test-core without compiling,MAPREDUCE-2183,12479711,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Won't Fix,,gcabrer,gcabrer,11/Nov/10 14:37,17/Mar/16 16:20,12/Jan/21 09:52,17/Mar/16 16:20,0.21.0,,,,,,,,build,,,,,,0,,,,,"While testing Apache Harmony Select (lightweight version of Harmony) with Hadoop mapreduce we had to first build with Harmony and then test using Harmony Select using the test-core target. This was done in an effort to investigate any issues with Harmony Select in running common. However, the test-core target also compiles the classes which we are unable to do with Harmony Select. A new target is proposed that only runs the tests without compiling them.","SLE v. 11, Apache Harmony 6",atm,aw,gkesavan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/10 14:38;gcabrer;MAPREDUCE-2183.patch;https://issues.apache.org/jira/secure/attachment/12459350/MAPREDUCE-2183.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2016-03-17 16:20:29.767,,,false,,,,,,,,,,,,,,,,,,150048,,,,,Thu Mar 17 16:20:29 UTC 2016,,,,,,,"0|i0jiev:",111913,,,,,,,,,,,testing,,,,,,,,,,"17/Mar/16 16:20;aw;stale issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repeatable Input File Format,MAPREDUCE-6453,12856656,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,AbdulRahman,AbdulRahman,AbdulRahman,17/Aug/15 16:59,17/Aug/15 16:59,12/Jan/21 09:52,,,,,,,,,,,,,,,,1,deeplearning,inputfileformat,newbie,repeatable,"We are interested in running the training process of deep learning architectures on Hadoop clusters. We developed an algorithm that can carry out this training process in a MapReduce fashion. However, there is still a problem that we can improve.

In deep learning, training data is usually repeated multiple times (10 or even more). However, we were not able to find a way to go through the input training file multiple times without having to reduce first and then go back and then map and reduce and so on so forth. So, to carry on the experiments, we were forced to phyiscally repeat the files 10 or 20 times. This is not the best solution, obviously, because first the file size is becoming much larger, and second, it is not a neat way to carry out the job.


Thus, what we aim to do is to create an interface that input file formats can implement that would provide them with the ability to repeat a file n times before eventually reducing, which will solve the problem and make Hadoop more suitable for the training of deep learning algorithms, or for such problems that require going over the data multiple times before reducing.",,AbdulRahman,cdouglas,jphegde,sprog,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2419200,2419200,,0%,2419200,2419200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2015-08-17 16:59:43.0,,,,,,,"0|i2j0gf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Be able to retrieve configuration keys by index,MAPREDUCE-5917,12718839,New Feature,Patch Available,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,,JoeM,JoeM,06/Jun/14 13:08,06/May/15 03:27,12/Jan/21 09:52,,,,,,,,,,pipes,,,,,,0,BB2015-05-TBR,,,,The pipes C++ side does not have a configuration key/value pair iterator.  It is useful to be able to iterate through all of the configuration keys without having to expose a C++ map iterator since that is specific to the JobConf internals.,,JoeM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/14 12:53;JoeM;MAPREDUCE-5917.patch;https://issues.apache.org/jira/secure/attachment/12652405/MAPREDUCE-5917.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2014-06-25 14:32:08.493,,,false,,,,,,,,,,,,,,,,,,397038,,,,,Tue Mar 10 07:20:14 UTC 2015,,,,,,,"0|i1wcsf:",397156,Add method to retrieve configuration key by index.,,,,,,,,,,,,,,,,,,,,"06/Jun/14 13:11;JoeM;Trunk patch that passes dev-support/test-patch and is ready for code review.

The patch adds two methods to the JobConf() class:

getCount() and getKey(size_t index).

This avoids having to expose an implementation specific map<string, string>::const_iterator.","25/Jun/14 12:53;JoeM;Update patch to be trunk root based.","25/Jun/14 14:32;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12652405/MAPREDUCE-5917.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4691//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4691//console

This message is automatically generated.","10/Mar/15 07:20;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12652405/MAPREDUCE-5917.patch
  against trunk revision 7711049.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5278//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5278//console

This message is automatically generated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support use of ephemeral client ports for shuffle/etc.,MAPREDUCE-6325,12822779,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,,qwertymaniac,qwertymaniac,21/Apr/15 20:02,21/Apr/15 20:03,12/Jan/21 09:52,,2.4.0,,,,,,,,applicationmaster,,,,,,0,,,,,"Forking out from MAPREDUCE-5036.

Please see comments https://issues.apache.org/jira/browse/MAPREDUCE-5036?focusedCommentId=14384339&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14384339 onwards for more discussion details that lead to this JIRA.",,brahmareddy,jlowe,Naganarasimha,qwertymaniac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2015-04-21 20:02:59.0,,,,,,,"0|i2dk5z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a plugin class for the TaskTracker to determine available slots,MAPREDUCE-1603,12459327,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Won't Fix,,stevel@apache.org,steve_l,16/Mar/10 22:00,23/Jul/14 22:11,12/Jan/21 09:52,23/Jul/14 22:11,0.22.0,,,,,,,,tasktracker,,,,,,0,,,,,"Currently the #of available map and reduce slots is determined by the configuration. MAPREDUCE-922 has proposed working things out automatically, but that is going to depend a lot on the specific tasks -hard to get right for everyone.

There is a Hadoop cluster near me that would like to use CPU time from other machines in the room, machines which cannot offer storage, but which will have spare CPU time when they aren't running code scheduled with a grid scheduler. The nodes could run a TT which would report a dynamic number of slots, the number depending upon the current grid workload. 

I propose we add a plugin point here, so that different people can develop plugin classes that determine the amount of available slots based on workload, RAM, CPU, power budget, thermal parameters, etc. Lots of space for customisation and improvement. And by having it as a plugin: people get to integrate with whatever datacentre schedulers they have without Hadoop itself needing to be altered: the base implementation would be as today: subtract the number of active map and reduce slots from the configured values, push that out. ",,aw,cdouglas,cutting,jaideep,jorda,macyang,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-03-16 22:05:51.321,,,false,,,,,,,,,,,,,,,,,,149634,,,,,Wed Jul 23 22:11:47 UTC 2014,,,,,,,"0|i0jh3z:",111702,,,,,,,,,,,,,,,,,,,,,"16/Mar/10 22:05;aw;I like this idea a lot.  It starts to crack open the door to much more advanced scheduling.  For example, it'd be great to be able to pass up to the scheduling system if a machine is 32-bit or 64-bit, Windows or some Unix flavor, etc.  This means at some other future point, jobs could request a particular environment. ","17/Mar/10 17:39;steve_l;-that would imply passing up machine metadata: cpu family/version, OS, etc. No reason why that couldn't be done, though you'd have to decide whether that is something you'd republish every heartbeat or just when the TT first registers. Of course, without the JT making decisions on where to route stuff based on those features, it's wasted effort. Which would imply you also need some plugin support for making the decisions as to where to run Mappers and Reducers; right now it's fairly straightforward: do it close to the data. ","01/May/11 11:31;eltonsky;I like the idea, but I don't think we can set the slots with hardware parameters, rather it's application dependant. For example, you have a Quad core cluster and a Dual core cluster. Both cluster have same disk and inter connection. When you run a ""Grep"", if you apply the same slot numbers on both cluster, I guess the processing times are similar. If you change you application to ""Sort"", still using same number of slots, then there could be noticeable difference. 

So I guess, to get a reasonable slots, we need to actually run the application. Somehow.","23/Jul/14 22:11;aw;Closing as won't fix.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The WebUI pages that display the list of map and reduce tasks should also include the hostname where the task is running,MAPREDUCE-446,12406669,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Fixed,,jothipn,jothipn,17/Oct/08 09:41,19/Jul/14 18:59,12/Jan/21 09:52,19/Jul/14 18:59,,,,,,,,,,,,,,,0,,,,,"Currently, the page that displays the list of map tasks shows ""Task, Complete, Status, Start time, End Time, Counters"". If this could show the hostname as well, it could be helpful in analyzing cases where tasks in a given set of nodes execute much slower than the rest.",,aw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-07-19 18:59:56.388,,,false,,,,,,,,,,,,,,,,,,148802,,,,,Sat Jul 19 18:59:56 UTC 2014,,,,,,,"0|i0iuyf:",108109,,,,,,,,,,,,,,,,,,,,,"17/Oct/08 10:16;jothipn;In the case where there are multiple attempts for a task, we could either one row for each attempt or provide a comma separated list of hostnames.","19/Jul/14 18:59;aw;Long ago fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Serialization for RecordIO,MAPREDUCE-447,12404616,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Invalid,,wyckoff,wyckoff,18/Sep/08 03:17,18/Jul/14 23:39,12/Jan/21 09:52,18/Jul/14 23:39,,,,,,,,,,,,,,,0,,,,,"Implement org.apache.hadoop.io.serialization.Serialization/Serializer/Deserializer interfaces

",,aw,cutting,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-376,,,,,,,,,,,,,,,,,,,,,"18/Sep/08 04:06;wyckoff;HADOOP-4199.0.txt;https://issues.apache.org/jira/secure/attachment/12390339/HADOOP-4199.0.txt","18/Sep/08 03:19;wyckoff;RecordIOSerialization.java;https://issues.apache.org/jira/secure/attachment/12390334/RecordIOSerialization.java",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2008-09-18 14:00:18.426,,,false,,,,,,,,,,,,,,,,,,148803,,,,,Fri Jul 18 23:39:37 UTC 2014,,,,,,,"0|i0iuq7:",108072,,,,,,,,,,,,,,,,,,,,,"18/Sep/08 03:19;wyckoff;This needs some cleaning up but unit tests pass. will add those and the cleaned up version soon.
","18/Sep/08 04:06;wyckoff;This is a full blown patch modeled after the ThriftSerialization code. 

No build file here as waiting for the ThriftSerialization to be committed to link with that build file.
","18/Sep/08 14:00;tomwhite;Since Record extends Writable, will WritableSerialization not work here?","18/Sep/08 17:36;wyckoff;yes, you are right - forgot a Record is a Writable. But, what about clearing the object?  How will that work for these?

Also, how will non-Binary streams work?  Admittedly, I didn't implement that, but we should support Csv and anything else Record supports??

","21/Sep/08 17:46;wyckoff;I guess this JIRA brings up the larger point that although a Serializer/Deserializer may be on something that extends Writable, you may still need more information that that?  i.e., Binary, CSV, .. for RecordIO.  This isn't a great example because admittedly CSV isn't a very useful format for long lived data.

So, maybe I should mark this as invalid? 

But, what if i did have data in CSV format? I could never get to it with the current Serialization framework and SequenceFileRecordReader - I would actually have to define my own SequenceFileRecordReader that knows not to add the default Writable Serialization implementation??

","18/Jul/14 23:39;aw;I'm going with Pete's suggestion and just marking this invalid.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskTracker's Memory resource should be considered when tasktracker asks for new task,MAPREDUCE-451,12402863,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Fixed,,buptzhugy,buptzhugy,22/Aug/08 10:42,18/Jul/14 22:23,12/Jan/21 09:52,18/Jul/14 22:23,,,,,,,,,,,,,,,0,,,,,"Currently, taskTracker only considers enough free disk space left when it asks for new task, memory resource should be considered too, or it may works badly in SMP environment.
",,aw,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2008-08-22 10:46:01.402,,,false,,,,,,,,,,,,,,,,,,148807,,,,,Fri Jul 18 22:23:43 UTC 2014,,,,,,,"0|i0iudj:",108015,,,,,,,,,,,,,,,,,,,,,"22/Aug/08 10:45;buptzhugy;    Currently, taskTracker only considers enough free disk space left when it asks for new task, memory resource should be considered too, or it may works badly in SMP environment. 

    Let's consider a scenario: one smp tasktracker with 8 cpu and 8 GB memory, and mapred.tasktracker.tasks.maximum=7. It has running 5 tasks while each task consumed 1.5GB memory, so it would asks for 2 new tasks which would make things go badly. 

    We should add free memory check just like 
         askForNewTask = enoughFreeMem(localMinMemStart);   in tasktracker.java just like 
         askForNewTask = enoughFreeSpace(localMinSpaceStart);

     I will attach a patch for this feature as soon as possible.

","22/Aug/08 10:46;vinodkv;Please see HADOOP-3759 and HADOOP-3581.","22/Aug/08 18:18;runping;
this is related to HADOOP-2676","01/Oct/08 14:06;ddas;Please mark it as a blocker for 0.19 if required","18/Jul/14 22:23;aw;Closing this as fixed, in particular the memory management added in the 0.20 time line.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
snapshot a map-reduce to DFS ... and restore,MAPREDUCE-443,12330365,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Duplicate,omalley,eric14,eric14,17/Mar/06 12:26,18/Jul/14 17:37,12/Jan/21 09:52,18/Jul/14 17:36,,,,,,,,,,,,,,,0,,,,,"The idea is to be able to issue a command to the job tracker that
will halt a map-reduce and archive it to a directory in such a way
that it can later be restarted.

We could also set a mode that would cause this to happen to a job
when it fails.  This would allow one to debug and restart a failing
job reasonably, which might be important, for long running jobs.  It
has certainly been important in similar systems I've seen before.  One 
could restart with a new jar or work bench a single failing map or reduce.
",,aw,qwertymaniac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-460,HADOOP-313,MAPREDUCE-452,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2006-03-21 02:53:55.0,,,false,,,,,,,,,,,,,,,,,,148799,,,,,Fri Jul 18 17:36:26 UTC 2014,,,,,,,"0|i0dxjb:",79363,,,,,,,,,,,,,,,,,,,,,"21/Mar/06 02:53;bpendleton;This would be very useful, although, it should be noted that snapshotting a job to DFS means that it will take as much extra space to store as the replication (well, replication+1) level. If you're running jobs that produce large intermediate results, then attempting to checkpoint with, say, the default 3x replication, requires 4 times as much space as the job would, otherwise. For no-side-effect jobs, perhaps the default should be to checkpoint but with replication of 1 (assuming per-file replication gets added to DFS), and just let lost blocks turn into lost tasks that just get re-run. Hadoop should minimize space usage wherever possible, if it's really going to scale up to huge workloads.","21/Mar/06 03:20;eric14;Good points.  Adding an option to specify replication level would be a good addition.  Some of this data will be automatically regeneratable, only meta-data may really need high level replication.","18/Jul/14 17:36;aw;Closing as a dupe of MAPREDUCE-457",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ability to pause/resume tasks,MAPREDUCE-457,12399544,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,md87,md87,md87,03/Jul/08 15:13,18/Jul/14 17:37,12/Jan/21 09:52,,,,,,,,,,,,,,,,1,,,,,"It would be nice to be able to pause (and subsequently resume) tasks that are currently running, in order to allow tasks from higher priority jobs to execute. At present it is quite easy for long-running tasks from low priority jobs to block a task from a newer high priority job, and there is no way to force the execution of the high priority task without killing the low priority jobs.",,acmurthy,cutting,fdesing,johanoskarsson,martind,matei@eecs.berkeley.edu,omalley,tomwhite,tucu00,vivekr,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-443,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/08 15:13;md87;hadoop-pausing.8.trunk.patch;https://issues.apache.org/jira/secure/attachment/12385987/hadoop-pausing.8.trunk.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2008-07-04 05:30:55.246,,,false,,,,,,,,,,,,,,,,,,148812,,,,,Tue Nov 11 00:41:28 UTC 2008,,,,,,,"0|i0itwn:",107939,,,,,,,,,,,,,,,,,,,,,"03/Jul/08 15:38;md87;I've started work on a way to do this. At the minute it allows reduce tasks to be manually paused and resumed from the command line. The paused/unpaused state of a task is included as a second boolean in the return value of the statusUpdate/ping methods of the umbilical protocol, which isn't particularly elegant but works well enough. Paused tasks sit in a sleep loop waiting to be unpaused (this obviously means that they're consuming resources while not actually doing anything useful — the ideal solution would be to store them somewhere and resume later, as described in HADOOP-91).

Suggestions for improvements/comments welcome...","04/Jul/08 05:30;amar_kamat;Chris,
We been thinking on this for sometime. This problem will be more visible once the new scheduler comes in since that will have the pre-emption feature. 

From our offline discussions it makes more sense to _suspend/resume_ *reduce* tasks. Since on an average the reducers run for longer time and mostly determine the job runtime. Its also easier to suspend reducers as one can always save the shuffled data and restart the _REDUCE_ phase. Saving shuffle data might be a huge gain but again there are issues with resources getting wasted and clean-up. 

With maps its difficult since mostly the maps run faster and the maps have just one phase i.e the MAP phase. When the map task runs following are the things that determines its state
1) The offset in the input that is read
2) The mapped <k,v> in the memory
3) The data spilled to disk
4) External connections 
One could probably optimise by using what is already spilled and move to the offset on restart/resume but its not clear how much gain this will give and if at all there are any use-cases that strongly demand this. 

Holding tasks in memory (i.e the pause) might not be scalable. Hence we should think on suspend-to-fs/resume. As rightly pointed out by Vivek (offline) that its not guaranteed that the job/org will get the same set of nodes back and hence saving this state on disk might not make sense. Saving to dfs will be a huge hit. Thoughts? Comments?","04/Jul/08 06:13;omalley;I don't think keeping the tasks in memory is feasible. They would at the very least need to be written to disk, and even that would be hard given the number of threads that we use in the framework to increase parallelism. In the current framework, you might make a command that lowers the priority of a job and kills any task that is still running N minutes later. That would be easy to do and have the right effect, wouldn't it?","04/Jul/08 09:52;md87;I agree that suspending/resuming reducers makes more sense than map tasks. I've only implemented the pause logic in reducers at the minute, and (assuming that it works well enough) I doubt there will be any need to add it to mappers.

As for keeping tasks in memory, it's obviously nowhere near ideal, but I view this more as an interim solution until HADOOP-91 is resolved; I'm thinking that with a bit of end-user diligence (ensuring that all high(er) priority jobs were queued before the low(er) one is paused) we could probably get away with having at most one paused task per node.  Whether or not that will be problematic wrt memory usage obviously depends on the tasks/hardware/config, and is something I'll have to look at. If keeping them in memory proves not to be feasible then I'll obviously have to concentrate on suspending to the [d]fs.

Owen: unfortunately a command that results in tasks being killed wouldn't really have the effect I'm after; we don't want to waste the work that's been done by the (possibly long-running) lower-priority tasks. At present the tasks are killed manually to make way for the high-priority job — the idea of pausing is to be able to preserve the progress that's already been made on those tasks until the high-pri job is out of the way.","04/Jul/08 10:11;amar_kamat;bq. In the current framework, you might make a command that lowers the priority of a job and kills any task that is still running N minutes later. That would be easy to do and have the right effect, wouldn't it?
Wouldn't HADOOP-3444 introduce this (pre-emption)? Unless we decide to implement pre-emption earlier and modify it to cater the scheduling needs.","14/Jul/08 15:13;md87;Attached a patch of my current progress on this issue. It defines a new job priority ({PAUSED}), which prevents new reducers from being started, and pauses existing reduces. You can also pause individual (reduce) tasks via the command line or web ui. Paused tasks (from non-paused jobs) are resumed when their tracker requests new work and there are no higher-priority tasks waiting.

The communication between the TaskTracker and in-progress tasks works by replacing the boolean response to ping/updateStatus in the TaskUmbilicalProtocol with a TaskPingResponse object which specifies both whether the task is known by the tracker, and whether it is paused or not. Once a task is paused, it sits in a sleep loop waiting to be unpaused.

As previously mentioned, paused tasks are kept in memory, so there's an obvious limit on how much you can pause. We're currently testing the patch on a cluster to see whether or not this is problematic in practice.

Comments/suggestions welcome!","14/Jul/08 16:43;ab;I think it would be good to add to the Task interface something like onPause() / onUnpause() methods. This way map / reduce tasks to be paused could prepare for this event (e.g. close DB conections or close the side-effect files), and similarly restore their state on un-pause.","06/Oct/08 21:50;vivekr;I'm wondering if you really need this feature, now that we have pluggable schedulers (HADOOP-3412, HADOOP-3444, HADOOP-3746). Schedulers decide what is the best task to run, given priorities and resources, and other constraints. If you're concerned about low-priority jobs blocking resources for higher-priority jobs that are submitted later, you may want to look at one of these schedulers, or write your own that deals with the situation. Granted that you still won't be able to pause tasks, but you may not need to do that if your scheduler works correctly. It may start running a task from a low-running job, and then assign a task from the higher-priority job to the next available TT. Yes, you can reach a situation where tasks from the lower-priority job are consuming all resources and all long-running, so the higher-priority job is waiting,but there are ways to counter that. HADOOP-3444 provides capacities and user limits, along with preemption, to prevent one user or one job from taking over the system. 

I think supporting pausing tasks is non-trivial so you want to make sure there is a real need for it. ","11/Nov/08 00:41;matei@eecs.berkeley.edu;The really hard challenge with pausing, in my opinion, will be how to decide when to resume the tasks or when to kill them. It's not clear that if you pause a task on some machine, you'll get the opportunity to run it again. In fact, maybe another machine becomes free and you'd be better off running the task on that one. So the whole scheduling problem becomes more difficult.

Another fix that we really have to strive for is making reduces smaller, e.g. by separating the copy phase into its own set of tasks (Joydeep has posted some comments on this in the MapReduce 2.0 discussion).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Facility to connect back to a job from the command line. ,MAPREDUCE-448,12392403,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Won't Fix,,amar_kamat,amar_kamat,27/Mar/08 04:52,17/Jul/14 21:49,12/Jan/21 09:52,17/Jul/14 21:49,,,,,,,,,,,,,,,0,,,,,"There should a facility to connect back to a job from the command line. Once the job is fired from the client and say for some reason the connection is lost, there is no way to connect back for tracking the job's progress from the command line. Something like 'bin/hadoop job -connect <job-id>'. ",,aw,tnelson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2008-04-11 10:01:14.731,,,false,,,,,,,,,,,,,,,,,,148804,,,,,Thu Jul 17 21:49:02 UTC 2014,,,,,,,"0|i0itbr:",107845,,,,,,,,,,,,,,,,,,,,,"11/Apr/08 10:01;omalley;I'd suggest using -track instead of connect, because you really aren't connecting as much as wanting to track the status of the job.","11/Apr/08 10:10;amar_kamat;_-resume_?","17/Jul/14 21:49;aw;Closing this as won't fix. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop needs a better XML Input,MAPREDUCE-455,12384739,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,,karlunho,karlunho,17/Dec/07 06:35,17/Jul/14 19:14,12/Jan/21 09:52,,,,,,,,,,,,,,,,2,,,,,Hadoop does not have a good XML parser for XML input. The XML parser in the streaming class is fairly difficult to work with and doesn't have proper test cases around it.,,aw,hammer,johanoskarsson,karlunho,lucat,sekikn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/07 07:12;karlunho;HADOOP-2439Patch.patch;https://issues.apache.org/jira/secure/attachment/12371780/HADOOP-2439Patch.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2007-12-17 18:57:01.972,,,false,,,,,,,,,,,,,,,,,,96873,,,,,Thu Jul 17 19:14:32 UTC 2014,,,,,,,"0|i0issf:",107758,,,,,,,,,,,,,,,,,,,,,"17/Dec/07 07:12;karlunho;This patch is for adding XML input support to hadoop. This patch does rely on J2EE 5 libraries, namely the libraries required for StAX parsing. 

The libraries it depends are are:
appserv-ws.jar
javaee.jar
webservices-rt.jar
webservices-tools.jar

I've attached the jar files seperately","17/Dec/07 07:16;karlunho;JAR files for StAX parsing","17/Dec/07 18:57;cutting;What license are these jar files distributed under?

","18/Dec/07 06:52;karlunho;I believe under GPL - http://java.sun.com/javaee/downloads/licenses/JavaEE5SDKU3.thirdpartylicensereadme.txt

","18/Dec/07 06:59;karlunho;The two jars webservices-rt.jar and webservices-tools.jar seems to be too big. I'm not sure what is the best way to get these libraries in. Please note that these libraries are part of the J2SE.1.6 distro.","18/Dec/07 07:08;jimwhite;As Doug will say, GPL licensed material cannot be hosted here.  But I doubt those JARs are necessarily GPL.  They are probably CDDL because I think they are from Glassfish.

https://glassfish.dev.java.net/public/faq/GF_FAQ_2.html#terms

The java.net JAX-WS RI is dual licensed CDDL and GPL:

https://jax-ws.dev.java.net/

It might also be Sun's Binary License:

https://sjsxp.dev.java.net/

In any case you'll need to identify exactly where these files are coming from and which license is applicable.  If it is not an Apache compatible license (such as GPL) then the build will have to download them separately.

Note that redistributing items less than the entire distro is prohibited by the J2SE license.

http://java.sun.com/javase/6/jdk-6u3-license.txt","18/Dec/07 17:29;cutting;Apache's policy for 3rd party libraries is stated here:

http://people.apache.org/~cliffs/3party.html

In particular, redistribution of GPL libraries is prohibited, while CDDL libraries are permitted in binary form only.

If these are included in Java 6, perhaps we should make this issue depend on HADOOP-2325 (upgrading Hadoop to require Java 6).
","18/Dec/07 18:07;karlunho;I believe James is right. J2EE 5 is basically bundled with the Glassfish Application server. This means that all the binaries are under CDDL. Regarding upgrading to Java 6, I personally haven't upgraded yet because there isn't any support for MacOSX. ","30/Jan/09 21:34;milindb;Now that Hadoop requires java 6, can this be included ?","30/Jan/09 22:45;karlunho;I'd like to. Doug - who should I ask to get it merged in ? I've already uploaded the patch, and should probably run a test to make sure it works with Java 6.","01/Feb/09 01:08;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12371782/javaee.jar
  against trunk revision 739416.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3787/console

This message is automatically generated.","09/Feb/09 16:46;tomwhite;I think the Hudson run failed because it tried to use the latest attachment, which in this case is a jar file, rather than the patch file. Also, I couldn't get the patch to apply. Could you regenerate it please?","09/Feb/09 17:56;karlunho;Sure. Its been a while since I've worked on the system. Do you want me to apply the patch to the trunk ?","13/Feb/09 21:35;tomwhite;Yes please.

BTW, how does this code compare with the approach described at http://www.nabble.com/Re%3A-map-reduce-function-on-xml-string-p15835195.html?","14/Feb/09 01:15;karlunho;The approach described doesn't use a StaX parser, and probably isn't going to be as robust to failure or as extensible as using a StaX parser. If you look at the code, my patch allows you to specify the XML element name, ""namespace_prefix"", and namespace_URI when identifying the correct tag.  My patch also makes it easier to massage the XML too when reading in data.

Initially when I tried to create a XML parser, I tried to hack something up like the previous approach described. But after trying to parse real-world data (e.g. a dump of wikipedia), I threw up my arms and decided to use a proper pull-parser.

","17/Jul/14 19:14;aw;Is it time to revisit this?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tasktracker checkpointing capability,MAPREDUCE-452,12370011,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Fixed,,wyckoff,wyckoff,23/May/07 00:24,17/Jul/14 17:12,12/Jan/21 09:52,17/Jul/14 17:12,,,,,,,,,,,,,,,0,,,,,"This relates to allowing a resource manager (e.g., hadoop on demand) to grow and (rarely) shrink jobs on the fly.

Growing is already supported. Shrinking could be done in 2 ways - (1) consider the machine dead and allow speculative execution to take care of it or (2) moving the existing map outputs from that machine somewhere else (another machine, dfs) - ""task tracker checkpointing"" 

In the case of IO only intensive jobs,  checkpointing the tasktracker doesn't do much for you.  But, in the case of CPU or other scarce resource (e.g., a DB or Webpage cache...), the checkpointing could be very useful.  The question is how often is this the case and how useful?





",,aw,eric14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2007-05-25 17:44:56.634,,,false,,,,,,,,,,,,,,,,,,148808,,,,,Thu Jul 17 17:12:19 UTC 2014,,,,,,,"0|i0is27:",107640,,,,,,,,,,,,,,,,,,,,,"25/May/07 17:44;eric14;The ability to save a task trackers state into HDFS is needed to usefully implement CPU intensive elastic jobs.  We need those.  Also jobs with side effects may be able to depend on speculative execution.

Finally, the ability to save a task trackers state would allow us to address HADOOP-91, which suggests we should be able to suspend a job.","17/Jul/14 17:12;aw;Marking this as fixed since YARN provides this capability.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable regular expression in the DistCp input,MAPREDUCE-5851,12709591,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,bhrar,bhrar,bhrar,21/Apr/14 18:09,13/May/14 09:35,12/Jan/21 09:52,,,,,,,,,,distcp,,,,,,0,distcp,expression,regular,,"DistCp doesn't support regular expression as the input. If the files to copy are in the different locations, it is quite verbose to put a long list of inputs in the command. ",,anthonyr,avinz50,bhrar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-05-13 09:35:02.56,,,false,,,,,,,,,,,,,,,,,,387913,,,,,Tue May 13 09:35:02 UTC 2014,,,,,,,"0|i1uto7:",388174,,,,,,,,,,,,,,,,,,,,,"13/May/14 09:35;avinz50;i can see many parameters in Distcp class. in which parameter do we need to enable regular expressions?

private static final String usage = NAME
      + "" [OPTIONS] <srcurl>* <desturl>"" +
      ""\n\nOPTIONS:"" +
      ""\n-p[rbugp]              Preserve status"" +
      ""\n                       r: replication number"" +
      ""\n                       b: block size"" +
      ""\n                       u: user"" +
      ""\n                       g: group"" +
      ""\n                       p: permission"" +
      ""\n                       -p alone is equivalent to -prbugp"" +
      ""\n-i                     Ignore failures"" +
      ""\n-log <logdir>          Write logs to <logdir>"" +
      ""\n-m <num_maps>          Maximum number of simultaneous copies"" +
      ""\n-overwrite             Overwrite destination"" +
      ""\n-update                Overwrite if src size different from dst size"" +
      ""\n-f <urilist_uri>       Use list at <urilist_uri> as src list"" +
      ""\n-filelimit <n>         Limit the total number of files to be <= n"" +
      ""\n-sizelimit <n>         Limit the total size to be <= n bytes"" +
      ""\n-delete                Delete the files existing in the dst but not in src"" +
      ""\n-mapredSslConf <f>     Filename of SSL configuration for mapper task"" +
     
      ""\n\nNOTE 1: if -overwrite or -update are set, each source URI is "" +
      ""\n      interpreted as an isomorphic update to an existing directory."" +
      ""\nFor example:"" +
      ""\nhadoop "" + NAME + "" -p -update \""hdfs://A:8020/user/foo/bar\"" "" +
      ""\""hdfs://B:8020/user/foo/baz\""\n"" +
      ""\n     would update all descendants of 'baz' also in 'bar'; it would "" +
      ""\n     *not* update /user/foo/baz/bar"" +

      ""\n\nNOTE 2: The parameter <n> in -filelimit and -sizelimit can be "" +
      ""\n     specified with symbolic representation.  For examples,"" +
      ""\n       1230k = 1230 * 1024 = 1259520"" +
      ""\n       891g = 891 * 1024^3 = 956703965184"" +
     
      ""\n"";",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create counters that count how many bytes of data were read from a local node and a local rack,MAPREDUCE-5881,12712802,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,,kawaa,kawaa,07/May/14 10:14,07/May/14 10:16,12/Jan/21 09:52,,,,,,,,,,mrv2,,,,,,0,,,,,"MapReduce provides the counters ""org.apache.hadoop.mapreduce.JobCounter DATA_LOCAL_MAPS"" and ""org.apache.hadoop.mapreduce.JobCounter RACK_LOCAL_MAPS"" that count how many local map tasks were started. This works great, if we have one HDFS block per a input split.

If we have multiple HDFS blocks in an input split (e.g. non-splittable file such as gzip, or use of CombineFileInputFormat), then these counters become inaccurate.

If it has not been proposed yet, I propose adding new counters that count how many bytes of data were read from a local node and a local rack. Thanks to them, a user will see the real data locality ratio.",,kawaa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,391118,,,,,2014-05-07 10:14:50.0,,,,,,,"0|i1vd3b:",391340,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update NativeS3FileSystem to issue copy commands for files with in a directory with a configurable number of threads,MAPREDUCE-5872,12711743,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Invalid,ted.m,ted.m,ted.m,01/May/14 17:31,01/May/14 17:32,12/Jan/21 09:52,01/May/14 17:32,,,,,,,,,performance,,,,,,0,performance,,,,"In NativeS3FileSystem if you do a copy of a directory it will copy all the files to the new location, but it will do this with one thread.  Code is below.  This jira will allow a configurable number of threads to be used to issue the copy commands to S3.

do {
        PartialListing listing = store.list(srcKey, S3_MAX_LISTING_LENGTH, priorLastKey, true);
        for (FileMetadata file : listing.getFiles()) {
          keysToDelete.add(file.getKey());
          store.copy(file.getKey(), dstKey + file.getKey().substring(srcKey.length()));
        }
        priorLastKey = listing.getPriorLastKey();
      } while (priorLastKey != null);",,ted.m,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,390064,,,,,2014-05-01 17:31:52.0,,,,,,,"0|i1v6r3:",390302,Needs to be a Hadoop Jira not a MapReduce Jira,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Off-the-shelf funtionality for processing of Image, Video, Audio files",MAPREDUCE-5615,12678177,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,rekhajoshm,rekhajoshm,rekhajoshm,08/Nov/13 08:41,22/Nov/13 07:50,12/Jan/21 09:52,,2.2.0,,,,,,,,client,,,,,,0,,,,,"The FileInputFormat,FileOutputFormat, FileRecordReader, FileRecordWriter 
are not ideal for processing of media data types.
Believe most use SequenceFile and repeat their own set of media transcoding /processing to enable big data processing for them.
There might be few non open source implementations to handle image, video, audio processing.Maybe MapR has some impl.

Suggest we have standard off-the-shelf functionality to handle these kind of input datatypes - 
ImageInputFormat, ImageOutputFormat, ImageRecordReader, ImageRecordWriter
VideoFileInputFormat,VideoFileOutputFormat, VideoRecordReader,VideoRecordWriter
AudioFileInputFormat,AudioFileOutputFormat, AudioRecordReader, AudioRecordWriter

",,jghoman,rekhajoshm,stevel@apache.org,yvesbastos,zjshen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-11-08 09:35:39.241,,,false,,,,,,,,,,,,,,,,,,357552,,,,,Fri Nov 08 09:35:39 UTC 2013,,,,,,,"0|i1pmzr:",357842,,,,,,,,,,,,,,,,,,,,,"08/Nov/13 09:35;stevel@apache.org;This could be good, but it may be better outside the core hadoop project simply because it can evolve and release on a separate timetable. If so, where should it go? Mahout isn't quite the right place",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add tag info to JH files,MAPREDUCE-5129,12640881,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Fixed,billie,billie,billie,04/Apr/13 22:11,15/May/13 05:15,12/Jan/21 09:52,06/Apr/13 05:36,,,,,,1.2.0,2.1.0-beta,,,,,,,,0,,,,,It will be useful to add tags to the existing workflow info logged by JH.  This will allow jobs to be filtered/grouped for analysis more easily.,,acmurthy,billie,hudson,mattf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/13 01:17;billie;MAPREDUCE-5129-branch-1.patch;https://issues.apache.org/jira/secure/attachment/12577341/MAPREDUCE-5129-branch-1.patch","06/Apr/13 03:15;acmurthy;MAPREDUCE-5129.patch;https://issues.apache.org/jira/secure/attachment/12577350/MAPREDUCE-5129.patch","06/Apr/13 01:17;billie;MAPREDUCE-5129.patch;https://issues.apache.org/jira/secure/attachment/12577340/MAPREDUCE-5129.patch","04/Apr/13 22:13;billie;MAPREDUCE-5129.patch;https://issues.apache.org/jira/secure/attachment/12577084/MAPREDUCE-5129.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2013-04-05 18:01:40.553,,,false,,,,,,,,,,,,,,,,,,321340,,,,,Wed May 15 05:15:49 UTC 2013,,,,,,,"0|i1jfzz:",321685,,,,,,,,,,,,,,,,,,,,,"05/Apr/13 18:01;acmurthy;Billie, the patch looks good. Couple of things - can you pls port this to trunk and a unit test? Thanks!","06/Apr/13 01:17;billie;Here are new patches with tests.","06/Apr/13 03:11;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12577341/MAPREDUCE-5129-branch-1.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3507//console

This message is automatically generated.","06/Apr/13 03:15;acmurthy;Re-attaching same patch again for hudson.","06/Apr/13 03:41;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12577350/MAPREDUCE-5129.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-tools/hadoop-rumen.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3508//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3508//console

This message is automatically generated.","06/Apr/13 05:36;acmurthy;+1. I just committed this, thanks Billie!","06/Apr/13 21:06;hudson;Integrated in Hadoop-trunk-Commit #3573 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3573/])
    MAPREDUCE-5129. Allow tags to JobHistory for deeper analytics. Contributed by Billie Rinaldi. (Revision 1465188)

     Result = SUCCESS
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1465188
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/JobImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestJobImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/avro/Events.avpr
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobSubmittedEvent.java
* /hadoop/common/trunk/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/Job20LineHistoryEventEmitter.java
","07/Apr/13 10:57;hudson;Integrated in Hadoop-Yarn-trunk #176 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/176/])
    MAPREDUCE-5129. Allow tags to JobHistory for deeper analytics. Contributed by Billie Rinaldi. (Revision 1465188)

     Result = SUCCESS
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1465188
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/JobImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestJobImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/avro/Events.avpr
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobSubmittedEvent.java
* /hadoop/common/trunk/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/Job20LineHistoryEventEmitter.java
","07/Apr/13 13:11;hudson;Integrated in Hadoop-Hdfs-trunk #1365 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1365/])
    MAPREDUCE-5129. Allow tags to JobHistory for deeper analytics. Contributed by Billie Rinaldi. (Revision 1465188)

     Result = FAILURE
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1465188
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/JobImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestJobImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/avro/Events.avpr
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobSubmittedEvent.java
* /hadoop/common/trunk/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/Job20LineHistoryEventEmitter.java
","07/Apr/13 14:06;hudson;Integrated in Hadoop-Mapreduce-trunk #1392 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1392/])
    MAPREDUCE-5129. Allow tags to JobHistory for deeper analytics. Contributed by Billie Rinaldi. (Revision 1465188)

     Result = SUCCESS
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1465188
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/JobImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestJobImpl.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/avro/Events.avpr
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRJobConfig.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobSubmittedEvent.java
* /hadoop/common/trunk/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/Job20LineHistoryEventEmitter.java
","15/May/13 05:15;mattf;Closed upon release of Hadoop 1.2.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backport YARN-40 to support listClusterNodes and printNodeStatus in command line tool,MAPREDUCE-4944,12627787,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,,decster,decster,16/Jan/13 08:06,14/May/13 05:14,12/Jan/21 09:52,,1.1.1,,,,,,,,,,,,,,0,,,,," support listClusterNodes and printNodeStatus in command line tool is useful for admin to create certain automation tools, this can also used by MAPREDUCE-4900 to get TaskTracker name so can set TT's slot dynamically",,decster,junping_du,mattf,tgraves,vicaya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-4900,YARN-40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-01-18 07:56:59.74,,,false,,,,,,,,,,,,,,,,,,304575,Incompatible change,,,,Tue May 14 05:14:44 UTC 2013,,,,,,,"0|i17ngn:",252770,,,,,,,,,,,,,1.3.0,,,,,,,,"18/Jan/13 07:26;decster;I look into the code and find some issue of this backport:
hadoop-1.x have similar command -list-active-trackers and -list-blacklisted-trackers, which just print trackerNames, they use JobSubmissionProtocol to talk to JobTracker, and their is no more information we can expose expect adding another protocol method into JobSubmissionProtocol, this wil break compatibility which I think is unacceptable for JobSubmissionProtocol, which is used by normal clients. 
Another option is add this to AdminOperationsProtocol(like MAPREDUCE-4900), it is a admin protocol, which I think has less compatibility requirement, still I don't think it's worth to break compatibility.
Another option is JMX, I haven't find out the proper way to write a command line tool using JMX, which needs to connect to some specific port in JobTracker host, this information is hard to get, because it is not in mapred-site.xml but depending some jvm config in hadoop-env.sh.
","18/Jan/13 07:56;junping_du;I prefer to break AdminOpProtocol if we have to break compatibility. But another option could be add another protocol for slot get/set? I think manipulate map/red slots on TT is a very useful feature especially in sharing environment (like HBase region server live with TT), so may be deserved to add another protocol?","01/Apr/13 09:19;vicaya;We don't need to use JMX-RMI directly, rather a JMX REST API. Either /jmx or /jolokia (after HADOOP-9160) would work with regular http port from the configs. The legacy /jmx already have these information.","14/May/13 05:14;mattf;Changed Target Version to 1.3.0 upon release of 1.2.0. Please change to 1.2.1 if you intend to submit a fix for branch-1.2.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cartesian product file split,MAPREDUCE-2070,12474227,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,,pburkhardt,pburkhardt,15/Sep/10 23:04,02/May/13 02:29,12/Jan/21 09:52,,0.22.0,,,,,,,,,,,,,,0,,,,,"Generates a Cartesian product of file pairs from two directory inputs and enables a RecordReader to optimally read the split in tuple order, eliminating extraneous read operations.

The new InputFormat generates a split comprised of file combinations as tuples. The size of the split is configurable. A RecordReader employs the convenience class, CartesianProductFileSplitReader, to generate file pairs in tuple ordering. The actual read operations are delegated to the RecordReader which must implement the CartesianProductTupleReader interface. An implementor of a RecordReader can perform file manipulations without restriction and also benefit from the optimization of tuple ordering.

In the Cartesian product of two sets with cardinalities, X and Y, each element x in {X } need only be referenced once, saving X(Y-1) references of the elements. If the Cartesian product is split into subsets of size N there are then X(Y/N) instead of XY references for a difference of XY(N-1)/N. Suppose each x is equal in size, s, this would save reading sXY(N-1)/N bytes.",,jghoman,pburkhardt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HDFS-1402,,,,"15/Sep/10 23:06;pburkhardt;MAPREDUCE-2070;https://issues.apache.org/jira/secure/attachment/12454718/MAPREDUCE-2070",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,149972,,,,,Fri Sep 17 16:11:37 UTC 2010,,,,,,,"0|i0ji3z:",111864,,,,,,,,,,,,,,,,,,,,,"15/Sep/10 23:06;pburkhardt;Patched against the trunk.","15/Sep/10 23:18;pburkhardt;Depends on HDFS-1402 to return the host locations. Alternatively, it can be modified to depend instead on HDFS-202.","17/Sep/10 16:11;pburkhardt;An example RecordReader that concatenates file pairs from the Cartesian product is as follows:

{code}
public class CPRecordReader
implements RecordReader<Text, BytesWritable>, CartesianProductTupleReader {
  private JobConf job = null;
  private long pos = 0;
  private long totalLength = 0;
  private byte[] keyContent = null;
  private byte[] valueContent = null;
  private String keyName = null;
  private String valueName = null;
  private String DELIMITER = null;
  private CartesianProductFileSplit split = null;
  private CartesianProductFileSplitReader reader = null;

  public CPRecordReader(JobConf job, CartesianProductFileSplit split)	
  throws IOException {
    this.job = job;
    this.split = split;
    this.totalLength = split.getLength();
    this.DELIMITER = job.get(""mapred.input.format.delimiter"");
    reader = new CartesianProductFileSplitReader(split, this);
  }

  @Override
  public boolean next(Text key, BytesWritable value) throws IOException {
    if (reader.next()) {
      setKey(key);
      setValue(value);
      pos += valueContent.length;
      return true;
    } else {
      return false;
    }
  }

  @Override
  public void readKey(Path p) throws IOException {
    long length = split.getLength(p);
    keyName = p.toString();
    keyContent = new byte[(int)length];
    FSDataInputStream stream = p.getFileSystem(job).open(p);
    stream.readFully(keyContent, 0, (int)length);
  }
  
  @Override
  public void readValue(Path p) throws IOException {
    long length = split.getLength(p);
    valueName = p.toString();
    valueContent = new byte[(int)(keyContent.length + length)];
    System.arraycopy(keyContent, 0, valueContent, 0, keyContent.length);
    FSDataInputStream stream = p.getFileSystem(job).open(p);
    stream.readFully(0, valueContent, (int)keyContent.length, (int)length);
  }

  @Override
  public void close() throws IOException {
    return;
  }

  @Override
  public Text createKey() {
    return new Text();
  }

  @Override
  public BytesWritable createValue() {
    return new BytesWritable();
  }

  @Override
  public long getPos() throws IOException {
    return pos;
  }

  @Override
  public long getPos() throws IOException {
    return pos;
  }

  @Override
  public float getProgress() throws IOException {
    return ((float) getPos()) / totalLength;
  }

  private void setKey(Text key) throws IOException {
    long length = valueContent.length - keyContent.length;
    String keystring = new String();
    keystring += keyName + "":"" + keyContent.length;
    keystring += DELIMITER;
    keystring += valueName + "":"" + length;
    key.set(keystring);
  }
  
  private void setValue(BytesWritable value) throws IOException {
    value.set(valueContent, 0, valueContent.length);
  }
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Building blocks for the  herriot test cases ,MAPREDUCE-1758,12463865,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,balajirg,balajirg,balajirg,06/May/10 11:15,02/May/13 02:29,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"There is so much commonality in the test cases that we are writing, so it is pertinent to create reusable code. The common methods will be added to herriot framework. ",,balajirg,cos,hammer,iyappans,vinaythota,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-1676,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-1710,MAPREDUCE-1676,MAPREDUCE-1654,,"06/May/10 11:18;balajirg;bb_patch.txt;https://issues.apache.org/jira/secure/attachment/12443850/bb_patch.txt","28/Jun/10 11:31;balajirg;bb_patch_1.txt;https://issues.apache.org/jira/secure/attachment/12448200/bb_patch_1.txt","02/Jul/10 09:00;balajirg;bb_patch_2.txt;https://issues.apache.org/jira/secure/attachment/12448550/bb_patch_2.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2010-05-06 11:44:18.356,,,false,,,,,,,,,,,,,,,,,,149733,,,,,Fri Jul 09 05:51:17 UTC 2010,,,,,,,"0|i0jhdr:",111746,,,,,,,,,,,herriot,,,,,,,,,,"06/May/10 11:18;balajirg;In this patch, I have moved the methods isJobStarted , isTaskStarted from MRCluster to JTClient, since the method was using more of jtclient member variables. The guideline is only methods that are common across all daemons must be present in MRCluster. also added couple of more reusable methods. Please start using this methods in the test cases, and also when you have time fix old testcases to use the new methods, and add new methods in the future to framework which can be reused more often. ","06/May/10 11:44;iyappans;Some comments:

1) I think, we shd change whatever tcs which are checked in, which uses these functions ina  single patch. Because after checking this in, other tcs which were using these methods will break.

2) +  public void isTaskStopped(TaskID tID) throws IOException {
+    
+    while (proxy.getTask(tID).getTaskStatus().getRunState() 
+        == TaskStatus.State.RUNNING) {
+      UtilsForTests.waitFor(1000);      
+    }
+  }

Here check ""null"" for proxy.getTask(tID)

3) +  public void signalAllTasks(JobID id) throws IOException{
+    TaskInfo[] taskInfos = getJTClient().getProxy().getTaskInfo(id);
+    for (TaskInfo taskInfoRemaining : taskInfos) {
+      FinishTaskControlAction action = new FinishTaskControlAction(TaskID
+         .downgrade(taskInfoRemaining.getTaskID()));
+      Collection<TTClient> tts = getTTClients();
+      for (TTClient cli : tts) {
+        cli.getProxy().sendAction(action);

Check for null in taskInfos, taskInfo, action.

4) +   * Allow the job to continue through MR control job.
+   * @param id,signal all the task for the current job. 
+   * @throws IOException when failed to get task info. 


here correct the javadoc. @param should come in next line.

5) +  public boolean isJobStarted(JobID id) throws IOException {
+    JobInfo jInfo = getProxy().getJobInfo(id);
+    int counter = 0;
+    while (counter < 60) {
+      if (jInfo.getStatus().getRunState() == JobStatus.RUNNING) {
+        break;
+      } else {
+        UtilsForTests.waitFor(1000);
+        jInfo = getProxy().getJobInfo(jInfo.getID());
+        Assert.assertNotNull(""Job information is null"",jInfo);
+      }
+      counter++;
+    }
+    return (counter != 60)? true : false ;
+  }


 a) Check null of jobInfo
 b) space between parameters in functions.

6) jobInfo shd be checked for null in is JobStopped also.

7) ttClientIns.isTaskStopped(tID);
   - Use AssertTrue here
","07/May/10 00:23;hammer;Hey,

Sorry if this is a dumb question, but what is the ""Herriot Framework""?

Thanks,
Jeff","07/May/10 03:14;balajirg;Iyappan thanks for quick response, I agree with all of your comments, expect for 1, I don't see checking in this code will break any existing test cases, since these are new methods. I have already refactored the test cases that was using the functionality in MRcluster to JTclient. 

Jeff Herriot is a fancy name given to the system testing framework that we have developed at yahoo for hadoop mapreduce/hdfs system testing. The herriot lets the test cases be run at the system level opposed to unit testing framework which is testing the code at module level. Most of the herriot framework code for aiding this will be forwarded ported soon right now most of the work resides in yahoo internal branch. ","07/May/10 16:30;cos;Jeff, this is the system test framework tracked by HADOOP-6332. This is a proposed name after [James Herriot | http://en.wikipedia.org/wiki/James_Herriot] aka Alf Wight, who was a famous veterinarian and writer (thanks Arun Murphy for digging up this name). We though it'd be cool to use this name for the framework because it designated to 'cure' our little elephant :)

All Herriot code is available in a form of Y20 patches at the above JIRA. I have started working on forward port last night and now patches for all components of Hadoop's trunk are available but aren't finished yet: there some known issues and gaps. I hope to finish the work in a next few days. Any reviews and comments will be very helpful.","28/Jun/10 11:31;balajirg;Implemented iyappans comment. ","29/Jun/10 05:43;iyappans;Some comments on the new patch:

1) isTaskStopped method:

a) If Task is in UNASSIGNED state, still this method will return false. I think, the method should wait for it to get assigned, finish running and then return true.

b) TaskID tID should be checked for null in the beginning of the method, before passing it as a parameter to a method.

2) isJobStopped method:

a) JobID id parameter might be null. Check for it before passing it to a  method.

b) jInfo can be returned null in jInfo = getProxy().getJobInfo(id);
 So , jInfo.getStatus() can be done as ""if (jInfo ! = null && jInfo.getStatus().isJobComplete())""

3) Formatting  issue at: 
+          FinishTaskControlAction action = new FinishTaskControlAction(TaskID
+     .    downgrade(taskInfoRemaining.getTaskID()));

4) +    Assert.assertTrue(""Task did not stop ""+tID,ttClientIns.isTaskStopped(tID));
 - Spacing between + and also spacing between comma.","02/Jul/10 09:00;balajirg;Implemented iyappan's comments. ","02/Jul/10 10:23;balajirg;For generating the patch for external, first the dependent patches needs to be forward ported first. ","06/Jul/10 03:18;iyappans;+1 ","07/Jul/10 02:41;cos;bq. For generating the patch for external, first the dependent patches needs to be forward ported first. 
You can apply the needed patches in order to generate one for this JIRA (the dependencies are already listed). Does it seem to be a problem?","09/Jul/10 05:51;balajirg;MAPREDUCE-1710 patch is still not available for trunk will work with the author so I can generate the patch for trunk. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ant tasks for job submission,MAPREDUCE-1332,12413281,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Won't Fix,stevel@apache.org,stevel@apache.org,steve_l,26/Jan/09 15:30,02/May/13 02:29,12/Jan/21 09:52,18/Feb/12 16:24,0.22.0,,,,,,,,,,,,,,1,,,,,"Ant tasks to make it easy to work with hadoop filesystem and submit jobs. 

<submit> : uploads JAR, submits job as user, with various settings

filesystem operations: mkdir, copyin, copyout, delete
 -We could maybe use Ant1.7 ""resources"" here, and so use hdfs as a source or dest in Ant's own tasks

# security. Need to specify user; pick up user.name from JVM as default?
# cluster binding: namenode/job tracker (hostname,port) or url are all that is needed?
#job conf: how to configure the job that is submitted? support a list of <property name=""name"" value=""something""> children
# testing. AntUnit to generate <junitreport> compatible XML files
# Documentation. With an example using Ivy to fetch the JARs for the tasks and hadoop client.
# Polling: ant task to block for a job finished? ","Both platforms, Linux and Windows",castagna,cdouglas,kaykay.unique,nidaley,philip,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-445,,,,"27/Jan/09 14:23;steve_l;JobSubmitTask.java;https://issues.apache.org/jira/secure/attachment/12398815/JobSubmitTask.java",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2009-01-26 15:36:22.499,,,false,,,,,,,,,,,,,,,,,,87669,,,,,Sat Feb 18 16:24:23 UTC 2012,,,,,,,"0|i0d3wn:",74409,,,,,,,,,,,,,,,,,,,,,"26/Jan/09 15:36;steve_l;Antunit probably doesnt integrate well with tests that need to set  up a mini cluster for the test run; use the ""legacy"" junit test case integration JARs instead.","26/Jan/09 20:41;steve_l;The use case for the {{<submit>}} ant task is to submit a job as part of a build; print
out enough information for you to track it's progress. Upload the JAR file.
{code}
<hadoop:submit tracker=""http://jobtracker:50030"" 
    in=""hdfs://host:port/tmp/in/something""
    out=""hdfs://host:port/tmp/out/something""
    jobProperty=""myJob""
    jar=""dist/myapp.jar""
>
  <property name=""dfs.replication.factor"" value=""4"" />
  <mapper classname=""org.example.identity"" /> 
  <reducer classname=""org.example.count"" />
 </hadoop:submit>
{code}

# No attempt to do a block for the job submission. The task will print out
  the jobID.
# jobProperty names a property to set for the job ID
# list zero or more JAR files. No attempt to do sanity checks like loading classes -the far end can do that.
# No separate configuration files for the map/reduce/combine
# Maybe, a configuration file attribute {{conf}}; defines a conf file to use. If set, no other properties can be set (would force the ant task to parse the XML, edit it, save it etc.
# JAR file is optional, but if listed, it had better be there

Tests without cluster
* fail to submit if the JAR is missing
* fail to submit if there is no tracker
* error if the mapper or reducer is not defined

Tests with MiniMR up
* submit a job

","26/Jan/09 21:10;steve_l;File operations

*Touch, copy in, copy out. Not using distcp, so for small data. 
* Rename, 
* add a condition for a file existing, maybe minimum size.
* DfsMkDir: create a directory

A first pass would use resources, [[http://ant.apache.org/manual/CoreTypes/resources.html#resource]], which can be used in existing Ant tasks; they extend the resource class
[[https://svn.apache.org/viewvc/ant/core/trunk/src/main/org/apache/tools/ant/types/Resource.java?view=markup]]
and can be used in the existing, {{<copy>}}, {{<touch>}} tasks, and the like. 
The resource would need to implement the getOutputStream() and getInputStream() operations, also, ideally, {{Touchable}}, for the touch() operation.  

Tests without a cluster
* Some meaningful failure if the hdfs:// URLS don't work

Tests with a cluster
* Copy in, copy-out, copy inside
* touch
* delete
* test for a resource existing
* some of the resource selection operations

Tests against other file systems
* S3:// URLs? Test that it works, but then assume that it stays working.
* Test that s3 urls fail gracefully if the URL is missing/forbidden","26/Jan/09 21:15;cdouglas;This sounds like a more thorough integration than the ant tasks added/proposed in HADOOP-1508 and HADOOP-2778. Would either of those be a reasonable base for some of the work you're considering?","27/Jan/09 09:12;steve_l;I'll take a look at the codebase in both of these. I'd initially expect to start with the minimal set of operations needed to get work into a cluster from a developer's desktop; let it evolve from there. While I know less about Hadoop than the other contributions, I do know more about Ant and how to test build files under JUnit, so what's really going to be new here are the regression tests. I have some job submit code of my own that I was going to start with, but HADOOP-2788 could be a good starting point.

What worries me is the whole configuration problem; I think the client settings are minimal enough now that the JT URL should be enough. 

The other problem is versioning; I will handle that by requiring tasks and cluster to be in sync, at least for now.","27/Jan/09 14:18;steve_l;Looking at HADOOP-2788, its actually more advanced than what I was thinking, as it 
# tries to block the Ant run until the job is finished; extracts counters afterwards
# does some classloader tricks to work out the JAR to include

I'm against the latter; more reliable to let the build file author point to the right place.

The blocking-the-job thing is also something I'm doubtful about, at least initially. Why? Because people will end up trying to use Ant as a long-lived workflow tool and it isn't optimised for that, either in availability or even memory management. People do try this - GridAnt is a case in point [http://www.globus.org/cog/projects/gridant/], but we don't encourage it. Better to move the workflow into the cluster and have some HA scheduler manage the sequence.

","27/Jan/09 14:23;steve_l;This is a first draft of a JobSubmit client.

1. no declaration/setting up of the inputs and outputs
2. no setup, yet, of the configuration above the default values. 

For #2 there's a choice.
(a) refer to an ant resource (including, once its in there, a resource in an HDFS filesystem)
(b) let you declare the various properties in Ant itself. 

(b) is more Ant-like, but less compatible with the rest of the hadoop configuration design, and may still need to support reading in XML files just to get the base configuration together. But a mixed-configuration is hardest to get right.

Thoughts?



","22/May/09 11:24;steve_l;looking at this and the configuration options, assuming everything is left to the XML files themselves.

# set it up on the classpath that you declare the task. Easiest to do, and what I would start with
# with a confdir attribute that points you at a configuration directory

the second option is more flexible.

If I were to do this (and one of my colleagues is pestering me for it), I'd do it as a contrib -it depends on both core and mapred, so once they get split up, it should be downstream of them. Nicely self contained, just need a cluster for testing. This could be done, incidentally, if the MiniMR cluster classes were moved from test/mapred to mapred, so I could add a <minimrcluster> task too. ","25/Jun/09 16:36;steve_l;Good ant tasks should be independent of hadoop versions, submit to remote clusters, etc. Hence clients to a RESTy job submission API

","15/Jan/10 03:56;kaykay.unique;So - what is the current status / interest in this ? A non-blocking ant task would be very useful. ","03/Feb/10 18:24;kaykay.unique;On a related note - have put up a maven hadoop mojo here at - http://github.com/akkumar/maven-hadoop . 

Pretty rudimentary at this point with a target for packing jar files , for submission .  Hopefully , down the road should be polished more. 

Interested people can follow more at - http://maven-hadoop.blogspot.com/ . ","18/Feb/12 16:24;stevel@apache.org;closing as wontfix unless anyone wants to do it. 

Better to submit a workflow to something like oozie or use cascading",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow for a plugin to process JH files as they are transferred from MR AM to JHS,MAPREDUCE-5135,12641180,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,,acmurthy,acmurthy,06/Apr/13 06:34,06/Apr/13 06:34,12/Jan/21 09:52,,,,,,,,,,jobhistoryserver,,,,,,0,,,,,"Allow for a plugin to process JH files as they are transferred from MR AM to JHS. This will allow, for e.g., Ambari to add a plugin to intercept JH data.",,acmurthy,jlowe,philip,sandyr,sseth,tgraves,vicaya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,321596,,,,,2013-04-06 06:34:46.0,,,,,,,"0|i1jhkf:",321941,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add possibility to set a custom system classloader for mapred child processes, separate from mapred.child.java.opts",MAPREDUCE-5126,12640737,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,,pkolaczk,pkolaczk,04/Apr/13 10:16,04/Apr/13 10:20,12/Jan/21 09:52,,1.0.4,,,,,,,,,,,,,,0,,,,,"Some third party frameworks / systems based on Hadoop might want to set a custom classloader for loading classes of their jobs to better resolve conflicts with their libraries. 

While it is possible to set a custom classloader using the mapred.child.java.opts, this field is often overriden by users in their job configuration. So in order to change e.g. heap sizes the user would need to remember also to include the custom classloader property from the framework-defaults or otherwise he would break the framework.

This small patch introduces another parameter: mapred.child.java.class.loader that allows to set the classloader separately. This gives custom frameworks built on top of Hadoop more flexibility to supply their own classloader, without need to force users to adjust any settings.",,jeromatron,jlowe,ozawa,pkolaczk,qwertymaniac,tanbamboo,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/13 10:17;pkolaczk;custom-classloader.patch;https://issues.apache.org/jira/secure/attachment/12576963/custom-classloader.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,321196,,,,,2013-04-04 10:16:36.0,,,,,,,"0|i1jf3z:",321541,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sort Avoidance,MAPREDUCE-4039,12547154,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,anty.rao,anty,anty,20/Mar/12 07:58,28/Jan/13 21:31,12/Jan/21 09:52,,0.23.2,,,,,0.23.2,,,mrv2,,,,,,3,,,,,"Inspired by [Tenzing|http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/37200.pdf], in 5.1 MapReduce Enhanceemtns:
{quote}*Sort Avoidance*. Certain operators such as hash join
and hash aggregation require shuffling, but not sorting. The
MapReduce API was enhanced to automatically turn off
sorting for these operations. When sorting is turned off, the
mapper feeds data to the reducer which directly passes the
data to the Reduce() function bypassing the intermediate
sorting step. This makes many SQL operators significantly
more ecient.{quote}

There are a lot of applications which need aggregation only, not sorting.Using sorting to achieve aggregation is costly and inefficient. Without sorting, up application can make use of hash table or hash map to do aggregation efficiently.But application should bear in mind that reduce memory is limited, itself is committed to manage memory of reduce, guard against out of memory. Map-side combiner is not supported, you can also do hash aggregation in map side  as a workaround.

the following is the main points of sort avoidance implementation
# add a configuration parameter ??mapreduce.sort.avoidance??, boolean type, to turn on/off sort avoidance workflow.Two type of workflow are coexist together.
# key/value pairs emitted by map function is sorted by partition only, using a more efficient sorting algorithm: counting sort.
# map-side merge, use a kind of byte merge, which just concatenate bytes from generated spills, read in bytes, write out bytes, without overhead of key/value serialization/deserailization, comparison, which current version incurs.
# reduce can start up as soon as there is any map output available, in contrast to sort workflow which must wait until all map outputs are fetched and merged.
# map output in memory can be directly consumed by reduce.When reduce can't catch up with the speed of incoming map outputs, in-memory merge thread will kick in, merging in-memory map outputs onto disk.
# sequentially read in on-disk files to feed reduce, in contrast to currently implementation which read multiple files concurrently, result in many disk seek. Map output in memory take precedence over on disk files in feeding reduce function.

I have already implement this feature based on hadoop CDH3U3 and done some performance evaluation, you can reference to [https://github.com/hanborq/hadoop] for details. Now,I'm willing to port it into yarn. Welcome for commenting.


",,acmurthy,ahmed.radwan,anty,ashutoshc,atm,avnerb,cdouglas,chandrap,cutting,daijy,decster,eli2,fei1223fei,gates,gemini5201314,gopalv,gortsleigh,jayf,jimhuang,jira.shegalov,johnvijoe,kam_iitkgp,kkambatl,lianhuiwang,longmi,lskuff,masokan,mayank_bansal,naiteix,nemon,nroberts,ozawa,priyank.rastogi,qwertymaniac,revans2,rohithsharma,schubertzhang,sharadag,sseth,tlipcon,tomwhite,vicaya,xiaokang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/12 14:09;anty;IndexedCountingSortable.java;https://issues.apache.org/jira/secure/attachment/12532411/IndexedCountingSortable.java","23/Jul/12 13:19;anty;MAPREDUCE-4039-branch-0.23.2.patch;https://issues.apache.org/jira/secure/attachment/12537563/MAPREDUCE-4039-branch-0.23.2.patch","26/Mar/12 05:59;anty;MAPREDUCE-4039-branch-0.23.2.patch;https://issues.apache.org/jira/secure/attachment/12519908/MAPREDUCE-4039-branch-0.23.2.patch","23/Mar/12 10:56;anty;MAPREDUCE-4039-branch-0.23.2.patch;https://issues.apache.org/jira/secure/attachment/12519605/MAPREDUCE-4039-branch-0.23.2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2012-03-20 09:52:57.026,,,false,,,,,,,,,,,,,,,,,,232312,,,,,Mon Jan 28 21:31:42 UTC 2013,,,,,,,"0|i0jnu7:",112792,,,,,,,,,,,,,2.0.0-alpha,,,,,,,,"20/Mar/12 09:52;schubertzhang;Nice summary Anty, and expect your patch on 0.23.

The more detailed description and benchmark on this feature, you can refer to http://www.slideshare.net/hanborq/hanborq-optimizations-on-hadoop-mapreduce-20120216a or http://www.slideshare.net/schubertzhang/hanborq-optimizations-on-hadoop-map-reduce-20120221a ","20/Mar/12 11:12;acmurthy;+1, looks like a good addition.

We should be careful to ensure we document this adequately.","21/Mar/12 06:52;anty;I am a little confused about the implementation of Reader of IFile.
In previous hadoop version, IFile reader will read in a bunch of key/value pairs from the disk one time, then serve it directly from in memory.I think this strategy is common and good.However, in yarn for each requested key/value pairs reader will go hit the disk(though pre-read will do some help). Am i miss something？Can someone shed light on me?","26/Mar/12 01:12;schubertzhang;Patch is available by Anty, someone to have a review? ","26/Mar/12 06:18;acmurthy;bq. Can someone shed light on me?

Anty - IFile.Reader in hadoop-0.23 uses IFileInputStream which does the 'full' read into a in-memory buffer similar to what was happening in IFile.Reader in hadoop-0.20.xxx and should give similar characteristics. Makes sense?","26/Mar/12 06:28;acmurthy;Anty - I spent a bit of time thinking of this. 

Some early thoughts... seems to me that we are better of doing a bit of surgery on the MR runtime before we do this.

We could consider making the MapOutputBuffer pluggable for a start so we can split the 'full sort' and the 'hash sort' implementations. Similarly, we could implement a pluggable Shuffle to not block until outputs of all maps are not available.

This way we can cleanly layer the necessary features.

Thoughts?","26/Mar/12 07:26;anty;Arun, Thanks for you reply!
{quote}Anty - IFile.Reader in hadoop-0.23 uses IFileInputStream which does the 'full' read into a in-memory buffer similar to what was happening in IFile.Reader in hadoop-0.20.xxx and should give similar characteristics. Makes sense?{quote}

IFileInputStream only do data checksum  not data buffering . Data buffering is enforced on IFile.Reader level in hadoop-0.20.xxx
1. In hadoop-0.20.xxx, a bunch of data is first read into a byte array

{code}    byte[] buffer = null;{code}the initial size of this buffer is controlled by the parameter _io.file.buffer.size_. Then key/value pair is read from this in-memory buffer.
2. In hadoop-0.23, this variable _buffer_ still exist ,but isn't in use actually. So, each key/value pair is read directly from the underling IFileInputStream.






","26/Mar/12 08:11;anty;{quote}We could consider making the MapOutputBuffer pluggable for a start so we can split the 'full sort' and the 'hash sort' implementations. Similarly, we could implement a pluggable Shuffle to not block until outputs of all maps are not available.

This way we can cleanly layer the necessary features.

Thoughts?{quote}
Arun, I like the idea of making MapOutputBuffer and Shuffle pluggable.I will take a crack at it, and give the feedback.

","08/Jun/12 07:39;fei1223fei;The patch Did not give the class org.apache.hadoop.util.IndexedCountingSortable.","18/Jun/12 12:24;xiaokang;@Schubert, could you give some typical applications that benefit from sort avoidance? It seems that using this feature simple aggregation app such as wordcount will use more memory to wait for all keys processed.","18/Jun/12 14:09;anty;the missing file.","18/Jun/12 14:25;anty;@Kang
Yes, you are right.
Using merge-sort to achieve aggregation maybe don't use so much memory as hash aggregation with this feature.But the process of merge-sort require much useless work to done, consume more resources, e.g. CPU, disk, network.
it's just a tradeoff according to your usecase, latency requirement, etc.","19/Jul/12 22:19;ahmed.radwan;Thanks Anty! a couple of initial comments:

* IndexedCountingSortable.java doesn't compile (seems the the same class contents are pasted twice). I think you want to add this file to the location: hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/IndexedCountingSortable.java. It'll be easier to create a new diff including this new file and upload an updated patch.

* The patch also doesn't include any new or updated unit tests.","23/Jul/12 13:19;anty;@Ahmed Radwan
Sorry for the delay
Submit a new batch.
","27/Jul/12 02:39;ahmed.radwan;Thanks Anty! I'll be happy to review your patch. I have noticed that the updated patch still have no unit tests, are you planning to add them? Also, will you be providing a corresponding patch for branch-1?","30/Jul/12 01:55;anty;Ahmed,thank you for spacing time to review this patch.
But, when sort plugin is finished, this patch will need be modified.","30/Jul/12 08:10;ahmed.radwan;bq. when sort plugin is finished, this patch will need be modified.

Sure Anty, are you already working on updating the patch with pluggable MapOutputBuffer and Shuffle then?","31/Jul/12 17:41;ahmed.radwan;Hi Anty, Do you think you'll be able to update the patch and add the unit tests? With a new feature like that, unit tests are essential. Please let us know if you need any help.","01/Aug/12 01:48;anty;Hi Ahmed
{quote}Sure Anty, are you already working on updating the patch with pluggable MapOutputBuffer and Shuffle then?{quote}
No, the sort plugin feature seems not completed, So i don't start out
{quote}Hi Anty, Do you think you'll be able to update the patch and add the unit tests? With a new feature like that, unit tests are essential. Please let us know if you need any help.{quote}
I will add some unit tests.","07/Aug/12 20:06;masokan;Hi Anty,
  I have submitted a patch for MAPREDUCE-2454 to support a pluggable sort for MR.  I have some ideas on implementing a NullSortPlugin which will be a special sort plugin that avoids sorting.  The NullSortPlugin can live outside the Hadoop MR code.  I can share my thoughts if you are interested.
Thanks.
-- Asokan
","20/Aug/12 06:41;anty;Hi: Asokan
Of course,i'm interested.Glad that you can share your thoughts!","30/Aug/12 21:49;masokan;Hi Anty,
  Sorry, I did not get back to you.  Please take a look at the patch for MAPREDUCE-2454.  I added a test that has a very simple implementation of NullSortPlugin.  You can take a look at the code there.

-- Asokan","26/Jan/13 14:28;masokan;Hi Anty,
  Now that MAPREDUCE-4809, MAPREDUCE-4807, MAPREDUCE-4808, and MAPREDUCE-4049 are all committed, it is possible to implement sort avoidance as implementation of plugins for {{MapOutputCollector}} and {{ShuffleConsumerPlugin}} with a special implementation of {{MergeManager.}}

If you don't mind, I can assign this Jira to me and work on it.  Please let me know.

Thanks.

-- Asokan
","28/Jan/13 01:50;anty;[~masokan]  
You can get on with it!
Looking forward to seeing this feature incorporated to trunk.

","28/Jan/13 18:45;acmurthy;bq.  it is possible to implement sort avoidance as implementation of plugins for MapOutputCollector and ShuffleConsumerPlugin with a special implementation of MergeManager

Asokan - as I've mentioned before several times on other jiras, I believe this is the wrong way to go about doing adding this feature. Happy to discuss alternates.","28/Jan/13 21:31;masokan;Hi Arun,
  If you have alternate ideas, please feel free to post.  We can discuss about them.

Thanks.

-- Asokan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Counters that track max values,MAPREDUCE-4709,12610604,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,,jeremy@lewi.us,jeremy@lewi.us,05/Oct/12 17:39,25/Jan/13 13:50,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"A nice feature to help monitor MR jobs would be mapreduce counters that track the maximum of some metric across all workers. These trackers would work just like regular counters except it would track the max value of all arguments passed to the ""increment"" function as opposed to summing them.

",,ak.arun@aol.com,ben.roling,jeremy@lewi.us,jlowe,qwertymaniac,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-01-25 10:01:00.541,,,false,,,,,,,,,,,,,,,,,,244015,,,,,Fri Jan 25 13:50:40 UTC 2013,,,,,,,"0|i05agn:",28792,,,,,,,,,,,,,,,,,,,,,"25/Jan/13 10:01;ak.arun@aol.com;@Jeremy Lewi, 
Could you please elaborate on the problem with an example? ","25/Jan/13 13:50;qwertymaniac;Hi Arun,

Sorry we forgot to link the discussion, but please also see http://search-hadoop.com/m/cuZMf2humC",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable ServicePlugins for the JobTracker,MAPREDUCE-461,12422467,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Fixed,fhedberg,fhedberg,fhedberg,09/Apr/09 12:02,07/Dec/12 22:42,12/Jan/21 09:52,17/May/11 03:51,,,,,,0.23.0,1.2.0,,,,,,,,0,,,,,Allow ServicePlugins (see HADOOP-5257) for the JobTracker.,,aaa,aw,bcwalrus,brandonli,dhruba,eli,hammer,omalley,pramod.thangali,tlipcon,vinodkv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/12 17:53;brandonli;MAPREDUCE-461.branch-1.patch;https://issues.apache.org/jira/secure/attachment/12546071/MAPREDUCE-461.branch-1.patch","10/Nov/10 05:22;tomwhite;MAPREDUCE-461.patch;https://issues.apache.org/jira/secure/attachment/12459224/MAPREDUCE-461.patch","09/Apr/09 12:02;fhedberg;sp-jt-1.diff;https://issues.apache.org/jira/secure/attachment/12405064/sp-jt-1.diff",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2009-04-10 01:35:55.143,,,false,,,,,,,,,,,,,,,,,,37396,Reviewed,,,,Mon Sep 24 22:23:10 UTC 2012,,,,,,,"0|i02thb:",14377,,,,,,,,,,,,,,,,,,,,,"10/Apr/09 01:35;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12405064/sp-jt-1.diff
  against trunk revision 763728.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/176/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/176/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/176/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/176/console

This message is automatically generated.","10/Apr/09 11:33;fhedberg;AFAIK, the failed tests are not related to this issue. I don't think this patch warrants any new unit tests.","27/May/09 09:19;amar_kamat;Fredrik, the patch looks good. Few minor comments.
# Can you add 'mapred.jobtracker.plugins' to mapred-default.xml and document it there. 
# bq. LOG.warn(""ServicePlugin "" + p + "" could not be started"", t);
   and
   bq. LOG.warn(""ServicePlugin "" + p + "" could not be stopped"", t);
Here *p* will log junk info about the object (Object.toString()). Either we can simply dump the exception or else define {{ServicePlugin.getName()}}.

Rest looks good.","01/Jun/09 10:30;ddas;Could you please write a unit test for this? Thanks!","29/Jun/09 11:24;sharadag;Waiting for unit test.","23/Oct/09 09:01;aaa;Fredrik, will you write a unit test for it?","24/Feb/10 22:35;aw;It looks like Cloudera has this patch in their distribution, yet it doesn't appear to have been committed upstream.  What is the status?  Are we still waiting for a unit test?","10/Nov/10 05:22;tomwhite;Here's an updated patch which includes a unit test. It also addresses Amar's feedback.","01/Mar/11 02:55;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12459224/MAPREDUCE-461.patch
  against trunk revision 1075422.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/98//testReport/
Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/98//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/98//console

This message is automatically generated.","17/May/11 00:36;tlipcon;+1, looks good to me","17/May/11 03:51;tomwhite;I've just committed this. Thanks, Fredrik!","17/May/11 05:53;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #678 (See [https://builds.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/678/])
    MAPREDUCE-461. Enable ServicePlugins for the JobTracker. Contributed by Fredrik Hedberg.
","17/May/11 15:40;hudson;Integrated in Hadoop-Mapreduce-trunk #682 (See [https://builds.apache.org/hudson/job/Hadoop-Mapreduce-trunk/682/])
    MAPREDUCE-461. Enable ServicePlugins for the JobTracker. Contributed by Fredrik Hedberg.
","21/Sep/12 17:54;brandonli;Uploaded a patch to back port this feature to branch-1.","23/Sep/12 18:09;vinodkv;Brandon, the patch for branch-1 looks good. But it doesn't apply against 1.1, can you please update? Tx.","24/Sep/12 03:59;brandonli;@Vinod, thanks for reviewing. The patch is actually for branch-1, not 1.1.","24/Sep/12 20:51;brandonli;Test-patch result of the branch-1 patch:
{noformat}
-1 overall.  
    +1 @author.  The patch does not contain any @author tags.
    +1 tests included.  The patch appears to include 2 new or modified tests.
    +1 javadoc.  The javadoc tool did not generate any warning messages.
    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
    -1 findbugs.  The patch appears to introduce 196 new Findbugs (version 2.0.0) warnings.
{noformat}
The patch doesn't introduce new findbuf warnings.","24/Sep/12 22:23;vinodkv;Just committed this to branch-1. Thanks for the backport, Brandon!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exposing MiniDFS and MiniMR clusters as a single process command-line,MAPREDUCE-987,12435766,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Fixed,ahmed.radwan,philip,philip,15/Sep/09 21:31,11/Oct/12 17:48,12/Jan/21 09:52,23/Jul/12 23:58,1.2.0,2.0.2-alpha,,,,1.2.0,2.0.2-alpha,,build,test,,,,,0,,,,,"It's hard to test non-Java programs that rely on significant mapreduce functionality.  The patch I'm proposing shortly will let you just type ""bin/hadoop jar hadoop-hdfs-hdfswithmr-test.jar minicluster"" to start a cluster (internally, it's using Mini{MR,HDFS}Cluster) with a specified number of daemons, etc.  A test that checks how some external process interacts with Hadoop might start minicluster as a subprocess, run through its thing, and then simply kill the java subprocess.

I've been using just such a system for a couple of weeks, and I like it.  It's significantly easier than developing a lot of scripts to start a pseudo-distributed cluster, and then clean up after it.  I figure others might find it useful as well.

I'm at a bit of a loss as to where to put it in 0.21.  hdfs-with-mr tests have all the required libraries, so I've put it there.  I could conceivably split this into ""minimr"" and ""minihdfs"", but it's specifically the fact that they're configured to talk to each other that I like about having them together.  And one JVM is better than two for my test programs.",,aaa,ahmed.radwan,atm,aw,cdouglas,cos,cutting,dhruba,eli,garymurry,goosmurf,hammer,hong.tang,hudson,jothipn,nidaley,omalley,rvs,sseth,szetszwo,tgraves,tomwhite,tucu00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HDFS-3167,,,,,,,,,,,,,,,,,,,,,"15/Sep/09 22:35;philip;HDFS-621-0.20-patch;https://issues.apache.org/jira/secure/attachment/12419696/HDFS-621-0.20-patch","15/Sep/09 22:02;philip;HDFS-621.patch;https://issues.apache.org/jira/secure/attachment/12419689/HDFS-621.patch","06/Jul/12 23:31;ahmed.radwan;MAPREDUCE-987.patch;https://issues.apache.org/jira/secure/attachment/12535470/MAPREDUCE-987.patch","15/Sep/09 22:46;philip;MAPREDUCE-987.patch;https://issues.apache.org/jira/secure/attachment/12419700/MAPREDUCE-987.patch","10/Jul/12 02:42;ahmed.radwan;MAPREDUCE-987_branch-1.0.patch;https://issues.apache.org/jira/secure/attachment/12535777/MAPREDUCE-987_branch-1.0.patch","23/Jul/12 01:46;ahmed.radwan;MAPREDUCE-987_branch-1.0_rev2.patch;https://issues.apache.org/jira/secure/attachment/12537535/MAPREDUCE-987_branch-1.0_rev2.patch","16/Jul/12 22:47;ahmed.radwan;MAPREDUCE-987_rev2.patch;https://issues.apache.org/jira/secure/attachment/12536742/MAPREDUCE-987_rev2.patch","19/Jul/12 06:52;ahmed.radwan;MAPREDUCE-987_rev3.patch;https://issues.apache.org/jira/secure/attachment/12537142/MAPREDUCE-987_rev3.patch","19/Jul/12 19:01;ahmed.radwan;MAPREDUCE-987_rev4.patch;https://issues.apache.org/jira/secure/attachment/12537227/MAPREDUCE-987_rev4.patch",,,,,,,,,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,2009-09-15 21:44:23.463,,,false,,,,,,,,,,,,,,,,,,37391,Reviewed,,,,Mon Jul 23 23:58:12 UTC 2012,,,,,,,"0|i01767:",4931,,,,,,,,,,,,,,,,,,,,,"15/Sep/09 21:44;szetszwo;I guess you need a separated MAPREDUCE patch for MiniMR.","15/Sep/09 21:48;cos;.bq And one JVM is better than two for my test programs.
Is it better for the testing of the product ? ;-)","15/Sep/09 22:02;philip;Uploading patch to start a single-process cluster.","15/Sep/09 22:08;philip;bq. I guess you need a separated MAPREDUCE patch for MiniMR. 

If you would, take a quick look to see how I use MiniMRCluster.  Do you feel I'm abusing the fact that hdfs-hdfswithmr-test exists?

bq. Is it better for the testing of the product?

It doesn't contribute anything to testing Hadoop.  It does make it easier to write tests for programs that are downstream of Hadoop.  One might argue that such facilities ought to be ""top-level"" and not shunted into a ""-test"" jar.  I'm open to suggestions.  (I could also see this being shunted to contrib, if folks have a strong preference.)","15/Sep/09 22:08;omalley;I would rather that the local runner be made to work better than fake a mini cluster. For testing the framework, the mini cluster makes sense. But I think that for testing applications, the local runner is a much better fit.","15/Sep/09 22:12;philip;Owen,

I totally agree that LocalJobRunner should be maximally useful.  That's great for testing jobs.

Let's say I have a python class that knows how to interact with HDFS and MR.  It knows how to look at files, start jobs, etc.  I call out to hadoop binaries to interact with HDFS, and I want to capture all the details that occur when I talk to my real cluster.  For this, if I were in Java, I'd spin up a Mini* cluster.  Since I'm not in Java, I resort to spinning up a subprocess.  I could also mock everything out, but at the end of the day, I want an integration test, and I really don't want to run it against a cluster that has to be setup externally: I'd rather the cluster be spun up and shut down by my test itself.

I'm happy to throw this contrib/ if you feel strongly about it.  I figure it'd be useful to other folks.

-- Philip","15/Sep/09 22:25;szetszwo;> If you would, take a quick look to see how I use MiniMRCluster. Do you feel I'm abusing the fact that hdfs-hdfswithmr-test exists?

No more mapreduce codes in hdfs, please.  Having hdfs-with-mr in hdfs is a mistake.  It leads to a circular dependence.  Indeed, we should move hdfs-with-mr to mapreduce.
","15/Sep/09 22:35;philip;Attaching the 0.20 version.  Conveniently, where to place it is not a problem there :)","15/Sep/09 22:46;philip;Nicholas,

Agreed that circular dependencies are to be avoided.  I've moved this issue into MAPREDUCE, and spun up a new patch.

Do we anticipate a world where MR doesn't depend statically on HDFS (i.e., it only depends on the FileSystem interfaces)?

-- Philip","15/Sep/09 22:58;szetszwo;> Agreed that circular dependencies are to be avoided. I've moved this issue into MAPREDUCE, and spun up a new patch.
Thanks, Philip.

> Do we anticipate a world where MR doesn't depend statically on HDFS (i.e., it only depends on the FileSystem interfaces)?
Theoretically, mapreduce does not depend on hdfs.  However, the performance of mapreduce would be bad without hdfs.  I guess there is no serious applications using mapreduce without hdfs, except for testing.","17/Sep/09 03:32;tlipcon;+1 - I code reviewed this internally before posting, and have also been using it for a couple of weeks, finding it very useful. Happy to post a copy of our internal review discussion if necessary.","12/Oct/09 00:32;philip;Submitting patch: our friend Hudson may have gotten confused.

Does anyone have any objections to exposing Mini*Cluster in this way?

Thanks,

-- Philip","12/Oct/09 03:58;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419700/MAPREDUCE-987.patch
  against trunk revision 823227.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 5 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/158/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/158/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/158/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/158/console

This message is automatically generated.","18/Oct/09 21:34;cdouglas;This seems appropriate for the test jar. Small notes:
* This picks up \-D params like the generic parser; would it make sense to also accept \-conf? The other params make less sense in this context, though it may be worth considering Tool/ToolRunner
* It'd be better if sleepForever monitored the Mini\*Cluster, rather than waking up every minute for no reason. Not sure if it makes sense to include a poison pill (Path?) + configurable polling interval that might signal an orderly shutdown.
* If this is intended for tests, should {{start}} wait for the TT/DNs to come up before returning?","24/Feb/10 22:54;aw;There is a patch for this in the Cloudera distribution but it doesn't appear to have been committed upstream.  What is the status?
","20/Oct/10 20:26;cos;Phillip, is there any intention to address last comments from Chris, so this can be committed into trunk?","06/Jul/12 23:31;ahmed.radwan;Here is an updated patch for trunk, various changes were done due to MR2 changes.","06/Jul/12 23:35;ahmed.radwan;Please note that the patch requires also the patches from the blocker tickets MAPREDUCE-4406 and MAPREDUCE-4407.","07/Jul/12 03:06;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12535470/MAPREDUCE-987.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified test files.

    -1 javac.  The applied patch generated 2071 javac compiler warnings (more than the trunk's current 2070 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2552//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2552//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2552//console

This message is automatically generated.","10/Jul/12 02:42;ahmed.radwan;Here is also the updated patch for branch-1.0.","10/Jul/12 09:16;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12535777/MAPREDUCE-987_branch-1.0.patch
  against trunk revision .

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2561//console

This message is automatically generated.","16/Jul/12 22:47;ahmed.radwan;Here is an updated version of the trunk patch based on new MAPREDUCE-4406 change for using properties to control if fixed ports are used or not.","16/Jul/12 22:56;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12536742/MAPREDUCE-987_rev2.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified test files.

    -1 javac.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2605//console

This message is automatically generated.","18/Jul/12 23:41;tucu00;+1. Ahmed, please add documentation for it (in the yarn-site submodule, in APT format), and we are good to go.","19/Jul/12 06:52;ahmed.radwan;Thanks Tucu! Here is the updated patch with the new docs.","19/Jul/12 07:46;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537142/MAPREDUCE-987_rev3.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified test files.

    -1 javac.  The applied patch generated 2067 javac compiler warnings (more than the trunk's current 2066 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site:

                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2625//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2625//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2625//console

This message is automatically generated.","19/Jul/12 08:04;ahmed.radwan;The Jenkins reported test failure doesn't seem to be related to this patch.","19/Jul/12 16:15;tucu00;Ahmed, a couple of Nits in the docs:

*1* 

The sentence:

+  The CLI MiniCLuster can
+  start a full Hadoop cluster including the <<<Yarn>>> <<<ResourceManager>>>,
+  <<<NodeManager(s)>>>, and <<<HDFS>>> <<<NameNode>>> and <<<DataNode(s)>>>, in
+  addition to the <<<MapReduce>>> <<<JobHistoryServer>>>.

it is a bit too complex, why not just replace it with:

The CLI MiniCluster starts both a YARN/MapReduce & HDFS clusters.

*2*

Instead using VERSION and a paragraph explaining what VERSION has to be replaced with, you can use ${project.version}, within *.apt.vm files you can use Maven variables.

","19/Jul/12 19:01;ahmed.radwan;Thanks Tucu! Here is the updated patch incorporating your comments.","19/Jul/12 19:56;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537227/MAPREDUCE-987_rev4.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified test files.

    -1 javac.  The applied patch generated 2067 javac compiler warnings (more than the trunk's current 2066 warnings).

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site:

                  org.apache.hadoop.mapreduce.v2.TestSpeculativeExecution
                  org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2628//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2628//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2628//console

This message is automatically generated.","20/Jul/12 23:08;tucu00;+1","20/Jul/12 23:27;hudson;Integrated in Hadoop-Hdfs-trunk-Commit #2575 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2575/])
    MAPREDUCE-987. Exposing MiniDFS and MiniMR clusters as a single process command-line. (ahmed via tucu) (Revision 1364020)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1364020
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/MiniHadoopClusterManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/test/MapredTestDriver.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/CLIMiniCluster.apt.vm
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/index.apt.vm
","20/Jul/12 23:28;hudson;Integrated in Hadoop-Common-trunk-Commit #2510 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2510/])
    MAPREDUCE-987. Exposing MiniDFS and MiniMR clusters as a single process command-line. (ahmed via tucu) (Revision 1364020)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1364020
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/MiniHadoopClusterManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/test/MapredTestDriver.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/CLIMiniCluster.apt.vm
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/index.apt.vm
","21/Jul/12 00:15;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #2531 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2531/])
    MAPREDUCE-987. Exposing MiniDFS and MiniMR clusters as a single process command-line. (ahmed via tucu) (Revision 1364020)

     Result = FAILURE
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1364020
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/MiniHadoopClusterManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/test/MapredTestDriver.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/CLIMiniCluster.apt.vm
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/index.apt.vm
","21/Jul/12 12:48;hudson;Integrated in Hadoop-Hdfs-trunk #1111 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1111/])
    MAPREDUCE-987. Exposing MiniDFS and MiniMR clusters as a single process command-line. (ahmed via tucu) (Revision 1364020)

     Result = FAILURE
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1364020
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/MiniHadoopClusterManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/test/MapredTestDriver.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/CLIMiniCluster.apt.vm
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/index.apt.vm
","21/Jul/12 14:03;hudson;Integrated in Hadoop-Mapreduce-trunk #1143 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1143/])
    MAPREDUCE-987. Exposing MiniDFS and MiniMR clusters as a single process command-line. (ahmed via tucu) (Revision 1364020)

     Result = FAILURE
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1364020
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/MiniHadoopClusterManager.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/test/MapredTestDriver.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/CLIMiniCluster.apt.vm
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/index.apt.vm
","21/Jul/12 16:27;tucu00;Thanks Ahmed. Committed to trunk and branch-2, I'll leave it open until we have the patch with docs for branch-1 and we commit it there.","23/Jul/12 01:46;ahmed.radwan;Thanks Tucu! Here is the updated patch for branch-1.0 including the new docs.","23/Jul/12 01:51;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537535/MAPREDUCE-987_branch-1.0_rev2.patch
  against trunk revision .

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2646//console

This message is automatically generated.","23/Jul/12 23:58;tucu00;Thanks Philip & Ahmed. Committed to branch-1 as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide more flexibility in the way tasks are run,MAPREDUCE-453,12399354,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Duplicate,un_brice,un_brice,un_brice,01/Jul/08 10:45,17/Jun/12 05:01,12/Jan/21 09:52,17/Jun/12 04:59,,,,,,,,,,,,,,,0,,,,,"*The aim*
With [HADOOP-3421] speaking about sharing a cluster among more than one organization (so potentially with non-cooperative users), and posts on the ML speaking about virtualization and the ability to re-use the TaskTracker's VM to run new tasks, it could be useful for admins to choose the way TaskRunners run their children. 

More specifically, it could be useful to provide a way to imprison a Task in its working directory, or in a virtual machine.
In some cases, reusing the VM might be useful, since it seems that this feature is really wanted ([HADOOP-249]).

*Concretely*
What I propose is a new class, called called SeperateVMTaskWrapper which contains the current logic for running tasks in another JVM. This class extends another, called TaskWrapper, which could be inherited to provide new ways of running tasks.
As part of this issue I would also like to provide two other TaskWrappers : the first would run the tasks as Thread of the TaskRunner's VM (if it is possible without too much changes), the second would use a fixed pool of local unix accounts to insulate tasks from each others (so potentially non-cooperating users will be hable to share a cluster, as described in [HADOOP-3421]).",,acmurthy,alexlod,aw,craigm,cutting,ddas,johanoskarsson,mahadev,matei@eecs.berkeley.edu,omalley,qwertymaniac,rickcox,tomwhite,tucu00,un_brice,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-3421,,,,,,,,HADOOP-249,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/08 10:48;un_brice;TaskWrapper_v0.patch;https://issues.apache.org/jira/secure/attachment/12385028/TaskWrapper_v0.patch","03/Jul/08 09:36;un_brice;userBasedInsulator.sh;https://issues.apache.org/jira/secure/attachment/12385191/userBasedInsulator.sh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2008-07-01 17:39:21.379,,,false,,,,,,,,,,,,,,,,,,148809,,,,,Sun Jun 17 05:01:13 UTC 2012,,,,,,,"0|i0itvz:",107936,,,,,,,,,,,,,,,,,,,,,"01/Jul/08 10:48;un_brice;First implementation. It is really not ready for use in production, but it seems to works and should illustrate my aim.
To provide compatibility, it tries to parse options from ""mapred.child.java.opts"" and to pass them to the TaskWrapper.
We might consider deprecating ""mapred.child.java.opts"" and spliting it into a few other options such as ""mapred.child.java.properties"" and ""mapred.child.java.memoryLimit"".","01/Jul/08 17:39;cutting;In general, I like this approach.

> As part of this issue I would also like to provide two other TaskWrappers

Yes, I think the initial patch should provide at least one other implementation, to prove the utility of the API.  The thread-per-task approach has often been requested, and is a thus a great candidate.

A ""jail"" implemented with 'chroot' that isolates users would also be very useful.  If a new root directory is created per user then we should not need more than one additional uid.  The tasktracker's uid would need sudo privledges in order to run 'chroot', so we would want to run user tasks as a different uid, but all user tasks could run as the same uid, but each with a different root filesystem.  However such a capability might better be added in a separate issue...

> We might consider deprecating ""mapred.child.java.opts""

If the TaskWrapper implementation is passed the Configuration, can't this property continue to be used by SeperateVMTaskWrapper?
","02/Jul/08 12:58;steve_l;An in-VM task runner should always run the task in a new security manager (or at least try to set a new security manager, and fail gracefully if it can't, in case someone else is running Hadoop under a new Security Manager already). The SM could block calls to System.exit()

The reason for doing this from the outset is if you leave it out, it sets up expectations that are hard to change later on. The rule should be in-VM == under a security manager. ","03/Jul/08 09:36;un_brice;*@Doug Cutting*
Glad you like it ^^

I don't think that chroot alone are a good idea, because tasks inside a chroot can still kill or ptrace tasks outside of the chroot. So it would not protect users from each other. FreeBSD jails or Linux vservers would be perfect, but they are non-standard and vservers require to patch the kernel.
My intention is to provide a TaskWrapper that delegate the security to a user script, and to write a script (which could go in contrib/ ?) that would use a pool of local Unix accounts to run user tasks.
So, if we have two users (Alice and Bob) whose tasks are going to be run on the same tasktracker, Alice's tasks will be run as the Unix user {{hadoop0}} and Bob's tasks as {{hadoop1}}.
When Alice's tasks are done, her files and process are killed atomically (via {{kill -PGROUP}}) to ensure there's nothing left. Then {{hadoop0}} is made available for use by another Hadoop user.
The benefit of using a separate shell script is that only this script (not the whole TaskTracker) needs root privileges. And it can get them via sudo (so we don't require yet another SUID binary).
An administrator wanting to use this would :
# Deploy hadoop as usual
# Create Unix accounts {{hadoopUser0}}...{{hadoopUserN}} for use by this wrapper
# Add in {{/etc/sudoers}} a permission for the hadoop user to run the wrapper script as root
# Set the right wrapper in Hadoop config

The attached script demonstrate the process. If there is shell guru available, I would really like his advices ^^.
We could also write a script that run tasks inside a VM, but I'm unsure that it is useful, considering the overhead.

bq. If the TaskWrapper implementation is passed the Configuration, can't this property continue to be used by SeperateVMTaskWrapper?
You're right, it would be the best way to ensure compatibility. For now I will continue to use parameters set by setMaximumMemory(), addArg() and so on, in order to test the API. But the ""release"" version of SeperateVMTaskWrapper will directly use mapred.child.java.opts.

*@Steve Loughran*
bq. An in-VM task runner should always run the task in a new security manager 
Good idea ! For some TaskWrapper we might be unhable to provide a true security (I think mainly to the ThreadWrapper that I'm writing, which is mainly intended to be used with the Streaming API), but the programmer should at least be protected against most obvious errors.
On the ML, Alejandro Abdelnur said that he already run his tasks under a security manager, I'm going to ask him if he can publish more information that I could integrate into a TaskWrapper.
You're also right about the fact that failing gracefully is very important, since some TaskWrapper might not be able to run all tasks. My next proposition will try to take that in account.


Thanks for your comments !","01/Aug/08 07:12;ddas;I was looking at the patch. Unfortunately, this patch has gone stale. Could you pls regenerate the patch. Alternatively, pls let me know the trunk revision you generated the patch against. Also it seems to me that TaskProcess.java should have been added as part of the patch but the patch file seems to try to delete that (currently) non-existent file.
Thanks!","04/Aug/08 09:15;ddas;I am looking at the JVM reuse issue particularly. I think that can be done independently of this jira. My idea is to still have the umbilical protocol for task executions. The difference with the current approach is that that JVM would not exit at the end of the current task execution but the TaskTracker.Child.main would have a loop inside. The crux of the loop would look something like:
{code}
while (true) {
   Task t = umbilical.getTask();
   if (t != null) {
     t.run(umbilical);
   }
}
{code}

If JVM reuse is enabled, the tasktracker would have 1 JVM per job. The tasktracker would kill the JVM process when the corresponding job finishes. If it is already running too many JVMs, it would kill one based on LRU. 

Also, the JVM would kill itself whenever it encounters an exception (a running task on encountering an exception would result in the shutdown of the JVM). That would take care of problems such as a long running JVM running out of memory. The only thing is a task attempt might get penalized for no fault of the task (in the case where the JVM is leaking memory, for example) since that attempt is bound to fail. So this is something to watch out for but I am not too worried about it at this point of time.

Overall, the above would probably benefit jobs with many short running tasks (like for maps which are short lived).

I am thinking of submitting the patch with the above for HADOOP-249.

Thoughts?","04/Aug/08 10:11;yhemanth;One thing to consider here is resources used per task. For e.g. memory that is used by a task attempt should be freed to as much extent as possible before the next task is run. This way, any per task limits like those in HADOOP-3581 can be enforced correctly.","04/Aug/08 12:29;steve_l;In the ant context In-VM execution of things like Javac and junit was always good for speed, but bad for long term memory consumption, both of the normal heap and of PermGenHeapSpace
* you can't unload a class until all references are removed
* you can't unload a classloader until all classes in it are removed
* its very hard to be sure that classes/classloaders are fully unloaded.
You also need to be sure that you dont pass down any of your own classes to the code, just those in the JDK, which forces you to add extra code and tests, especially for the com.sun stuff

Here is some of the code, which is intended to give a hint as to how hard it is, rather than provide a starting point:

http://svn.apache.org/viewvc/ant/core/trunk/src/main/org/apache/tools/ant/util/JavaEnvUtils.java?view=markup
http://svn.apache.org/viewvc/ant/core/trunk/src/main/org/apache/tools/ant/AntClassLoader.java?view=markup

Unless you are planning on killing the tasktracker process on a regular basis and restarting it, a more reliable alternative might be for the task tracker to start and communicate with a secondary process that the tasktracker can keep alive, but which lets you start even more work. This is effectively what smartfrog does, using RMI to communicate between processes. Again, classloaders are painful to work with, but you can kill the children more easily. It has proven to work better over long-lived deployments -but took a lot more engineering effort. 

","04/Aug/08 12:59;ddas;Hemanth, yes, the memory _should_ be freed up but in the cases where a task is using direct buffers and so on, which makes GC hard, one of the downstream tasks would suffer. But that's not the most typical use case. I hate to make the jvm reuse a per job user configurable option but if I do so, then users can turn this feature off if they see their job behaving erratically. Would that work?
Steve, actually, the task is run as a separate process. Its just that the same JVM would be reused for running more tasks of the same job (so classloader issue is a non-issue here). So the TT is not affected by this really. ","04/Aug/08 13:26;steve_l;@Devaraj

If its just for tasks of the same job, then yes, life is simple: no classloader problems, and when the job is finished it will ultimately die. As long as the job's code doesnt create giant static data structures or do bad things with native libraries, you should be ok. ","04/Aug/08 17:07;cutting;Devaraj: This sounds like a fine design.  One JVM is launched per job, so the per-job overhead should be much reduced, with threads in that job per task, right?  This should ideally be configurable per-job (rather than per-cluster) and we could consider switching to this by default if performance is considerably faster for common jobs (e.g., sort).","04/Aug/08 20:29;ddas;Doug, I hadn't considered the thread-per-task approach. I was considering sequential executions of tasks (perhaps we would sometimes have more than one JVM for the same job in memory subject to the available free slots). The slots used in the thread-per-task case would be the number of concurrently running threads (read tasks) across all the JVMs, right? 
It does bring in a complication to do with integration with HADOOP-3581 but it should be possible to count how many task slots a JVM is currently using (number of concurrently running tasks), and, factor that in into the resource utilization issue that HADOOP-3581 deals with. 
The other complication is to figure out whether the framework code is clean enough (or threadsafe) that multiple instances of the Map/Reduce task can be active within one process at any given point of time. Ditto with the application code - can we assume that apps have been written to be thread safe.","04/Aug/08 21:00;tomwhite;Deveraj, This approach looks good to me. One of the challenges is to handle the user logs, which are managed using the shell (since HADOOP-1553). If the JVM is reused for more tasks, how does the shell redirection know about this? If the JVM handled the logging (by redirecting System.out and System.err), then it would be fine, but the reason it was moved to the shell was because of performance (see numbers in HADOOP-1553).","04/Aug/08 21:13;cutting;The motivation for re-using the JVM is presumably performance.  Since most nodes are multicore these days, it would be a shame if tasks of a job were executed serially on each node, no?  So, if you don't intend to use threads, then I think you need to lift the one-jvm-per-job limit.  The tasktracker could run instead one jvm per task slot, restarting them when tasks arrive from a different job.  Could that work?","04/Aug/08 21:41;craigm;
HADOOP-3280 and other places note that longer term Hadoop task tracker will run as root and setuid to the user named in the JobConf. 

Brice, I note that you are using a script to setuid to one of a pool of users. 

My question is whether this patch implements enough to the su to the specified user.","05/Aug/08 04:04;ddas;Doug, yes, as i had stated in my earlier comment, we might have more than one active JVM for a job. I was thinking of having the option to do with max JVMs in memory configurable where the default max could be equal to the #slots. But maybe it makes sense to just limit it to the number of slots.
Tom, you raise an interesting question on the details. Let me see how that would work.","05/Aug/08 06:25;ddas;Everyone, pls continue discussion on HADOOP-249 on the specific topic of JVM reuse.","17/Jun/12 04:59;qwertymaniac;HADOOP-3280 and MAPREDUCE-249 covered these ideas already. I am resolving this as a dupe of those alternatives - I do believe the planned feature (minus of the threads-per-task goal) are present in one version or another already today (in different forms).

For threads per task, if that is still a valid gain beyond what MR2's AM does, can be discussed over a new issue.","17/Jun/12 05:00;qwertymaniac;Sorry, previous comment should have said HADOOP-249, not MAPREDUCE-249.","17/Jun/12 05:01;qwertymaniac;And just for ref, MR2 details can be drilled down from via MAPREDUCE-279.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job should be able to specify whether task vm is 32 or 64 bit,MAPREDUCE-444,12360435,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Fixed,,nidaley,nidaley,13/Jan/07 01:50,17/Jan/12 03:49,12/Jan/21 09:52,17/Jan/12 03:49,,,,,,,,,,,,,,,0,,,,,"Perhaps a job should be able to specify whether it wants it's task VM's to be 32 or 64 bit.  This could be accomplished by the -d32 and -d64 java options when the task VM is exec'd.  This becomes important for native libs.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-01-16 10:31:39.754,,,false,,,,,,,,,,,,,,,,,,148800,,,,,Tue Jan 17 03:49:52 UTC 2012,,,,,,,"0|i0988v:",51768,,,,,,,,,,,,,,,,,,,,,"16/Jan/12 10:31;qwertymaniac;Could be done with child opts/etc. I think. Closing out since this has not seen any demand nor does it appear to be useful from my experience.","17/Jan/12 03:49;acmurthy;Fixed via MR-279.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add a link to the dfs from job tracker WI,MAPREDUCE-456,12354716,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Won't Fix,omalley,yarnon,yarnon,03/Nov/06 17:46,16/Jan/12 10:14,12/Jan/21 09:52,16/Jan/12 10:14,,,,,,,,,,,,,,,0,,,,,add a link to the dfs from job tracker WI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-01-16 10:14:30.485,,,false,,,,,,,,,,,,,,,,,,148811,,,,,Mon Jan 16 10:14:30 UTC 2012,,,,,,,"0|i0irin:",107552,,,,,,,,,,,,,,,,,,,,,"16/Jan/12 10:14;qwertymaniac;Don't see why we ought to do this cause JT isn't related to HDFS in any way.

Job XMLs still show up full FS URIs, so can be accessed from UI already.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dynamic configuration of mapred.map/reduce.tasks,MAPREDUCE-2700,12353411,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,,oae,oae,17/Oct/06 15:39,16/Jan/12 10:08,12/Jan/21 09:52,,,,,,,,,,job submission,,,,,,0,newbie,,,,"Since the optimal value for these 2 configuration properties heavily depends on the numer of nodes, also examined here
http://wiki.apache.org/lucene-hadoop/HowManyMapsAndReduces
 would it be not more comfortable to configure relations to numberOfNodes ?
So you hav'nt to change it all the time the number of nodes changes...",,jrideout,neelesh77,oae,qwertymaniac,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-01-16 10:08:57.746,,,false,,,,,,,,,,,,,,,,,,67904,,,,,Mon Jan 16 10:08:57 UTC 2012,,,,,,,"0|i0irgf:",107542,,,,,,,,,,,,,,,,,,,,,"16/Jan/12 10:08;qwertymaniac;Agree that it makes sense to have #-reducers auto-tuned to cluster size.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding caching to Hadoop which is independent of the task trackers.,MAPREDUCE-458,12347835,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Not A Problem,omalley,mahadev,mahadev,12/Aug/06 00:01,16/Jan/12 09:51,12/Jan/21 09:52,16/Jan/12 09:51,,,,,,,,,,,,,,,0,,,,,"It would be nice to have a feature in Hadoop that could cache files locally that is independent of TaskTrackers and JobTrackers. Hadoop-288 caching is dependent on the tasktrackers. In an environment where you would dynamically bring up and down the TaskTrackers for resource sharing, that is problematic. It would be good to have this feature wherein you can install tasktrackers/jobtrackers on these machines using this caching mechanism. The caching feature could use something like Bittorent /http/rsync to copy the main hadoop.jar.",,brianmackay,gautamk,martind,qwertymaniac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-288,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2006-09-05 20:41:11.0,,,false,,,,,,,,,,,,,,,,,,148813,,,,,Mon Jan 16 09:51:44 UTC 2012,,,,,,,"0|i0ird3:",107527,,,,,,,,,,,,,,,,,,,,,"05/Sep/06 20:41;cutting;Perhaps this argues that all file caching should be outside of the TaskTracker, in a separate library.  The TaskTracker could use this for the job.xml and job.jar.","12/Jan/07 10:49;tucu00;Need to run MR jobs that add up to 10Mb in the size of the of the JAR file being submitted (dependent jars being the main culprits).

This slows down things as the JAR has to be copied to all nodes participating of the MR in order to run the job.

Many of these jobs are the same MR using different input/output and arguments.

These MR are fired thousand of times a day, even if copying them with high priority it is a few seconds per job.

If somehow we could upload a Job JAR with and ID and then use it repeatedly just by sending the JOB JAR ID and a JobConf file it would be great.

It is not possible/practical to set the JARs in hadoop/lib as the clusters are shared and we need to be able to udpate JARs without bringing down the cluster.
","16/Jan/12 09:51;qwertymaniac;This discussion has gone stale. I wonder if we see major issues with current DistributedCache features today that does proper caching and works across jobs too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
write Junit test cases that run the Hadoop examples,MAPREDUCE-2008,12470477,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,,kanjilal,kanjilal,29/Jul/10 22:40,13/Jan/11 02:28,12/Jan/21 09:52,,0.20.2,,,,,,,,examples,test,,,,,0,,,,,"I will be adding junit test cases that run the hadoop examples, specifically I'll be adding a test suite that tests everything under the src/examples/org/apache/hadoop/examples package.  Since this is my first contribution to the project I'll need some help to figure out how to submit this",N/A,chaitk,cos,eli,garymurry,jghoman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,72000,72000,,0%,72000,72000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/10 03:30;kanjilal;HADOOP-MR-2008.patch;https://issues.apache.org/jira/secure/attachment/12455006/HADOOP-MR-2008.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2010-07-29 22:48:35.779,,,false,,,,,,,,,,,,,,,,,,149927,,,,,Mon Sep 27 06:17:54 UTC 2010,,,,,,,"0|i0ixxb:",108590,Additional junit tests written to run to drive the examples,,,,,,,,,,junit driver for the examples,,,,,,,,,,"29/Jul/10 22:48;jghoman;Hi Saikat.  What were you hoping to accomplish with these new unit tests? The example programs are meant to be a quick demo of Hadoop's abilitiles; how you are hoping to fit them into the unit tests?","29/Jul/10 22:55;kanjilal;Hello Jakob,
I am interested in contributing to the hadoop project overall and am new to the community, to start this effort I took a look at the Project Suggestions page and picked the first task, please take a look at http://wiki.apache.org/hadoop/ProjectSuggestions and follow through to the first task under the Test Projects that has 16 hours allocated to it.  This is what I was hoping to tackle,  please let me know if I am misunderstanding what this task is asking but I thought this would be a good way to begin my contributions.

Regards","29/Jul/10 23:11;jghoman;Ah, I see.  I think I know what the intention of this project idea was.  The examples could used as good integration tests and junit would be a reasonable way to execute them.  The tests themselves would consist of starting up minidfsclusters, prepping them for each example as needed, and executing them.  Post-project split the examples are located in the MapReduce project, so this JIRA should probably end up there.  This would be a good step toward clarifying the difference between unit and integration tests, which is a bit muddled at the moment in the source tree.","29/Jul/10 23:20;kanjilal;Jakob,
Thanks for the big picture explanation, I'll first finish the test suite containing test classes to drive all the examples, move this jira over to the MapReduce project and then as I mentioned earlier may need some help to submit this (docs etc on the submittal process for this would be most helpful, I see some docs on submitting patches so not sure if this would be considered a patch or would follow some other process) from y'all since this will be my first submittal.
Regards","30/Jul/10 02:20;cos;bq. This would be a good step toward clarifying the difference between unit and integration tests, which is a bit muddled at the moment in the source tree.

Right on! Because examples aren't unit tests in any sense of this word. They might be functional tests at best. As for integration tests: I'd agree that having something around mini-clusters might pass as integration. However, I tend to suggest that actual integration validation needs to be done in a real cluster environment.
","20/Sep/10 03:30;kanjilal;Submitting patch to address this issue, added examples for all the tests, all tests running successfully","20/Sep/10 04:08;amareshwari;The issue is not committed to any branch. So, reopening it.

The attached patch needs a review and commit.","20/Sep/10 04:09;amareshwari;Making it Patch available","20/Sep/10 05:15;kanjilal;Hi Amareshwari,
Just following up since this is my first time submitting, is there anything else I need to do on this on my end, I tested this against the trunk for mapreduce.

Sent from my iPhone



","27/Sep/10 06:10;amareshwari;Saikat, some comments on the patch:
* The tests should not run by calling Main directly. They should be run as ToolRunner().run. See TestTeraSort for example.
* System.exit call is commented in all the examples. That should be removed.
* The input for the examples is never generated. Are they running on empty input?
* There should be a validation for every test to ensure that the example is run as expected and has produced the expected output.","27/Sep/10 06:17;amareshwari;bq. System.exit call is commented in all the examples. That should be removed.
I meant ""It should not be commented."".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Temporary Fix when using Apache Harmony, can't handle default access modifier in JvmContext.java",MAPREDUCE-2190,12480128,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,,gcabrer,gcabrer,16/Nov/10 16:41,16/Nov/10 17:48,12/Jan/21 09:52,,0.21.0,,,,,,,,,,,,,,0,,,,,"Using Apache Harmony Select as the JRE to test Hadoop Map/Reduce, we came across the following error:

INFO: In JvmRunner constructed JVM ID: jvm_20101023030236354_0001_m_-37103043 
Oct 23, 2010 3:03:09 AM org.apache.hadoop.mapred.JvmManager$JvmManagerForType spawnNewJvm
INFO: JVM Runner jvm_20101023030236354_0001_m_-37103043 spawned.
Oct 23, 2010 3:03:20 AM org.apache.hadoop.mapred.JvmManager$JvmManagerForType$JvmRunner runChild
INFO: JVM : jvm_20101023030236354_0001_m_-37103043 exited with exit
code 1. Number of tasks it ran: 0
Oct 23, 2010 3:03:20 AM org.apache.hadoop.mapred.TaskRunner run
WARNING: attempt_20101023030236354_0001_m_000000_0 : Child Error
Throwable occurred: java.io.IOException: Task process exit with
nonzero status of 1.
       at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:236)


Oct 23, 2010 3:03:31 AM org.apache.hadoop.security.Groups <init>
INFO: Group mapping
impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping;
cacheTimeout=300000
Uncaught exception in main:
java.lang.IllegalAccessError: from $Proxy0 to
org/apache/hadoop/mapred/JvmContext
       at $Proxy0.<clinit>(Unknown Source)
       at java.lang.reflect.VMReflection.newClassInstance(VMReflection.java)
       at java.lang.reflect.Constructor.newInstance(Constructor.java:282)
       at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:218)
       at org.apache.hadoop.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:224)
       at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:225)
       at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:214)
       at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:242)
       at org.apache.hadoop.mapred.Child$1.run(Child.java:100)
       at org.apache.hadoop.mapred.Child$1.run(Child.java:97)
       at java.security.AccessController.doPrivilegedImpl(AccessController.java:112)
       at java.security.AccessController.doPrivileged(AccessController.java:86)
       at javax.security.auth.Subject.doAs_PrivilegedExceptionAction(Subject.java:284)
       at javax.security.auth.Subject.doAs(Subject.java:225)
       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:742)
       at org.apache.hadoop.mapred.Child.main(Child.java:96)

As a temporary fix, we are including a patch that will add the public access modifier to constructors in JvmContext.java and SortedRanges.java. It seems Apache Harmony has issues with the default access modifier. This is a problem with Apache Harmony and not Hadoop Map/Reduce, however, we wanted to introduce a temporary fix for anyone working with Hadoop and Apache Harmony.","SLE v. 11, Apache Harmony 6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/10 16:42;gcabrer;MAPREDUCE-2190.patch;https://issues.apache.org/jira/secure/attachment/12459710/MAPREDUCE-2190.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,150053,,,,,Tue Nov 16 17:48:55 UTC 2010,,,,,,,"0|i0d387:",74299,,,,,,,,,,,access modifiers,,,,,,,,,,"16/Nov/10 17:48;gcabrer;For more information relating to building Hadoop maprecue using Apache Harmony, please refer to http://wiki.apache.org/hadoop/HarmonyMapreduce",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Forrest Documentation for Dynamic Priority Scheduler,MAPREDUCE-2040,12472762,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Fixed,sandholm,sandholm,sandholm,28/Aug/10 00:29,29/Oct/10 02:04,12/Jan/21 09:52,06/Oct/10 03:48,0.21.0,,,,,0.21.1,,,contrib/dynamic-scheduler,,,,,,0,,,,,New Forrest documentation for dynamic priority scheduler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/10 00:41;sandholm;MAPREDUCE-2040.patch;https://issues.apache.org/jira/secure/attachment/12453325/MAPREDUCE-2040.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2010-10-06 03:42:44.006,,,false,,,,,,,,,,,,,,,,,,43694,Reviewed,,,,Fri Oct 29 02:04:17 UTC 2010,,,,,,,"0|i0ji2n:",111858,Forrest Documentation for Dynamic Priority Scheduler,,,,,,,,,,,,,,,,,,,,"28/Aug/10 00:41;sandholm;xdoc file and a link from the scheduler menu","06/Oct/10 03:42;tomwhite;+1

Results from test-patch:

{noformat}
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no new tests are needed for this patch.
     [exec]                         Also please list what manual steps were performed to verify this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec] 
     [exec]     -1 system tests framework.  The patch failed system tests framework compile.
{noformat}

Since this is a documentation patch the -1s are not a problem.","06/Oct/10 03:48;tomwhite;I've just committed this. Thanks Thomas!","29/Oct/10 02:04;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #523 (See [https://hudson.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/523/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a Common Data-Generator for Testing Hadoop,MAPREDUCE-2112,12475846,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,,ranjit,ranjit,05/Oct/10 11:40,05/Oct/10 16:51,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"It is useful to have a common data-generator for testing Hadoop and related projects. Such a tool
should be able to generate data in a specified format and should be able to use a Hadoop cluster
for speeding up the data-generation. This tool can then be used across Hadoop (e.g. GridMix3),
Pig, Hive, etc. reducing the need for each project to invent something like this itself.

We can use the data-generator used in PigMix2 (PIG-200) as a starting point. It is described
in [http://wiki.apache.org/pig/DataGeneratorHadoop]. Since it depends on the SDSU
Java library ([http://www.eli.sdsu.edu/java-SDSU/]) released under the GNU GPL, it has to be
modified a bit to eliminate this dependency before it can be included in Apache Hadoop.",,eli,gates,hammer,olgan,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-10-05 16:51:57.147,,,false,,,,,,,,,,,,,,,,,,150000,,,,,Tue Oct 05 16:51:57 UTC 2010,,,,,,,"0|i0ji6v:",111877,,,,,,,,,,,,,,,,,,,,,"05/Oct/10 16:51;olgan;It is important that the tool supports different column distribution so that we can simulate different kinds of data with it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Show why the job failed  (e.g. Job ___ failed because task ____ failed 4 times),MAPREDUCE-2075,12474331,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Duplicate,,knoguchi,knoguchi,16/Sep/10 22:07,17/Sep/10 04:02,12/Jan/21 09:52,17/Sep/10 04:02,,,,,,0.22.0,,,,,,,,,0,,,,,"When our users have questions about their jobs' failure, they tend to copy&paste all the userlog exceptions they see on the webui/console.  However, most of them are not the one that caused the job to fail.   When we tell them 'This task failed 4 times"", sometimes that's enough information for them to solve the problem on their own.

It would be nice if jobclient or job status page shows the reason for the job being flagged as fail.
",,rvadali,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-09-17 03:40:34.167,,,false,,,,,,,,,,,,,,,,,,149976,,,,,Fri Sep 17 04:02:02 UTC 2010,,,,,,,"0|i0ji47:",111865,,,,,,,,,,,,,,,,,,,,,"17/Sep/10 03:40;amar_kamat;Seems like a duplicate of MAPREDUCE-343.","17/Sep/10 04:02;knoguchi;Duplicate of  MAPREDUCE-343.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide sample fair scheduler config file in conf/ and use it by default if no other config file is specified,MAPREDUCE-546,12409863,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Fixed,matei,matei,matei,04/Dec/08 01:37,24/Aug/10 21:13,12/Jan/21 09:52,19/Jul/09 00:27,,,,,,0.21.0,,,,,,,,,0,,,,,"The capacity scheduler includes a config file template in hadoop/conf, so it would make sense to create a similar one for the fair scheduler and mention it in the README.",,hammer,matei,rksingh,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/09 18:12;matei;mapreduce-546-v1.patch;https://issues.apache.org/jira/secure/attachment/12412768/mapreduce-546-v1.patch","06/Jul/09 22:46;matei;mapreduce-546.patch;https://issues.apache.org/jira/secure/attachment/12412664/mapreduce-546.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2009-07-07 08:39:56.826,,,false,,,,,,,,,,,,,,,,,,148882,Reviewed,,,,Sat Jul 25 15:45:16 UTC 2009,,,,,,,"0|i0ivl3:",108211,,,,,,,,,,,,,,,,,,,,,"29/Jun/09 16:13;matei;An update on this issue: MAPREDUCE-551 added a fair-scheduler.xml.template but didn't make the mapred.fairscheduler.allocation.file point to it by default. I'll try to figure out how to do that (mainly how to find the path of the conf dir from inside the fair scheduler).","06/Jul/09 22:46;matei;Here's a patch for this issue that makes the scheduler use fair-scheduler.xml off the classpath if no other allocation file is specified through mapred.fairscheduler.allocation.file. (We keep this parameter for backwards compatibility).

The only tricky part was using the URL from the ClassLoader's getResource method instead of a String for the path to the file. I followed the example set in Configuration of having an Object as the allocation file name that may be either an URL or a String, because I didn't want to force users of the existing jobconf parameter to supply a file:// URL, and I also didn't want to append file:// in front (this doesn't work with relative paths, and although the docs have said to use an absolute path since the scheduler was released, it could be confusing).

I haven't included a unit test because the code changes are minor and I can't think of an easy way to unit test this. However, I did manually test that configurations are found whether the default config file fair-scheduler.xml is used or another file is specified through the config parameter, and also that the allocation file is reloaded at runtime in both situations.

I've also updated the docs to reflect the new functionality and emptied-out the fair-scheduler.xml template so that it creates no pools by default. The docs cover all the features that were described in the old fair-scheduler.xml.template.","07/Jul/09 08:39;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12412664/mapreduce-546.patch
  against trunk revision 791418.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/360/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/360/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/360/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/360/console

This message is automatically generated.","07/Jul/09 13:20;tomwhite;This looks reasonable to me. Could you use Configuration#getResource(String) or is there a classloader problem?

Longer term, it would be nice if the fair scheduler could migrate to using a Hadoop Configuration just like the capacity scheduler does.","07/Jul/09 18:12;matei;Here's a new patch that uses Configuration.getResource instead of the classloader code.","07/Jul/09 18:18;matei;I've thought a bit about using a Configuration for the pools file, but I'm afraid it would become very verbose. A config file like this:

{code}
<allocations>
  <pool name=""ads"">
     <minMaps>10</minMaps>
     <minReduces>5</minReduces>
     <minSharePreemptionTimeout>300</minSharePreemptionTimeout>
  </pool>
  <user name=""bob"">
     <maxRunningJobs>2</maxRunningJobs>
  </user>
</allocations>
{code}

Would become something like this:

{code}
<configuration>
  <property>
     <name>mapred.fairscheduler.pool.ads.minMaps</name>
     <value>10</value>
  </property>
  <property>
     <name>mapred.fairscheduler.pool.ads.minReduces</name>
     <value>5</value>
  </property>
  <property>
     <name>mapred.fairscheduler.pool.ads.minSharePreemptionTimeout</name>
     <value>600</value>
  </property>
  <property>
     <name>mapred.fairscheduler.user.bob.maxRunningJobs</name>
     <value>2</value>
  </property>
</configuration>
{code}

I find the first one more readable and more maintainable, especially as properties relating to the same pool are grouped together. Do you think the code reuse benefits of using Configuration outweigh the loss in usability? I actually don't think the configuration reading code will be much smaller using Configuration because we'd have to parse string names like mapred.fairscheduler.pool.ads.minReduces instead of parsing XML.","08/Jul/09 07:15;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12412768/mapreduce-546-v1.patch
  against trunk revision 791909.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/365/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/365/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/365/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/365/console

This message is automatically generated.","17/Jul/09 11:11;tomwhite;+1

I think having a configuration that works like the other configuration in Hadoop is a win for users, even if it is more verbose. And you would get improvements like HADOOP-5670 for free. But this is a discussion for another Jira - the current patch is fine for now.","19/Jul/09 00:27;matei;I've committed this. Thanks for the review, Tom!","25/Jul/09 15:45;hudson;Integrated in Hadoop-Mapreduce-trunk #29 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/29/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sqoop should have an option to create hive tables and skip the table import step,MAPREDUCE-1341,12444292,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Fixed,lfurman,lfurman,lfurman,29/Dec/09 01:50,02/Jul/10 06:31,12/Jan/21 09:52,12/Feb/10 05:16,0.22.0,,,,,,,,,,,,,,0,,,,,"In case the client only needs to create tables in hive, it would be helpful if Sqoop had an optional parameter:

--hive-create-only

which would omit the time consuming table import step, generate hive create table statements and run them.

If this feature seems useful, I can generate the patch. I have modified the Sqoop code and built it on my development machine, and it seems to be working well.",,hammer,kimballa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/10 01:51;lfurman;MAPREDUCE-1341.2.patch;https://issues.apache.org/jira/secure/attachment/12434770/MAPREDUCE-1341.2.patch","04/Feb/10 17:00;lfurman;MAPREDUCE-1341.3.patch;https://issues.apache.org/jira/secure/attachment/12434833/MAPREDUCE-1341.3.patch","05/Feb/10 22:28;lfurman;MAPREDUCE-1341.4.patch;https://issues.apache.org/jira/secure/attachment/12435023/MAPREDUCE-1341.4.patch","09/Feb/10 22:28;lfurman;MAPREDUCE-1341.5.patch;https://issues.apache.org/jira/secure/attachment/12435365/MAPREDUCE-1341.5.patch","10/Feb/10 20:16;lfurman;MAPREDUCE-1341.6.patch;https://issues.apache.org/jira/secure/attachment/12435484/MAPREDUCE-1341.6.patch","04/Feb/10 01:43;lfurman;MAPREDUCE-1341.patch;https://issues.apache.org/jira/secure/attachment/12434768/MAPREDUCE-1341.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,2010-01-06 21:43:59.349,,,false,,,,,,,,,,,,,,,,,,37320,Reviewed,,,,Fri Feb 12 16:22:37 UTC 2010,,,,,,,"0|i02swn:",14284,,,,,,,,,,,,,,,,,,,,,"06/Jan/10 21:43;kimballa;Please submit a patch for this","04/Feb/10 01:42;lfurman;Added two command line options to Sqoop:

1. --hive-create-only
This option ensures that tables are created in hive without loading data from database to HDFS

2. -- hive-overwrite
This option indicates that existing Hive tables will be replaced with new ones.","04/Feb/10 01:51;lfurman;The patch contains external directories specific to my local development machine.","04/Feb/10 01:52;lfurman;New patch has been uploaded - it contains no external directories that could break the build.","04/Feb/10 09:48;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12434770/MAPREDUCE-1341.2.patch
  against trunk revision 906228.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 24 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/300/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/300/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/300/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/300/console

This message is automatically generated.","04/Feb/10 17:00;lfurman;Fixed the unit test TestTableDefWriter and uploaded a new patch: MAPREDUCE-1341.3.patch.","04/Feb/10 20:48;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12434833/MAPREDUCE-1341.3.patch
  against trunk revision 906228.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 27 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/435/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/435/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/435/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/435/console

This message is automatically generated.","05/Feb/10 00:42;kimballa;Patch looks pretty good -- just a couple of small issues:

Sqoop.java -- Please use spaces not tabs.

SqoopOptions.doCreateTableOnly(), doOverwriteTable(): maybe call these doCreateHiveTableOnly / doOverwriteHiveTable? There are a lot of ""tables"" floating around... the field names include ""Hive"" which is good. Getters should reflect that too.

Please add --hive-create-only, --hive-overwrite to the documentation in src/contrib/sqoop/doc/Sqoop-manpage.txt and hive.txt. These files are in ASCIIDOC format -- see http://powerman.name/doc/asciidoc and http://www.methods.co.nz/asciidoc/userguide.html if you need syntax help here.
","05/Feb/10 22:42;lfurman;Aaron,

Thank you for your feedback. I have made the following changes to the patch:

1. In SqoopOptions class, renamed the variables to createHiveTableOnly and overwriteHiveTable, as well as their corresponding getter methods.

2. Modified the documentation in src/contrib/sqoop/doc/Sqoop-manpage.txt and hive.txt files. Please let me know if the updated instructions are clear.

3. Sqoop.java shouldn't have tab characters anymore. I use IntelliJ IDEA for my development, and it's possible that last time it didn't convert tabs to spaces properly. Let me know if you still see the occurrences of tab characters, and I will resubmit the patch.","09/Feb/10 22:10;lfurman;It looks like the hudson build hasn't picked up the latest patch - MAPREDUCE-1341.4.patch. Should I flip the ticket status in order to restart the build?

Thanks!","09/Feb/10 22:20;kimballa;I don't see the patch listed in http://hudson.zones.apache.org/hudson/view/Hadoop/job/Mapreduce-Patch-Admin/lastSuccessfulBuild/artifact/MAPREDUCE_PatchQueue.html so yea, go through cancel patch / submit patch again.","09/Feb/10 22:22;lfurman;Cycling patch to retrigger hudson build.","09/Feb/10 22:28;lfurman;For the purpose of updating the hudson build queue, I am uploading another copy of the patch - MAPREDUCE-1341.5.patch, which is the same as the previous one - MAPREDUCE-1341.4.patch.","09/Feb/10 22:31;lfurman;Aaron, it doesn't seem to populate the queue - does it usually happen immediately or after some time?","09/Feb/10 22:33;lfurman;Never mind, it is there now. Thank you!","10/Feb/10 14:19;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12435365/MAPREDUCE-1341.5.patch
  against trunk revision 908321.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 21 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/311/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/311/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/311/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/311/console

This message is automatically generated.","10/Feb/10 20:17;lfurman;Uploaded new patch - MAPREDUCE-1341.6.patch, this should fix the most recent problems with build.","11/Feb/10 22:22;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12435484/MAPREDUCE-1341.6.patch
  against trunk revision 908321.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 27 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/314/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/314/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/314/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/314/console

This message is automatically generated.","12/Feb/10 02:19;kimballa;+1; patch #6 looks good to me. If someone could commit this, that'd be superb.
","12/Feb/10 02:31;lfurman;Thanks, Aaron!","12/Feb/10 05:16;tomwhite;I've just committed this. Thanks Leonid!","12/Feb/10 11:05;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #235 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/235/])
    ","12/Feb/10 16:22;hudson;Integrated in Hadoop-Mapreduce-trunk #233 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/233/])
    . Sqoop should have an option to create hive tables and skip the table import step. Contributed by Leonid Furman.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
There is little information provided when the TaskTracker kills a Task that has not reported within the timeout (600 sec) interval - this patch provides a stack trace of the task ,MAPREDUCE-449,12402818,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Duplicate,,jason_attributor,jason_attributor,21/Aug/08 21:18,07/May/10 05:32,12/Jan/21 09:52,07/May/10 05:32,,,,,,,,,,,,,,,0,,,,,"When we have a task that is killed for not reporting, sometimes there is an obvious programming error, and sometimes the reason the job didn't report is unclear.
This patch will cause the TaskTracker to try to generate a stack trace of the offending task before the task is killed.
Given how opaque process control is in java, a program is run to generate the stack trace, using the PID extracted from the undocumented UNIXProcess class

The attached patch is against 0.16.0, as that is the release we use.
This will only work on Unix machines -- or JVM's what use the java.lang.UNIXProcess implementation for the java Process object.
The script that generates the stack trace is very linux specific.
The code changes will run on jvm's where the UNIXProcess class is not available, without failure, but no stack trace will be generated.
",,cutting,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/08 21:18;jason_attributor;0.16_patch;https://issues.apache.org/jira/secure/attachment/12388695/0.16_patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2008-08-21 22:28:10.008,,,false,,,,,,,,,,,,,,,,,,148805,,,,,Fri May 07 05:32:28 UTC 2010,,,,,,,"0|i0iucn:",108011,,,,,,,,,,,,,,,,,,,,,"21/Aug/08 21:18;jason_attributor;Patch against 0.16","21/Aug/08 22:28;steve_l;This could be really useful; anything to get the PID of a forked process would be handy. As you note, UNIXProcess is undocumented and only likely to surface on sun-derived JVMs; the other risk is instability of their private code. But it would be useful, in other places in the apache portfolio.

* all code to deal with this class should be outside TaskRunner; a separate class for use on demand, 
* the class should include a condition that warns that that the operation is going to work 
* To test, fork a process that Sleeps for 30s or so, and before that sleep has finished, try to get a stack dump. 
* I could imagine a kill() method being useful too.

","21/Aug/08 22:47;jason_attributor;The window's jvm equivalent is java.lang.ProcessImpl.java and has the process handle in the private long variable handle.

I suspect having an equivalent for this would be straight forward but I don't currently develop under windows so I didn't try to implement an equivalent.

The code uses reflection right now to get at the private variables - definitly  fragile. I tried very hard to ensure that there would be no crashes if the something was not as expected.
","22/Aug/08 06:13;vinodkv;Elsewhere on HADOOP-3581(yet to be committed), we used a different method of obtaining the pid of the launching task. For this, just before the task is launched, the launching shell prints out the pid to a pid file(echo $$ > pidfile), and then the task is exec'ed. Later this pid file is read and then is used for memory management of the process. I guess the same pidfile can be used for this issue too. This method works everywhere the shell feature works.

But I agree in general that a getPid() method is a good to have.

bq. [..] UNIXProcess is undocumented and only likely to surface on sun-derived JVMs; the other risk is instability of their private code [..]
We can write native code to avoid the above. But yes, this implies adding another native library.

As, a side note, the JAVA getPid() bug(http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4244896) is past 9 year celebrations :).

Also can we do anything similar to get more information when streaming/pipe tasks timeout too?","22/Aug/08 11:00;steve_l;Jason> I suspect having an equivalent for this would be straight forward but I don't currently develop under windows so I didn't try to implement an equivalent.

no direct equivalent to kill -QUIT, I think. And it makes testing harder. Probably best to stick to unix systems.

Using reflection there's a risk this wont work under the security manager; the code should catch SecurityExceptions. But I'd be happier with a bit of reflection abuse than another native library. 

Vinod> Also can we do anything similar to get more information when streaming/pipe tasks timeout too?

It's not so easy if they are native code; they wont have java stacks. 


Vinod> As, a side note, the JAVA getPid() bug(http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4244896) is past 9 year celebrations .

it is not alone, try searching for  happy birthday site:bugs.sun.com","07/May/10 05:32;amareshwari;Fixed by MAPREDUCE-1119",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
job output chroot support,MAPREDUCE-1666,12460970,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Invalid,,aw,aw,01/Apr/10 20:43,01/Apr/10 22:13,12/Jan/21 09:52,01/Apr/10 22:13,0.20.2,,,,,,,,job submission,,,,,,0,,,,,"It would be useful to be able to submit the same job and have it chroot the output to a different base directory before execution.  This would allow for input to be the same, but output different for the same job over multiple runs (potentially by different users).",,acmurthy,hammer,jghoman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-04-01 21:14:32.326,,,false,,,,,,,,,,,,,,,,,,149673,,,,,Thu Apr 01 22:13:17 UTC 2010,,,,,,,"0|i0jh7z:",111720,,,,,,,,,,,,,,,,,,,,,"01/Apr/10 21:14;acmurthy;By 'output' you mean on HDFS? ","01/Apr/10 21:17;aw;Yes.  I'm not sure whether this should be a HDFS or MR bug tho.  So I started here. :)","01/Apr/10 21:32;acmurthy;Why doesn't '-output' switch (-Dmapred.output.dir) suffice then?","01/Apr/10 21:35;aw;Oooo. Maybe.  I'll see if that works for the use case these guys have.  Thanks.  I didn't know that existed. :)","01/Apr/10 21:37;acmurthy;No worries, once your highness is satisfied please close this one! :)","01/Apr/10 22:13;aw;Muhaha!  I'm quite stunned to see I'm held in such high regard!
 
Anyway, I've been told to close this out, as the people who really wanted to do this say this will work.

Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"OutputFormat should have a ""close"" method called after all the reducers have completed",MAPREDUCE-450,12410840,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,Duplicate,,marz,marz,16/Dec/08 21:23,11/Feb/10 06:18,12/Jan/21 09:52,11/Feb/10 06:18,,,,,,,,,,,,,,,0,,,,,"It would be very useful for OutputFormat's to have a ""close"" method so that any global logic for outputting can take place there. For example, to output a meta data file along with all the reduce outputs. The close method should have the following signature and be run after all the reducers have finished, but before the ""work output path"" is renamed to the final output path:

void close(FileSystem fs, JobConf job, Progressable progress) throws IOException",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-02-11 06:18:19.877,,,false,,,,,,,,,,,,,,,,,,148806,,,,,Thu Feb 11 06:18:19 UTC 2010,,,,,,,"0|i0ivpz:",108233,,,,,,,,,,,,,,,,,,,,,"11/Feb/10 06:18;amareshwari;This functionality is provided by OutputCommitter. See HADOOP-3150",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow specifying min shares as percentage of cluster,MAPREDUCE-549,12414793,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,,matei,matei,14/Feb/09 08:10,20/Jun/09 08:01,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"Currently the guaranteed shares for pools in the fair scheduler are specified as a number of slots. For organizations where a group pays X% of the cluster and the actual number of nodes in the cluster varies due to failures, expansion, etc over time, it would be useful to support a guaranteed share given as a percentage too. This would just let you write in the config file something like <minMaps>5%</minMaps> instead of <minMaps>42</minMaps>. The scheduler would need to recompute what this means in terms of number of slots on every update (probably through some kind of update(ClusterStatus) method in PoolManager).",,matei,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-02-14 15:18:00.989,,,false,,,,,,,,,,,,,,,,,,148884,,,,,Tue Feb 17 04:14:55 UTC 2009,,,,,,,"0|i0iw5z:",108305,,,,,,,,,,,,,,,,,,,,,"14/Feb/09 15:18;yhemanth;+1. This would be a great addition. However, are you suggesting that we support both modes ? So, if specified without a '%', this would become an absolute number, and otherwise it would be a percentage ?

Expressing it purely as percentages has a very simple operational advantage that it can be validated. For e.g. if the percentages add up to more than 100, then we could warn the administrators or even stop the scheduler. Allowing both modes would probably prevent us from such validations, no ?","15/Feb/09 00:47;matei;I suggested we allow both modes because we have been using the fixed-number-of-slots model at Facebook. We mostly use min share to guarantee that ETL jobs and other jobs with deadlines get enough slots to finish in time. Thus it makes more sense to set the min share to something small but just enough to be sure that the job will finish, and this is easier to think of in terms of slots than percents. I understand that the Yahoo use case is different and requires assigning percent shares to organizations.","16/Feb/09 11:14;yhemanth;Matei, the fixed number of slots use case makes sense. So, I guess we need both modes. 

I am considering how this would compare to having a separate config variable for expressing this as a percentage. i.e we could either specify minMaps and minReduces, or minCapacityPercent in the config file. 

The advantages are:

- I think it is a little simpler to manage. If someone misses a percentage symbol without intending to, it could lead to some weird results in the earlier suggestion because it would still be a valid value for the configuration. And like I mentioned, one can add up the percentages and do some simple check that it doesn't exceed 100 and so on.
- I don't see a need of having two separate variables in the percentage mode. Like you've mentioned in the description, groups pay for X% of the cluster capacity, and generally not X% maps and Y% reduces. So, setting up two numbers, which almost always are going to be the same doesn't seem to be required.

The disadvantage I see is:
- One more configuration variable
- And what happens if both are specified. We could either have one of the two modes take precendence, or just error out. But either way, the semantics should be decided.

In spite of the disadvantages, given how central this, I would favor separate configuration options.

Thoughts ?","16/Feb/09 19:20;matei;I think the second config variable makes sense. When we describe it in the documentation, we can say ""either you configure the pool to a percent of total cluster capacity, or specify an exact number of maps and reduces"". Certainly nobody pays for 10% of reduces but 20% of maps.

The main difficulty I see with this and fair scheduler configuration in general is how to provide feedback to the operator. Right now, if the fair scheduler detects something wrong in the config file at startup, it throws a RuntimeException to prevent the JobTracker from starting, thus bringing the problem to the operator's attention. If it detects a problem when reloading a config file at runtime, it logs it as an ERROR in the JobTracker's log4j log but it keeps the old settings of the config variables so that the cluster can continue operating. Because most of the config info is visible on the web UI, the idea is that an operator will look at the web UI to see whether their change went through. There are several possible enhancements to improve this:
* Rather than making the config reloading something passive, maybe we can require the operator to say ""reload the config file now"" through either a command-like invocation or a button on the web UI. In response to this, we can display any errors.
* If things don't make sense at runtuime, e.g. total min shares add up to more than 100% because nodes went away, maybe we can display a warning on the web UI or email the operator (?). (By the way, right now, if min shares exceed 100%, the scheduler doesn't crash or behave weirdly; it just treats them as weights and normalizes them to 100%; so it's not the end of the world, but the operator should know about it.)","17/Feb/09 04:14;yhemanth;+1 for a more passive load of the configuration.
+1 also for logging warnings on runtime errors. IMO, it is reasonable to have a place to display events that need attention on the web ui or have some other alerting option, and this is not just for the scheduler.

One more suggestion we've had from our Operations team was to provide a validator tool that can validate configuration (given a conf file, for instance), before it can be deployed on the cluster.

These all make sense, but I believe are a subject of different JIRAs.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow a pool to hold onto its reservation for a while even if it contains no jobs,MAPREDUCE-547,12427693,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,,matei,matei,11/Jun/09 23:24,20/Jun/09 08:01,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"To better handle the case where a pool is submitting a sequence of jobs one at a time, it could be useful to let a pool hold onto its min share of slots for some timeout after its last job finishes. If no new job is submitted during this timeout, the slots can go back to being shared by other pools.",,aaa,matei,vinodkv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,148883,,,,,2009-06-11 23:24:14.0,,,,,,,"0|i0ixfr:",108511,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jobqueue_details.jsp should support a 'refresh' attribute,MAPREDUCE-535,12406663,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Minor,,sreekanth,vinodkv,vinodkv,17/Oct/08 08:42,20/Jun/09 07:59,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"jobqueue_details.jsp should support a 'refresh' attribute for automatic page-refresh, just like jobdetails.jsp.",,sreekanth,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/08 11:26;sreekanth;HADOOP-4442.patch;https://issues.apache.org/jira/secure/attachment/12394898/HADOOP-4442.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2008-10-20 06:25:47.184,,,false,,,,,,,,,,,,,,,,,,148871,,,,,Fri Nov 28 11:26:59 UTC 2008,,,,,,,"0|i0iuy7:",108108,,,,,,,,,,,,,,,,,,,,,"20/Oct/08 06:25;sreekanth;When the jobqueue_details.jsp was written it was meant to be a static reference to the list of the jobs which are currently scheduled the job queue. The job queue details page just provides the user the position of his job with respect to the other jobs in the queue. The extra detail like actual progress is just an added information. Moreover, the actual position changes with respect to scheduling would happen due to user actions like another job submitted with higher priority while user job is in waiting or user changing its priority. Both of these actions are highly infrequent compared to normal submission of the job. In those cases user can just manually refresh once in a while on his own to check for the position of the job in the job queue.","28/Nov/08 11:26;sreekanth;Attaching a patch which auto-refreshes the page every two minutes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rest API for retrieving job / task statistics ,MAPREDUCE-2818,12407604,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Trivial,Implemented,,fleibert,fleibert,31/Oct/08 15:55,28/Sep/15 21:10,12/Jan/21 09:52,10/Jul/12 16:21,,,,,,,,,,,,,,,3,,,,,"a rest api that returns a simple JSON containing information about a given job such as:  min/max/avg times per task, failed tasks, etc. This would be useful in order to allow external restart or modification of parameters of a run.
",,alexlod,amirhyoussefi,cdouglas,cutting,cwensel,dehora,fleibert,hammer,kimballa,mantonov,pacoid,philip,qwertymaniac,romainr,sabir ayappalli,svenkat,tlipcon,vinodkv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/08 15:54;fleibert;HADOOP-4559v2.patch;https://issues.apache.org/jira/secure/attachment/12396607/HADOOP-4559v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2008-11-03 11:14:01.42,,,false,,,,,,,,,,,,,,,,,,65397,,,,,Mon Sep 28 21:10:31 UTC 2015,,,,,,,"0|i08aw7:",46364,adds api features to the webapp part of hadoop allowing to retrieve task stats for a given job,,,,,,,,,,,,,,,,,,,,"31/Oct/08 16:14;fleibert;This will provide a very simple api that allows to retrieve statistics about the tasks for a given jobid - such as average, min and max times per task, failed tasks per job, total job runtime, etc. ","03/Nov/08 11:14;steve_l;- although its a JSP page, everything, including printing, is done in Java code. It would either be better implemented as a pure servlet, or the output redone as <%= %> operations to produce something more JSP-y

- I recommend HtmlUnit as the best extension to JUnit for testing web pages; it could grab the pages and look at the content. ","03/Nov/08 17:00;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12393159/HADOOP-4559.patch
  against trunk revision 709609.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3515/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3515/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3515/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3515/console

This message is automatically generated.","03/Nov/08 19:44;pacoid;HADOOP-4559 provides a workaround for part of the issue described in HADOOP-3850.  Can now access log data by making REST calls to JSP provided in 3850. For example:

   RunningJob currentjob = JobClient.runJob(job_conf);

   JobID id = currentjob.getID();
   String url = ""http://localhost:50030/api.jsp?info=jobdetails&id="" + id.getId();

   HttpClient client = new HttpClient();
   HttpMethod method = new GetMethod(url);

   client.executeMethod(method);
   String logData = method.getResponseBodyAsString();
   method.releaseConnection();
","08/Nov/08 00:49;cdouglas;bq. although its a JSP page, everything, including printing, is done in Java code. It would either be better implemented as a pure servlet
+1

* Please format the code according to the [conventions|http://wiki.apache.org/hadoop/HowToContribute#head-59ae13df098fbdcc46abdf980aa8ee76d3ee2e3b].
* There's a fair amount of dead code in this patch, e.g.
{noformat}
+	    StringBuffer sb = new StringBuffer();
+	    boolean isFirst = true;
+	    for (String kv : kv_pairs) {
+	    	
+			sb.append(kv);	
+		}
{noformat}
{{kv_pairs}} is initialized, but empty. {{sb}} is unused, save in this loop. The loop above it doesn't appear to do any productive work. StringBuilder should be used instead of StringBuffer in this context.
* If you're proposing this as a public API, it must at least have a unit test.
* Isn't most of this provided through job history?","26/Nov/08 01:51;pacoid;> Isn't most of this provided through job history?

No, not really. Not if a long-running workflow requires these measurements for automated decisions.

While a human can *read* the job history data from JSP pages, there's no current means for the app code which calls ToolRunner to obtain that data and use it to alter the workflow.","28/Nov/08 01:28;dehora;

{code}
JobID id = currentjob.getID();
String url = ""http://localhost:50030/api.jsp?info=jobdetails&id="" + id.getId();
{code}

Can't you just call this a JSP into the jobtracker instead? I hate to nitpick, but it's not REST style (client url construction), nor is the response (no links), and ASF code should (imvho) know the difference. If you want to be build REST style tooling around the tracker, I'd be happy to help with that. For example to scale this up to a lot of jobs and/or a lot of clients will require something that doesn't hammer the tracker. And iterating over the tracker seems like a linear bottleneck - O(1) key lookup would be much better. ","22/Dec/08 15:54;fleibert;the previous version was a bit dirty. I think this one is quite an improvement. We're using it to gather a lot of stats for our job runs. It's not a servlet and doesn' contain HtmlUnit - I think one stats JSP doesn't justify adding another library to the distribution - also for the sake of simplicity this remains a JSP... Hope this is valuable for someone else as well - it really is useful for us to track performance when modifying our algorithm...
","04/Feb/09 23:19;steve_l;+1 to Bill's idea for a RESTy API, one that works long-haul. ","10/Jul/12 16:21;qwertymaniac;This is no longer an issue on 2.x, with MR2 today. See http://hadoop.apache.org/common/docs/current/hadoop-yarn/hadoop-yarn-site/MapredAppMasterRest.html for the MR REST API for getting such data.","01/Aug/13 10:04;mantonov;Do you mean it's not going to be implemented for MRv1 job tracker?","01/Aug/13 15:28;qwertymaniac;Mikhail,

See MAPREDUCE-4837 perhaps.","21/Aug/13 22:20;mantonov;Thanks! but it's talking about 1.2.0, I was curious if it's going to be ported to 0.20.2.","28/Sep/15 21:10;vinodkv;Dropping fix-version from 'non-fixed' (didn't have code-fixes) JIRAs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flexible CSV text parser InputFormat,MAPREDUCE-2208,12491957,New Feature,Open,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Trivial,,,lancenorskog,lancenorskog,03/Dec/10 03:59,20/Mar/13 18:58,12/Jan/21 09:52,,,,,,,,,,,,,,,,0,,,,,"CSVTextInputFormat is a configurable CSV parser tuned to most of the csv-style datasets I've found. The Hadoop samples I've seen all FileInputFormat and Mapper<LongWritable,Text>. They drop the Longwritable key and parse the Text value as a CSV line. But, they are all custom-coded for the format.

CSVTextInputFormat takes any csv-encoded file and rearrange the fields into the format required by a Mapper. You can drop fields & rearrange them. There is also a random sampling option to make training/test runs easier.

Attached are CSVTextInputFormat.java and a unit test for it. Both go into org.apache.hadoop.mapreduce.lib.input under src/java and test/mapred/src.

This is compiled against hadoop-0.0.20.

",,aw,cdouglas,cutting,devaraj,lancenorskog,mvallebr,piccolbo,qwertymaniac,tomwhite,tzolov,xiaobogu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/10 04:01;lancenorskog;CSVTextInputFormat.java;https://issues.apache.org/jira/secure/attachment/12465205/CSVTextInputFormat.java","03/Dec/10 04:01;lancenorskog;TestCSVTextFormat.java;https://issues.apache.org/jira/secure/attachment/12465204/TestCSVTextFormat.java",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2010-12-03 20:54:44.41,,,false,,,,,,,,,,,,,,,,,,67193,,,,,Wed Mar 20 18:58:36 UTC 2013,,,,,,,"0|i0jihz:",111927,,,,,,,,,,,,,,,,,,,,,"03/Dec/10 20:54;aw;Any chance this could get changed to CombineFile/MultiFile instead?","06/Dec/10 04:30;lancenorskog;Artfully phrased. Ah, the virtues of the passive voice.

I only learned enough file i/o to make this work. And I only work in small development datasets, not production. So, no, it never impinged that it would need more stuff to support multifile directories. This is in hadoop-20.0.2. I work in Mahout, not Hadoop. I'm not upgrading Hadoop until Mahout makes me.

How did you envision this modification? It looks like the RecordReader would be public and would need a constructor matching this line:

org.apache.hadoop.mapred.lib.CombineFileRecordReader<K, V>:144
      curReader =  rrConstructor.newInstance(new Object [] 
                            {split, jc, reporter, Integer.valueOf(idx)});







","11/Dec/10 03:07;lancenorskog;Another use case: one Wikipedia format is:
{code}
1: 1664968
2: 3 747213 1664968 1691047 4095634 5535664
{code}
which would read in as:
{code}
1: 1664968
2: 3 
2: 747213 
2: 1664968
etc.
{code}

","21/Jul/11 14:33;xiaobogu;How do you handle CSV file header, or is it not supported?","22/Jul/11 08:57;lancenorskog;Hadoop assumes that it will process several files of the same format. Will every CSV file have the same header? If you split a giant CSV file into many pieces, will you reproduce the header line on the 2nd through N file?

Hadoop jobs are generally configured with total knowledge of the data. The mappers are hard-coded for the input formats.

The code could include a rule for how to decide that the first line is a header and skip over it. That would be worth adding.","22/Jul/11 10:39;xiaobogu;There are two senarioes,
1. Single huge CSV file with header.
2. Many middle CSV files with the same format and header.","27/Oct/11 01:39;mkovalenko;So what regex one would need to specify to parse the ""normal"" CSV that uses comma as a delimiter and happen to have comma in one of the values, for example:

value1,value2,""more,complex,with,commas,value3""

just providing "","" as the pattern1 will no longer work as it will produce 7 columns for the above case instead of 3.

Also consider the following use case when value contains a double quoute. In this case according to CSV escaping rules it has to be escaped by another double quote, for example:

column1,""thank you, """"User"""" for the report, again, thank you"",column3

Considering above two cases what value for pattern1 should I provide?

I think configuration of CSVTextInputFormat would be more natural if instead of patterns, one had to provide delimiter character (comma by default) and quote character (double quote by default). Then I and other users won't have to struggle with possible regex patterns (see my questions above, I'm still curious if you can come up with one).

Another benefit is that from delimiter and quote characters you can create any regexes that you need if necessary (if you want to stick to current implementation). By the way, right now you have some fragility in the implementation when you prepend user provided regex with a ""\\"". This will break in case when user supplied pattern itself starts with ""\\"".","27/Oct/11 04:23;qwertymaniac;I'd suggest reusing OpenCSV instead, if it is possible to. I do think the
license is compatible, and it is well maintained.

On Thursday, October 27, 2011, Maksym Kovalenko (Commented) (JIRA) <
https://issues.apache.org/jira/browse/MAPREDUCE-2208?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13136680#comment-13136680]
uses comma as a delimiter and happen to have comma in one of the values, for
example:
7 columns for the above case instead of 3.
In this case according to CSV escaping rules it has to be escaped by another
double quote, for example:
instead of patterns, one had to provide delimiter character (comma by
default) and quote character (double quote by default). Then I and other
users won't have to struggle with possible regex patterns (see my questions
above, I'm still curious if you can come up with one).
any regexes that you need if necessary (if you want to stick to current
implementation). By the way, right now you have some fragility in the
implementation when you prepend user provided regex with a ""\\"". This will
break in case when user supplied pattern itself starts with ""\\"".
csv-style datasets I've found. The Hadoop samples I've seen all
FileInputFormat and Mapper<LongWritable,Text>. They drop the Longwritable
key and parse the Text value as a CSV line. But, they are all custom-coded
for the format.
into the format required by a Mapper. You can drop fields & rearrange them.
There is also a random sampling option to make training/test runs easier.
org.apache.hadoop.mapreduce.lib.input under src/java and test/mapred/src.
administrators:
https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa

-- 
Harsh J
","25/Jan/13 14:01;mvallebr;Created an improved version of a CSVInputFormat, able to read multiline CSVs, just in case it interests: https://github.com/mvallebr/CSVInputFormat","20/Mar/13 14:51;tzolov;Hi Marcelo, the multiline CSVInputFormat inherits the getSplits() implementation from the parent FileInputFormat. Therefore I see a potential risk of splitting one multiline record across two (or more) different splits. 
Is this a valid concern or I might be missing something?","20/Mar/13 15:29;mvallebr;Christian, this is a valid concern. Actually, when I created the first version of this input format, I had chosen to have the CSV line numbers as the keys. Indeed, it worked well until I tested it on a cluster (amazon EMR with 15 instances). When I did, I realized the line numbers wasn't a good key, as it wouldn't get the right results among cluster nodes.
I fixed that to use the file position as input key, just as NLineInputFormat does (http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/lib/NLineInputFormat.html)
I have tested it a lot and so far I found no problems. However, if you find some problem I didn't see, please tell me, as I would be very interested in fixing it.","20/Mar/13 15:34;mvallebr;Oh, just to complement, I realized you possible meant something different from your question... You are concerned about a single CSV line be split in two among different splits, right? No, that won't happen because I wrote a custom reader, that reads N lines at a time. The getSplits method uses the reader to correct get N lines and perform the splits, so getSplits will never return half of a line, you can actually configure how many lines you want on each split.
Yes, this is also a valid concern and I took care about it. I am sorry, I hadn't understood well your question the first time I read it.","20/Mar/13 18:28;tzolov;Ah, I've only looked at the CSVTextInputFormat, which doesn't override the getSplits(). CSVNLineInputFormat does indeed.

So the CSVNLineInputFormat implementation reads the entire data set twice? Once to compute the splits and second pass for the actual read in the map tasks. 
While the double-passing approach is unavoidable (IMO) I wonder what is the performance (and perhaps the scalability) impact. Do you have any numbers comparing the standart vs. multiline implementations?
Thanks, Chris","20/Mar/13 18:58;mvallebr;CSVTextInputFormat was my first try of doing this inputFormat, but I should remove it from github later... If you take a look at the example, you will see I am only using CSVNLineInputFormat. Please don't consider using this class (CSVTextInputFormat) as it probably doesn't work.

Honestly, I would have the same concern you had when considering to use CSVTextInputFormat, as looking at getSplits code (http://grepcode.com/file/repo1.maven.org/maven2/org.jvnet.hudson.hadoop/hadoop-core/0.19.1-hudson-2/org/apache/hadoop/mapred/FileInputFormat.java#FileInputFormat.getSplits%28org.apache.hadoop.mapred.JobConf%2Cint%29) I have the impression the file could be split in the middle of a line, even in a case where you have single line text files. I could be wrong, but to the best of my knowledge, this is how it works.

However, if you use CSVTextInputFormat overriding the isSplittable() method to return FALSE, it could be useful and avoid two parses of the same file, if you have 1000s of small files instead of one huge file, like in my case. By doing that, you would assure 1 split per file.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change org.apache.hadoop.mapreduce.Cluster methods to allow for extending,MAPREDUCE-1678,12461327,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Trivial,Duplicate,qwertymaniac,kellrott,kellrott,06/Apr/10 23:24,15/Nov/11 00:48,12/Jan/21 09:52,14/Jun/11 11:51,0.23.0,,,,,0.23.0,,,job submission,,,,,,0,,,,,"Change methods in org.apache.hadoop.mapreduce.Cluster from private to protected to allow extension of cluster.
If the method createRPCProxy is changed from private to protected, then alternate cluster implementations could be written that return other ClientProtocol's. 
For example, changing the protocol some custom implementation called SimpleClient
 
ie:
public class SimpleCluster extends Cluster {
  @Override
  protected ClientProtocol createRPCProxy(InetSocketAddress addr, Configuration conf) throws IOException {
    return new SimpleClient(conf);
  } 
}


",,acmurthy,cutting,qwertymaniac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,300,300,,0%,300,300,,,,,,,,,,MAPREDUCE-2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/10 11:09;qwertymaniac;MAPREDUCE-1678.r1.patch;https://issues.apache.org/jira/secure/attachment/12451664/MAPREDUCE-1678.r1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2010-08-10 11:09:18.373,,,false,,,,,,,,,,,,,,,,,,71296,,,,,Tue Jun 14 11:51:04 UTC 2011,,,,,,,"0|i0jh8f:",111722,,,,,,,,,,,,,,,,,,,,,"10/Aug/10 11:09;qwertymaniac;Attaching patch for trivial change since there was none provided.","13/Jun/11 22:48;tlipcon;I think this use case might be provided for in MAPREDUCE-2400, right?","14/Jun/11 09:54;acmurthy;Agreed. I think we can close this in favour of MAPREDUCE-2400.","14/Jun/11 11:51;qwertymaniac;This issue's use case is handled by the changes in MAPREDUCE-2400

Resolving as duplicate.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide a jobconf property for explicitly assigning a job to a pool,MAPREDUCE-707,12429567,New Feature,Closed,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Trivial,Fixed,aheirich,matei,matei,05/Jul/09 17:29,24/Aug/10 21:14,12/Jan/21 09:52,05/Nov/09 18:42,,,,,,0.21.0,,,contrib/fair-share,,,,,,0,,,,,"A common use case of the fair scheduler is to have one pool per user, but then to define some special pools for various production jobs, import jobs, etc. Therefore, it would be nice if jobs went by default to the pool of the user who submitted them, but there was a setting to explicitly place a job in another pool. Today, this can be achieved through a sort of trick in the JobConf:

{code}
<property>
  <name>mapred.fairscheduler.poolnameproperty</name>
  <value>pool.name</value>
</property>

<property>
  <name>pool.name</name>
  <value>${user.name}</value>
</property>
{code}

This JIRA proposes to add a property called mapred.fairscheduler.pool that allows a job to be placed directly into a pool, avoiding the need for this trick.",,aaa,dhruba,hammer,matei,tomwhite,yhemanth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/09 22:40;aheirich;MAPREDUCE-707-1-apache.patch;https://issues.apache.org/jira/secure/attachment/12424062/MAPREDUCE-707-1-apache.patch","05/Nov/09 02:39;aheirich;MAPREDUCE-707-2-apache.patch;https://issues.apache.org/jira/secure/attachment/12424090/MAPREDUCE-707-2-apache.patch","04/Nov/09 20:08;aheirich;MAPREDUCE-707-apache.patch;https://issues.apache.org/jira/secure/attachment/12424051/MAPREDUCE-707-apache.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2009-11-02 23:57:37.548,,,false,,,,,,,,,,,,,,,,,,37322,Reviewed,,,,Sat Nov 07 15:52:43 UTC 2009,,,,,,,"0|i02sx3:",14286,add mapred.fairscheduler.pool property to define which pool a job belongs to.,,,,,,,,,,,,,,,,,,,,"02/Nov/09 23:57;aheirich;I think the right steps are:
Create a new property mapred.fairscheduler.pool with default value user.name
Change default value of mapred.fairscheduler.poolnameproperty to mapred.fairscheduler.pool
Add a unit test

Can anyone tell me where to find the initialization of these properties?","03/Nov/09 03:53;matei;Hi Alan,

If you just want to get this behaviour as a fair scheduler user, the properties can be set in your mapred-site.xml. If you want to submit a patch so that this behaviour is the default, you have to edit mapred-default.xml, which is available in src or perhaps src/java in the mapreduce project.","03/Nov/09 03:55;matei;I should also add that my original intent in the JIRA was to have a separate way of setting the pool that doesn't require using the mapred.fairscheduler.poolnameproperty. This way, users could set poolnameproperty to something else (e.g. group.name for a Unix group). However, I think it's overkill to add a second way of setting the pool name, so I'd be okay with just a patch to mapred-default.xml if that works.","03/Nov/09 04:42;dhruba;> I think it's overkill to add a second way of setting the pool name, so I'd be okay with just a patch to mapred-default.xml if that works.

+1.","03/Nov/09 04:44;aheirich;What about PoolManager.java initialize() which reads the value of mapred.fairscheduler.poolnameproperty?  It looks like this enforces the default values in conf.get().  If I do something like this then do we need to patch mapred-default.xml?
   public void initialize() throws IOException, SAXException,
       AllocationConfigurationException, ParserConfigurationException {
     Configuration conf = scheduler.getConf();
+    this.pool = conf.get(
+        ""mapred.fairscheduler.pool"", JobContext.USER_NAME);
     this.poolNameProperty = conf.get(
-        ""mapred.fairscheduler.poolnameproperty"", JobContext.USER_NAME);
+        ""mapred.fairscheduler.poolnameproperty"", this.pool);
","03/Nov/09 08:09;matei;I don't think that does what you want it to do, because it just sets poolNameProperty to a different value. What you want is for each *individual* job's pool to be determined based on which properties are in its JobConf (if it has mapred.fairscheduler.pool, use that; otherwise use whatever property poolNameProperty is set to; and if that doesn't exist in the JobConf either, use DEFAULT_POOL_NAME). The XML code I posted makes the job's JobConf implement this logic, by having the ""pool"" property be whatever the user sets it to if the user provides a setting, and making it default to the value of user.name otherwise. But there's no way to achieve this same behavior by just changing PoolManager.initialize to set poolNameProperty another way.

Having thought about this some more, I think the most elegant implementation is to actually change PoolManger.getPoolName(JobInProgress job) to explicitly check whether the job's JobConf has the key ""mapred.fairscheduler.pool"" set, and if so, return that value; otherwise, it should return conf.get(poolNameProperty) as it does now. This should be a simple change to PoolManager.getPoolName. You will also need to change PoolManager.setPool() to set the ""mapred.fairscheduler.pool"" property rather than the poolNameProperty property on the job. Let me know if this makes sense.","03/Nov/09 08:26;yhemanth;Matei, maybe it makes more sense to add fairshare scheduler properties to a scheduler specific file rather than mapred-*.xml, no ? This way any framework level changes that handle the framework's configuration would not be tied up by specific scheduler implementation. And also vice versa. Let me know what you think.","03/Nov/09 08:33;matei;I agree, and the solution I proposed above doesn't require changing mapred-default.xml. Many fair scheduler properties are in fair-scheduler.xml, but it would be nice to move some of the other ones that are still in mapred-site.xml to there. This would also make it easier to toggle them at runtime.","03/Nov/09 21:31;aheirich;Sure Matei, what you suggest sounds really simple and I'm testing it now.  Thanks.","04/Nov/09 17:37;aheirich;I'm finding that the demand for the mapSchedulable and reduceSchedulable objects are not updating as a result of removing and adding jobs to a pool.  As a result of this calls to PoolManager.setPool do not cause the pool demands to update.  (setPool calls removeJob() and addJob()).

I've written a unit test that submits jobs to pools, tries to change their pool, and checks getDemand() to verify the right thing happened.  This test is failing because getDemand() shows no changes in the demand.

Is this the expected behavior of getDemand()???","04/Nov/09 18:01;matei;Hi Alan,

Demands are only updated when the fair scheduler's update() function is called (which calls updateDemand in turn). All the code that calls setPool calls scheduler.update() afterwards. So you should do that in the unit test too.","04/Nov/09 18:02;aheirich;I guess another way to put this is: if we need to call updateDemand() to keep the demand up to date, should setPool() call updateDemand() after changing the pool for a job?
","04/Nov/09 18:27;matei;The reason I haven't made the PoolManager methods call updateDemand is that FairScheduler.update() does other things as well, and doing updateDemand without doing a full update() could potentially break some of the algorithms. (I'm not sure that it does so right now, but it would have been a problem in earlier versions). Therefore, I wanted all the updates to always happen through FairScheduler.update(). I'd rather not make the PoolManager call update() all the time because it would be better if the PoolManager didn't have to be modified whenever the structure of FairScheduler changes. All of the other unit tests call update() too, so I think it's fine not to do it in setPool.","04/Nov/09 18:29;matei;In other words, I want to treat PoolManager, PoolSchedulable, JobSchedulable, etc as data structures, and decide externally when updates need to happen and when they don't, so that all that control logic is in one or two places (the event handlers in the FairScheduler and the UI code in FairSchedulerServlet).","04/Nov/09 20:08;aheirich;adds mapred.fairscheduler.pool property, use it to specify pool name ","04/Nov/09 20:10;aheirich;I would like to request a code review of MAPREDUCE-707-apache.patch, it is intended to resolve this JIRA.","04/Nov/09 21:20;matei;Here are some comments on the patch:

# Instead of using the string ""mapred.fairscheduler.pool"" in multiple places in PoolManager, make it a constant at the top of the file (something like EXPLICIT_POOL_PROPERTY).
# Add a comment to PoolManager.getPoolName to explain the logic (first look for the explicit pool property, then for the property named by poolNameProperty, and finally default to DEFAULT_POOL_NAME).
# Add a unit test for PoolManager.getPoolName that tries each of those cases (explicit property is set, no explicit property but poolNameProperty is used, or neither is used). Right now your existing unit test checks that setPool works but there's no test that submits a job with mapred.fairscheduler.pool directly.
# Instead of assertEquals(0,    scheduler.getPoolManager().getPoolName(job2).compareTo(""poolA"")) you can probably use a version of assertEquals that works on strings.
# In the documentation, instead of saying ""This property is ignored if mapred.fairscheduler.pool is specified."" for the poolnameproperty, it would be better to say that the poolnameproperty is used only for jobs in which mapred.fairscheduler.pool is not explicitly set.","04/Nov/09 22:24;aheirich;revised patch per review comments","04/Nov/09 22:40;aheirich;A patch after incorporating changes suggested in the review comments.","04/Nov/09 22:41;aheirich;Please review MAPREDUCE-707-1-apache.patch.  Thanks.","05/Nov/09 00:41;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12424051/MAPREDUCE-707-apache.patch
  against trunk revision 832362.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/224/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/224/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/224/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/224/console

This message is automatically generated.","05/Nov/09 00:55;matei;This looks pretty good, except that testPoolAssignment fails when I run the unit tests. I think the problem is with job4, where you set ""mapred.fairscheduler.poolnameproperty"" in the job's Configuration (jobConf2), not in the fair scheduler's configuration. You need to set the poolNameProperty when you create the fair scheduler object. That's what the code used to do with the POOL_PROPERTY string at the top, but you can't set the pool name property to mapred.fairscheduler.pool, because that wouldn't be testing anything. I'd suggest leaving the POOL_PROPERTY as ""pool"" and trying to set job4's pool through that.

Also, for sanity, in job1 (where you set mapred.fairscheduler.pool directly), you should say the ""pool"" property to something other than poolA to make sure it isn't used.

Finally, two small nitpicks:

# In the test line with assertEquals(scheduler.getPoolManager().getPoolName(job2), ""poolA""), you should switch the two parameters (put ""poolA"" first); the first parameter is always supposed to be the value expected.
# Regarding the comment on getPoolName - the pool name property used by default is ""user.name"", not ""project"". I think I forgot to fix that comment a while back.","05/Nov/09 00:55;matei;By the way, here's a tip if you want to run the unit tests faster - you can run just the fair scheduler's unit test with ant -Dtestcase=TestFairScheduler test.","05/Nov/09 02:19;aheirich;further revisions per comments.","05/Nov/09 02:21;aheirich;Oops - I thought TestFairScheduler would be run as part of ""ant test"".  I guess not.  It passes now.

Hudson reported that TestGridmixSubmission failed, but that passes in my workspace.  I'm on Mac OS X and I saw some tests fail from a fresh build that should have passed.

Please see MAPREDUCE-707-2-apache.patch","05/Nov/09 02:39;aheirich;corrected patch","05/Nov/09 03:53;matei;Thanks Alan, this looks good! +1 from me. I'll wait for the Hudson automated test and then commit it if there are no warnings.","05/Nov/09 06:57;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12424090/MAPREDUCE-707-2-apache.patch
  against trunk revision 832362.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/225/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/225/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/225/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h6.grid.sp2.yahoo.net/225/console

This message is automatically generated.","05/Nov/09 17:01;aheirich;This Hudson result is frustrating.  TestGridmixSubmission passes when I run it in my workspace on Mac OS X.  I cannot reproduce the failure.  But Hudson compains that testSubmit fails!  The diagnostic is:
Error Message

Mismatched output bytes 1737801/1764471
Stacktrace

junit.framework.AssertionFailedError: Mismatched output bytes 1737801/1764471
	at org.apache.hadoop.mapred.gridmix.TestGridmixSubmission$TestMonitor.check(TestGridmixSubmission.java:231)
	at org.apache.hadoop.mapred.gridmix.TestGridmixSubmission$TestMonitor.verify(TestGridmixSubmission.java:140)
	at org.apache.hadoop.mapred.gridmix.TestGridmixSubmission$DebugGridmix.checkMonitor(TestGridmixSubmission.java:263)
	at org.apache.hadoop.mapred.gridmix.TestGridmixSubmission.testSubmit(TestGridmixSubmission.java:297","05/Nov/09 18:39;matei;The test failure seems to be due to Hudson being flaky, because the test passes on my laptop too. Therefore I've committed the patch. Thanks Alan!","05/Nov/09 18:42;matei;Alan, you should edit the JIRA to assign it to yourself through the ""edit this issue"" link.","05/Nov/09 19:01;hudson;Integrated in Hadoop-Mapreduce-trunk-Commit #110 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/110/])
    . Provide a jobconf property for explicitly assigning a job to 
a pool in the Fair Scheduler. Contributed by Alan Heirich.
","05/Nov/09 21:17;aheirich;I don't seem able to assign to myself, perhaps because the issue is closed.  That's ok.","07/Nov/09 15:52;hudson;Integrated in Hadoop-Mapreduce-trunk #136 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/136/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit both numMapTasks and numReduceTasks,MAPREDUCE-1379,12445602,New Feature,Resolved,MAPREDUCE,Hadoop Map/Reduce,software,omalley,Hadoop's map/reduce framework,http://hadoop.apache.org,Trivial,Won't Fix,,sinofool,sinofool,15/Jan/10 06:30,06/May/10 04:25,12/Jan/21 09:52,24/Jan/10 23:51,0.22.0,,,,,,,,tasktracker,,,,,,0,,,,,"In some environment, the number of concurrent running process is very sensitive.

  mapreduce.tasktracker.map.tasks.maximum
and
  mapreduce.tasktracker.reduce.tasks.maximum
limit tasks running on each tasktracker separately.

This patch limits them together, using mapreduce.tasktracker.total.tasks.maximum",,aah,acmurthy,aw,cdouglas,hong.tang,matei,schen,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-6749,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/10 06:33;sinofool;limit-both-numMapTasks-and-numReduceTasks.patch;https://issues.apache.org/jira/secure/attachment/12430364/limit-both-numMapTasks-and-numReduceTasks.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2010-01-15 06:58:11.068,,,false,,,,,,,,,,,,,,,,,,149477,,,,,Sun Jan 24 23:51:28 UTC 2010,,,,,,,"0|i0jghr:",111602,,,,,,,,,,,,,,,,,,,,,"15/Jan/10 06:37;sinofool;submit patch","15/Jan/10 06:58;acmurthy;Bochun, when you can limit them separately why do be need this new config?","15/Jan/10 07:10;sinofool;Arun, this patch added mapreduce.tasktracker.total.tasks.maximum property in mapred-site.xml.
This value set to map+reduce default.
","15/Jan/10 07:42;hong.tang;Bochun, have you done any evaluation as to in what situations this additional knob helps?","15/Jan/10 07:49;matei;Is the goal to be able to set ""total.tasks.maximum"" to less than map.tasks.maximum + reduce.tasks.maximum? If so, you should watch out because you might possibly starve tasks of one type. For example, if you set max map slots to 5, max reduce slots to 5, and max total slots to 5, most of the current job schedulers will probably cause reduce tasks to be starved, because they always look for a map to launch before looking for a reduce. In fact, given that schedulers ultimately decide which tasks to launch anyway, I wouldn't add a ""max total slots"" concept in the TaskTracker to begin with; I would add this functionality to one of the schedulers.","15/Jan/10 08:13;sinofool;Tang, this helps lower the number of concurrent tasks running by each task tracker.
some of my machine is shared with other service, running 1 mapper and 1 reducer is not good enough.
","15/Jan/10 08:46;sinofool;Matei, I use 1 with all the three parameters. The situation of starving tasks do exists, numTotal > numMap && numTotal > numReduce will improve it.
A custom scheduler solve the situation too, but in different layer and a little bit more difficult. I will try to implement one later.
","15/Jan/10 09:06;sinofool;Arun, answering your first question ""why do need this new config"".
Limit separately is minimal 2 tasks running concurrently, 1 mapper and 1 reducer. I have seen some questions in the maillist asked how to limit them both. I think he must face the same problem I did.
Some of my servers are shared with other service. I use map=1 reduce=1 total=1.

And these 2 situations made me this patch.
Situation A:
  Most jobs have thousand of map tasks and only a few reducer. 
Situation B:
  But some of the tasks set the number of reduce tasks large.

Solve A:  For 8-core CPU, m=7,r=1 seted
Solve B:  For 8-core CPU, m=8,r=8,t=9 seted. to avoid high loadavg.

I have to config both of them, so a total config is needed.","15/Jan/10 11:41;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12430364/limit-both-numMapTasks-and-numReduceTasks.patch
  against trunk revision 899501.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/273/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/273/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/273/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h3.grid.sp2.yahoo.net/273/console

This message is automatically generated.","24/Jan/10 23:51;cdouglas;Matei is exactly right. As far as I'm aware, no scheduler avoids giving out reduce work fearing that it could starve itself of map slots. This introduces corner cases that, without corresponding audit of schedulers, is unlikely to be handled correctly.

There are many proposals for handling/sharing resources more efficiently, e.g. MAPREDUCE-297, MAPREDUCE-922, MAPREDUCE-961, MAPREDUCE-279... the current solution, in contrast, is a violation of the current slot model to get around the coarse utilization implicit in its design.

Closing as ""won't fix""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
