Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Cloners),Outward issue link (Completes),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Tags),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Flaky Test KTableKTableForeignKeyJoinIntegrationTest.doLeftJoinFromRightThenDeleteRightEntity,KAFKA-9000,13261160,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,vvcephei,cadonna,cadonna,08/Oct/19 17:49,17/Oct/19 05:49,12/Jan/21 11:54,17/Oct/19 05:49,2.4.0,,,,2.4.0,,,,,streams,,,,0,flaky-test,,,"{code:java}
java.lang.AssertionError: expected:<[KeyValue(2, value1=1.77,value2=10), KeyValue(1, value1=1.33,value2=10), KeyValue(3, value1=3.77,value2=30)]> but was:<[KeyValue(3, value1=3.77,value2=null), KeyValue(1, value1=1.33,value2=10), KeyValue(3, value1=3.77,value2=30)]>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:120)
	at org.junit.Assert.assertEquals(Assert.java:146)
	at org.apache.kafka.streams.integration.KTableKTableForeignKeyJoinIntegrationTest.doLeftJoinFromRightThenDeleteRightEntity(KTableKTableForeignKeyJoinIntegrationTest.java:313)
{code}",,ableegoldman,cadonna,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-10-11 02:59:37.274,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 17 05:41:02 UTC 2019,,,,,,,"0|z07edk:",9223372036854775807,,,,,,,,,,,,,,,,"08/Oct/19 19:44;cadonna;This test is also flaky on my local machine.","10/Oct/19 07:39;cadonna;Failed in https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/25666/

with slightly different error:
{code:java}
java.lang.AssertionError: expected:<[KeyValue(2, value1=1.77,value2=10), KeyValue(1, value1=1.33,value2=10), KeyValue(3, value1=3.77,value2=30)]> but was:<[KeyValue(2, value1=1.77,value2=10), KeyValue(3, value1=3.77,value2=null), KeyValue(3, value1=3.77,value2=30)]>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:120)
	at org.junit.Assert.assertEquals(Assert.java:146)
	at org.apache.kafka.streams.integration.KTableKTableForeignKeyJoinIntegrationTest.doLeftJoinFromRightThenDeleteRightEntity(KTableKTableForeignKeyJoinIntegrationTest.java:313)
{code}","10/Oct/19 11:21;cadonna;Failed in  https://builds.apache.org/job/kafka-pr-jdk11-scala2.12/8484/

again with slightly different error:

{code:java}
java.lang.AssertionError: expected:<[KeyValue(2, value1=1.77,value2=10), KeyValue(1, value1=1.33,value2=10), KeyValue(3, value1=3.77,value2=30)]> but was:<[KeyValue(2, value1=1.77,value2=null), KeyValue(2, value1=1.77,value2=10), KeyValue(3, value1=3.77,value2=30)]>
        at org.junit.Assert.fail(Assert.java:89)
         at org.junit.Assert.failNotEquals(Assert.java:835)
         at org.junit.Assert.assertEquals(Assert.java:120)
         at org.junit.Assert.assertEquals(Assert.java:146)
         at org.apache.kafka.streams.integration.KTableKTableForeignKeyJoinIntegrationTest.doLeftJoinFromRightThenDeleteRightEntity(KTableKTableForeignKeyJoinIntegrationTest.java:313)
{code}","11/Oct/19 02:59;ableegoldman;Failed locally: 

java.lang.AssertionError: expected:<[KeyValue(2, value1=1.77,value2=10), KeyValue(1, value1=1.33,value2=10), KeyValue(3, value1=3.77,value2=30)]> but was:<[KeyValue(3, value1=3.77,value2=null), KeyValue(1, value1=1.33,value2=10), KeyValue(3, value1=3.77,value2=30)]>","15/Oct/19 02:59;githubbot;vvcephei commented on pull request #7517: KAFKA-9000: fix flaky FK join test by using TTD
URL: https://github.com/apache/kafka/pull/7517
 
 
   Migrate this integration test to use TopologyTestDriver instead of running 3 Streams instances.
   
   Dropped one test that was attempting to produce specific interleavings. If anything, these should be verified deterministically by unit testing.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","16/Oct/19 05:35;mjsax;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/2668/testReport/junit/org.apache.kafka.streams.integration/KTableKTableForeignKeyJoinIntegrationTest/doLeftJoinFromRightThenDeleteRightEntity/]","16/Oct/19 05:37;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/25869/testReport/junit/org.apache.kafka.streams.integration/KTableKTableForeignKeyJoinIntegrationTest/doLeftJoinFromRightThenDeleteRightEntity/]","17/Oct/19 05:41;githubbot;guozhangwang commented on pull request #7517: KAFKA-9000: fix flaky FK join test by using TTD
URL: https://github.com/apache/kafka/pull/7517
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,
Kafka is going down with issue ERROR Failed to clean up log for __consumer_offsets-0 in dir /tmp/kafkadev2-logs due to IOException (kafka.server.LogDirFailureChannel),KAFKA-7184,13173112,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Not A Problem,,smuddamsetty,smuddamsetty,19/Jul/18 06:34,19/Jul/18 14:18,12/Jan/21 11:54,19/Jul/18 14:18,1.1.0,,,,,,,,,admin,log,,,0,,,,"Kafka is going down with issue ERROR Failed to clean up log for __consumer_offsets-0 in dir /tmp/kafkadev2-logs due to IOException (kafka.server.LogDirFailureChannel).

This  error we are seeing very frequently for every  168 hours(7 days) where the defualt value of log retention in kafka configuration . After modifying it to 240 hours this thing is happening again after 240 hours . I ahve gone thorugh some google groups some artiicles and have observed this is happening in windows system but here i am facing this issue in linux system . Below are my configuration details . 

kafka_2.11-1.0.0

OS: OEL 7.1",,dhruvilshah,omkreddy,smuddamsetty,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/18 07:36;smuddamsetty;log-cleaner.log;https://issues.apache.org/jira/secure/attachment/12932210/log-cleaner.log","19/Jul/18 07:36;smuddamsetty;server.log.2018-07-18-15;https://issues.apache.org/jira/secure/attachment/12932209/server.log.2018-07-18-15",,,,,,2.0,,,,,,,,,,,,,,,,,,,,2018-07-19 07:20:17.504,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 19 14:18:03 UTC 2018,,,,,,,"0|i3w207:",9223372036854775807,,,,,,,,,,,,,,,,"19/Jul/18 07:20;dhruvilshah;What kind of storage are you using? Could you please upload server.log and log_cleaner.log from around the time of the error.","19/Jul/18 07:39;smuddamsetty;[~dhruvilshah]

Please find the attached the logs > please let me know if any further details required . We are using SSD storage.","19/Jul/18 12:09;omkreddy;looks like your data directory is in /tmp/ folder.  /tmp may get cleared during system OS cleanups etc..
For production deployments, you may need to configure the ""log.dirs"" property  with any other valid directory path.","19/Jul/18 13:43;smuddamsetty;[~omkreddy]

Yes . My log location is /tmp  but i guess  this error is not occuring due to  OS clean up's as i am facing this issue after reaching to retention period hours which was set to 240 hours . If this is from OS level how can we track and get a conclusion to that ?.","19/Jul/18 14:18;dhruvilshah;[~smuddamsetty] after reaching the retention point, Kafka will try to delete segments which are past retention. It is possible that these files no longer exist because the /tmp directory was cleaned out before. Kafka requires a persistent store be used for the log directory so you should test with that.",,,,,,,,,,,,,,,,,
Why kafka sync send message with  10 seconds delay,KAFKA-6403,13127303,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,,,change,change,27/Dec/17 03:04,09/Jan/18 04:01,12/Jan/21 11:54,,1.0.0,,,,,,,,,producer ,,,,0,,,,"I have a timertask to send a message to kafka  every half an hour, Statistics reports 


||send starttime|send successfully time|delay/ms||
|2017-12-26 15:50:25.413 |2017-12-26 15:50:35,447|10034|
|2017-12-26 16:20:35.419 |2017-12-26 16:20:45,483|10064|
|2017-12-26 17:28:20.708|2017-12-26 17:28:25,743|5035	|
|2017-12-26 18:44:20.447|2017-12-26 18:44:25,516|5069|
|2017-12-26 19:14:25.518|2017-12-26 19:14:30,547|5029|


 ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.0.179:39092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[2017-12-26 03:30:58,042] INFO KafkaConfig values: 
        advertised.host.name = kafka-1.default.svc.cluster.local
        advertised.listeners = null
        advertised.port = null
        alter.config.policy.class.name = null
        authorizer.class.name = 
        auto.create.topics.enable = true
        auto.leader.rebalance.enable = true
        background.threads = 10
        broker.id = 1
        broker.id.generation.enable = true
        broker.rack = null
        compression.type = producer
        connections.max.idle.ms = 600000
        controlled.shutdown.enable = true
        controlled.shutdown.max.retries = 3
        controlled.shutdown.retry.backoff.ms = 5000
        controller.socket.timeout.ms = 30000
        create.topic.policy.class.name = null
        default.replication.factor = 3
        delete.records.purgatory.purge.interval.requests = 1
        delete.topic.enable = true
        fetch.purgatory.purge.interval.requests = 1000
        group.initial.rebalance.delay.ms = 3000
        group.max.session.timeout.ms = 300000
        group.min.session.timeout.ms = 6000
        host.name = 
        inter.broker.listener.name = null
        inter.broker.protocol.version = 1.0-IV0
        leader.imbalance.check.interval.seconds = 300
        leader.imbalance.per.broker.percentage = 10
        listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
        listeners = null
        log.cleaner.backoff.ms = 15000
        log.cleaner.dedupe.buffer.size = 134217728
        log.cleaner.delete.retention.ms = 86400000
        log.cleaner.enable = true
        log.cleaner.io.buffer.load.factor = 0.9
        log.cleaner.io.buffer.size = 524288
        log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
        log.cleaner.min.cleanable.ratio = 0.5
        log.cleaner.min.compaction.lag.ms = 0
        log.cleaner.threads = 1
        log.cleanup.policy = [delete]
        log.dir = /data
        log.dirs = /data
        log.flush.interval.messages = 9223372036854775807
        log.flush.interval.ms = null
        log.flush.offset.checkpoint.interval.ms = 60000
        log.flush.scheduler.interval.ms = 9223372036854775807
        log.flush.start.offset.checkpoint.interval.ms = 60000
        log.index.interval.bytes = 4096
        log.index.size.max.bytes = 10485760
        log.message.format.version = 1.0-IV0
        log.message.timestamp.difference.max.ms = 9223372036854775807
        log.message.timestamp.type = CreateTime
        log.preallocate = false
        log.retention.bytes = -1
        log.retention.check.interval.ms = 300000
        log.retention.hours = 168
        log.retention.minutes = null
        log.retention.ms = null
        log.roll.hours = 168
        log.roll.jitter.hours = 0
        log.roll.jitter.ms = null
        log.roll.ms = null
        log.segment.bytes = 1073741824
        log.segment.delete.delay.ms = 60000
        max.connections.per.ip = 2147483647
        max.connections.per.ip.overrides = 
        message.max.bytes = 1000012
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        min.insync.replicas = 1
        num.io.threads = 8
        num.network.threads = 3
        num.partitions = 3
        num.recovery.threads.per.data.dir = 1
        num.replica.fetchers = 1
        offset.metadata.max.bytes = 4096
        offsets.commit.required.acks = -1
        offsets.commit.timeout.ms = 5000
        offsets.load.buffer.size = 5242880
        offsets.retention.check.interval.ms = 600000
        offsets.retention.minutes = 1440
        offsets.topic.compression.codec = 0
        offsets.topic.num.partitions = 50
        offsets.topic.replication.factor = 3
        offsets.topic.segment.bytes = 104857600
        port = 9092
        principal.builder.class = null
        producer.purgatory.purge.interval.requests = 1000
        queued.max.request.bytes = -1
        queued.max.requests = 500
        quota.consumer.default = 9223372036854775807
        quota.producer.default = 9223372036854775807
        quota.window.num = 11
        quota.window.size.seconds = 1
        replica.fetch.backoff.ms = 1000
        replica.fetch.max.bytes = 1048576
        replica.fetch.min.bytes = 1
        replica.fetch.response.max.bytes = 10485760
        replica.fetch.wait.max.ms = 500
        replica.high.watermark.checkpoint.interval.ms = 5000
        replica.lag.time.max.ms = 10000
        replica.socket.receive.buffer.bytes = 65536
        replica.socket.timeout.ms = 30000
        replication.quota.window.num = 11
        replication.quota.window.size.seconds = 1
        request.timeout.ms = 30000
        reserved.broker.max.id = 1000
        sasl.enabled.mechanisms = [GSSAPI]
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.principal.to.local.rules = [DEFAULT]
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.mechanism.inter.broker.protocol = GSSAPI
        security.inter.broker.protocol = PLAINTEXT
        socket.receive.buffer.bytes = 102400
        socket.request.max.bytes = 104857600
        socket.send.buffer.bytes = 102400
        ssl.cipher.suites = null
        ssl.client.auth = none
        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
        ssl.endpoint.identification.algorithm = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLS
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
        transaction.max.timeout.ms = 900000
        transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
        transaction.state.log.load.buffer.size = 5242880
        transaction.state.log.min.isr = 2
        transaction.state.log.num.partitions = 50
        transaction.state.log.replication.factor = 3
        transaction.state.log.segment.bytes = 104857600
        transactional.id.expiration.ms = 604800000
        unclean.leader.election.enable = false
        zookeeper.connect = zookeeper-1,zookeeper-2,zookeeper-3
        zookeeper.connection.timeout.ms = 10000
        zookeeper.session.timeout.ms = 10000
        zookeeper.set.acl = false
        zookeeper.sync.time.ms = 2000
",,change,ewencp,juparker31,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-01-09 04:01:40.247,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 09 04:01:40 UTC 2018,,,,,,,"0|i3ob07:",9223372036854775807,,,,,,,,,,,,,,,,"09/Jan/18 04:01;ewencp;[~change] Since you've only provided some configs, it'll probably be difficult to track down the issue. You'd probably need to provide at least more information about how you are measuring the timing, and even so the issue could be anywhere from the client to any of the brokers involved. Probably most useful here would be logs -- if this is reproducible, turn the logging on the client up to DEBUG level.

I've removed the fix version as it does not make sense for a bug that affects 1.0.0 to also be fixable in 1.0.0. We can update the fix version if the issue is tracked down and we determine whether including it in a bugfix version or just the next major/minor release makes sense.",,,,,,,,,,,,,,,,,,,,,
Add cases for concurrent transactional reads and writes in system tests,KAFKA-5366,13076746,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,apurva,apurva,apurva,02/Jun/17 07:05,07/Jun/17 00:24,12/Jan/21 11:54,07/Jun/17 00:23,0.11.0.0,,,,0.11.0.0,,,,,,,,,0,exactly-once,,,"Currently the transactions system test does a transactional copy while bouncing brokers and clients, and then does a verifying read on the output topic to ensure that it exactly matches the input. 

We should also have a transactional consumer reading the tail of the output topic as the writes are happening, and then assert that the values _it_ reads also exactly match the values in the source topics. 

This test really exercises the abort index, and we don't have any of them in the system or integration tests right now. ",,apurva,githubbot,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-06-02 22:31:32.634,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 07 00:24:29 UTC 2017,,,,,,,"0|i3fshj:",9223372036854775807,,,,,,,,,,,,,,,,"02/Jun/17 22:31;githubbot;GitHub user apurvam opened a pull request:

    https://github.com/apache/kafka/pull/3217

    KAFKA-5366: Add concurrent reads to transactions system test

    This currently fails in multiple ways. One of which is most likely KAFKA-5355, where the concurrent consumer reads duplicates.
    
    During broker bounces, the concurrent consumer misses messages completely. This is another bug.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apurvam/kafka KAFKA-5366-add-concurrent-reads-to-transactions-system-test

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3217.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3217
    
----
commit cd0990784aaa26fa6485e9f369a600b85c1647f9
Author: Apurva Mehta <apurva@confluent.io>
Date:   2017-06-02T22:25:00Z

    Add a concurrent consumer in the transactions system tests. This will exercise the abort index

commit 71fcad197b403ff7873d646feec287d92793cbe6
Author: Apurva Mehta <apurva@confluent.io>
Date:   2017-06-02T22:29:47Z

    Bounce brokers as well

----
","05/Jun/17 18:59;apurva;[~ijuma] I originally marked this as a 'Major' item for 0.11.0.1, but am upgrading it to a blocker for 0.11.0.0 . 

The reason is that with these improvements, we are testing a behavior which would be used at the core of streams EOS (reading and writing transactionally and concurrently at the tail of the log). If this behavior doesn't work, streams EOS won't work.

So we need these upgraded tests to pass (have filed KAFKA-5375 and KAFKA-5376 to fix the failures), and then we can merge this. Let me know if  you have any objections.","07/Jun/17 00:23;hachikuji;Issue resolved by pull request 3217
[https://github.com/apache/kafka/pull/3217]","07/Jun/17 00:24;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/3217
",,,,,,,,,,,,,,,,,,
Change one SASL system test to use new JAAS config property,KAFKA-4580,13031622,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,rsivaram,rsivaram,rsivaram,03/Jan/17 09:55,17/Jan/17 18:44,12/Jan/21 11:54,17/Jan/17 18:44,,,,,0.10.2.0,,,,,system tests,,,,0,,,,Change one of the existing system tests using the static JAAS configuration to use the new dynamic JAAS config property introduced in KAFKA-4259.,,ewencp,githubbot,ijuma,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-06 10:14:57.16,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 17 18:44:03 UTC 2017,,,,,,,"0|i3872n:",9223372036854775807,,ijuma,,,,,,,,,,,,,,"06/Jan/17 10:14;githubbot;GitHub user rajinisivaram opened a pull request:

    https://github.com/apache/kafka/pull/2323

    KAFKA-4580: Use sasl.jaas.config for some system tests

    Switched console_consumer, verifiable_consumer and verifiable_producer to use new sasl.jaas_config property instead of static JAAS configuration file when used with SASL_PLAINTEXT.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rajinisivaram/kafka KAFKA-4580

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2323.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2323
    
----
commit dbe6d36aa7b74d3a4d24cb6990eb3301e03fcba6
Author: Rajini Sivaram <rajinisivaram@googlemail.com>
Date:   2017-01-05T15:16:21Z

    KAFKA-4580: Use sasl.jaas.config for some system tests

----
","16/Jan/17 05:01;ewencp;[~rsivaram] Moving to blocker since it'd be ideal to have real tests that this works (and thanks for being prompt with a PR for the system tests!). Will follow up on the PR and we can drop priority if absolutely necessary between now and code freeze. [~ijuma] [~junrao] would be nice to have someone familiar with the original KIP reviewing this PR as well.","17/Jan/17 18:43;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2323
","17/Jan/17 18:44;ijuma;Issue resolved by pull request 2323
[https://github.com/apache/kafka/pull/2323]",,,,,,,,,,,,,,,,,,
ClientCompatibilityTest system test failing since KIP-31/KIP-32 was merged,KAFKA-3371,12948719,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,enothereska,ijuma,ijuma,10/Mar/16 09:33,16/Mar/16 15:48,12/Jan/21 11:54,16/Mar/16 15:48,,,,,0.10.0.0,,,,,,,,,0,,,,"ClientCompatibilityTest system test has been failing since we merged KIP-31/32. We need to fix this for 0.10.0.0. Latest failure below:


test_id:    2016-03-09--001.kafkatest.tests.compatibility_test.ClientCompatibilityTest.test_producer_back_compatibility
status:     FAIL
run time:   1 minute 4.864 seconds
http://confluent-kafka-system-test-results.s3-us-west-2.amazonaws.com/2016-03-09--001.1457539618--apache--trunk--324b0c8/report.html

cc [~becket_qin]",,enothereska,ewencp,githubbot,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-11 14:40:15.489,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 16 15:48:19 UTC 2016,,,,,,,"0|i2ugf3:",9223372036854775807,,,,,,,,,,,,,,,,"11/Mar/16 14:40;githubbot;GitHub user enothereska opened a pull request:

    https://github.com/apache/kafka/pull/1051

    KAFKA-3371: ClientCompatibilityTest system test failing

    @becketqin have a look if this looks reasonable to you. Thanks.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/enothereska/kafka kafka-3371

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1051.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1051
    
----
commit d18a35556c98ca116e5bee0c4c56bbe6d80d2463
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-03-11T14:35:50Z

    Removed unneeded string

----
","16/Mar/16 15:48;ewencp;Issue resolved by pull request 1051
[https://github.com/apache/kafka/pull/1051]","16/Mar/16 15:48;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1051
",,,,,,,,,,,,,,,,,,,
Kafka 2.3.0 Transient Unit Test Failures SocketServerTest. testControlPlaneRequest,KAFKA-8711,13246899,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,,,chandranc.rao@oracle.com,chandranc.rao@oracle.com,24/Jul/19 19:32,30/Jul/19 14:29,12/Jan/21 11:54,,2.3.0,,,,,,,,,core,unit tests,,,0,,,,"Cloned Kakfa 2.3.0 source to our git repo and compiled it using 'gradle build', we see the following error consistently:

Gradle Version 4.7

 

testControlPlaneRequest
java.net.BindException: Address already in use (Bind failed)
        at java.net.PlainSocketImpl.socketBind(Native Method)
        at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:387)
        at java.net.Socket.bind(Socket.java:644)
        at java.net.Socket.<init>(Socket.java:433)
        at java.net.Socket.<init>(Socket.java:286)
        at kafka.network.SocketServerTest.connect(SocketServerTest.scala:140)
        at kafka.network.SocketServerTest.$anonfun$testControlPlaneRequest$1(SocketServerTest.scala:200)
        at kafka.network.SocketServerTest.$anonfun$testControlPlaneRequest$1$adapted(SocketServerTest.scala:199)
        at kafka.network.SocketServerTest.withTestableServer(SocketServerTest.scala:1141)
        at kafka.network.SocketServerTest.testControlPlaneRequest(SocketServerTest.scala:199)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:305)
        at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:365)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
        at org.junit.runners.ParentRunner$4.run(ParentRunner.java:330)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:78)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:328)
        at org.junit.runners.ParentRunner.access$100(ParentRunner.java:65)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:292)
        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:305)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:412)
        at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:106)
        at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
        at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
        at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:66)
        at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
        at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
        at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
        at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32)
        at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93)
        at com.sun.proxy.$Proxy1.processTestClass(Unknown Source)
        at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:109)
        at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
        at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
        at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:155)
        at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:137)
        at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:404)
        at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)
        at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)
        at java.lang.Thread.run(Thread.java:748)",,chandranc.rao@oracle.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/19 19:33;chandranc.rao@oracle.com;KafkaAUTFailures07242019_PASS2.txt;https://issues.apache.org/jira/secure/attachment/12975697/KafkaAUTFailures07242019_PASS2.txt","24/Jul/19 19:33;chandranc.rao@oracle.com;KafkaUTFailures07242019_PASS2.GIF;https://issues.apache.org/jira/secure/attachment/12975698/KafkaUTFailures07242019_PASS2.GIF",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 30 14:29:09 UTC 2019,,,,,,,"0|z04ztk:",9223372036854775807,,,,,,,,,,,,,,,,"24/Jul/19 19:36;chandranc.rao@oracle.com;Attached find the logs...Please let us know if we can comment this test case for this build and move forward until there is a release available with this fix. If so where is this code in repo and how should we run the subset of tc's rather than waiting ~2hrs to find the failures...

 

Thanks Much

 

Chandra

[^KafkaAUTFailures07242019_PASS2.txt]!KafkaUTFailures07242019_PASS2.GIF!","30/Jul/19 14:29;chandranc.rao@oracle.com;Any feedback?",,,,,,,,,,,,,,,,,,,,
DynamicBrokerReconfigurationTest#testAdvertisedListenerUpdate is flaky,KAFKA-6904,13159230,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Duplicate,,yuzhihong@gmail.com,yuzhihong@gmail.com,14/May/18 23:15,01/Mar/19 07:25,12/Jan/21 11:54,01/Mar/19 07:25,,,,,,,,,,core,unit tests,,,0,flaky-test,,,"From https://builds.apache.org/job/kafka-pr-jdk10-scala2.12/820/testReport/junit/kafka.server/DynamicBrokerReconfigurationTest/testAdvertisedListenerUpdate/ :
{code}
kafka.server.DynamicBrokerReconfigurationTest.testAdvertisedListenerUpdate

Failing for the past 1 build (Since Failed#820 )
Took 21 sec.
Error Message
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.
Stacktrace
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.valueOrError(FutureRecordMetadata.java:94)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:77)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at kafka.server.DynamicBrokerReconfigurationTest.$anonfun$verifyProduceConsume$3(DynamicBrokerReconfigurationTest.scala:996)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234)
	at scala.collection.Iterator.foreach(Iterator.scala:944)
	at scala.collection.Iterator.foreach$(Iterator.scala:944)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1432)
	at scala.collection.IterableLike.foreach(IterableLike.scala:71)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:70)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike.map(TraversableLike.scala:234)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:227)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at kafka.server.DynamicBrokerReconfigurationTest.verifyProduceConsume(DynamicBrokerReconfigurationTest.scala:996)
	at kafka.server.DynamicBrokerReconfigurationTest.testAdvertisedListenerUpdate(DynamicBrokerReconfigurationTest.scala:742)
{code}
The above happened with jdk 10.

",,guozhang,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-08-17 19:13:07.143,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 07 16:06:25 UTC 2018,,,,,,,"0|i3tp3j:",9223372036854775807,,,,,,,,,,,,,,,,"16/May/18 21:23;yuzhihong@gmail.com;Another subtest failed:

https://builds.apache.org/job/kafka-trunk-jdk8/2648/testReport/junit/kafka.server/DynamicBrokerReconfigurationTest/testUncleanLeaderElectionEnable/","05/Jul/18 00:47;yuzhihong@gmail.com;Haven't seen this failure lately.","17/Aug/18 19:13;guozhang;Have seen this failure again in Jenkins.","21/Aug/18 20:20;yuzhihong@gmail.com;testUncleanLeaderElectionEnable was the latest which may fail.

https://builds.apache.org/job/kafka-pr-jdk10-scala2.12/3599/testReport/junit/kafka.server/DynamicBrokerReconfigurationTest/testUncleanLeaderElectionEnable/
{code}
java.lang.AssertionError: Unclean leader not elected
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at kafka.server.DynamicBrokerReconfigurationTest.testUncleanLeaderElectionEnable(DynamicBrokerReconfigurationTest.scala:478)
{code}","21/Aug/18 20:24;yuzhihong@gmail.com;In the above test output, there was:
{code}
2018-08-21 18:29:47,614] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=1] Error for partition testtopic-6 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2018-08-21 18:29:47,615] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=1] Error for partition testtopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
{code}
Running the test locally, there is no such error in test output.","07/Sep/18 16:06;yuzhihong@gmail.com;Haven't seen this failure lately .",,,,,,,,,,,,,,,,
issue for extracting JSON from https web page,KAFKA-3180,12935598,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Not A Problem,,swayam,swayam,01/Feb/16 11:52,18/May/18 08:41,12/Jan/21 11:54,18/May/18 08:41,0.9.0.0,,,,,,,,,clients,,,,0,,,,"Hi Team,

Could you help me how to extract JSON info from https web page by help of kafka into HDFS . 

here is the json available URL : 
https://affiliate-api.flipkart.net/affiliate/api/8924b177d4c64fcab4db860b94fbcea2.json

Please help me to get the info ..",cloudera 5.4.2.0,omkreddy,swayam,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-05-18 08:41:12.558,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 18 08:41:12 UTC 2018,,,,,,,"0|i2s8b3:",9223372036854775807,,,,,,,,,,,,,,,,"18/May/18 08:41;omkreddy; I suggest to post these kind of queries to [[users@kafka.apache.org|mailto:users@kafka.apache.org]|mailto:[users@kafka.apache.org|mailto:users@kafka.apache.org]] mailing list ([[http://kafka.apache.org/contact]]) for more visibility.",,,,,,,,,,,,,,,,,,,,,
getting one test failed when building apache kafka,KAFKA-2501,12861268,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Duplicate,ewencp,gundun,gundun,02/Sep/15 09:55,26/Jan/17 03:03,12/Jan/21 11:54,26/Jan/17 03:03,0.8.2.0,,,,,,,,,build,,,,0,,,,"I have run steps from github https://github.com/apache/kafka
cd source-code
gradle
./gradlew jar
./gradlew srcJar
./gradlew test 

error :

org.apache.kafka.common.record.MemoryRecordsTest > testIterator[2] FAILED
    org.apache.kafka.common.KafkaException: java.lang.reflect.InvocationTargetException
        at org.apache.kafka.common.record.Compressor.wrapForOutput(Compressor.java:217)
        at org.apache.kafka.common.record.Compressor.<init>(Compressor.java:73)
        at org.apache.kafka.common.record.Compressor.<init>(Compressor.java:77)
        at org.apache.kafka.common.record.MemoryRecords.<init>(MemoryRecords.java:43)
        at org.apache.kafka.common.record.MemoryRecords.emptyRecords(MemoryRecords.java:51)
        at org.apache.kafka.common.record.MemoryRecords.emptyRecords(MemoryRecords.java:55)
        at org.apache.kafka.common.record.MemoryRecordsTest.testIterator(MemoryRecordsTest.java:42)

        Caused by:
        java.lang.reflect.InvocationTargetException
            at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
            at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
            at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
            at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
            at org.apache.kafka.common.record.Compressor.wrapForOutput(Compressor.java:213)
            ... 6 more

            Caused by:
            java.lang.UnsatisfiedLinkError: /tmp/snappy-unknown-fe798961-3b66-41f3-808a-68ebd27cc82d-libsnappyjava.so: /tmp/snappy-u                                    nknown-fe798961-3b66-41f3-808a-68ebd27cc82d-libsnappyjava.so: cannot open shared object file: No such file or directory (Possible ca                                    use: endianness mismatch)
                at java.lang.ClassLoader$NativeLibrary.load(Native Method)
                at java.lang.ClassLoader.loadLibrary1(ClassLoader.java:1965)
                at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1890)
                at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1851)
                at java.lang.Runtime.load0(Runtime.java:795)
                at java.lang.System.load(System.java:1062)
                at org.xerial.snappy.SnappyLoader.loadNativeLibrary(SnappyLoader.java:166)
                at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:145)
                at org.xerial.snappy.Snappy.<clinit>(Snappy.java:47)
                at org.xerial.snappy.SnappyOutputStream.<init>(SnappyOutputStream.java:90)
                at org.xerial.snappy.SnappyOutputStream.<init>(SnappyOutputStream.java:83)
                ... 11 more


267 tests completed, 1 failed
:clients:test FAILED


please help me fix the failure test case",Ubuntu 15.04 ppc64 le,ewencp,gundun,,,,,,,,,,172800,172800,,0%,172800,172800,,,,,,KAFKA-2543,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-09-03 04:34:20.92,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 26 03:03:05 UTC 2017,,,,,,,"0|i2jp2n:",9223372036854775807,,,,,,,,,,,,,,,,"03/Sep/15 04:34;ewencp;This looks like there's something wrong with the Snappy dependency, not with Kafka itself. What platform are you running this on? It also looks like it's trying to load the snappy library from a pretty unusual location -- normally libs shouldn't be under /tmp.","03/Sep/15 04:50;gundun;i am working on rhel7.1 and Ubuntu 15.04 ppc64 le ","03/Sep/15 05:18;ewencp;Can you find the snappy-java jar on your classpath and verify it actually contains the necessary file? It looks like the version of snappy-java from Central does contain a ppc64le library:

org/xerial/snappy/native/Linux/ppc64le/libsnappyjava.so

and the /tmp filename is probably required since the library has to be extracted to the filesystem before it's loaded. Perhaps something else cleaned up the file before it was loaded?

Also, is this the only test that's failing, and is it failing repeatedly? There should be more tests that require loading that same jar.","03/Sep/15 06:23;gundun;Hi Even,

I following the below steps to build the apache kafka 
git clone https://github.com/apache/kafka.git --branch 0.8.2
cd kafka
gradle
./gradlew jar
./gradlew srcJar
./gradlew test

and I have only one test case failure and it is failing repeatedly.
","03/Sep/15 06:28;gundun;root@69bb9e99fb8d:/kafka/core/build/dependant-libs-2.10.4# jar -tf snappy-java-1.1.1.6.jar
META-INF/MANIFEST.MF
org/
org/xerial/
org/xerial/snappy/
org/xerial/snappy/OSInfo.class
org/xerial/snappy/PureJavaCrc32C.class
org/xerial/snappy/Snappy.class
org/xerial/snappy/SnappyBundleActivator.class
org/xerial/snappy/SnappyCodec.class
org/xerial/snappy/SnappyError.class
org/xerial/snappy/SnappyErrorCode.class
org/xerial/snappy/SnappyException.class
org/xerial/snappy/SnappyFramed.class
org/xerial/snappy/SnappyFramedInputStream$FrameAction.class
org/xerial/snappy/SnappyFramedInputStream$FrameData.class
org/xerial/snappy/SnappyFramedInputStream$FrameMetaData.class
org/xerial/snappy/SnappyFramedInputStream.class
org/xerial/snappy/SnappyFramedOutputStream.class
org/xerial/snappy/SnappyIOException.class
org/xerial/snappy/SnappyInputStream.class
org/xerial/snappy/SnappyLoader.class
org/xerial/snappy/SnappyNative.class
org/xerial/snappy/SnappyOutputStream.class
org/xerial/snappy/buffer/
org/xerial/snappy/buffer/BufferAllocator.class
org/xerial/snappy/buffer/BufferAllocatorFactory.class
org/xerial/snappy/buffer/CachedBufferAllocator$1.class
org/xerial/snappy/buffer/CachedBufferAllocator.class
org/xerial/snappy/buffer/DefaultBufferAllocator$1.class
org/xerial/snappy/buffer/DefaultBufferAllocator.class
org/xerial/snappy/native/
org/xerial/snappy/native/AIX/
org/xerial/snappy/native/AIX/ppc64/
org/xerial/snappy/native/AIX/ppc64/libsnappyjava.a
org/xerial/snappy/native/Linux/
org/xerial/snappy/native/Linux/arm/
org/xerial/snappy/native/Linux/arm/libsnappyjava.so
org/xerial/snappy/native/Linux/armhf/
org/xerial/snappy/native/Linux/armhf/libsnappyjava.so
org/xerial/snappy/native/Linux/ppc64/
org/xerial/snappy/native/Linux/ppc64/libsnappyjava.so
org/xerial/snappy/native/Linux/ppc64le/
org/xerial/snappy/native/Linux/ppc64le/libsnappyjava.so
org/xerial/snappy/native/Linux/x86/
org/xerial/snappy/native/Linux/x86/libsnappyjava.so
org/xerial/snappy/native/Linux/x86_64/
org/xerial/snappy/native/Linux/x86_64/libsnappyjava.so
org/xerial/snappy/native/Mac/
org/xerial/snappy/native/Mac/x86/
org/xerial/snappy/native/Mac/x86/libsnappyjava.jnilib
org/xerial/snappy/native/Mac/x86_64/
org/xerial/snappy/native/Mac/x86_64/libsnappyjava.jnilib
org/xerial/snappy/native/README
org/xerial/snappy/native/SunOS/
org/xerial/snappy/native/SunOS/sparc/
org/xerial/snappy/native/SunOS/sparc/libsnappyjava.so
org/xerial/snappy/native/SunOS/x86/
org/xerial/snappy/native/SunOS/x86/libsnappyjava.so
org/xerial/snappy/native/SunOS/x86_64/
org/xerial/snappy/native/SunOS/x86_64/libsnappyjava.so
org/xerial/snappy/native/Windows/
org/xerial/snappy/native/Windows/x86/
org/xerial/snappy/native/Windows/x86/snappyjava.dll
org/xerial/snappy/native/Windows/x86_64/
org/xerial/snappy/native/Windows/x86_64/snappyjava.dll
","26/Jan/17 03:02;ewencp;Exact duplicate by same reporter","26/Jan/17 03:03;ewencp;See duplicate, which was resolve as Can't Reproduce. This was also tagged for an old release that is unlikely to see fixes. The other bug can be reopened if needed.",,,,,,,,,,,,,,,
Closing socket connection to /10.118.192.104. (kafka.network.Processor),KAFKA-1830,12763807,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Not A Problem,,tapas.030uce@gmail.com,tapas.030uce@gmail.com,26/Dec/14 09:50,28/Oct/15 16:10,12/Jan/21 11:54,30/Dec/14 04:26,0.8.1,,,,,,,,,log,,,,0,,,,"I was testing Spark-Kafka integration . Created one producer which pushes data to kafka topic. One consumer reads that data and processes it and publish results to another kafka topic. Suddenly the following log is seen in the console.
[2014-12-26 15:20:04,643] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:04,848] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:05,053] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:05,257] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:05,462] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:05,666] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:05,870] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:06,074] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:06,280] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:06,484] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:06,689] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:06,911] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:07,116] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:07,320] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:07,525] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:07,729] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:07,934] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:08,140] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:08,345] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:08,551] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:08,756] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:08,960] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:09,165] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:09,370] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:09,574] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:09,778] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:09,983] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:10,189] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:10,394] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:10,599] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:10,804] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:11,009] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:11,214] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:11,418] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:11,623] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:11,827] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)
[2014-12-26 15:20:12,043] INFO Closing socket connection to /10.118.192.104. (kafka.network.Processor)

 ","Linux OS, 5 node CDH5.12 cluster, Scala 2.10.4",guozhang,mingjielai,spowis@salesforce.com,tapas.030uce@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-12-30 04:25:35.346,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 28 16:10:22 UTC 2015,,,,,,,"0|i23s7j:",9223372036854775807,,,,,,,,,,,,,,,,"30/Dec/14 04:25;guozhang;Tapas, this log entry is expected as clients periodically refresh their metadata using a separate / transient socket and is removed in the latest trunk.","21/Jan/15 01:31;mingjielai;[~guozhang] Can you tell me which JIRA or git commit fixed the issue? Can you help to port it to 0.8.2 branch?","11/Mar/15 17:01;guozhang;0.8.2 release already changed the log level from INFO to DEBUG.","28/Oct/15 16:10;spowis@salesforce.com;Running 0.8.2.1 and this is still an INFO log.  Reviewing the release notes back thru 8.1.0 I don't see KAFKA-1830 included anywhere.",,,,,,,,,,,,,,,,,,
Add integration test for DumpLogSegments,KAFKA-4928,13057933,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,ijuma,ijuma,21/Mar/17 15:34,11/Jan/21 12:23,12/Jan/21 11:54,,,,,,,,,,,log,tools,,,0,newbie,,,"DumpLogSegments is an important tool to analyse log files, but we have no JUnit tests for it. It would be good to have some tests that verify that the output is sane for a populated log.

Our system tests call DumpLogSegments, but we should be able to detect regressions via the JUnit test suite.",,githubbot,guozhang,high.lee,ijuma,lindong,mjsax,original-brownbear,sammers,smurakozi,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-03-27 15:14:49.724,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 11 12:23:44 UTC 2021,,,,,,,"0|i3cl3b:",9223372036854775807,,,,,,,,,,,,,,,,"27/Mar/17 15:14;original-brownbear;Trying this one :)","21/Apr/17 14:45;githubbot;GitHub user original-brownbear opened a pull request:

    https://github.com/apache/kafka/pull/2889

    KAFKA-4928: Add integration test for DumpLogSegments

    Adding tests for `kafka.tools.DumpLogSegments`

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/original-brownbear/kafka KAFKA-4928

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2889.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2889
    
----

----
","12/Sep/17 07:16;githubbot;Github user original-brownbear closed the pull request at:

    https://github.com/apache/kafka/pull/2889
","23/Sep/17 04:49;guozhang;*Reminder to the contributor / reviewer of the PR*: please note that the code deadline for 1.0.0 is less than 2 weeks away (Oct. 4th). Please re-evaluate your JIRA and see if it still makes sense to be merged into 1.0.0 or it could be pushed out to 1.1.0, or be closed directly if the JIRA itself is not valid any more, or re-assign yourself as contributor / committer if you are no longer working on the JIRA.","18/Oct/17 15:10;sammers;I'd like to work on that","27/Oct/17 15:18;githubbot;GitHub user Sammers21 opened a pull request:

    https://github.com/apache/kafka/pull/4145

    KAFKA-4928: Add integration test for DumpLogSegments

    Adding tests for `kafka.tools.DumpLogSegments`

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/Sammers21/kafka test_for_dump_log_segments

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/4145.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #4145
    
----

----
","02/Oct/18 14:30;lindong;Moving this to 2.2.0 since there is no progress in the past few months.","17/Feb/19 19:13;mjsax;Moving all major/minor/trivial tickets that are not merged yet out of 2.2 release.","11/Feb/20 11:31;githubbot;Sammers21 commented on pull request #4145: KAFKA-4928: Add integration test for DumpLogSegments
URL: https://github.com/apache/kafka/pull/4145
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","11/Jan/21 12:23;high.lee;[~ijuma] 
Is it okay for me to do this?
thanks",,,,,,,,,,,,
Fix flaky kafka.network.SocketServerTest.testConnectionRatePerIp test,KAFKA-10854,13346025,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,15/Dec/20 03:25,15/Dec/20 17:12,12/Jan/21 11:54,15/Dec/20 16:54,,,,,2.8.0,,,,,,,,,0,,,,"org.scalatest.exceptions.TestFailedException: expected exception when writing to closed plain socket
 at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530)
 at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529)
 at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1389)
 at org.scalatest.Assertions.fail(Assertions.scala:1091)
 at org.scalatest.Assertions.fail$(Assertions.scala:1087)
 at org.scalatest.Assertions$.fail(Assertions.scala:1389)
 at kafka.network.SocketServerTest.verifyRemoteConnectionClosed(SocketServerTest.scala:922)
 at kafka.network.SocketServerTest.testConnectionRatePerIp(SocketServerTest.scala:884)
 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)
 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 15 06:00:10 UTC 2020,,,,,,,"0|z0lij4:",9223372036854775807,,,,,,,,,,,,,,,,"15/Dec/20 06:00;showuon;[https://ci-builds.apache.org/job/Kafka/job/kafka-trunk-jdk11/308/testReport/junit/kafka.network/SocketServerTest/testConnectionRatePerIp/]

[https://ci-builds.apache.org/job/Kafka/job/kafka-trunk-jdk15/332/testReport/junit/kafka.network/SocketServerTest/testConnectionRatePerIp/]

 ",,,,,,,,,,,,,,,,,,,,,
Fix flaky shouldShutdownSingleThreadApplication test,KAFKA-10754,13341725,Test,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,20/Nov/20 07:04,30/Nov/20 19:13,12/Jan/21 11:54,25/Nov/20 02:43,,,,,2.8.0,,,,,streams,unit tests,,,0,,,,"org.apache.kafka.streams.integration.StreamsUncaughtExceptionHandlerIntegrationTest.shouldShutdownSingleThreadApplication failed, log available in /home/jenkins/jenkins-agent/workspace/Kafka/kafka-trunk-jdk11/streams/build/reports/testOutput/org.apache.kafka.streams.integration.StreamsUncaughtExceptionHandlerIntegrationTest.shouldShutdownSingleThreadApplication.test.stdout

org.apache.kafka.streams.integration.StreamsUncaughtExceptionHandlerIntegrationTest > shouldShutdownSingleThreadApplication FAILED
 java.lang.AssertionError: Expected all streams instances in [org.apache.kafka.streams.KafkaStreams@36c1250, org.apache.kafka.streams.KafkaStreams@124268b5] to be ERROR within 30000 ms, but the following were not: \{org.apache.kafka.streams.KafkaStreams@124268b5=RUNNING, org.apache.kafka.streams.KafkaStreams@36c1250=RUNNING}
 at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:26)
 at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitForApplicationState$12(IntegrationTestUtils.java:933)
 at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:450)
 at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:418)
 at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitForApplicationState(IntegrationTestUtils.java:916)
 at org.apache.kafka.streams.integration.StreamsUncaughtExceptionHandlerIntegrationTest.shouldShutdownSingleThreadApplication(StreamsUncaughtExceptionHandlerIntegrationTest.java:186)

 

 

[https://ci-builds.apache.org/blue/rest/organizations/jenkins/pipelines/Kafka/pipelines/kafka-trunk-jdk15/runs/267/log/?start=0]

[https://ci-builds.apache.org/blue/rest/organizations/jenkins/pipelines/Kafka/pipelines/kafka-trunk-jdk11/runs/241/log/?start=0]

[https://ci-builds.apache.org/blue/rest/organizations/jenkins/pipelines/Kafka/pipelines/kafka-trunk-jdk15/runs/270/log/?start=0]

 ",,ableegoldman,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-11-20 07:04:04.0,,,,,,,"0|z0ks20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve `testRollingBrokerRestartsWithSmallerMaxGroupSizeConfigDisruptsBigGroup`,KAFKA-8266,13229163,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,hachikuji,hachikuji,19/Apr/19 23:52,30/Nov/20 16:05,12/Jan/21 11:54,30/Nov/20 16:05,,,,,,,,,,,,,,0,,,,Some additional validation could be done after the member gets kicked out. The main thing is showing that the group can continue to consume data and commit offsets.,,ableegoldman,cadonna,chia7712,dajac,hachikuji,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-02-05 23:20:38.856,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 30 16:05:15 UTC 2020,,,,,,,"0|z01z60:",9223372036854775807,,,,,,,,,,,,,,,,"05/Feb/20 23:20;mjsax;This test is rather flaky, cf. KAFKA-7965. It might be best to merge both tickets?","20/Mar/20 18:29;ableegoldman;Can we close this as a duplicate?","19/Jun/20 01:05;mjsax;The other ticket is resolved, but I just saw another failure for this test: [https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/7034/testReport/junit/kafka.api/ConsumerBounceTest/testRollingBrokerRestartsWithSmallerMaxGroupSizeConfigDisruptsBigGroup/]
{quote}org.scalatest.exceptions.TestFailedException: The remaining consumers in the group could not fetch the expected records at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530) at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529) at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1389) at org.scalatest.Assertions.fail(Assertions.scala:1091) at org.scalatest.Assertions.fail$(Assertions.scala:1087) at org.scalatest.Assertions$.fail(Assertions.scala:1389) at kafka.api.ConsumerBounceTest.testRollingBrokerRestartsWithSmallerMaxGroupSizeConfigDisruptsBigGroup(ConsumerBounceTest.scala:329){quote}
\cc [~dajac]","19/Jun/20 07:36;dajac;Thanks for reporting [~mjsax]. I just checked the logs and there is nothing to diagnose the issue in there. It seems that the test ran slower than usual though. It may indicate an overloaded Jenkins worker. I will keep an eye on it.","21/Sep/20 14:07;cadonna;This test failed for me with:
{code:java}
org.scalatest.exceptions.TestFailedException: The remaining consumers in the group could not fetch the expected records
	at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530)
	at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529)
	at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1389)
	at org.scalatest.Assertions.fail(Assertions.scala:1091)
	at org.scalatest.Assertions.fail$(Assertions.scala:1087)
	at org.scalatest.Assertions$.fail(Assertions.scala:1389)
	at kafka.api.ConsumerBounceTest.testRollingBrokerRestartsWithSmallerMaxGroupSizeConfigDisruptsBigGroup(ConsumerBounceTest.scala:331)
{code}

The job can be found here:
https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-9262/6/tests","25/Sep/20 05:37;chia7712;[~dajac] Could I take over this issue? I have encountered this flaky many times recently.","25/Sep/20 07:43;dajac;[~chia7712] I will take a look at this. I wonder why this has come back more frequently now...","25/Sep/20 07:48;chia7712;{quote}
I will take a look at this. I wonder why this has come back more frequently now...
{quote}

[~dajac] Thank you so much :)","29/Sep/20 02:26;chia7712;flaky again :(

https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-9284/6/testReport/kafka.api/ConsumerBounceTest/Build___JDK_11___testRollingBrokerRestartsWithSmallerMaxGroupSizeConfigDisruptsBigGroup/","30/Nov/20 16:05;dajac;It seems that we haven't seen this one for a while now. I will close it. Please, re-open if necessary.",,,,,,,,,,,,
Flaky Test org.apache.kafka.streams.integration.PurgeRepartitionTopicIntegrationTest.shouldRestoreState,KAFKA-10405,13322798,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,bbejeck,bbejeck,14/Aug/20 19:04,13/Nov/20 18:04,12/Jan/21 11:54,,,,,,,,,,,streams,,,,0,,,,"From build [https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/1979/]

 
{noformat}
org.apache.kafka.streams.integration.PurgeRepartitionTopicIntegrationTest > shouldRestoreState FAILED
14:25:19     java.lang.AssertionError: Condition not met within timeout 60000. Repartition topic restore-test-KSTREAM-AGGREGATE-STATE-STORE-0000000002-repartition not purged data after 60000 ms.
14:25:19         at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:26)
14:25:19         at org.apache.kafka.test.TestUtils.lambda$waitForCondition$6(TestUtils.java:401)
14:25:19         at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:449)
14:25:19         at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:417)
14:25:19         at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:398)
14:25:19         at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:388)
14:25:19         at org.apache.kafka.streams.integration.PurgeRepartitionTopicIntegrationTest.shouldRestoreState(PurgeRepartitionTopicIntegrationTest.java:206){noformat}",,ableegoldman,bbejeck,cadonna,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-09-08 10:29:42.801,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 13 18:04:20 UTC 2020,,,,,,,"0|z0hsbs:",9223372036854775807,,,,,,,,,,,,,,,,"08/Sep/20 10:29;cadonna;Another failure:

https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-9258/1/tests

{code}
java.lang.AssertionError: Condition not met within timeout 60000. Repartition topic restore-test-KSTREAM-AGGREGATE-STATE-STORE-0000000002-repartition not purged data after 60000 ms.
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:26)
	at org.apache.kafka.test.TestUtils.lambda$waitForCondition$6(TestUtils.java:401)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:449)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:417)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:398)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:388)
	at org.apache.kafka.streams.integration.PurgeRepartitionTopicIntegrationTest.shouldRestoreState(PurgeRepartitionTopicIntegrationTest.java:206)
{code}","15/Sep/20 21:07;ableegoldman;Same error as above – [https://github.com/apache/kafka/pull/8892/checks?check_run_id=1115481931]","05/Oct/20 22:44;bbejeck;Saw same error again - https://github.com/apache/kafka/pull/9099/checks?check_run_id=1210606325","19/Oct/20 18:06;mjsax;[https://github.com/apache/kafka/pull/9368/checks?check_run_id=1267096954] ","13/Nov/20 18:04;ableegoldman;https://github.com/apache/kafka/pull/9489/checks?check_run_id=1394380768",,,,,,,,,,,,,,,,,
Mock consumer should behave consistent with actual consumer,KAFKA-9679,13290233,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,arafsheikh,bchen225242,bchen225242,07/Mar/20 01:01,17/Oct/20 19:09,12/Jan/21 11:54,,,,,,,,,,,consumer,streams,,,1,help-wanted,newbie,newbie++,"Right now in MockConsumer we shall return illegal state exception when the buffered records are not able to find corresponding assigned partitions. This is not the case for KafkaConsumer where we shall just not return those data during `poll()` call. This inconsistent behavior should be fixed.

Note that if we are going to take this fix, the full unit tests need to be executed to make sure no regression is introduced, as some tests are potentially depending on the current MockConsumer behavior.",,albert02lowis,arafsheikh,bchen225242,epinto,quanuw,rakicodes,sujayopensource,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-03-15 07:03:08.381,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 17 19:09:03 UTC 2020,,,,,,,"0|z0c9yw:",9223372036854775807,,,,,,,,,,,,,,,,"15/Mar/20 07:03;sujayopensource;Hi [~bchen225242],



I would take up this. Im a newbie and would be a good way for me to start off via Tests.

Assigning it to myself. Let me know if there are concerns.

Thanks","15/Mar/20 12:19;sujayopensource;[~bchen225242],

 

Do we still have this issue?

I went through MockConsumer and came across:

public synchronized void addRecord(ConsumerRecord<K, V> record) {
 ensureNotClosed();
 TopicPartition tp = new TopicPartition(record.topic(), record.partition());
 Set<TopicPartition> currentAssigned = this.subscriptions.assignedPartitions();
 if (!currentAssigned.contains(tp))
 throw new IllegalStateException(""Cannot add records for a partition that is not assigned to the consumer"");
 List<ConsumerRecord<K, V>> recs = this.records.computeIfAbsent(tp, k -> new ArrayList<>());
 recs.add(record);
}

Is this what you were referring to?
I feel this is a bit different from whats mentioned in description(I may be wrong).
In above we dont add records to a partition if we cannot find a consumer to which the partition is assigned to.","15/Mar/20 15:38;bchen225242;Thanks for taking this up! [~sujayopensource] Yes, the behavior we want to fix is in MockConsumer.poll(). The addRecords fence adding a record into a non-existing partition, but wont' help if we add records to an existing partition, subscription changes and loses the partition, and mock consumer fail into illegal state due to unknown partition.

I would recommend you to read this context: [https://github.com/apache/kafka/pull/8220/files#r389201119]

to better understand the issue. Let me know if you have further questions.","08/Sep/20 12:04;albert02lowis;Hello [~sujayopensource] and [~bchen225242] , I'm a newbie looking to contribute, it seems that it has been some time since there is activity in this issue, do you mind if I take this up?

Thank you!","11/Oct/20 19:27;arafsheikh;Hi [~bchen225242], I've taken a stab at this issue and raised a PR. My understanding is that during poll the MockConsumer should filter records from partitions it is currently not subscribed to. Correct me if I'm missing something.

I've also run all the unit + integration tests on my commit and they seem to pass locally (except `SocketServerTest.testIdleConnection` which fails on trunk as well and I believe is unrelated).","17/Oct/20 19:09;arafsheikh;Hi [~bchen225242], did you get a chance to take a look at the PR? Let me know if I'm on the right path or if the commit can be improved.",,,,,,,,,,,,,,,,
Fix flaky testDynamicListenerConnectionCreationRateQuota,KAFKA-10482,13327595,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,15/Sep/20 06:37,24/Sep/20 06:56,12/Jan/21 11:54,24/Sep/20 06:56,,,,,,,,,,,,,,0,,,,"[https://ci-builds.apache.org/blue/rest/organizations/jenkins/pipelines/Kafka/pipelines/kafka-trunk-jdk11/runs/64/log/?start=0]

 

[https://ci-builds.apache.org/blue/rest/organizations/jenkins/pipelines/Kafka/pipelines/kafka-trunk-jdk15/runs/63/log/?start=0]

kafka.network.DynamicConnectionQuotaTest > testDynamicListenerConnectionCreationRateQuota FAILED
 java.util.concurrent.ExecutionException: org.scalatest.exceptions.TestFailedException: Connections not closed (initial = 2 current = 1)
 at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
 at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:205)
 at kafka.network.DynamicConnectionQuotaTest.testDynamicListenerConnectionCreationRateQuota(DynamicConnectionQuotaTest.scala:219)

Caused by:
 org.scalatest.exceptions.TestFailedException: Connections not closed (initial = 2 current = 1)
 at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530)
 at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529)
 at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1389)
 at org.scalatest.Assertions.fail(Assertions.scala:1091)
 at org.scalatest.Assertions.fail$(Assertions.scala:1087)
 at org.scalatest.Assertions$.fail(Assertions.scala:1389)
 at kafka.network.DynamicConnectionQuotaTest.verifyConnectionRate(DynamicConnectionQuotaTest.scala:349)
 at kafka.network.DynamicConnectionQuotaTest.$anonfun$testDynamicListenerConnectionCreationRateQuota$5(DynamicConnectionQuotaTest.scala:217)",,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 22 06:56:27 UTC 2020,,,,,,,"0|z0ilw8:",9223372036854775807,,,,,,,,,,,,,,,,"18/Sep/20 06:43;showuon;https://ci-builds.apache.org/blue/rest/organizations/jenkins/pipelines/Kafka/pipelines/kafka-trunk-jdk15/runs/69/log/?start=0","21/Sep/20 09:23;showuon;[https://ci-builds.apache.org/blue/rest/organizations/jenkins/pipelines/Kafka/pipelines/kafka-trunk-jdk15/runs/71/log/?start=0]

https://ci-builds.apache.org/blue/rest/organizations/jenkins/pipelines/Kafka/pipelines/kafka-trunk-jdk8/runs/72/log/?start=0","22/Sep/20 06:56;showuon;https://ci-builds.apache.org/blue/rest/organizations/jenkins/pipelines/Kafka/pipelines/kafka-trunk-jdk15/runs/78/log/?start=0

[https://ci-builds.apache.org/blue/rest/organizations/jenkins/pipelines/Kafka/pipelines/kafka-trunk-jdk15/runs/77/log/?start=0]",,,,,,,,,,,,,,,,,,,
Flaky Test kafka.api.SaslSslConsumerTest.testCoordinatorFailover,KAFKA-10404,13322784,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,bbejeck,bbejeck,14/Aug/20 16:42,16/Aug/20 14:11,12/Jan/21 11:54,16/Aug/20 14:11,,,,,2.5.2,2.6.1,2.7.0,,,core,unit tests,,,0,,,,"From build [https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3829/]

 
{noformat}
kafka.api.SaslSslConsumerTest > testCoordinatorFailover FAILED
11:27:15     java.lang.AssertionError: expected:<None> but was:<Some(org.apache.kafka.clients.consumer.CommitFailedException: Offset commit cannot be completed since the consumer is not part of an active group for auto partition assignment; it is likely that the consumer was kicked out of the group.)>
11:27:15         at org.junit.Assert.fail(Assert.java:89)
11:27:15         at org.junit.Assert.failNotEquals(Assert.java:835)
11:27:15         at org.junit.Assert.assertEquals(Assert.java:120)
11:27:15         at org.junit.Assert.assertEquals(Assert.java:146)
11:27:15         at kafka.api.AbstractConsumerTest.sendAndAwaitAsyncCommit(AbstractConsumerTest.scala:195)
11:27:15         at kafka.api.AbstractConsumerTest.ensureNoRebalance(AbstractConsumerTest.scala:302)
11:27:15         at kafka.api.BaseConsumerTest.testCoordinatorFailover(BaseConsumerTest.scala:76)
11:27:15         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
11:27:15         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
11:27:15         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
11:27:15         at java.lang.reflect.Method.invoke(Method.java:498)
11:27:15         at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
11:27:15         at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
11:27:15         at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
11:27:15         at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
11:27:15         at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
11:27:15         at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
11:27:15         at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
11:27:15         at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
11:27:15         at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
11:27:15         at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
11:27:15         at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
11:27:15         at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
11:27:15         at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
11:27:15         at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
11:27:15         at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
11:27:15         at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
11:27:15         at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
11:27:15         at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
11:27:15         at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
11:27:15         at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
11:27:15         at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
11:27:15         at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
11:27:15         at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
11:27:15         at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
11:27:15         at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
11:27:15         at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
11:27:15         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
11:27:15         at java.lang.reflect.Method.invoke(Method.java:498)
11:27:15         at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
11:27:15         at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
11:27:15         at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
11:27:15         at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
11:27:15         at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
11:27:15         at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:119)
11:27:15         at sun.reflect.GeneratedMethodAccessor25.invoke(Unknown Source)
11:27:15         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
11:27:15         at java.lang.reflect.Method.invoke(Method.java:498)
11:27:15         at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
11:27:15         at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
11:27:15         at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
11:27:15         at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
11:27:15         at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:414)
11:27:15         at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
11:27:15         at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
11:27:15         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
11:27:15         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
11:27:15         at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
11:27:15         at java.lang.Thread.run(Thread.java:748){noformat}",,bbejeck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-08-14 16:42:30.0,,,,,,,"0|z0hs8o:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor AbstractJoinIntegrationTest and Sub-classes,KAFKA-9273,13272544,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,albert02lowis,bbejeck,bbejeck,05/Dec/19 16:06,15/Aug/20 16:45,12/Jan/21 11:54,15/Aug/20 16:31,2.7.0,,,,2.7.0,,,,,streams,,,,0,newbie,,,"The  AbstractJoinIntegrationTest uses an embedded broker, but not all the sub-classes require the use of an embedded broker anymore.  Additionally, there are two test remaining that require an embedded broker, but they don't perform joins, the are tests validating other conditions, so ideally those tests should move into a separate test",,ableegoldman,albert02lowis,bbejeck,cadonna,gauravs,mjsax,sujayopensource,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-12-14 13:00:02.513,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 31 04:37:52 UTC 2020,,,,,,,"0|z09c5c:",9223372036854775807,,,,,,,,,,,,,,,,"14/Dec/19 13:00;sujayopensource;HI [~bbejeck],

 

I am new to Kafka(as developer/contributor).

Shall I take this up as a starting task?

 

Thanks,

Sujay","16/Dec/19 23:26;bbejeck;Hi [~sujayopensource]

Yes, feel free to start working on this task.  I've assigned this ticket to you.  

Thanks for your interest in contributing.

Bill","29/Jul/20 15:38;albert02lowis;Hi [~bbejeck] , [~sujayopensource] I am a newbie looking to contribute,

Can I take up this task? Since I see that it has been sometime since the last activity

 

Thank you,

Albert","29/Jul/20 16:44;bbejeck;Hi [~albert02lowis],

 

Thanks for your interest.  It looks like [~sujayopensource] has not started work on this ticket yet.  I'd give another day or so to respond, then if you don't hear anything back, feel free to pick this ticket up.  

I've taken the liberty of adding you to the contributors list, so you should be able to self-assign this ticket and any others in the future.

 

-Bill","31/Jul/20 02:07;albert02lowis;Hi [~bbejeck] thanks for the update and for adding me into the contributors list!

Since it has been more than a day, I will assign myself to this ticket.

Looks like this task can be broken down to 3 PRs:
 # Extract out testShouldAutoShutdownOnIncompleteMetadata from StreamTableJoinIntegrationTest into its own test
 # Extract out shouldNotAccessJoinStoresWhenGivingName from StreamStreamJoinIntegrationTest into its own test
 # Finally, remove the unused embedded broker from AbstractJoinIntegrationTest

I will try to create the PR for the 1st one, let me know if my understanding is incorrect","31/Jul/20 04:37;albert02lowis;PR Created: https://github.com/apache/kafka/pull/9108",,,,,,,,,,,,,,,,
Flaky Test kafka.api.SaslPlaintextConsumerTest.testCoordinatorFailover,KAFKA-9007,13261372,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,bbejeck,bbejeck,09/Oct/19 16:30,11/Aug/20 15:58,12/Jan/21 11:54,,,,,,,,,,,core,,,,0,flaky-test,,,"Failed in [https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/25644/testReport/junit/kafka.api/SaslPlaintextConsumerTest/testCoordinatorFailover/]

 
{noformat}
Error Messagejava.lang.AssertionError: expected:<None> but was:<Some(org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing max.poll.interval.ms or by reducing the maximum size of batches returned in poll() with max.poll.records.)>Stacktracejava.lang.AssertionError: expected:<None> but was:<Some(org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing max.poll.interval.ms or by reducing the maximum size of batches returned in poll() with max.poll.records.)>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:120)
	at org.junit.Assert.assertEquals(Assert.java:146)
	at kafka.api.AbstractConsumerTest.sendAndAwaitAsyncCommit(AbstractConsumerTest.scala:195)
	at kafka.api.AbstractConsumerTest.ensureNoRebalance(AbstractConsumerTest.scala:302)
	at kafka.api.BaseConsumerTest.testCoordinatorFailover(BaseConsumerTest.scala:76)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:305)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:365)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:330)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:78)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:328)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:65)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:292)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:305)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:412)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.GeneratedMethodAccessor21.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)
	at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.lang.Thread.run(Thread.java:748)
Standard Output[2019-10-09 04:16:11,167] WARN SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/kafka8699070131166161961.tmp'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it. (org.apache.zookeeper.ClientCnxn:1094)
[2019-10-09 04:16:11,167] ERROR [ZooKeeperClient] Auth failed. (kafka.zookeeper.ZooKeeperClient:74)
[2019-10-09 04:16:11,173] WARN SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/kafka8699070131166161961.tmp'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it. (org.apache.zookeeper.ClientCnxn:1094)
[2019-10-09 04:16:11,173] ERROR [ZooKeeperClient Kafka server] Auth failed. (kafka.zookeeper.ZooKeeperClient:74)
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka1004276285089639183.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 04:16:11,446] WARN SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/kafka8699070131166161961.tmp'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it. (org.apache.zookeeper.ClientCnxn:1094)
[2019-10-09 04:16:11,447] ERROR [ZooKeeperClient Kafka server] Auth failed. (kafka.zookeeper.ZooKeeperClient:74)
[2019-10-09 04:16:11,751] WARN SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/kafka8699070131166161961.tmp'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it. (org.apache.zookeeper.ClientCnxn:1094)
[2019-10-09 04:16:11,751] ERROR [ZooKeeperClient Kafka server] Auth failed. (kafka.zookeeper.ZooKeeperClient:74)
[2019-10-09 04:16:11,911] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 04:16:11,911] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 04:16:12,131] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 04:16:12,135] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 04:16:12,169] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka5852180608788496098.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 04:17:47,303] WARN SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/kafka6314540047031769315.tmp'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it. (org.apache.zookeeper.ClientCnxn:1094)
[2019-10-09 04:17:47,304] ERROR [ZooKeeperClient] Auth failed. (kafka.zookeeper.ZooKeeperClient:74)
[2019-10-09 04:17:47,310] WARN SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/kafka6314540047031769315.tmp'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it. (org.apache.zookeeper.ClientCnxn:1094)
[2019-10-09 04:17:47,310] ERROR [ZooKeeperClient Kafka server] Auth failed. (kafka.zookeeper.ZooKeeperClient:74)
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka2446781755760053585.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 04:17:47,474] WARN SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/kafka6314540047031769315.tmp'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it. (org.apache.zookeeper.ClientCnxn:1094)
[2019-10-09 04:17:47,475] ERROR [ZooKeeperClient Kafka server] Auth failed. (kafka.zookeeper.ZooKeeperClient:74)
[2019-10-09 04:17:47,608] WARN SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/kafka6314540047031769315.tmp'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it. (org.apache.zookeeper.ClientCnxn:1094)
[2019-10-09 04:17:47,609] ERROR [ZooKeeperClient Kafka server] Auth failed. (kafka.zookeeper.ZooKeeperClient:74)
[2019-10-09 04:17:47,772] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 04:17:47,772] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 04:17:48,165] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 04:17:48,165] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8412381019780226270.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 
{noformat}",,bbejeck,bchen225242,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-02-02 18:25:38.104,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 11 15:58:50 UTC 2020,,,,,,,"0|z07foo:",9223372036854775807,,,,,,,,,,,,,,,,"02/Feb/20 18:25;mjsax;This also failed for SaslSslCosumerTest
{quote}java.lang.AssertionError: expected:<None> but was:<Some(org.apache.kafka.clients.consumer.CommitFailedException: Offset commit cannot be completed since the consumer is not part of an active group for auto partition assignment; it is likely that the consumer was kicked out of the group.)> at org.junit.Assert.fail(Assert.java:89) at org.junit.Assert.failNotEquals(Assert.java:835) at org.junit.Assert.assertEquals(Assert.java:120) at org.junit.Assert.assertEquals(Assert.java:146) at kafka.api.AbstractConsumerTest.sendAndAwaitAsyncCommit(AbstractConsumerTest.scala:195) at kafka.api.AbstractConsumerTest.ensureNoRebalance(AbstractConsumerTest.scala:302) at kafka.api.BaseConsumerTest.testCoordinatorFailover(BaseConsumerTest.scala:76){quote}","05/Feb/20 23:21;mjsax;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/4509/testReport/junit/kafka.api/SaslPlaintextConsumerTest/testCoordinatorFailover/]","11/Aug/20 15:58;bchen225242;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3773/testReport/junit/kafka.api/SaslSslConsumerTest/testCoordinatorFailover/]

 
h3. Error Message

java.lang.AssertionError: expected:<None> but was:<Some(org.apache.kafka.clients.consumer.CommitFailedException: Offset commit cannot be completed since the consumer is not part of an active group for auto partition assignment; it is likely that the consumer was kicked out of the group.)>
h3. Stacktrace

java.lang.AssertionError: expected:<None> but was:<Some(org.apache.kafka.clients.consumer.CommitFailedException: Offset commit cannot be completed since the consumer is not part of an active group for auto partition assignment; it is likely that the consumer was kicked out of the group.)> at org.junit.Assert.fail(Assert.java:89) at org.junit.Assert.failNotEquals(Assert.java:835) at org.junit.Assert.assertEquals(Assert.java:120) at org.junit.Assert.assertEquals(Assert.java:146) at kafka.api.AbstractConsumerTest.sendAndAwaitAsyncCommit(AbstractConsumerTest.scala:195) at kafka.api.AbstractConsumerTest.ensureNoRebalance(AbstractConsumerTest.scala:302) at kafka.api.BaseConsumerTest.testCoordinatorFailover(BaseConsumerTest.scala:76) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)",,,,,,,,,,,,,,,,,,,
Fix flaky testOneWayReplicationWithAutorOffsetSync1 test,KAFKA-10255,13315914,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,09/Jul/20 14:16,03/Aug/20 21:13,12/Jan/21 11:54,30/Jul/20 21:01,,,,,2.7.0,,,,,,,,,0,,,,"https://builds.apache.org/blue/rest/organizations/jenkins/pipelines/kafka-trunk-jdk14/runs/279/log/?start=0

org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest > testOneWayReplicationWithAutorOffsetSync1 STARTED
org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testOneWayReplicationWithAutorOffsetSync1 failed, log available in /home/jenkins/jenkins-slave/workspace/kafka-trunk-jdk14/connect/mirror/build/reports/testOutput/org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testOneWayReplicationWithAutorOffsetSync1.test.stdout

org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest > testOneWayReplicationWithAutorOffsetSync1 FAILED
 java.lang.AssertionError: consumer record size is not zero expected:<0> but was:<2>
 at org.junit.Assert.fail(Assert.java:89)
 at org.junit.Assert.failNotEquals(Assert.java:835)
 at org.junit.Assert.assertEquals(Assert.java:647)
 at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testOneWayReplicationWithAutorOffsetSync1(MirrorConnectorsIntegrationTest.java:349)",,ableegoldman,cadonna,mjsax,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-07-10 01:01:09.261,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 30 07:53:28 UTC 2020,,,,,,,"0|z0glzs:",9223372036854775807,,,,,,,,,,,,,,,,"10/Jul/20 01:01;ableegoldman;Failed again 
h3. Stacktrace

java.lang.AssertionError: consumer record size is not zero expected:<0> but was:<3> at org.junit.Assert.fail(Assert.java:89) at org.junit.Assert.failNotEquals(Assert.java:835) at org.junit.Assert.assertEquals(Assert.java:647) at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testOneWayReplicationWithAutorOffsetSync1(MirrorConnectorsIntegrationTest.java:349)","11/Jul/20 18:26;mjsax;One more: [https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/1512/testReport/org.apache.kafka.connect.mirror/MirrorConnectorsIntegrationTest/testOneWayReplicationWithAutorOffsetSync1/]","11/Jul/20 18:38;mjsax;Another one: [https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/7372/testReport/junit/org.apache.kafka.connect.mirror/MirrorConnectorsIntegrationTest/testOneWayReplicationWithAutorOffsetSync1/]","23/Jul/20 02:51;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3489/testReport/junit/org.apache.kafka.connect.mirror/MirrorConnectorsIntegrationTest/testOneWayReplicationWithAutorOffsetSync1/]","23/Jul/20 16:32;mjsax;Two more:
 * [https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/1630/testReport/junit/org.apache.kafka.connect.mirror/MirrorConnectorsIntegrationTest/testOneWayReplicationWithAutorOffsetSync1/]
 * [https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3480/testReport/junit/org.apache.kafka.connect.mirror/MirrorConnectorsIntegrationTest/testOneWayReplicationWithAutorOffsetSync1/]","23/Jul/20 17:29;mjsax;And one more: [https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/1645/]","23/Jul/20 17:31;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3497/testReport/junit/org.apache.kafka.connect.mirror/MirrorConnectorsIntegrationTest/testOneWayReplicationWithAutorOffsetSync1/]","25/Jul/20 20:58;mjsax;[https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/1680/console]","30/Jul/20 07:53;cadonna;https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3608

{code:java}
java.lang.AssertionError: consumer record size is not zero expected:<0> but was:<4>
{code}
",,,,,,,,,,,,,
Flaky test kafka.api.SaslSslAdminIntegrationTest.testCreateTopicsResponseMetadataAndConfig,KAFKA-8967,13259961,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,chia7712,enether,enether,01/Oct/19 16:38,25/Jul/20 16:18,12/Jan/21 11:54,,,,,,,,,,,core,security,unit tests,,0,flaky-test,,,"{code:java}
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)
	at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)
	at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)
	at kafka.api.SaslSslAdminClientIntegrationTest.testCreateTopicsResponseMetadataAndConfig(SaslSslAdminClientIntegrationTest.scala:452)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:288)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:282)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.{code}
Failed in [https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/25374]",,ableegoldman,chia7712,enether,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-10-08 02:24:20.838,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 19 13:53:11 UTC 2020,,,,,,,"0|z077ds:",9223372036854775807,,,,,,,,,,,,,,,,"08/Oct/19 02:24;mjsax;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.12/8378/consoleFull]","10/Oct/19 20:35;mjsax;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.12/8485/consoleFull]","14/Oct/19 22:33;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/25807/testReport/kafka.api/SaslSslAdminClientIntegrationTest/testCreateTopicsResponseMetadataAndConfig/]","15/Oct/19 04:47;mjsax;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/2626/testReport/junit/kafka.api/SaslSslAdminClientIntegrationTest/testCreateTopicsResponseMetadataAndConfig/]","16/Oct/19 07:09;mjsax;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/2659/testReport/junit/kafka.api/SaslSslAdminClientIntegrationTest/testCreateTopicsResponseMetadataAndConfig/]","17/Oct/19 04:04;ableegoldman;h3. Error Message

java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.","29/Jan/20 06:18;mjsax;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/4393/testReport/junit/kafka.api/SaslSslAdminIntegrationTest/testCreateTopicsResponseMetadataAndConfig/]
{quote}org.scalatest.exceptions.TestFailedException: Expected CompletableFuture.get to return an exception at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530) at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529) at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1389) at org.scalatest.Assertions.fail(Assertions.scala:1091) at org.scalatest.Assertions.fail$(Assertions.scala:1087) at org.scalatest.Assertions$.fail(Assertions.scala:1389) at kafka.utils.TestUtils$.assertFutureExceptionTypeEquals(TestUtils.scala:1610) at kafka.api.SaslSslAdminIntegrationTest.validateMetadataAndConfigs$1(SaslSslAdminIntegrationTest.scala:418) at kafka.api.SaslSslAdminIntegrationTest.testCreateTopicsResponseMetadataAndConfig(SaslSslAdminIntegrationTest.scala:422){quote}","17/Feb/20 14:31;chia7712;SaslSslAdminClientIntegrationTest was renamed to SaslSslAdminIntegrationTest (see [this|https://github.com/apache/kafka/commit/400185421f008662ee6f92298154151493486c1e]) so I updated the title.","19/Feb/20 13:53;githubbot;chia7712 commented on pull request #8137: KAFKA-8967 Flaky test kafka.api.SaslSslAdminIntegrationTest.testCreat…
URL: https://github.com/apache/kafka/pull/8137
 
 
   The other brokers sync ACLs from zk notification so the sync may be slower than the Assert. The fix is to wait all brokers to sync the ACLs. 
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,
fix flaky StreamsOptimizedTest,KAFKA-10191,13312784,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,chia7712,chia7712,chia7712,22/Jun/20 07:56,22/Jul/20 20:51,12/Jan/21 11:54,07/Jul/20 18:20,,,,,2.7.0,,,,,streams,unit tests,,,0,,,,"{quote}Exception in thread ""StreamsOptimizedTest-53c7d3b1-12b2-4d02-90b1-15757dfd2735-StreamThread-1"" java.lang.IllegalStateException: Tried to lookup lag for unknown task 2_0Exception in thread ""StreamsOptimizedTest-53c7d3b1-12b2-4d02-90b1-15757dfd2735-StreamThread-1"" java.lang.IllegalStateException: Tried to lookup lag for unknown task 2_0 at org.apache.kafka.streams.processor.internals.assignment.ClientState.lagFor(ClientState.java:306) at java.util.Comparator.lambda$comparingLong$6043328a$1(Comparator.java:511) at java.util.Comparator.lambda$thenComparing$36697e65$1(Comparator.java:216) at java.util.TreeMap.put(TreeMap.java:552) at java.util.TreeSet.add(TreeSet.java:255) at java.util.AbstractCollection.addAll(AbstractCollection.java:344) at java.util.TreeSet.addAll(TreeSet.java:312) at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.getPreviousTasksByLag(StreamsPartitionAssignor.java:1250) at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.assignTasksToThreads(StreamsPartitionAssignor.java:1164) at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.computeNewAssignment(StreamsPartitionAssignor.java:920) at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.assign(StreamsPartitionAssignor.java:391) at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.performAssignment(ConsumerCoordinator.java:583) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.onJoinLeader(AbstractCoordinator.java:689) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.access$1400(AbstractCoordinator.java:111) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:602) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:575) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1132) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1107) at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:206) at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:169) at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:129) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:602) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:412) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:297) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:215) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:419) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:359) at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:506) at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1263) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1229) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1204) at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:762) at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:622) at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:549) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:508)
{quote}
 

this issue may be related to [https://github.com/apache/kafka/commit/0f68dc7a640b26a8edea154ea4ea2b6d93b5104b] since the test passes If  the commit is reverted",,ableegoldman,chia7712,junrao,mhmdchebbi,mjsax,,,,,,,,,,,,,,,,,,,,KAFKA-10194,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-06-22 16:12:46.426,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 11:25:25 UTC 2020,,,,,,,"0|z0g2wg:",9223372036854775807,,,,,,,,,,,,,,,,"22/Jun/20 16:12;ableegoldman;This looks like it could be an actual bug cc [~vvcephei] might be another blocker?","22/Jun/20 17:23;ableegoldman;Hey [~chia7712], do you have the logs or a link to this failure?","22/Jun/20 17:33;ableegoldman;I think this could be caused by a client having a task in its prevTasks set that is not also in its taskLagTotals, ie the task is no longer among the current stateful tasks for this rebalance.

How we could _lose_ a task, I'm not sure, but I don't think we should crash over it.","22/Jun/20 17:43;junrao;This test failure occurred [http://testing.confluent.io/confluent-kafka-branch-builder-system-test-results/?prefix=2020-06-19--001.1592614513--chia7712--fix_8334_avoid_deadlock--142a6c4c0/] when testing [https://github.com/apache/kafka/pull/8657]. But the failure doesn't seem to be related to that PR since it fails locally too.","22/Jun/20 19:22;ableegoldman;Awesome, thanks Jun.

I think the hypothesis above fits. We actually are ""losing"" a task in this system test – we start up with no optimizations, then restart with optimizations on. The optimization does some shuffling of repartition topics to reduce the total number of repartitions and we end up with fewer tasks. So the ClientState will have tasks in its previousTasksForConsumer map that aren't in taskLagTotals. We need to filter these tasks out.

 

Did you want to submit a fix for this [~chia7712]?","23/Jun/20 02:15;chia7712; 
{quote}Did you want to submit a fix for this Chia-Ping Tsai?
{quote}
sure :)","23/Jun/20 02:52;ableegoldman;Awesome!

That said, I want to qualify my earlier claims: after thinking about this a bit more, I would actually say that this is not a real bug but just a bug in the test.

It seems like the system test was testing an illegal upgrade to begin with. Optimized and non-optimized topologies are not compatible in general, and in this specific situation the problem is that changing tasks/numbering will cause a change in the mapping of task to TaskId. You would actually need to reset your application in this case before you can safely turn optimizations on.

So, that's exactly what we should do in this test. We need to 
 # call KafkaStreams#cleanup before starting the application up the second time (ie when we turn optimizations on)
 # run the reset tool between stopping the original application and starting the new one

I think that 1. alone would technically be enough to stop this test from failing, but we should really be doing both. [~chia7712] does that make sense? ","23/Jun/20 04:23;chia7712;sorry for delayed response

{quote}
call KafkaStreams#cleanup before starting the application up the second time (ie when we turn optimizations on)
{quote}

this makes sense as it is what users should do in production.

{quote}
run the reset tool between stopping the original application and starting the new one
{quote}

Could we address it in another issue so as to make this fix simpler.






","23/Jun/20 06:43;chia7712;{quote}
run the reset tool between stopping the original application and starting the new one
{quote}

I have filed KAFKA-10194 to address this comment.","23/Jun/20 16:49;ableegoldman;Cool. Since this is failing pretty regularly, I agree that we should get the bare minimum fix in quickly and it's fine to follow up with the full ""correct"" fix next","25/Jun/20 23:13;mhmdchebbi;i want to create a PR for KAFKA-10194 , but i dont understant what do you mean by reset tool.","25/Jun/20 23:25;ableegoldman;[~mhmdchebbi] There's something called the ""application reset tool"" for Streams applications. It just resets some of the internal metadata and state for the application and its consumer group. You can read up on it here: [https://kafka.apache.org/25/documentation/streams/developer-guide/app-reset-tool.html]

Let me know if you have any questions about it or how to use it","30/Jun/20 02:58;chia7712;[~mhmdchebbi] Have you completed the PR for reset tool?","30/Jun/20 07:58;chia7712;[~mhmdchebbi] I have addressed [~ableegoldman] comment in the same PR. It would be better to get your reviews :)","30/Jun/20 09:38;mhmdchebbi;[~chia7712] sorry to be late, i read the reset tool documentation and if i understand it's about adding KafkaStreams#cleanUp() in the code, the other thing to do are done manually out of the code.","30/Jun/20 11:25;chia7712;{quote}
about adding KafkaStreams#cleanUp() in the code,
{quote}

yep, this is initial fix to this issue.

{quote}
the other thing to do are done manually out of the code.
{quote}

Yep, resetting stream application should be manually executed. However, as [~ableegoldman] suggested, we can do more for this fix so streams_optimized_test.py, now, always reset the stream application before running optimization.",,,,,,
Flaky Test kafka.integration.MetricsDuringTopicCreationDeletionTest.testMetricsDuringTopicCreateDelete,KAFKA-9009,13261377,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,bbejeck,bbejeck,09/Oct/19 16:35,01/Jul/20 17:02,12/Jan/21 11:54,,2.5.0,2.6.0,,,,,,,,core,,,,0,flaky-test,,,"Failure seen in [https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/25644/testReport/junit/kafka.integration/MetricsDuringTopicCreationDeletionTest/testMetricsDuringTopicCreateDelete/]

 
{noformat}
Error Messagejava.lang.AssertionError: assertion failed: UnderReplicatedPartitionCount not 0: 1Stacktracejava.lang.AssertionError: assertion failed: UnderReplicatedPartitionCount not 0: 1
	at scala.Predef$.assert(Predef.scala:170)
	at kafka.integration.MetricsDuringTopicCreationDeletionTest.testMetricsDuringTopicCreateDelete(MetricsDuringTopicCreationDeletionTest.scala:123)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:305)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:365)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:330)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:78)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:328)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:65)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:292)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:305)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:412)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.lang.Thread.run(Thread.java:748)
Standard Output[2019-10-09 02:27:41,128] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic0-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 02:27:41,144] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition topic0-2 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 02:27:41,142] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition topic0-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 02:27:41,128] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic0-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 02:27:41,144] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition topic0-2 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 02:27:41,168] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition topic0-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 02:27:41,618] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition topic1-2 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 02:27:41,619] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition topic1-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 02:27:41,619] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic1-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 02:27:41,619] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 02:27:41,618] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition topic1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 02:27:41,618] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition topic1-2 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.{noformat}",,bbejeck,bchen225242,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-07-01 17:02:31.626,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 01 17:02:31 UTC 2020,,,,,,,"0|z07fps:",9223372036854775807,,,,,,,,,,,,,,,,"01/Jul/20 17:02;bchen225242;Failed again:[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3200/testReport/junit/kafka.integration/MetricsDuringTopicCreationDeletionTest/testMetricsDuringTopicCreateDelete/]


java.lang.AssertionError: assertion failed: UnderReplicatedPartitionCount not 0: 1 at kafka.integration.MetricsDuringTopicCreationDeletionTest.testMetricsDuringTopicCreateDelete(MetricsDuringTopicCreationDeletionTest.scala:122) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38) at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62) at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51) at sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33) at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94) at com.sun.proxy.$Proxy2.processTestClass(Unknown Source) at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:119) at sun.reflect.GeneratedMethodAccessor41.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164) at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:414) at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64) at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56) at java.lang.Thread.run(Thread.java:748)",,,,,,,,,,,,,,,,,,,,,
Flaky Test OptimizedKTableIntegrationTest#shouldApplyUpdatesToStandbyStore,KAFKA-9974,13303696,Test,Reopened,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,guozhang,mjsax,mjsax,08/May/20 17:34,30/Jun/20 03:28,12/Jan/21 11:54,,,,,,,,,,,streams,unit tests,,,0,flaky-test,,,"[https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/321/testReport/junit/org.apache.kafka.streams.integration/OptimizedKTableIntegrationTest/shouldApplyUpdatesToStandbyStore/]
{quote}java.lang.AssertionError: Expected: is <0L> but: was <43L> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6) at org.apache.kafka.streams.integration.OptimizedKTableIntegrationTest.shouldApplyUpdatesToStandbyStore(OptimizedKTableIntegrationTest.java:138){quote}",,ableegoldman,mjsax,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-05-11 08:55:38.624,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 19 03:27:33 UTC 2020,,,,,,,"0|z0eiy0:",9223372036854775807,,mjsax,,,,,,,,,,,,,,"11/May/20 08:55;showuon;PR: [https://github.com/apache/kafka/pull/8646]

 ","03/Jun/20 03:26;ableegoldman;Failed again at a different point:
h3. Stacktrace

java.lang.AssertionError: Expected: is <true> but: was <false> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6) at org.apache.kafka.streams.integration.OptimizedKTableIntegrationTest.shouldApplyUpdatesToStandbyStore(OptimizedKTableIntegrationTest.java:159)","17/Jun/20 16:27;ableegoldman;Unfortunately this just failed again, at a different place:
h3. Stacktrace

java.lang.AssertionError: Expected: is <true>

but: was <false>

at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)

at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)

at org.apache.kafka.streams.integration.OptimizedKTableIntegrationTest.shouldApplyUpdatesToStandbyStore(OptimizedKTableIntegrationTest.java:150)","19/Jun/20 03:27;showuon;I don't have thought on fixing the errors. See if anyone wants to investigate it. Thanks.",,,,,,,,,,,,,,,,,,
Fix flaky test MirrorConnectorsIntegrationTest.testReplication,KAFKA-9509,13283579,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,skaundinya,skaundinya,05/Feb/20 21:33,30/Jun/20 03:15,12/Jan/21 11:54,30/Jun/20 03:15,2.4.0,,,,2.5.0,,,,,mirrormaker,,,,0,,,,"The test org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testReplication is a flaky test for MirrorMaker 2.0. Its flakiness lies in the timing of when the connectors and tasks are started up. The fix for this would make it such that when the connectors are started up, to wait until the REST endpoint returns a positive number of tasks to be confident that we can start testing.",,ableegoldman,bchen225242,githubbot,lthomas,mjsax,showuon,skaundinya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-02-05 21:59:29.349,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 03:15:04 UTC 2020,,,,,,,"0|z0b788:",9223372036854775807,,,,,,,,,,,,,,,,"05/Feb/20 21:59;githubbot;skaundinya15 commented on pull request #8048: KAFKA-9509: Fixing flakiness of MirrorConnectorsIntegrationTest.testReplication
URL: https://github.com/apache/kafka/pull/8048
 
 
   JIRA: https://issues.apache.org/jira/browse/KAFKA-9509
   
   As the JIRA indicates, `org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testReplication` has shown to be an increasingly flaky test recently. This PR aims to make this test more deterministic. Specifically, the flakiness was due to a timing issue between the tasks not starting up in time for the test to start running. This PR remediates that by introducing a status check after every connector is started up. These status checks include that the connector is found on the connect cluster as well as there are tasks created and up and running for that connector. These checks are introduced before the test starts running so that there is a confidence that the connectors and tasks are started up correctly before the test runs.
   
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behavior change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","07/Feb/20 21:58;githubbot;hachikuji commented on pull request #8048: KAFKA-9509: Fixing flakiness of MirrorConnectorsIntegrationTest.testReplication
URL: https://github.com/apache/kafka/pull/8048
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","17/Jun/20 17:57;ableegoldman;Saw this fail again on a PR build:
h3. Stacktrace

java.lang.RuntimeException: Could not find enough records. found 0, expected 100 at org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster.consume(EmbeddedKafkaCluster.java:435) at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testReplication(MirrorConnectorsIntegrationTest.java:222)

 

[https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/1130/testReport/junit/org.apache.kafka.connect.mirror/MirrorConnectorsIntegrationTest/testReplication/]","18/Jun/20 08:36;showuon;[https://builds.apache.org/blue/rest/organizations/jenkins/pipelines/kafka-trunk-jdk8/runs/4650/log/?start=0]
[https://builds.apache.org/blue/rest/organizations/jenkins/pipelines/kafka-trunk-jdk11/runs/1578/log/?start=0]
[https://builds.apache.org/blue/rest/organizations/jenkins/pipelines/kafka-trunk-jdk14/runs/226/log/?start=0]
[https://builds.apache.org/blue/rest/organizations/jenkins/pipelines/kafka-trunk-jdk11/runs/1579/log/?start=0]

 

org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest > testReplication FAILED
 java.lang.RuntimeException: Could not find enough records. found 0, expected 100
 at org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster.consume(EmbeddedKafkaCluster.java:435)
 at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testReplication(MirrorConnectorsIntegrationTest.java:222)

[https://builds.apache.org/blue/rest/organizations/jenkins/pipelines/kafka-trunk-jdk8/runs/4651/log/?start=0]


org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest > testReplication FAILED
 java.lang.RuntimeException: Could not find enough records. found 0, expected 100
 at org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster.consume(EmbeddedKafkaCluster.java:435)
 at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testReplication(MirrorConnectorsIntegrationTest.java:218)","18/Jun/20 08:36;showuon;I take it over since it failed quite often recently, and also failed my PR testing!! :<

 ","19/Jun/20 04:28;mjsax;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/7061/testReport/junit/org.apache.kafka.connect.mirror/MirrorConnectorsIntegrationTest/testReplication/]
{quote}ava.lang.AssertionError: Connector MirrorCheckpointConnector tasks did not start in time on cluster: org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster@5376246b at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.assertConnectorAndAtLeastNumTasksAreRunning(EmbeddedConnectClusterAssertions.java:120) at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.waitUntilMirrorMakerIsRunning(MirrorConnectorsIntegrationTest.java:191) at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:184){quote}","20/Jun/20 01:37;showuon;PR: [https://github.com/apache/kafka/pull/8894]

 ","23/Jun/20 03:37;showuon;[https://builds.apache.org/blue/rest/organizations/jenkins/pipelines/kafka-trunk-jdk8/runs/4661/log/?start=0]

[https://builds.apache.org/blue/rest/organizations/jenkins/pipelines/kafka-trunk-jdk14/runs/238/log/?start=0]

org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest > testReplication FAILED
 java.lang.RuntimeException: Could not find enough records. found 0, expected 100
 at org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster.consume(EmbeddedKafkaCluster.java:435)
 at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testReplication(MirrorConnectorsIntegrationTest.java:222)","23/Jun/20 21:58;bchen225242;Failed again:
h3. Stacktrace

java.lang.RuntimeException: Could not find enough records. found 0, expected 100 at org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster.consume(EmbeddedKafkaCluster.java:435) at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testReplication(MirrorConnectorsIntegrationTest.java:222) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)

 

[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/7083/testReport/junit/org.apache.kafka.connect.mirror/MirrorConnectorsIntegrationTest/testReplication/]","26/Jun/20 20:20;lthomas;Failed again:
h3. Stacktrace

java.lang.RuntimeException: Could not find enough records. found 0, expected 100 at org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster.consume(EmbeddedKafkaCluster.java:435) at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testReplication(MirrorConnectorsIntegrationTest.java:221) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564)

 

[https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/1292/testReport/junit/org.apache.kafka.connect.mirror/MirrorConnectorsIntegrationTest/testReplication/]

 ","30/Jun/20 03:15;showuon;Fixed by increasing timeout when consuming records to fix flaky test in MM2. Thanks.",,,,,,,,,,,
"Enable TLSv.1.3 in system tests ""run all"" execution.",KAFKA-9943,13302152,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nizhikov,nizhikov,nizhikov,30/Apr/20 16:47,25/Jun/20 08:25,12/Jan/21 11:54,25/Jun/20 08:25,,,,,2.7.0,,,,,,,,,0,,,,"We need to enable system tests with the TLSv1.3 in ""run all"" execution.",,nizhikov,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-06-24 20:47:07.699,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 25 08:25:28 UTC 2020,,,,,,,"0|z0e9q8:",9223372036854775807,,,,,,,,,,,,,,,,"24/Jun/20 20:47;rhauch;Since this is not a blocker issue, as part of the 2.6.0 release process I'm changing the fix version to `2.7.0`. If this is incorrect, please respond and discuss on the ""[DISCUSS] Apache Kafka 2.6.0 release"" discussion mailing list thread.","25/Jun/20 08:25;nizhikov;It seems to me that we cover this ticket with the KAFKA-9320.

Closing for now.",,,,,,,,,,,,,,,,,,,,
Flaky Test org.apache.kafka.streams.integration.KTableSourceTopicRestartIntegrationTest.shouldRestoreAndProgressWhenTopicWrittenToDuringRestorationWithEosEnabled,KAFKA-9182,13268086,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,bbejeck,bbejeck,13/Nov/19 16:51,10/Jun/20 21:10,12/Jan/21 11:54,10/Jun/20 21:10,,,,,2.6.0,,,,,streams,,,,0,flaky-test,tests,,"Failed in [https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/26571/testReport/junit/org.apache.kafka.streams.integration/KTableSourceTopicRestartIntegrationTest/shouldRestoreAndProgressWhenTopicWrittenToDuringRestorationWithEosEnabled/]

 
{noformat}
Error Messagejava.lang.AssertionError: Condition not met within timeout 30000. Table did not read all valuesStacktracejava.lang.AssertionError: Condition not met within timeout 30000. Table did not read all values
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:24)
	at org.apache.kafka.test.TestUtils.lambda$waitForCondition$4(TestUtils.java:369)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:417)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:385)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:368)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:356)
	at org.apache.kafka.streams.integration.KTableSourceTopicRestartIntegrationTest.assertNumberValuesRead(KTableSourceTopicRestartIntegrationTest.java:187)
	at org.apache.kafka.streams.integration.KTableSourceTopicRestartIntegrationTest.shouldRestoreAndProgressWhenTopicWrittenToDuringRestorationWithEosEnabled(KTableSourceTopicRestartIntegrationTest.java:141)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:305)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:365)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:330)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:78)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:328)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:65)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:292)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:305)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:412)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.lang.Thread.run(Thread.java:748)
Standard Output[2019-11-13 06:04:49,595] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog:103)
[2019-11-13 06:04:49,595] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase:117)
[2019-11-13 06:04:49,596] INFO minSessionTimeout set to 1600 (org.apache.zookeeper.server.ZooKeeperServer:938)
[2019-11-13 06:04:49,596] INFO maxSessionTimeout set to 16000 (org.apache.zookeeper.server.ZooKeeperServer:947)
[2019-11-13 06:04:49,596] INFO Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 datadir /tmp/kafka-8546929362712488246/version-2 snapdir /tmp/kafka-5999627801426978574/version-2 (org.apache.zookeeper.server.ZooKeeperServer:166)
[2019-11-13 06:04:49,596] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 3 selector thread(s), 48 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory:673)
[2019-11-13 06:04:49,597] INFO binding to port /127.0.0.1:0 (org.apache.zookeeper.server.NIOServerCnxnFactory:686)
[2019-11-13 06:04:49,598] INFO Snapshotting: 0x0 to /tmp/kafka-5999627801426978574/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog:384)
[2019-11-13 06:04:49,598] INFO Snapshotting: 0x0 to /tmp/kafka-5999627801426978574/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog:384)
[2019-11-13 06:04:49,600] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit6760496041183144599/junit6110490013242643385
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:34936
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:347)
[2019-11-13 06:04:49,602] INFO starting (kafka.server.KafkaServer:66)
[2019-11-13 06:04:49,602] INFO Connecting to zookeeper on 127.0.0.1:34936 (kafka.server.KafkaServer:66)
[2019-11-13 06:04:49,603] INFO [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:34936. (kafka.zookeeper.ZooKeeperClient:66)
[2019-11-13 06:04:49,603] INFO Initiating client connection, connectString=127.0.0.1:34936 sessionTimeout=10000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@6a99449d (org.apache.zookeeper.ZooKeeper:868)
[2019-11-13 06:04:49,603] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket:237)
[2019-11-13 06:04:49,604] INFO zookeeper.request.timeout value is 0. feature enabled= (org.apache.zookeeper.ClientCnxn:1653)
[2019-11-13 06:04:49,604] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient:66)
[2019-11-13 06:04:49,604] INFO Opening socket connection to server localhost/127.0.0.1:34936. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn:1112)
[2019-11-13 06:04:49,605] INFO Socket connection established, initiating session, client: /127.0.0.1:46826, server: localhost/127.0.0.1:34936 (org.apache.zookeeper.ClientCnxn:959)
[2019-11-13 06:04:49,606] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog:216)
[2019-11-13 06:04:49,607] INFO Session establishment complete on server localhost/127.0.0.1:34936, sessionid = 0x100df7ef30e0000, negotiated timeout = 10000 (org.apache.zookeeper.ClientCnxn:1394)
[2019-11-13 06:04:49,607] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient:66)
[2019-11-13 06:04:49,634] INFO Cluster ID = kot2gqC3SryRv2wZkPzlAA (kafka.server.KafkaServer:66)
[2019-11-13 06:04:49,635] WARN No meta.properties file under dir /tmp/junit6760496041183144599/junit6110490013242643385/meta.properties (kafka.server.BrokerMetadataCheckpoint:70)
[2019-11-13 06:04:49,639] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit6760496041183144599/junit6110490013242643385
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:34936
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:347)
[2019-11-13 06:04:49,642] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit6760496041183144599/junit6110490013242643385
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:34936
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:347)
[2019-11-13 06:04:49,645] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-11-13 06:04:49,646] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-11-13 06:04:49,646] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-11-13 06:04:49,648] INFO Loading logs. (kafka.log.LogManager:66)
[2019-11-13 06:04:49,649] INFO Logs loading complete in 0 ms. (kafka.log.LogManager:66)
[2019-11-13 06:04:49,649] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager:66)
[2019-11-13 06:04:49,650] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager:66)
[2019-11-13 06:04:49,650] INFO Starting the log cleaner (kafka.log.LogCleaner:66)
[2019-11-13 06:04:49,652] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner:66)
[2019-11-13 06:04:49,677] INFO Awaiting socket connections on localhost:37288. (kafka.network.Acceptor:66)
[2019-11-13 06:04:49,681] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(localhost,0,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer:66)
[2019-11-13 06:04:49,681] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer:66)
[2019-11-13 06:04:49,682] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:04:49,682] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:04:49,683] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:04:49,683] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:04:49,685] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2019-11-13 06:04:49,687] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient:66)
[2019-11-13 06:04:49,688] INFO Stat of the created znode at /brokers/ids/0 is: 24,24,1573625089687,1573625089687,1,0,0,72303330374582272,190,0,24
 (kafka.zk.KafkaZkClient:66)
[2019-11-13 06:04:49,689] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(localhost,37288,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 24 (kafka.zk.KafkaZkClient:66)
[2019-11-13 06:04:49,831] INFO [ControllerEventThread controllerId=0] Starting (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2019-11-13 06:04:49,831] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:04:49,832] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:04:49,832] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:04:49,833] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator:66)
[2019-11-13 06:04:49,834] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator:66)
[2019-11-13 06:04:49,834] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager:66)
[2019-11-13 06:04:49,834] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient:66)
[2019-11-13 06:04:49,836] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager:66)
[2019-11-13 06:04:49,836] INFO [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,836] INFO [Controller id=0] Registering handlers (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,837] INFO [Controller id=0] Deleting log dir event notifications (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,838] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2019-11-13 06:04:49,838] INFO [Controller id=0] Deleting isr change notifications (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,838] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2019-11-13 06:04:49,839] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2019-11-13 06:04:49,839] INFO [Controller id=0] Initializing controller context (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,840] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:04:49,841] INFO [Controller id=0] Initialized broker epochs cache: Map(0 -> 24) (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,842] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2019-11-13 06:04:49,845] INFO [Controller id=0] Currently active brokers in the cluster: Set(0) (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,845] INFO [RequestSendThread controllerId=0] Starting (kafka.controller.RequestSendThread:66)
[2019-11-13 06:04:49,845] INFO [Controller id=0] Currently shutting brokers in the cluster: Set() (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,846] INFO [Controller id=0] Current list of topics in the cluster: Set() (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,846] INFO [Controller id=0] Fetching topic deletions in progress (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,846] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer:66)
[2019-11-13 06:04:49,846] INFO Kafka version: 2.5.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser:117)
[2019-11-13 06:04:49,846] INFO Kafka commitId: 6d664f6df7f721ae (org.apache.kafka.common.utils.AppInfoParser:118)
[2019-11-13 06:04:49,846] INFO [Controller id=0] List of topics to be deleted:  (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,847] INFO Kafka startTimeMs: 1573625089594 (org.apache.kafka.common.utils.AppInfoParser:119)
[2019-11-13 06:04:49,847] INFO [Controller id=0] List of topics ineligible for deletion:  (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,847] INFO [KafkaServer id=0] started (kafka.server.KafkaServer:66)
[2019-11-13 06:04:49,847] INFO [Controller id=0] Initializing topic deletion manager (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,847] INFO [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: Set() (kafka.controller.TopicDeletionManager:66)
[2019-11-13 06:04:49,847] INFO [Controller id=0] Sending update metadata request (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,848] INFO [ReplicaStateMachine controllerId=0] Initializing replica state (kafka.controller.ZkReplicaStateMachine:66)
[2019-11-13 06:04:49,848] INFO [ReplicaStateMachine controllerId=0] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2019-11-13 06:04:49,848] INFO [ReplicaStateMachine controllerId=0] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2019-11-13 06:04:49,848] INFO [PartitionStateMachine controllerId=0] Initializing partition state (kafka.controller.ZkPartitionStateMachine:66)
[2019-11-13 06:04:49,848] INFO [PartitionStateMachine controllerId=0] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine:66)
[2019-11-13 06:04:49,848] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit5414548266621944218/junit92696306455778432
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:34936
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:347)
[2019-11-13 06:04:49,849] INFO [RequestSendThread controllerId=0] Controller 0 connected to localhost:37288 (id: 0 rack: null) for sending state change requests (kafka.controller.RequestSendThread:66)
[2019-11-13 06:04:49,849] INFO [Controller id=0] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,851] INFO starting (kafka.server.KafkaServer:66)
[2019-11-13 06:04:49,851] INFO Connecting to zookeeper on 127.0.0.1:34936 (kafka.server.KafkaServer:66)
[2019-11-13 06:04:49,851] INFO [Controller id=0] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,851] INFO [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:34936. (kafka.zookeeper.ZooKeeperClient:66)
[2019-11-13 06:04:49,851] INFO [Controller id=0] Partitions that completed preferred replica election:  (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,851] INFO Initiating client connection, connectString=127.0.0.1:34936 sessionTimeout=10000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@52276192 (org.apache.zookeeper.ZooKeeper:868)
[2019-11-13 06:04:49,851] INFO [Controller id=0] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,852] INFO [Controller id=0] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,852] INFO [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,852] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket:237)
[2019-11-13 06:04:49,852] INFO zookeeper.request.timeout value is 0. feature enabled= (org.apache.zookeeper.ClientCnxn:1653)
[2019-11-13 06:04:49,853] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient:66)
[2019-11-13 06:04:49,853] INFO [Controller id=0] Starting the controller scheduler (kafka.controller.KafkaController:66)
[2019-11-13 06:04:49,853] INFO Opening socket connection to server localhost/127.0.0.1:34936. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn:1112)
[2019-11-13 06:04:49,854] INFO Socket connection established, initiating session, client: /127.0.0.1:46830, server: localhost/127.0.0.1:34936 (org.apache.zookeeper.ClientCnxn:959)
[2019-11-13 06:04:49,855] INFO Session establishment complete on server localhost/127.0.0.1:34936, sessionid = 0x100df7ef30e0001, negotiated timeout = 10000 (org.apache.zookeeper.ClientCnxn:1394)
[2019-11-13 06:04:49,855] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient:66)
[2019-11-13 06:04:49,861] INFO Cluster ID = kot2gqC3SryRv2wZkPzlAA (kafka.server.KafkaServer:66)
[2019-11-13 06:04:49,862] WARN No meta.properties file under dir /tmp/junit5414548266621944218/junit92696306455778432/meta.properties (kafka.server.BrokerMetadataCheckpoint:70)
[2019-11-13 06:04:49,864] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit5414548266621944218/junit92696306455778432
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	tran
...[truncated 1188894 chars]...
a.util.Try$.apply(Try.scala:192)
	at kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1163)
	at kafka.controller.KafkaController.process(KafkaController.scala:1868)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:53)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply$mcV$sp(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:136)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2019-11-13 06:06:14,057] ERROR [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __transaction_state-18 from OnlinePartition to OnlinePartition (state.change.logger:76)
kafka.common.StateChangeFailedException: Failed to elect leader for partition __transaction_state-18 under strategy ControlledShutdownPartitionLeaderElectionStrategy
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:427)
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:424)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:424)
	at kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:335)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:233)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:154)
	at kafka.controller.KafkaController.kafka$controller$KafkaController$$doControlledShutdown(KafkaController.scala:1201)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at scala.util.Try$.apply(Try.scala:192)
	at kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1163)
	at kafka.controller.KafkaController.process(KafkaController.scala:1868)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:53)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply$mcV$sp(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:136)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2019-11-13 06:06:14,058] ERROR [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __transaction_state-26 from OnlinePartition to OnlinePartition (state.change.logger:76)
kafka.common.StateChangeFailedException: Failed to elect leader for partition __transaction_state-26 under strategy ControlledShutdownPartitionLeaderElectionStrategy
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:427)
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:424)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:424)
	at kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:335)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:233)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:154)
	at kafka.controller.KafkaController.kafka$controller$KafkaController$$doControlledShutdown(KafkaController.scala:1201)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at scala.util.Try$.apply(Try.scala:192)
	at kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1163)
	at kafka.controller.KafkaController.process(KafkaController.scala:1868)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:53)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply$mcV$sp(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:136)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2019-11-13 06:06:14,058] ERROR [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __transaction_state-36 from OnlinePartition to OnlinePartition (state.change.logger:76)
kafka.common.StateChangeFailedException: Failed to elect leader for partition __transaction_state-36 under strategy ControlledShutdownPartitionLeaderElectionStrategy
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:427)
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:424)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:424)
	at kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:335)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:233)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:154)
	at kafka.controller.KafkaController.kafka$controller$KafkaController$$doControlledShutdown(KafkaController.scala:1201)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at scala.util.Try$.apply(Try.scala:192)
	at kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1163)
	at kafka.controller.KafkaController.process(KafkaController.scala:1868)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:53)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply$mcV$sp(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:136)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2019-11-13 06:06:14,058] ERROR [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __transaction_state-5 from OnlinePartition to OnlinePartition (state.change.logger:76)
kafka.common.StateChangeFailedException: Failed to elect leader for partition __transaction_state-5 under strategy ControlledShutdownPartitionLeaderElectionStrategy
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:427)
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:424)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:424)
	at kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:335)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:233)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:154)
	at kafka.controller.KafkaController.kafka$controller$KafkaController$$doControlledShutdown(KafkaController.scala:1201)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at scala.util.Try$.apply(Try.scala:192)
	at kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1163)
	at kafka.controller.KafkaController.process(KafkaController.scala:1868)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:53)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply$mcV$sp(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:136)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2019-11-13 06:06:14,059] ERROR [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __transaction_state-8 from OnlinePartition to OnlinePartition (state.change.logger:76)
kafka.common.StateChangeFailedException: Failed to elect leader for partition __transaction_state-8 under strategy ControlledShutdownPartitionLeaderElectionStrategy
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:427)
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:424)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:424)
	at kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:335)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:233)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:154)
	at kafka.controller.KafkaController.kafka$controller$KafkaController$$doControlledShutdown(KafkaController.scala:1201)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at scala.util.Try$.apply(Try.scala:192)
	at kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1163)
	at kafka.controller.KafkaController.process(KafkaController.scala:1868)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:53)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply$mcV$sp(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:136)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2019-11-13 06:06:14,059] ERROR [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __transaction_state-16 from OnlinePartition to OnlinePartition (state.change.logger:76)
kafka.common.StateChangeFailedException: Failed to elect leader for partition __transaction_state-16 under strategy ControlledShutdownPartitionLeaderElectionStrategy
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:427)
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:424)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:424)
	at kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:335)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:233)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:154)
	at kafka.controller.KafkaController.kafka$controller$KafkaController$$doControlledShutdown(KafkaController.scala:1201)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at scala.util.Try$.apply(Try.scala:192)
	at kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1163)
	at kafka.controller.KafkaController.process(KafkaController.scala:1868)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:53)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply$mcV$sp(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:136)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2019-11-13 06:06:14,059] ERROR [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __transaction_state-11 from OnlinePartition to OnlinePartition (state.change.logger:76)
kafka.common.StateChangeFailedException: Failed to elect leader for partition __transaction_state-11 under strategy ControlledShutdownPartitionLeaderElectionStrategy
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:427)
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:424)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:424)
	at kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:335)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:233)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:154)
	at kafka.controller.KafkaController.kafka$controller$KafkaController$$doControlledShutdown(KafkaController.scala:1201)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at scala.util.Try$.apply(Try.scala:192)
	at kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1163)
	at kafka.controller.KafkaController.process(KafkaController.scala:1868)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:53)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply$mcV$sp(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:136)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2019-11-13 06:06:14,060] ERROR [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __transaction_state-40 from OnlinePartition to OnlinePartition (state.change.logger:76)
kafka.common.StateChangeFailedException: Failed to elect leader for partition __transaction_state-40 under strategy ControlledShutdownPartitionLeaderElectionStrategy
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:427)
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:424)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:424)
	at kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:335)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:233)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:154)
	at kafka.controller.KafkaController.kafka$controller$KafkaController$$doControlledShutdown(KafkaController.scala:1201)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at scala.util.Try$.apply(Try.scala:192)
	at kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1163)
	at kafka.controller.KafkaController.process(KafkaController.scala:1868)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:53)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply$mcV$sp(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:136)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2019-11-13 06:06:14,060] ERROR [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __transaction_state-19 from OnlinePartition to OnlinePartition (state.change.logger:76)
kafka.common.StateChangeFailedException: Failed to elect leader for partition __transaction_state-19 under strategy ControlledShutdownPartitionLeaderElectionStrategy
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:427)
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:424)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:424)
	at kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:335)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:233)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:154)
	at kafka.controller.KafkaController.kafka$controller$KafkaController$$doControlledShutdown(KafkaController.scala:1201)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at scala.util.Try$.apply(Try.scala:192)
	at kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1163)
	at kafka.controller.KafkaController.process(KafkaController.scala:1868)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:53)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply$mcV$sp(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:136)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2019-11-13 06:06:14,061] ERROR [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __transaction_state-27 from OnlinePartition to OnlinePartition (state.change.logger:76)
kafka.common.StateChangeFailedException: Failed to elect leader for partition __transaction_state-27 under strategy ControlledShutdownPartitionLeaderElectionStrategy
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:427)
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:424)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:424)
	at kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:335)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:233)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:154)
	at kafka.controller.KafkaController.kafka$controller$KafkaController$$doControlledShutdown(KafkaController.scala:1201)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at scala.util.Try$.apply(Try.scala:192)
	at kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1163)
	at kafka.controller.KafkaController.process(KafkaController.scala:1868)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:53)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply$mcV$sp(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:136)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2019-11-13 06:06:14,061] ERROR [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __transaction_state-41 from OnlinePartition to OnlinePartition (state.change.logger:76)
kafka.common.StateChangeFailedException: Failed to elect leader for partition __transaction_state-41 under strategy ControlledShutdownPartitionLeaderElectionStrategy
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:427)
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:424)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:424)
	at kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:335)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:233)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:154)
	at kafka.controller.KafkaController.kafka$controller$KafkaController$$doControlledShutdown(KafkaController.scala:1201)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at scala.util.Try$.apply(Try.scala:192)
	at kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1163)
	at kafka.controller.KafkaController.process(KafkaController.scala:1868)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:53)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply$mcV$sp(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:136)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2019-11-13 06:06:14,062] ERROR [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __transaction_state-1 from OnlinePartition to OnlinePartition (state.change.logger:76)
kafka.common.StateChangeFailedException: Failed to elect leader for partition __transaction_state-1 under strategy ControlledShutdownPartitionLeaderElectionStrategy
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:427)
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:424)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:424)
	at kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:335)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:233)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:154)
	at kafka.controller.KafkaController.kafka$controller$KafkaController$$doControlledShutdown(KafkaController.scala:1201)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at scala.util.Try$.apply(Try.scala:192)
	at kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1163)
	at kafka.controller.KafkaController.process(KafkaController.scala:1868)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:53)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply$mcV$sp(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:136)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2019-11-13 06:06:14,062] ERROR [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __transaction_state-34 from OnlinePartition to OnlinePartition (state.change.logger:76)
kafka.common.StateChangeFailedException: Failed to elect leader for partition __transaction_state-34 under strategy ControlledShutdownPartitionLeaderElectionStrategy
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:427)
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:424)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:424)
	at kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:335)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:233)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:154)
	at kafka.controller.KafkaController.kafka$controller$KafkaController$$doControlledShutdown(KafkaController.scala:1201)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at scala.util.Try$.apply(Try.scala:192)
	at kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1163)
	at kafka.controller.KafkaController.process(KafkaController.scala:1868)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:53)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply$mcV$sp(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:136)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2019-11-13 06:06:14,062] ERROR [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __transaction_state-35 from OnlinePartition to OnlinePartition (state.change.logger:76)
kafka.common.StateChangeFailedException: Failed to elect leader for partition __transaction_state-35 under strategy ControlledShutdownPartitionLeaderElectionStrategy
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:427)
	at kafka.controller.ZkPartitionStateMachine$$anonfun$doElectLeaderForPartitions$2.apply(PartitionStateMachine.scala:424)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:424)
	at kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:335)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:233)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:154)
	at kafka.controller.KafkaController.kafka$controller$KafkaController$$doControlledShutdown(KafkaController.scala:1201)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at kafka.controller.KafkaController$$anonfun$23.apply(KafkaController.scala:1163)
	at scala.util.Try$.apply(Try.scala:192)
	at kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1163)
	at kafka.controller.KafkaController.process(KafkaController.scala:1868)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:53)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply$mcV$sp(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.controller.ControllerEventManager$ControllerEventThread$$anonfun$doWork$1.apply(ControllerEventManager.scala:137)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:136)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2019-11-13 06:06:14,065] INFO [KafkaServer id=2] Remaining partitions to move: [RemainingPartition(topicName='__transaction_state', partitionIndex=42), RemainingPartition(topicName='__transaction_state', partitionIndex=31), RemainingPartition(topicName='__transaction_state', partitionIndex=45), RemainingPartition(topicName='__transaction_state', partitionIndex=15), RemainingPartition(topicName='__transaction_state', partitionIndex=12), RemainingPartition(topicName='__transaction_state', partitionIndex=7), RemainingPartition(topicName='__transaction_state', partitionIndex=46), RemainingPartition(topicName='__transaction_state', partitionIndex=48), RemainingPartition(topicName='__transaction_state', partitionIndex=49), RemainingPartition(topicName='__transaction_state', partitionIndex=28), RemainingPartition(topicName='__transaction_state', partitionIndex=2), RemainingPartition(topicName='__transaction_state', partitionIndex=20), RemainingPartition(topicName='__transaction_state', partitionIndex=24), RemainingPartition(topicName='__transaction_state', partitionIndex=13), RemainingPartition(topicName='__transaction_state', partitionIndex=0), RemainingPartition(topicName='__transaction_state', partitionIndex=37), RemainingPartition(topicName='__transaction_state', partitionIndex=3), RemainingPartition(topicName='__transaction_state', partitionIndex=21), RemainingPartition(topicName='__transaction_state', partitionIndex=29), RemainingPartition(topicName='__transaction_state', partitionIndex=39), RemainingPartition(topicName='__transaction_state', partitionIndex=38), RemainingPartition(topicName='__transaction_state', partitionIndex=6), RemainingPartition(topicName='__transaction_state', partitionIndex=14), RemainingPartition(topicName='__transaction_state', partitionIndex=10), RemainingPartition(topicName='__transaction_state', partitionIndex=44), RemainingPartition(topicName='__transaction_state', partitionIndex=9), RemainingPartition(topicName='__transaction_state', partitionIndex=22), RemainingPartition(topicName='__transaction_state', partitionIndex=43), RemainingPartition(topicName='__transaction_state', partitionIndex=4), RemainingPartition(topicName='__transaction_state', partitionIndex=30), RemainingPartition(topicName='__transaction_state', partitionIndex=33), RemainingPartition(topicName='__transaction_state', partitionIndex=32), RemainingPartition(topicName='__transaction_state', partitionIndex=25), RemainingPartition(topicName='__transaction_state', partitionIndex=17), RemainingPartition(topicName='__transaction_state', partitionIndex=23), RemainingPartition(topicName='__transaction_state', partitionIndex=47), RemainingPartition(topicName='__transaction_state', partitionIndex=18), RemainingPartition(topicName='__transaction_state', partitionIndex=26), RemainingPartition(topicName='__transaction_state', partitionIndex=36), RemainingPartition(topicName='__transaction_state', partitionIndex=5), RemainingPartition(topicName='__transaction_state', partitionIndex=8), RemainingPartition(topicName='__transaction_state', partitionIndex=16), RemainingPartition(topicName='__transaction_state', partitionIndex=11), RemainingPartition(topicName='__transaction_state', partitionIndex=40), RemainingPartition(topicName='__transaction_state', partitionIndex=19), RemainingPartition(topicName='__transaction_state', partitionIndex=27), RemainingPartition(topicName='__transaction_state', partitionIndex=41), RemainingPartition(topicName='__transaction_state', partitionIndex=1), RemainingPartition(topicName='__transaction_state', partitionIndex=34), RemainingPartition(topicName='__transaction_state', partitionIndex=35)] (kafka.server.KafkaServer:66)
[2019-11-13 06:06:14,065] INFO [KafkaServer id=2] Error from controller: NONE (kafka.server.KafkaServer:66)
[2019-11-13 06:06:19,066] WARN [KafkaServer id=2] Retrying controlled shutdown after the previous attempt failed... (kafka.server.KafkaServer:70)
[2019-11-13 06:06:19,069] WARN [KafkaServer id=2] Proceeding to do an unclean shutdown as all the controlled shutdown attempts failed (kafka.server.KafkaServer:70)
[2019-11-13 06:06:19,069] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2019-11-13 06:06:19,069] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2019-11-13 06:06:19,069] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2019-11-13 06:06:19,070] INFO [SocketServer brokerId=2] Stopping socket server request processors (kafka.network.SocketServer:66)
[2019-11-13 06:06:19,074] INFO [SocketServer brokerId=2] Stopped socket server request processors (kafka.network.SocketServer:66)
[2019-11-13 06:06:19,074] INFO [data-plane Kafka Request Handler on Broker 2], shutting down (kafka.server.KafkaRequestHandlerPool:66)
[2019-11-13 06:06:19,075] INFO [data-plane Kafka Request Handler on Broker 2], shut down completely (kafka.server.KafkaRequestHandlerPool:66)
[2019-11-13 06:06:19,076] INFO [ExpirationReaper-2-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,143] INFO [ExpirationReaper-2-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,143] INFO [ExpirationReaper-2-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,144] INFO [KafkaApi-2] Shutdown complete. (kafka.server.KafkaApis:66)
[2019-11-13 06:06:19,144] INFO [ExpirationReaper-2-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,332] INFO [ExpirationReaper-2-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,332] INFO [ExpirationReaper-2-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,333] INFO [TransactionCoordinator id=2] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2019-11-13 06:06:19,333] INFO [ProducerId Manager 2]: Shutdown complete: last producerId assigned 2001 (kafka.coordinator.transaction.ProducerIdManager:66)
[2019-11-13 06:06:19,333] INFO [Transaction State Manager 2]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager:66)
[2019-11-13 06:06:19,334] INFO [Transaction Marker Channel Manager 2]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2019-11-13 06:06:19,334] INFO [Transaction Marker Channel Manager 2]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2019-11-13 06:06:19,334] INFO [Transaction Marker Channel Manager 2]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2019-11-13 06:06:19,335] INFO [TransactionCoordinator id=2] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2019-11-13 06:06:19,335] INFO [GroupCoordinator 2]: Shutting down. (kafka.coordinator.group.GroupCoordinator:66)
[2019-11-13 06:06:19,335] INFO [ExpirationReaper-2-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,532] INFO [ExpirationReaper-2-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,532] INFO [ExpirationReaper-2-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,533] INFO [ExpirationReaper-2-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,534] INFO [ExpirationReaper-2-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,534] INFO [ExpirationReaper-2-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,535] INFO [GroupCoordinator 2]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator:66)
[2019-11-13 06:06:19,535] INFO [ReplicaManager broker=2] Shutting down (kafka.server.ReplicaManager:66)
[2019-11-13 06:06:19,535] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2019-11-13 06:06:19,536] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2019-11-13 06:06:19,536] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2019-11-13 06:06:19,536] INFO [ReplicaFetcherManager on broker 2] shutting down (kafka.server.ReplicaFetcherManager:66)
[2019-11-13 06:06:19,536] INFO [ReplicaFetcherManager on broker 2] shutdown completed (kafka.server.ReplicaFetcherManager:66)
[2019-11-13 06:06:19,537] INFO [ReplicaAlterLogDirsManager on broker 2] shutting down (kafka.server.ReplicaAlterLogDirsManager:66)
[2019-11-13 06:06:19,537] INFO [ReplicaAlterLogDirsManager on broker 2] shutdown completed (kafka.server.ReplicaAlterLogDirsManager:66)
[2019-11-13 06:06:19,537] INFO [ExpirationReaper-2-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,630] INFO [ExpirationReaper-2-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,630] INFO [ExpirationReaper-2-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,630] INFO [ExpirationReaper-2-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,696] INFO [ExpirationReaper-2-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,696] INFO [ExpirationReaper-2-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,696] INFO [ExpirationReaper-2-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,896] INFO [ExpirationReaper-2-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,896] INFO [ExpirationReaper-2-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:19,897] INFO [ExpirationReaper-2-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:20,096] INFO [ExpirationReaper-2-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:20,096] INFO [ExpirationReaper-2-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2019-11-13 06:06:20,135] INFO [ReplicaManager broker=2] Shut down completely (kafka.server.ReplicaManager:66)
[2019-11-13 06:06:20,135] INFO Shutting down. (kafka.log.LogManager:66)
[2019-11-13 06:06:20,135] INFO Shutting down the log cleaner. (kafka.log.LogCleaner:66)
[2019-11-13 06:06:20,136] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner:66)
[2019-11-13 06:06:20,136] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner:66)
[2019-11-13 06:06:20,136] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner:66)
[2019-11-13 06:06:20,154] INFO [ProducerStateManager partition=__transaction_state-25] Writing producer snapshot at offset 19 (kafka.log.ProducerStateManager:66)
[2019-11-13 06:06:20,231] INFO Shutdown complete. (kafka.log.LogManager:66)
[2019-11-13 06:06:20,231] INFO [ControllerEventThread controllerId=2] Shutting down (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2019-11-13 06:06:20,231] INFO [ControllerEventThread controllerId=2] Stopped (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2019-11-13 06:06:20,231] INFO [ControllerEventThread controllerId=2] Shutdown completed (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2019-11-13 06:06:20,233] INFO [PartitionStateMachine controllerId=2] Stopped partition state machine (kafka.controller.ZkPartitionStateMachine:66)
[2019-11-13 06:06:20,233] INFO [ReplicaStateMachine controllerId=2] Stopped replica state machine (kafka.controller.ZkReplicaStateMachine:66)
[2019-11-13 06:06:20,233] INFO [RequestSendThread controllerId=2] Shutting down (kafka.controller.RequestSendThread:66)
[2019-11-13 06:06:20,234] INFO [RequestSendThread controllerId=2] Stopped (kafka.controller.RequestSendThread:66)
[2019-11-13 06:06:20,234] INFO [RequestSendThread controllerId=2] Shutdown completed (kafka.controller.RequestSendThread:66)
[2019-11-13 06:06:20,237] INFO [Controller id=2] Resigned (kafka.controller.KafkaController:66)
[2019-11-13 06:06:20,237] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient:66)
[2019-11-13 06:06:20,340] INFO Session: 0x100df7ef30e0002 closed (org.apache.zookeeper.ZooKeeper:1422)
[2019-11-13 06:06:20,340] INFO EventThread shut down for session: 0x100df7ef30e0002 (org.apache.zookeeper.ClientCnxn:524)
[2019-11-13 06:06:20,340] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient:66)
[2019-11-13 06:06:20,341] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-11-13 06:06:21,049] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-11-13 06:06:21,049] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-11-13 06:06:21,049] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-11-13 06:06:22,049] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-11-13 06:06:22,049] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-11-13 06:06:22,049] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-11-13 06:06:23,049] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-11-13 06:06:23,049] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2019-11-13 06:06:23,049] INFO [SocketServer brokerId=2] Shutting down socket server (kafka.network.SocketServer:66)
[2019-11-13 06:06:23,082] INFO [SocketServer brokerId=2] Shutdown completed (kafka.network.SocketServer:66)
[2019-11-13 06:06:23,084] INFO [KafkaServer id=2] shut down completed (kafka.server.KafkaServer:66)
[2019-11-13 06:06:23,097] INFO ConnnectionExpirerThread interrupted (org.apache.zookeeper.server.NIOServerCnxnFactory:583)
[2019-11-13 06:06:23,098] INFO accept thread exitted run method (org.apache.zookeeper.server.NIOServerCnxnFactory:219)
[2019-11-13 06:06:23,099] INFO selector thread exitted run method (org.apache.zookeeper.server.NIOServerCnxnFactory:420)
[2019-11-13 06:06:23,099] INFO selector thread exitted run method (org.apache.zookeeper.server.NIOServerCnxnFactory:420)
[2019-11-13 06:06:23,099] INFO selector thread exitted run method (org.apache.zookeeper.server.NIOServerCnxnFactory:420)
[2019-11-13 06:06:23,102] INFO shutting down (org.apache.zookeeper.server.ZooKeeperServer:558)
[2019-11-13 06:06:23,103] INFO Shutting down (org.apache.zookeeper.server.SessionTrackerImpl:237)
[2019-11-13 06:06:23,103] INFO Shutting down (org.apache.zookeeper.server.PrepRequestProcessor:1007)
[2019-11-13 06:06:23,103] INFO Shutting down (org.apache.zookeeper.server.SyncRequestProcessor:191)
[2019-11-13 06:06:23,103] INFO PrepRequestProcessor exited loop! (org.apache.zookeeper.server.PrepRequestProcessor:155)
[2019-11-13 06:06:23,103] INFO SyncRequestProcessor exited! (org.apache.zookeeper.server.SyncRequestProcessor:169)
[2019-11-13 06:06:23,104] INFO shutdown of request processor complete (org.apache.zookeeper.server.FinalRequestProcessor:514){noformat}",,bbejeck,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,KAFKA-9991,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-06-10 21:10:30.598,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 10 21:10:30 UTC 2020,,,,,,,"0|z08knk:",9223372036854775807,,,,,,,,,,,,,,,,"10/Jun/20 21:10;guozhang;Resolved as part of https://issues.apache.org/jira/browse/KAFKA-9991",,,,,,,,,,,,,,,,,,,,,
Improve stickiness verification for AbstractStickyAssignorTest,KAFKA-10118,13310014,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,showuon,showuon,08/Jun/20 03:03,08/Jun/20 03:04,12/Jan/21 11:54,,,,,,,,,,,,,,,0,,,,"In KAFKA-9987 , we implemented an optimized sticky partition assignor algorithm (i.e. {{constrainedAssign}} mehtod), so the original *isSticky* validation is not suitable for all situations, anymore. In this PR: [https://github.com/apache/kafka/pull/8788,] we removed the unnecessary isSticky verification for {{constrainedAssign}} method. But we should have a replace the check we removed with a better one. For more discussion, please see: [https://github.com/apache/kafka/pull/8788#pullrequestreview-425551903]

 

We haven't got any better idea so far, so welcome to provide suggestion. Thanks.",,ableegoldman,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-08 03:03:18.0,,,,,,,"0|z0flug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test kafka.api.SaslGssapiSslEndToEndAuthorizationTest.testNoConsumeWithoutDescribeAclViaSubscribe,KAFKA-9181,13268085,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,bbejeck,bbejeck,13/Nov/19 16:49,28/May/20 10:11,12/Jan/21 11:54,24/Jan/20 10:40,,,,,2.5.0,,,,,core,,,,0,flaky-test,tests,,"Failed in [https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/26571/testReport/junit/kafka.api/SaslGssapiSslEndToEndAuthorizationTest/testNoConsumeWithoutDescribeAclViaSubscribe/]

 
{noformat}
Error Messageorg.apache.kafka.common.errors.TopicAuthorizationException: Not authorized to access topics: [topic2]Stacktraceorg.apache.kafka.common.errors.TopicAuthorizationException: Not authorized to access topics: [topic2]
Standard OutputAdding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka6494439724844851846.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-13 04:43:16,187] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:43:16,191] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:43:16,384] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:43:16,384] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Read from hosts: *
	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka3083328529571706878.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka3083328529571706878.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-13 04:43:17,549] ERROR [Consumer clientId=consumer-group-255, groupId=group] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka2790389872392408045.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-13 04:43:43,513] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:43:43,513] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:43:43,682] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:43:43,683] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka3639280670011131711.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Create from hosts: * 

[2019-11-13 04:43:45,193] ERROR [Consumer clientId=consumer-group-256, groupId=group] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Read from hosts: *
	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

[2019-11-13 04:43:45,698] ERROR [Consumer clientId=consumer-group-256, groupId=group] Topic authorization failed for topics [topic2] (org.apache.kafka.clients.Metadata:283)
[2019-11-13 04:43:45,700] ERROR [Consumer clientId=consumer-group-256, groupId=group] Topic authorization failed for topics [topic2] (org.apache.kafka.clients.Metadata:283)
[2019-11-13 04:43:47,205] ERROR [Consumer clientId=consumer-group-256, groupId=group] Topic authorization failed for topics [topic2] (org.apache.kafka.clients.Metadata:283)
Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka2853735962705514989.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-13 04:44:10,330] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:44:10,331] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:44:10,531] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:44:10,531] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2e, patternType=PREFIXED)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=gr, patternType=PREFIXED)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:PREFIXED:e2e`: 
 	User:client has Allow permission for operations: Read from hosts: *
	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Current ACLs for resource `Group:PREFIXED:gr`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka3614118284002283090.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka3343806238750489399.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-13 04:44:42,245] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:44:42,262] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:44:42,453] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:44:42,461] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Read from hosts: *
	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka2293718255849931457.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8247896269514574204.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-13 04:45:09,679] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:45:09,687] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:45:09,901] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:45:09,902] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka6165264118808778893.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka5588216719148826164.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-13 04:45:44,360] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:45:44,366] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:45:44,553] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:45:44,553] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=topic2, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:topic2`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=topic2, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:topic2`: 
 	User:client has Allow permission for operations: Read from hosts: *
	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8937168000923149954.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-13 04:45:45,239] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic2-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:45:45,239] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition topic2-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka1753713975062534365.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-13 04:46:13,255] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:46:13,261] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:46:13,522] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:46:13,527] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=*, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Read from hosts: *
	User:kafka has Allow permission for operations: Read from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Current ACLs for resource `Group:LITERAL:*`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka4579277064687399324.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka1211513173897695002.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-13 04:46:38,320] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:46:38,323] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:46:38,521] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:46:38,526] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka4859545116775501587.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka5936495479832727441.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-13 04:47:13,934] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:47:13,939] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:47:14,142] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:47:14,143] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka5030780877268645002.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Create from hosts: * 

[2019-11-13 04:47:15,675] ERROR [Consumer clientId=consumer-group-263, groupId=group] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka2819838905642285118.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-13 04:47:43,623] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:47:43,635] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:47:43,831] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:47:43,831] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka7821381619195888442.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka7294747755465828555.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-13 04:48:13,692] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:48:13,705] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:48:13,880] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:48:13,884] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka2522835534939702730.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8641154754165823659.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-13 04:48:39,064] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:48:39,074] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:48:39,267] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:48:39,268] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka4420357362687484380.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-13 04:48:39,713] ERROR [Producer clientId=producer-422] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
[2019-11-13 04:48:39,740] ERROR [Consumer clientId=consumer-group-265, groupId=group] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=topic2, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:topic2`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=topic2, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:topic2`: 
 	User:client has Allow permission for operations: Read from hosts: *
	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

[2019-11-13 04:48:40,327] ERROR [Producer clientId=producer-422] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
[2019-11-13 04:48:40,429] ERROR [Producer clientId=producer-422] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
[2019-11-13 04:48:41,185] ERROR [Consumer clientId=consumer-group-265, groupId=group] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Read from hosts: *
	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8725022246516193653.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-13 04:49:10,578] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:49:10,578] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:49:10,775] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-13 04:49:10,780] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Read from hosts: *
	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka6140470407592077851.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded {noformat}",,bbejeck,githubbot,hai_lin,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-01-12 16:51:14.123,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 10:11:58 UTC 2020,,,,,,,"0|z08knc:",9223372036854775807,,hachikuji,,,,,,,,,,,,,,"14/Nov/19 15:08;bbejeck;Failed again in [https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/3358/testReport/junit/kafka.api/SaslGssapiSslEndToEndAuthorizationTest/testNoConsumeWithoutDescribeAclViaSubscribe/] on 11/14/19

 
{noformat}
Error Messageorg.apache.kafka.common.errors.TopicAuthorizationException: Not authorized to access topics: [topic2]Stacktraceorg.apache.kafka.common.errors.TopicAuthorizationException: Not authorized to access topics: [topic2]
Standard OutputAdding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka15290895624380393501.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-14 00:58:37,287] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 00:58:37,304] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 00:58:37,474] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 00:58:37,490] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Read from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka6561302204353356505.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka6561302204353356505.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-14 00:58:38,702] ERROR [Consumer clientId=consumer-group-123, groupId=group] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka14571018816684750392.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-14 00:59:01,454] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 00:59:01,453] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 00:59:01,638] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 00:59:01,638] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8347317469778808373.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Create from hosts: * 

[2019-11-14 00:59:03,227] ERROR [Consumer clientId=consumer-group-124, groupId=group] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Read from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

[2019-11-14 00:59:03,763] ERROR [Consumer clientId=consumer-group-124, groupId=group] Topic authorization failed for topics [topic2] (org.apache.kafka.clients.Metadata:283)
[2019-11-14 00:59:03,766] ERROR [Consumer clientId=consumer-group-124, groupId=group] Topic authorization failed for topics [topic2] (org.apache.kafka.clients.Metadata:283)
[2019-11-14 00:59:05,270] ERROR [Consumer clientId=consumer-group-124, groupId=group] Topic authorization failed for topics [topic2] (org.apache.kafka.clients.Metadata:283)
Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka13029856880541949269.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-14 00:59:26,542] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 00:59:26,542] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 00:59:26,750] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 00:59:26,757] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2e, patternType=PREFIXED)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=gr, patternType=PREFIXED)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:PREFIXED:e2e`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Read from hosts: * 

Current ACLs for resource `Group:PREFIXED:gr`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka6088574624537718013.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka7469658214928529291.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-14 00:59:49,789] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 00:59:49,993] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 00:59:50,000] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Read from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka10092050242584436880.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka4752948686662024739.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-14 01:00:19,801] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:00:19,802] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:00:19,999] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:00:19,999] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka17975749946103788919.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka6211608820863669201.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-14 01:00:43,661] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:00:43,666] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:00:43,880] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:00:43,881] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=topic2, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:topic2`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=topic2, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:topic2`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Read from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka13580752757834126408.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-14 01:00:44,578] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition topic2-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:00:44,578] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic2-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka9221698973638093027.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-14 01:01:12,267] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:01:12,278] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:01:12,469] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:01:12,486] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=*, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Read from hosts: *
	User:kafka has Allow permission for operations: Read from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Current ACLs for resource `Group:LITERAL:*`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka7749679852872205789.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka777035443037790769.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-14 01:01:37,428] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:01:37,428] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:01:37,643] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:01:37,648] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka1993279303170600061.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka10137267044708183264.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-14 01:02:02,047] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:02:02,051] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:02:02,260] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:02:02,266] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka2378892307444116935.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Create from hosts: * 

[2019-11-14 01:02:03,887] ERROR [Consumer clientId=consumer-group-131, groupId=group] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka17697454570176860746.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-14 01:02:26,341] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:02:26,351] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:02:26,563] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:02:26,563] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka9624121587150447769.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka13866155282540711107.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-14 01:02:48,677] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:02:48,689] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:02:48,882] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:02:48,886] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka12414618500290383241.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka4719537482188960558.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-14 01:03:08,341] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:03:08,354] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:03:08,545] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:03:08,545] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka10429787789810189752.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-14 01:03:08,982] ERROR [Producer clientId=producer-128] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
[2019-11-14 01:03:09,009] ERROR [Consumer clientId=consumer-group-133, groupId=group] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=topic2, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:topic2`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=topic2, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:topic2`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Read from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

[2019-11-14 01:03:09,582] ERROR [Producer clientId=producer-128] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
[2019-11-14 01:03:09,612] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition topic2-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:03:09,621] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic2-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:03:09,683] ERROR [Producer clientId=producer-128] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
[2019-11-14 01:03:10,799] ERROR [Consumer clientId=consumer-group-133, groupId=group] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Read from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8280054279144836016.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-14 01:03:33,512] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:03:33,715] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 01:03:33,720] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition e2etopic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Read from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka710823969989408103.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded {noformat}","12/Jan/20 16:51;githubbot;rajinisivaram commented on pull request #7941: KAFKA-9181; Ensure SubscriptionState.groupSubscription is updated even if onJoinPrepare not invoked after subscribe
URL: https://github.com/apache/kafka/pull/7941
 
 
   As described in KAFKA-9181, kafka.api.SaslGssapiSslEndToEndAuthorizationTest.testNoConsumeWithoutDescribeAclViaSubscribe occasionally hits unexpected TopicAuthorizationException even after the topic is removed from the subscription. The test uses small metadata refresh time and hence can see metadata responses before JoinGroup is processed. We currently rely on `onJoinPrepare` to reset SubscriptionState.groupSubscription, which accumulates topics until reset. If we process JoinGroup after a subscribe without a new `onJoinPrepare`, we leave the topic in `SubscriptionState.groupSubscription` and hence in metadata. This PR resets `groupSubscription` when sending JoinGroup request, when `ConsumerCoordinator.joinedSubscription` is updated.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","24/Jan/20 10:38;githubbot;rajinisivaram commented on pull request #7941: KAFKA-9181; Maintain clean separation between local and group subscriptions in consumer's SubscriptionState
URL: https://github.com/apache/kafka/pull/7941
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","11/Feb/20 00:19;githubbot;soondenana commented on pull request #8084: KAFKA-9181; Maintain clean separation between local and group subscriptions in consumer's SubscriptionState (#7941)
URL: https://github.com/apache/kafka/pull/8084
 
 
   
   Reviewers: Jason Gustafson <jason@confluent.io>, Guozhang Wang <wangguoz@gmail.com>
   (cherry picked from commit a565d1a182cc69c9994c4512b5e9877e97f06cdf)
   
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","11/Feb/20 18:11;githubbot;hachikuji commented on pull request #8084: KAFKA-9181; Maintain clean separation between local and group subscriptions in consumer's SubscriptionState (#7941)
URL: https://github.com/apache/kafka/pull/8084
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","30/Apr/20 17:28;hai_lin;I did notice some issue after this patch, here is what I observe.

Consumer metadata might skip first metadata update, cause grouopSubscription is not reset. In my case, the consumer coordinator thread hijack the update by calling newMetadataRequestAndVersion with outdated groupSubscription before joinPrepare() happen. The groupSubscription will get reset later and it will eventually get update later, and this won't be an issue for initial consumer subscribe(since the groupSubscription is empty anyway), but it might happen the following subscribe when groupSubscription is not empty. This will create a discrepancy between subscription and groupSubscription, if any new metadata request happened in between, metadataTopics will return outdated group information. 

 
h4. The happy path
 * Consumer call subscribe > Update {{needUpdated}}, bump up {{requestVersion}} and update {{subscription}} in {{SubscriptionState}} > {{prepareJoin()}} was call in first {{poll()}} to reset {{groupSubscription}} -> next time when metadata update was call and {{metadataTopics()}} returns {{subscription}} since {{groupSubscription}} is empty -> update call issue to broker to fetch partition information for new topic

h4. In our case
 * Consumer call subscribe > Update {{needUpdated}}, bump up {{requestVersion}} and update {{subscription}}(not {{groupSubscription}}) in {{SubscriptionState}} > Consumer Coordinator heartbeat thread call metadata request and {{SubscriptionState}} gave away the current requestVersion and outdated {{groupSubscription}} > making request for metadata update with outdated subscription -> request comes back to client and since {{requestVersion}} is up to latest, it reset {{needUpdated}} flag -> {{joinPrepare()}} called and reset {{groupSubscription}} > no new metadata update request follow cause {{needUpdated}} was reset -> metadata request will happen when {{metadata.max.age}} reaches.

 

I saw some discussion in the pull request, don't know if I miss anything here. cc [~rsivaram] [~bbejeck]","28/May/20 10:11;rsivaram;[~hai_lin] Thanks for reporting this issue. Opened KAFKA-10056 to follow this up.",,,,,,,,,,,,,,,
Add test coverage for new ActiveTaskCreator and StandbyTaskCreator,KAFKA-9676,13290164,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,bchen225242,bchen225242,06/Mar/20 18:52,14/May/20 20:57,12/Jan/21 11:54,14/May/20 06:25,,,,,,,,,,streams,,,,0,help-wanted,newbie,,The newly separated ActiveTaskCreator and StandbyTaskCreator have no unit test coverage. We should add corresponding tests.,,bchen225242,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-03-06 21:57:29.074,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 14 20:57:52 UTC 2020,,,,,,,"0|z0c9jk:",9223372036854775807,,,,,,,,,,,,,,,,"06/Mar/20 21:57;mjsax;This PR already adds some tests: [https://github.com/apache/kafka/pull/8218/files]","19/Mar/20 20:24;mjsax;[https://github.com/apache/kafka/pull/8318] adds all missing tests for `ActiveTaskCreator`.","14/May/20 06:25;bchen225242;The current unit test coverage is pretty good now, closing the ticket.","14/May/20 18:01;mjsax;[~bchen225242] There is still no `StandbyTaskCreatorTest` – IMHO we should keep this ticket open.","14/May/20 20:57;bchen225242;The current trunk has logic coverage for StandbyTaskCreator in TaskManagerTest and StreamThreadTest. I feel we don't necessarily need a dedicated test class for it.",,,,,,,,,,,,,,,,,
Flaky test: org.apache.kafka.streams.integration.QueryableStateIntegrationTest.shouldAllowConcurrentAccesses,KAFKA-9798,13295542,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Not A Problem,guozhang,bbejeck,bbejeck,01/Apr/20 13:59,13/May/20 14:50,12/Jan/21 11:54,13/May/20 14:50,,,,,,,,,,streams,unit tests,,,0,flaky-test,test,,,,ableegoldman,bbejeck,guozhang,gw524119574,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-04-01 15:45:01.431,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 11 15:36:45 UTC 2020,,,,,,,"0|z0d628:",9223372036854775807,,,,,,,,,,,,,,,,"01/Apr/20 14:00;bbejeck;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1523/testReport/junit/org.apache.kafka.streams.integration/QueryableStateIntegrationTest/shouldAllowConcurrentAccesses/]

 
{noformat}
Stacktracejava.lang.AssertionError: Did not receive all 1 records from topic output-concurrent-windowed-0 within 120000 ms
Expected: is a value equal to or greater than <1>
     but: <0> was less than <1>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinValuesRecordsReceived$6(IntegrationTestUtils.java:691)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:415)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:383)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinValuesRecordsReceived(IntegrationTestUtils.java:687)
	at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.waitUntilAtLeastNumRecordProcessed(QueryableStateIntegrationTest.java:1189)
	at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.shouldAllowConcurrentAccesses(QueryableStateIntegrationTest.java:649)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.lang.Thread.run(Thread.java:748){noformat}","01/Apr/20 15:45;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1526/testReport/junit/org.apache.kafka.streams.integration/QueryableStateIntegrationTest/shouldAllowConcurrentAccesses/]

Different issue:
{quote}[2020-04-01 06:14:42,679] ERROR stream-thread [queryable-state-1-3b9f384a-d048-4bd9-bcc0-934fdfde525d-StreamThread-1] task [2_0] Failed to flush state store windowed-word-count-store-stream-concurrent-0: (org.apache.kafka.streams.processor.internals.ProcessorStateManager:412) org.apache.kafka.streams.errors.ProcessorStateException: Error while executing flush from store windowed-word-count-store-stream-concurrent-0.1585612800000 at org.apache.kafka.streams.state.internals.RocksDBStore.flush(RocksDBStore.java:389) at org.apache.kafka.streams.state.internals.AbstractSegments.flush(AbstractSegments.java:148) at org.apache.kafka.streams.state.internals.AbstractRocksDBSegmentedBytesStore.flush(AbstractRocksDBSegmentedBytesStore.java:198) at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84) at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84) at org.apache.kafka.streams.state.internals.CachingWindowStore.flush(CachingWindowStore.java:297) at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84) at org.apache.kafka.streams.state.internals.MeteredWindowStore.lambda$flush$4(MeteredWindowStore.java:200) at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:808) at org.apache.kafka.streams.state.internals.MeteredWindowStore.flush(MeteredWindowStore.java:200) at org.apache.kafka.streams.processor.internals.ProcessorStateManager.flush(ProcessorStateManager.java:402) at org.apache.kafka.streams.processor.internals.StreamTask.prepareCommit(StreamTask.java:315) at org.apache.kafka.streams.processor.internals.TaskManager.commitInternal(TaskManager.java:745) at org.apache.kafka.streams.processor.internals.TaskManager.commitAll(TaskManager.java:733) at org.apache.kafka.streams.processor.internals.StreamThread.maybeCommit(StreamThread.java:833) at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:696) at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:545) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:504) Caused by: org.rocksdb.RocksDBException: While open a file for random read: /tmp/state-queryable-state-1534964904159798192/queryable-state-1/2_0/windowed-word-count-store-stream-concurrent-0/windowed-word-count-store-stream-concurrent-0.1585612800000/000230.sst: No such file or directory at org.rocksdb.RocksDB.flush(Native Method) at org.rocksdb.RocksDB.flush(RocksDB.java:2394) at org.apache.kafka.streams.state.internals.RocksDBStore$SingleColumnFamilyAccessor.flush(RocksDBStore.java:584) at org.apache.kafka.streams.state.internals.RocksDBStore.flush(RocksDBStore.java:387) ... 17 more [2020-04-01 06:14:42,692] ERROR stream-thread [queryable-state-1-3b9f384a-d048-4bd9-bcc0-934fdfde525d-StreamThread-1] Encountered the following exception during processing and the thread is going to shut down: (org.apache.kafka.streams.processor.internals.StreamThread:524) org.apache.kafka.streams.errors.ProcessorStateException: Error while executing flush from store windowed-word-count-store-stream-concurrent-0.1585612800000 at org.apache.kafka.streams.state.internals.RocksDBStore.flush(RocksDBStore.java:389) at org.apache.kafka.streams.state.internals.AbstractSegments.flush(AbstractSegments.java:148) at org.apache.kafka.streams.state.internals.AbstractRocksDBSegmentedBytesStore.flush(AbstractRocksDBSegmentedBytesStore.java:198) at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84) at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84) at org.apache.kafka.streams.state.internals.CachingWindowStore.flush(CachingWindowStore.java:297) at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84) at org.apache.kafka.streams.state.internals.MeteredWindowStore.lambda$flush$4(MeteredWindowStore.java:200) at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:808) at org.apache.kafka.streams.state.internals.MeteredWindowStore.flush(MeteredWindowStore.java:200) at org.apache.kafka.streams.processor.internals.ProcessorStateManager.flush(ProcessorStateManager.java:402) at org.apache.kafka.streams.processor.internals.StreamTask.prepareCommit(StreamTask.java:315) at org.apache.kafka.streams.processor.internals.TaskManager.commitInternal(TaskManager.java:745) at org.apache.kafka.streams.processor.internals.TaskManager.commitAll(TaskManager.java:733) at org.apache.kafka.streams.processor.internals.StreamThread.maybeCommit(StreamThread.java:833) at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:696) at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:545) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:504) Caused by: org.rocksdb.RocksDBException: While open a file for random read: /tmp/state-queryable-state-1534964904159798192/queryable-state-1/2_0/windowed-word-count-store-stream-concurrent-0/windowed-word-count-store-stream-concurrent-0.1585612800000/000230.sst: No such file or directory at org.rocksdb.RocksDB.flush(Native Method) at org.rocksdb.RocksDB.flush(RocksDB.java:2394) at org.apache.kafka.streams.state.internals.RocksDBStore$SingleColumnFamilyAccessor.flush(RocksDBStore.java:584) at org.apache.kafka.streams.state.internals.RocksDBStore.flush(RocksDBStore.java:387) ... 17 more

...

[2020-04-01 06:14:42,706] ERROR stream-thread [queryable-state-1-3b9f384a-d048-4bd9-bcc0-934fdfde525d-StreamThread-1] task [2_0] Failed to flush state store windowed-word-count-store-stream-concurrent-0: (org.apache.kafka.streams.processor.internals.ProcessorStateManager:412) org.apache.kafka.streams.errors.ProcessorStateException: Error while executing flush from store windowed-word-count-store-stream-concurrent-0.1585612800000 at org.apache.kafka.streams.state.internals.RocksDBStore.flush(RocksDBStore.java:389) at org.apache.kafka.streams.state.internals.AbstractSegments.flush(AbstractSegments.java:148) at org.apache.kafka.streams.state.internals.AbstractRocksDBSegmentedBytesStore.flush(AbstractRocksDBSegmentedBytesStore.java:198) at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84) at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84) at org.apache.kafka.streams.state.internals.CachingWindowStore.flush(CachingWindowStore.java:297) at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84) at org.apache.kafka.streams.state.internals.MeteredWindowStore.lambda$flush$4(MeteredWindowStore.java:200) at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:808) at org.apache.kafka.streams.state.internals.MeteredWindowStore.flush(MeteredWindowStore.java:200) at org.apache.kafka.streams.processor.internals.ProcessorStateManager.flush(ProcessorStateManager.java:402) at org.apache.kafka.streams.processor.internals.AbstractTask.executeAndMaybeSwallow(AbstractTask.java:110) at org.apache.kafka.streams.processor.internals.StreamTask.prepareClose(StreamTask.java:459) at org.apache.kafka.streams.processor.internals.StreamTask.prepareCloseDirty(StreamTask.java:420) at org.apache.kafka.streams.processor.internals.TaskManager.shutdown(TaskManager.java:626) at org.apache.kafka.streams.processor.internals.StreamThread.completeShutdown(StreamThread.java:888) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:528) Caused by: org.rocksdb.RocksDBException: While open a file for random read: /tmp/state-queryable-state-1534964904159798192/queryable-state-1/2_0/windowed-word-count-store-stream-concurrent-0/windowed-word-count-store-stream-concurrent-0.1585612800000/000230.sst: No such file or directory at org.rocksdb.RocksDB.flush(Native Method) at org.rocksdb.RocksDB.flush(RocksDB.java:2394) at org.apache.kafka.streams.state.internals.RocksDBStore$SingleColumnFamilyAccessor.flush(RocksDBStore.java:584) at org.apache.kafka.streams.state.internals.RocksDBStore.flush(RocksDBStore.java:387) ... 16 more{quote}","01/Apr/20 20:18;mjsax;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/5559/testReport/junit/org.apache.kafka.streams.integration/QueryableStateIntegrationTest/shouldAllowConcurrentAccesses/]","01/Apr/20 22:58;guozhang;{code}
java.lang.AssertionError: Did not receive all 1 records from topic output-concurrent-windowed-0 within 120000 ms
Expected: is a value equal to or greater than <1>
     but: <0> was less than <1>
Stacktrace
java.lang.AssertionError: Did not receive all 1 records from topic output-concurrent-windowed-0 within 120000 ms
Expected: is a value equal to or greater than <1>
     but: <0> was less than <1>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinValuesRecordsReceived$6(IntegrationTestUtils.java:691)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:415)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:383)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinValuesRecordsReceived(IntegrationTestUtils.java:687)
	at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.waitUntilAtLeastNumRecordProcessed(QueryableStateIntegrationTest.java:1189)
	at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.shouldAllowConcurrentAccesses(QueryableStateIntegrationTest.java:649)
{code}","01/Apr/20 23:16;guozhang;I pushed a hotfix commit into trunk, let's see if it fixed the flaky test.","06/Apr/20 19:40;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1624/testReport/junit/org.apache.kafka.streams.integration/QueryableStateIntegrationTest/shouldAllowConcurrentAccesses/]

Different error message (Seem the hotfix did not do? The PR should contain the hotfix \cc [~guozhang] ):
{quote}java.nio.file.DirectoryNotEmptyException: /tmp/state-queryable-state-18623682413158663595/queryable-state-1/1_0 at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242) at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103) at java.nio.file.Files.delete(Files.java:1126) at org.apache.kafka.common.utils.Utils$2.postVisitDirectory(Utils.java:802) at org.apache.kafka.common.utils.Utils$2.postVisitDirectory(Utils.java:772) at java.nio.file.Files.walkFileTree(Files.java:2688) at java.nio.file.Files.walkFileTree(Files.java:2742) at org.apache.kafka.common.utils.Utils.delete(Utils.java:772) at org.apache.kafka.common.utils.Utils.delete(Utils.java:758) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState(IntegrationTestUtils.java:125) at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.shutdown(QueryableStateIntegrationTest.java:225){quote}","18/Apr/20 17:09;guozhang;I looked through the recent traces up to 1809 but did not find a reoccurrence, and I failed to reproduce it locally as well.

I will close this ticket for now and if it happens again let's re-open it.","27/Apr/20 15:09;gw524119574;I think this happen again.

See here: [https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1968/]

 ","27/Apr/20 16:55;ableegoldman;h3. Stacktrace

java.nio.file.NoSuchFileException: /tmp/state-queryable-state-shouldAllowConcurrentAccesses5845383974269013532/queryable-state-shouldAllowConcurrentAccesses/1_0/.checkpoint.tmp at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92) at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111) at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116) at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55) at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149) at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99) at java.base/java.nio.file.Files.readAttributes(Files.java:1763) at java.base/java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219) at java.base/java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276) at java.base/java.nio.file.FileTreeWalker.next(FileTreeWalker.java:373) at java.base/java.nio.file.Files.walkFileTree(Files.java:2760) at java.base/java.nio.file.Files.walkFileTree(Files.java:2796) at org.apache.kafka.common.utils.Utils.delete(Utils.java:777) at org.apache.kafka.common.utils.Utils.delete(Utils.java:763) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState(IntegrationTestUtils.java:125) at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.shutdown(QueryableStateIntegrationTest.java:228)","27/Apr/20 16:55;ableegoldman;h3. Stacktrace

java.lang.AssertionError: Did not receive all 1 records from topic output-concurrent-shouldAllowConcurrentAccesses within 120000 ms Expected: is a value equal to or greater than <1> but: <0> was less than <1> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinValuesRecordsReceived$6(IntegrationTestUtils.java:691) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:415) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:383) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinValuesRecordsReceived(IntegrationTestUtils.java:687) at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.waitUntilAtLeastNumRecordProcessed(QueryableStateIntegrationTest.java:1189) at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.shouldAllowConcurrentAccesses(QueryableStateIntegrationTest.java:649)","27/Apr/20 21:24;guozhang;One more try to fix this: https://github.com/apache/kafka/pull/8565","05/May/20 17:30;mjsax;Failed again: [https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2127/testReport/junit/org.apache.kafka.streams.integration/QueryableStateIntegrationTest/shouldAllowConcurrentAccesses/]
{quote}java.lang.AssertionError: Did not receive all 1 records from topic output-concurrent-QueryableStateIntegrationTestshouldAllowConcurrentAccesses within 120000 ms Expected: is a value equal to or greater than <1> but: <0> was less than <1> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinValuesRecordsReceived$6(IntegrationTestUtils.java:747) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:429) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:397) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinValuesRecordsReceived(IntegrationTestUtils.java:743) at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.waitUntilAtLeastNumRecordProcessed(QueryableStateIntegrationTest.java:1159) at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.shouldAllowConcurrentAccesses(QueryableStateIntegrationTest.java:650){quote}","05/May/20 18:52;guozhang;This is due to a known issue which is fixed in a hotfix --- let's see if it fails again from now on.","08/May/20 05:54;mjsax;Failed again here: [https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2154/testReport/junit/org.apache.kafka.streams.integration/QueryableStateIntegrationTest/shouldAllowConcurrentAccesses/]

Not sure if the hotfix was in this build on not – what hotfix do you refer to?","08/May/20 18:37;guozhang;Sigh... it was after my hotfix indeed, I think it still have something to do with the background producer since that's the only difference with others. 

Looking into that test, I'm now feeling if it is really necessary to have this test given the other `shouldBeAbleToQueryDuringRebalance` etc. WDYT if I just remove that test?","08/May/20 18:56;mjsax;Would like to hear [~vvcephei] opinion on that.","09/May/20 18:19;vvcephei;Funny you should ask my opinion, since I was just saying to Bill yesterday that I don’t understand what this test is doing here. Is it’s purpose really to ensure that multiple readers can concurrently access the store? Depending on how you look at the problem of verifying this, it seems either trivial in a unit test or impossible in general. 

Or if it’s verifying that queries can be served while the app is concurrently updating the store, then it seems like this is a basic condition that would be automatically ensured by every single IQ test. ","10/May/20 04:42;guozhang;Haha, fair point. I think the original motivation is to verify the second, and also feel that every IQ test should do this already..

Cool, I will just go ahead and remove this test then.","11/May/20 15:36;vvcephei;Great, thanks!",,,
java client api can not completely take out the kafka-consumer-groups.sh output of information,KAFKA-9902,13300228,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,startjava,startjava,22/Apr/20 09:49,22/Apr/20 09:49,12/Jan/21 11:54,,,,,,,,,,,,,,,0,,,,"Why the java client api is not with:
.kafka-consumer-groups.sh --bootstrap-server localhost:9081 --describe --group test
The method corresponding to the command, I can not get together GROUP, TOPIC, PARTITION, CURRENT-OFFSET, LOG-END-OFFSET, LAG, CONSUMER-ID, HOST, CLIENT-ID these columns of information, search materials know need to be taken separately, which makes our developers very troublesome, and this feature is very common.

 ",,startjava,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-04-22 09:49:06.0,,,,,,,"0|z0dxuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka.api.SaslGssapiSslEndToEndAuthorizationTest.testNoDescribeProduceOrConsumeWithoutTopicDescribeAcl,KAFKA-9187,13268340,Test,Reopened,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,bbejeck,bbejeck,14/Nov/19 15:02,14/Apr/20 06:04,12/Jan/21 11:54,,,,,,2.4.0,,,,,core,,,,0,flaky-test,,,"Failed in [https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/26593/]

 
{noformat}
Error Messageorg.scalatest.exceptions.TestFailedException: Consumed 0 records before timeout instead of the expected 1 recordsStacktraceorg.scalatest.exceptions.TestFailedException: Consumed 0 records before timeout instead of the expected 1 records
	at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:530)
	at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1389)
	at org.scalatest.Assertions$class.fail(Assertions.scala:1091)
	at org.scalatest.Assertions$.fail(Assertions.scala:1389)
	at kafka.utils.TestUtils$.waitUntilTrue(TestUtils.scala:842)
	at kafka.utils.TestUtils$.pollRecordsUntilTrue(TestUtils.scala:793)
	at kafka.utils.TestUtils$.pollUntilAtLeastNumRecords(TestUtils.scala:1334)
	at kafka.utils.TestUtils$.consumeRecords(TestUtils.scala:1343)
	at kafka.api.EndToEndAuthorizationTest.consumeRecords(EndToEndAuthorizationTest.scala:530)
	at kafka.api.EndToEndAuthorizationTest.testNoDescribeProduceOrConsumeWithoutTopicDescribeAcl(EndToEndAuthorizationTest.scala:369)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:305)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:365)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:330)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:78)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:328)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:65)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:292)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:305)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:412)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.lang.Thread.run(Thread.java:748)
Standard OutputAdding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka6956053092858073839.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Read from hosts: *
	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka7544779694857809493.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka7544779694857809493.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-14 00:05:16,760] ERROR [Consumer clientId=consumer-group-64, groupId=group] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka1164866442124187878.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka5859610215295612432.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Create from hosts: * 

[2019-11-14 00:05:27,930] ERROR [Consumer clientId=consumer-group-65, groupId=group] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Read from hosts: *
	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

[2019-11-14 00:05:28,437] ERROR [Consumer clientId=consumer-group-65, groupId=group] Topic authorization failed for topics [topic2] (org.apache.kafka.clients.Metadata:283)
[2019-11-14 00:05:28,440] ERROR [Consumer clientId=consumer-group-65, groupId=group] Topic authorization failed for topics [topic2] (org.apache.kafka.clients.Metadata:283)
Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8747414312546582585.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2e, patternType=PREFIXED)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=gr, patternType=PREFIXED)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:PREFIXED:e2e`: 
 	User:client has Allow permission for operations: Read from hosts: *
	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Current ACLs for resource `Group:PREFIXED:gr`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka9087269612476946936.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8309184149332187367.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Read from hosts: *
	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka1141175465808765899.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka4492820688667369631.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka7127417062223263778.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka4842878770720273118.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=topic2, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:topic2`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=topic2, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:topic2`: 
 	User:client has Allow permission for operations: Read from hosts: *
	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka5625345472493495489.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka5120046464615928618.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=READ, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=*, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:client has Allow permission for operations: Write from hosts: *
	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Read from hosts: *
	User:kafka has Allow permission for operations: Read from hosts: *
	User:client has Allow permission for operations: Create from hosts: * 

Current ACLs for resource `Group:LITERAL:*`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8065847533733093202.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka5718433880170346361.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka2477903581865723182.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka2468726472607360276.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka5732682815670095435.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Create from hosts: * 

[2019-11-14 00:06:46,746] ERROR [Consumer clientId=consumer-group-72, groupId=group] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka2013270490890764303.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka2326244927512378494.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8457104099673983066.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka2302048904671721331.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka6086036635542397633.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8629762638726138051.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-11-14 00:07:19,447] ERROR [Producer clientId=producer-178] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
[2019-11-14 00:07:19,472] ERROR [Consumer clientId=consumer-group-74, groupId=group] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=topic2, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:topic2`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=topic2, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:topic2`: 
 	User:client has Allow permission for operations: Read from hosts: *
	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

[2019-11-14 00:07:20,026] ERROR [Producer clientId=producer-178] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
[2019-11-14 00:07:20,128] ERROR [Producer clientId=producer-178] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Read from hosts: *
	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

[2019-11-14 00:07:21,067] ERROR [Consumer clientId=consumer-group-74, groupId=group] Topic authorization failed for topics [e2etopic] (org.apache.kafka.clients.Metadata:283)
Adding ACLs for resource `ResourcePattern(resourceType=CLUSTER, name=kafka-cluster, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=CLUSTER_ACTION, permissionType=ALLOW) 

Current ACLs for resource `Cluster:LITERAL:kafka-cluster`: 
 	User:kafka has Allow permission for operations: ClusterAction from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`: 
 	(principal=User:kafka, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:*`: 
 	User:kafka has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka3830268151830041256.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=CREATE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=WRITE, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=e2etopic, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=DESCRIBE, permissionType=ALLOW)
	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Adding ACLs for resource `ResourcePattern(resourceType=GROUP, name=group, patternType=LITERAL)`: 
 	(principal=User:client, host=*, operation=READ, permissionType=ALLOW) 

Current ACLs for resource `Topic:LITERAL:e2etopic`: 
 	User:client has Allow permission for operations: Read from hosts: *
	User:client has Allow permission for operations: Describe from hosts: *
	User:client has Allow permission for operations: Create from hosts: *
	User:client has Allow permission for operations: Write from hosts: * 

Current ACLs for resource `Group:LITERAL:group`: 
 	User:client has Allow permission for operations: Read from hosts: * 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka3967813509415878539.tmp refreshKrb5Config is false principal is client@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client@EXAMPLE.COM
Will use keytab
Commit Succeeded {noformat}",,bbejeck,cadonna,chia7712,mjsax,,,,,,,,,,,,,,,,,KAFKA-9190,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-12-02 12:23:31.027,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 14 06:04:08 UTC 2020,,,,,,,"0|z08m80:",9223372036854775807,,,,,,,,,,,,,,,,"02/Dec/19 12:23;cadonna;https://builds.apache.org/job/kafka-pr-jdk11-scala2.12/9656/testReport/junit/kafka.api/SaslPlainSslEndToEndAuthorizationTest/testNoDescribeProduceOrConsumeWithoutTopicDescribeAcl/","09/Apr/20 17:55;mjsax;This test failed again with a different error message ([https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/5711/testReport/junit/kafka.api/SaslGssapiSslEndToEndAuthorizationTest/testNoDescribeProduceOrConsumeWithoutTopicDescribeAcl/])
{quote}org.apache.kafka.common.protocol.types.SchemaException: Error reading field 'responses': Error reading array of size 65542, only 6 bytes available at org.apache.kafka.common.protocol.types.Schema.read(Schema.java:110) at org.apache.kafka.common.protocol.ApiKeys.parseResponse(ApiKeys.java:319) at org.apache.kafka.clients.NetworkClient.parseStructMaybeUpdateThrottleTimeMetrics(NetworkClient.java:725) at org.apache.kafka.clients.NetworkClient.handleCompletedReceives(NetworkClient.java:839) at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:558) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233) at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1297) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1237) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1205) at kafka.utils.TestUtils$.pollUntilAtLeastNumRecords(TestUtils.scala:795) at kafka.utils.TestUtils$.consumeRecords(TestUtils.scala:1354) at kafka.api.EndToEndAuthorizationTest.consumeRecords(EndToEndAuthorizationTest.scala:537) at kafka.api.EndToEndAuthorizationTest.consumeRecordsIgnoreOneAuthorizationException(EndToEndAuthorizationTest.scala:556) at kafka.api.EndToEndAuthorizationTest.testNoDescribeProduceOrConsumeWithoutTopicDescribeAcl(EndToEndAuthorizationTest.scala:376){quote}","14/Apr/20 06:04;chia7712;I'm writing test case for KAFKA-9854 and then notices this issue. Maybe this is related to KAFKA-9854 that the order of parsing responses is broken by re-authentication.",,,,,,,,,,,,,,,,,,,
Order-Preserving Mirror Maker Testcase,KAFKA-976,12658076,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Won't Fix,jfung,guozhang,guozhang,16/Jul/13 18:24,08/Apr/20 19:23,12/Jan/21 11:54,08/Apr/20 19:23,,,,,,,,,,,,,,0,,,,"A new testcase (5007) for mirror_maker_testsuite is needed for the key-dependent order-preserving mirror maker, this is related to KAFKA-957.",,granders,guozhang,jfung,junrao,sliebau,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/13 21:51;jfung;kafka-976-v1.patch;https://issues.apache.org/jira/secure/attachment/12592637/kafka-976-v1.patch",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2013-07-16 21:56:01.966,,,false,,,,,,,,,,,,,,,,,338270,,,Wed Apr 08 19:23:03 UTC 2020,,,,,,,"0|i1mcg7:",338590,,,,,,,,,,,,,,,,"16/Jul/13 21:56;jfung;Uploaded kafka-976-v1.patch to add testcase_5007. Please do the following to run the test case:

1. Check out latest 0.8 branch
2. Apply KAFKA-957.v1.patch
3. Apply KAFKA-967.v2.patch
4. Apply kafka-976-v1.patch
5. Build kafka
6. Edit <kafka>/config/log4j.properties to uncomment the following 2 lines:
    #log4j.logger.kafka.perf=DEBUG, kafkaAppender
    #log4j.logger.kafka.perf.ProducerPerformance$ProducerThread=DEBUG, kafkaAppender
7. Edit <kafka>/system_test/testcase_to_run.json to the following:
{
    ""MirrorMakerTest""   : [
        ""testcase_5007""
    ]
}

8. Execute the test under <kafka>/system_test :

$ python -u -B system_test_runner.py 2>&1 | tee system_test_output.log","16/Jul/13 22:11;guozhang;+1, passed test with the above steps.","13/May/15 00:48;granders;Just making sure you're aware of work we're doing at Confluent on system tests. I'll be posting a KIP for this soon, but here's some info:

The original plan is sketched here:
https://cwiki.apache.org/confluence/display/KAFKA/System+Test+Improvements

This is the core library/test framework (WIP) which aids in writing and running the tests
https://github.com/confluentinc/ducktape/

This has system tests we've written to date for the Confluent Platform
https://github.com/confluentinc/muckrake","19/May/15 16:18;junrao;The current patch is too old.","01/Apr/20 12:07;sliebau;This has lain dormant for a long time now and was apparently (based on [~junrao]'s comment, haven't looked at the code) too old five years ago.
Also as MM 2.0 has now been released I think we can close this. I will do so in a few days if no one complains.","08/Apr/20 19:23;sliebau;As this has been dormant for a long time and no one reacted to my comment I'll close this for now.",,,,,,,,,,,,,,,,
"java kafka-client use ""props.put(""retries"", ""5"")"" ,why print 2 log ? Should 6 log !",KAFKA-9822,13296332,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,startjava,startjava,05/Apr/20 03:52,08/Apr/20 04:53,12/Jan/21 11:54,,,,,,,,,,,,,,,0,,,,"package test2;package test2;
import java.util.Properties;
import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;
public class ProduceMessage { public static void main(String[] args) {

Properties props = new Properties();

props.put(""bootstrap.servers"", ""192.168.1.113:9091"");/////////////wrong ip address

props.put(""acks"", ""1"");

props.put(""retries"", ""5"");

props.put(""key.serializer"", ""org.apache.kafka.common.serialization.StringSerializer""); props.put(""value.serializer"", ""org.apache.kafka.common.serialization.StringSerializer"");


 Producer<String, String> producer = new KafkaProducer<>(props);

for (int i = 0; i < 1; i++) {

producer.send(new ProducerRecord<String, String>(""myTopic1"", ""key"" + (i + 1), ""value"" + (i + 1))); } producer.close(); }}

 

console print result:

[kafka-producer-network-thread | producer-1] WARN org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node -1 (/192.168.1.113:9091) could not be established. Broker may not be available.
 [kafka-producer-network-thread | producer-1] WARN org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Bootstrap broker 192.168.1.113:9091 (id: -1 rack: null) disconnected
 [kafka-producer-network-thread | producer-1] WARN org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node -1 (/192.168.1.113:9091) could not be established. Broker may not be available.
 [kafka-producer-network-thread | producer-1] WARN org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Bootstrap broker 192.168.1.113:9091 (id: -1 rack: null) disconnected
 [main] INFO org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.

 

 

log4j.properties:

log4j.rootLogger=INFO,console
 log4j.logger.com.demo.kafka=DEBUG,kafka
 log4j.appender.kafka=kafka.producer.KafkaLog4jAppender
 log4j.appender.console=org.apache.log4j.ConsoleAppender
 log4j.appender.console.target=System.out
 log4j.appender.console.layout=org.apache.log4j.PatternLayout
 log4j.appender.console.layout.ConversionPattern=%d [%-5p] [%t] - [%l] %m%n",,startjava,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-04-08 04:53:16.446,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 04:53:16 UTC 2020,,,,,,,"0|z0daeg:",9223372036854775807,,,,,,,,,,,,,,,,"08/Apr/20 04:53;cricket007;If you are looking for support, this question would be better suited for StackOverflow.

 

Besides, You have failed to mention your OS, Java version, Kafka version, etc.",,,,,,,,,,,,,,,,,,,,,
Flaky Test ResetConsumerGroupOffsetTest#testResetOffsetsToLatest,KAFKA-9799,13295566,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,mjsax,mjsax,01/Apr/20 15:47,01/Apr/20 15:47,12/Jan/21 11:54,,,,,,,,,,,tools,unit tests,,,0,flaky-test,,,"[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1526/testReport/junit/kafka.admin/ResetConsumerGroupOffsetTest/testResetOffsetsToLatest/]
{quote}org.scalatest.exceptions.TestFailedException: Expected that consumer group has consumed all messages from topic/partition. Expected offset: 100. Actual offset: 0 at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530) at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529) at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1389) at org.scalatest.Assertions.fail(Assertions.scala:1091) at org.scalatest.Assertions.fail$(Assertions.scala:1087) at org.scalatest.Assertions$.fail(Assertions.scala:1389) at kafka.admin.ResetConsumerGroupOffsetTest.awaitConsumerProgress(ResetConsumerGroupOffsetTest.scala:478) at kafka.admin.ResetConsumerGroupOffsetTest.produceConsumeAndShutdown(ResetConsumerGroupOffsetTest.scala:465) at kafka.admin.ResetConsumerGroupOffsetTest.testResetOffsetsToLatest(ResetConsumerGroupOffsetTest.scala:237){quote}",,chia7712,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-04-01 15:47:02.0,,,,,,,"0|z0d67k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"if one machine has four broker, i want three is cluster ,how do ?",KAFKA-9789,13295162,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Not A Bug,,startjava,startjava,31/Mar/20 06:29,31/Mar/20 09:14,12/Jan/21 11:54,31/Mar/20 09:14,,,,,,,,,,,,,,0,,,,"if  me has one machine has four broker, i want three broker is cluster ,how do ?",,sliebau,startjava,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-03-31 09:14:14.663,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 31 09:14:14 UTC 2020,,,,,,,"0|z0d3q0:",9223372036854775807,,,,,,,,,,,,,,,,"31/Mar/20 09:14;sliebau;Hi [~startjava], 

this jira is to track bugs related to Kafka, for questions on how to use it, please post to the [Kafka Users mailing list|https://lists.apache.org/list.html?users@kafka.apache.org]",,,,,,,,,,,,,,,,,,,,,
MetricsTest.testMetricsLeak is flaky,KAFKA-5889,13102126,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Won't Fix,,yuzhihong@gmail.com,yuzhihong@gmail.com,14/Sep/17 03:01,30/Mar/20 08:02,12/Jan/21 11:54,30/Mar/20 08:02,,,,,,,,,,unit tests,,,,0,,,,"The following appeared in several recent builds (e.g. https://builds.apache.org/job/kafka-trunk-jdk7/2758) :
{code}
kafka.metrics.MetricsTest > testMetricsLeak FAILED
    java.lang.AssertionError: expected:<1216> but was:<1219>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:834)
        at org.junit.Assert.assertEquals(Assert.java:645)
        at org.junit.Assert.assertEquals(Assert.java:631)
        at kafka.metrics.MetricsTest$$anonfun$testMetricsLeak$1.apply$mcVI$sp(MetricsTest.scala:68)
        at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
        at kafka.metrics.MetricsTest.testMetricsLeak(MetricsTest.scala:66)
{code}",,chia7712,guozhang,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-10-04 19:22:11.252,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 30 08:02:59 UTC 2020,,,,,,,"0|i3k1of:",9223372036854775807,,,,,,,,,,,,,,,,"22/Sep/17 20:13;yuzhihong@gmail.com;As of commit 125d8d6f70829b9a0dbeabfef8f6b2df438dc12b , I got:
{code}
kafka.metrics.MetricsTest > testMetricsLeak FAILED
    java.lang.AssertionError: expected:<971> but was:<974>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:834)
        at org.junit.Assert.assertEquals(Assert.java:645)
        at org.junit.Assert.assertEquals(Assert.java:631)
        at kafka.metrics.MetricsTest$$anonfun$testMetricsLeak$1.apply$mcVI$sp(MetricsTest.scala:68)
{code}","02/Oct/17 19:54;yuzhihong@gmail.com;https://builds.apache.org/job/kafka-trunk-jdk7/2836/display/redirect?page=changes
{code}
kafka.metrics.MetricsTest > testMetricsLeak FAILED
    java.lang.AssertionError: expected:<1594> but was:<1597>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:834)
        at org.junit.Assert.assertEquals(Assert.java:645)
        at org.junit.Assert.assertEquals(Assert.java:631)
        at kafka.metrics.MetricsTest$$anonfun$testMetricsLeak$1.apply$mcVI$sp(MetricsTest.scala:68)
        at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
        at kafka.metrics.MetricsTest.testMetricsLeak(MetricsTest.scala:66)
{code}","04/Oct/17 19:22;guozhang;Thanks for reporting this Ted. cc [~bbejeck] for this week.","24/Oct/17 17:07;yuzhihong@gmail.com;From https://builds.apache.org/job/kafka-trunk-jdk8/2175:
{code}
kafka.metrics.MetricsTest > testMetricsLeak FAILED
    java.lang.AssertionError: expected:<1694> but was:<1697>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:834)
        at org.junit.Assert.assertEquals(Assert.java:645)
        at org.junit.Assert.assertEquals(Assert.java:631)
        at kafka.metrics.MetricsTest$$anonfun$testMetricsLeak$1.apply$mcVI$sp(MetricsTest.scala:68)
        at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
        at kafka.metrics.MetricsTest.testMetricsLeak(MetricsTest.scala:66)
{code}","03/Nov/17 02:46;yuzhihong@gmail.com;https://builds.apache.org/job/kafka-trunk-jdk7/2942 :
{code}
kafka.metrics.MetricsTest > testMetricsLeak FAILED
    java.lang.AssertionError: expected:<1694> but was:<1697>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:834)
        at org.junit.Assert.assertEquals(Assert.java:645)
        at org.junit.Assert.assertEquals(Assert.java:631)
        at kafka.metrics.MetricsTest$$anonfun$testMetricsLeak$1.apply$mcVI$sp(MetricsTest.scala:68)
        at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
        at kafka.metrics.MetricsTest.testMetricsLeak(MetricsTest.scala:66)
{code}","06/Feb/18 18:46;yuzhihong@gmail.com;As of 332e698ac9c74ce29317021b03a54512c92ac8b3 , I got:
{code}
kafka.metrics.MetricsTest > testMetricsLeak FAILED
    java.lang.AssertionError: expected:<1421> but was:<1424>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:834)
        at org.junit.Assert.assertEquals(Assert.java:645)
        at org.junit.Assert.assertEquals(Assert.java:631)
        at kafka.metrics.MetricsTest$$anonfun$testMetricsLeak$1.apply$mcVI$sp(MetricsTest.scala:68)
        at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
        at kafka.metrics.MetricsTest.testMetricsLeak(MetricsTest.scala:66)
{code}","10/Mar/18 17:52;yuzhihong@gmail.com;From https://builds.apache.org/job/kafka-trunk-jdk9/462/testReport/kafka.api/MetricsTest/testMetrics/ :
{code}
java.lang.AssertionError: Broker metric not recorded correctly for kafka.network:type=RequestMetrics,name=MessageConversionsTimeMs,request=Produce value 0.0
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at kafka.api.MetricsTest.verifyYammerMetricRecorded(MetricsTest.scala:289)
	at kafka.api.MetricsTest.verifyBrokerMessageConversionMetrics(MetricsTest.scala:198)
	at kafka.api.MetricsTest.testMetrics(MetricsTest.scala:93)
{code}","30/Mar/20 08:02;chia7712;MetricsTest.testMetricsLeak was removed by https://github.com/apache/kafka/commit/7132a85fc394bc0627fe1763c17cb523d8a8ff37",,,,,,,,,,,,,,
use kafka-topics.sh can't --alter replication-factor???,KAFKA-9785,13294912,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,startjava,startjava,30/Mar/20 07:18,30/Mar/20 07:45,12/Jan/21 11:54,,,,,,,,,,,,,,,0,,,,"ghy@ghy-VirtualBox:~/T/k/bin$ ./kafka-topics.sh --alter --topic my3 --bootstrap-server localhost:9081 --replication-factor 4 --partitions 3
Option ""[replication-factor]"" can't be used with option ""[alter]""

 

show bottom info !!

 

Can update the documentin in the new kafka *.sh, now show old kafak doc info.....",,showuon,startjava,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-03-30 07:23:16.343,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 30 07:45:15 UTC 2020,,,,,,,"0|z0d26w:",9223372036854775807,,,,,,,,,,,,,,,,"30/Mar/20 07:23;showuon;I will update the doc. Thanks.","30/Mar/20 07:32;startjava;[~showuon]  i question how do it ok???use what command ?","30/Mar/20 07:40;startjava;[~showuon] 小陈会说中文吧，我现在非常头痛的是，kafka2.4.1已经发官网发布最新的版本了，由于我是初学KAFKA的学习者，学习的过程中全是坑，，，执行sh脚本出现的help文本有的全是旧版，，新版本的或许使用其它新的命令实现了，但旧的文档信息还是在sh help的提示中出现，能不能再发版的时候，把sh help的文档也同步了，这样对我们初学者来讲是非常友好的","30/Mar/20 07:44;showuon;[~startjava], please check below link for how to increasing replication factor. In short, use _{{bin}}{{/kafka-reassign-partitions}}{{.sh}}_ instead. Thanks.

https://kafka.apache.org/documentation/#basic_ops_increase_replication_factor","30/Mar/20 07:45;showuon;好的! Will do!",,,,,,,,,,,,,,,,,
"Option combination ""[bootstrap-server],[config]"" can't be used with option ""[alter]""",KAFKA-9773,13294410,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,startjava,startjava,27/Mar/20 09:19,27/Mar/20 10:27,12/Jan/21 11:54,,,,,,,,,,,,,,,0,,,,"ghy@ghy-VirtualBox:~/T/k/bin$ ./kafka-topics.sh --topic my3 --bootstrap-server localhost:9081,localhost:9082,localhost:9083 --alter --config max.message.bytes=20480
Option combination ""[bootstrap-server],[config]"" can't be used with option ""[alter]""

 

use kafka2.4.1 version bottom error!

how do not show bottom error ?

 

 ",,showuon,startjava,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-03-27 10:20:39.87,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 27 10:27:19 UTC 2020,,,,,,,"0|z0cz3c:",9223372036854775807,,,,,,,,,,,,,,,,"27/Mar/20 10:20;showuon;Please use kafka-config.sh --alter instead.

Ex: 
{code:java}
kafka-configs.sh --zookeeper localhost:2181 --entity-type topics --entity-name my-topic --alter --add-config max.message.bytes=128000
{code}
 ","27/Mar/20 10:27;showuon;Meanwhile, I'll work on this issue.",,,,,,,,,,,,,,,,,,,,
Flaky Test kafka.admin.DescribeConsumerGroupTest.testDescribeGroupWithShortInitializationTimeout,KAFKA-9530,13284170,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,bbejeck,bbejeck,09/Feb/20 21:13,23/Feb/20 18:38,12/Jan/21 11:54,23/Feb/20 18:16,,,,,,,,,,core,,,,0,flaky-test,test,,"[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/4570/testReport/junit/kafka.admin/DescribeConsumerGroupTest/testDescribeGroupWithShortInitializationTimeout/]

 
{noformat}
Error Messagejava.lang.AssertionError: assertion failedStacktracejava.lang.AssertionError: assertion failed
	at scala.Predef$.assert(Predef.scala:267)
	at kafka.admin.DescribeConsumerGroupTest.testDescribeGroupWithShortInitializationTimeout(DescribeConsumerGroupTest.scala:585)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at jdk.internal.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)
	at jdk.internal.reflect.GeneratedMethodAccessor27.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.base/java.lang.Thread.run(Thread.java:834)
Standard OutputGROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID
test.group      foo             0          0               0               0               -               -               -

GROUP               TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID
test.group--offsets foo             0          0               0               0               -               -               -

GROUP                     COORDINATOR (ID)          ASSIGNMENT-STRATEGY  STATE           #MEMBERS
test.group--state         localhost:45389 (0)                            Empty           0{noformat}",,bbejeck,enether,githubbot,mjsax,,,,,,,,,,,,,,,,,,KAFKA-9541,,,KAFKA-9541,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-02-11 22:01:49.418,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 23 18:16:37 UTC 2020,,,,,,,"0|z0bavk:",9223372036854775807,,,,,,,,,,,,,,,,"11/Feb/20 22:01;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/630/testReport/junit/kafka.admin/DescribeConsumerGroupTest/testDescribeGroupWithShortInitializationTimeout/]","12/Feb/20 20:26;mjsax;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/4672/testReport/junit/kafka.admin/DescribeConsumerGroupTest/testDescribeGroupWithShortInitializationTimeout/]","12/Feb/20 20:36;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/659/testReport/junit/kafka.admin/DescribeConsumerGroupTest/testDescribeGroupWithShortInitializationTimeout/]","18/Feb/20 11:49;enether;I managed to reproduce the failure with 1 run (out of 96) locally - the issue is that DisconnectException gets raised:
{code:java}
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.DisconnectException: Cancelled findCoordinator request with correlation id 3 due to node 0 being disconnected
....
kafka.admin.ConsumerGroupCommand$ConsumerGroupService.$anonfun$describeConsumerGroups$1(ConsumerGroupCommand.scala:497)
...
kafka.admin.ConsumerGroupCommand$ConsumerGroupService.describeConsumerGroups(ConsumerGroupCommand.scala:496)
...
kafka.admin.ConsumerGroupCommand$ConsumerGroupService.collectGroupsMembers(ConsumerGroupCommand.scala:552)
kafka.admin.ConsumerGroupCommand$ConsumerGroupService.describeGroups(ConsumerGroupCommand.scala:318)
kafka.admin.DescribeConsumerGroupTest.testDescribeGroupWithShortInitializationTimeout(DescribeConsumerGroupTest.scala:582){code}

-----------
It seems like the AdminClient handles timed out calls by disconnecting ([https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L1075]) - this adds a disconnected flag to the response which  we then handle via a DisconnectedException ([https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L1126])","18/Feb/20 19:27;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/736/testReport/junit/kafka.admin/DescribeConsumerGroupTest/testDescribeGroupWithShortInitializationTimeout/]
{quote}java.lang.AssertionError: expected:<class org.apache.kafka.common.errors.TimeoutException> but was:<class org.apache.kafka.common.errors.DisconnectException> at org.junit.Assert.fail(Assert.java:89) at org.junit.Assert.failNotEquals(Assert.java:835) at org.junit.Assert.assertEquals(Assert.java:120) at org.junit.Assert.assertEquals(Assert.java:146) at kafka.admin.DescribeConsumerGroupTest.testDescribeGroupWithShortInitializationTimeout(DescribeConsumerGroupTest.scala:585){quote}","21/Feb/20 21:59;githubbot;hachikuji commented on pull request #8154: KAFKA-9530; Fix flaky test `testDescribeGroupWithShortInitializationTimeout`
URL: https://github.com/apache/kafka/pull/8154
 
 
   This should fix the flakiness with this test case. With a short timeout, the call may fail and the client might disconnect. Rather than exposing the underlying retriable error, we give the user a TimeoutException.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","23/Feb/20 18:16;githubbot;hachikuji commented on pull request #8154: KAFKA-9530; Fix flaky test `testDescribeGroupWithShortInitializationTimeout`
URL: https://github.com/apache/kafka/pull/8154
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,
Run some system tests using TLSv1.3,KAFKA-9319,13275514,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nizhikov,rsivaram,rsivaram,19/Dec/19 14:17,19/Feb/20 09:25,12/Jan/21 11:54,19/Feb/20 09:25,,,,,2.5.0,,,,,,,,,0,,,,"KAFKA-7251 enables TLSv1.3 for Kafka. We should get some system tests to run using TLSv1.3. Since TLSv1.3 is only supported from Java 11 onwards, we need a system test build that runs with JDK 11 to enable these tests.",,githubbot,nizhikov,rajinisivaram@gmail.com,rsivaram,SumitAgrawal117,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-01-29 12:28:08.104,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 19 09:25:17 UTC 2020,,,,,,,"0|z09uhk:",9223372036854775807,,,,,,,,,,,,,,,,"29/Jan/20 12:28;nizhikov;Tests with SSL enabled:
{noformat}
./test/kafkatest/services/kafka_log4j_appender.py
./test/kafkatest/services/listener_security_config.py
./test/kafkatest/services/security/security_config.py
./test/kafkatest/tests/core/security_test.py
{noformat}","29/Jan/20 16:05;rajinisivaram@gmail.com;Hi Nikolay,

I don't think that is the full list. The ones under `services` are not real
tests, they are classes used by tests. Any test that uses SSL in any form
needs to be run. e.g. replication_test uses SASL_SSL.

On Wed, Jan 29, 2020 at 12:29 PM Nikolay Izhikov (Jira) <jira@apache.org>

","04/Feb/20 11:18;nizhikov;Hello, [~rsivaram].

Sorry, for the first comment. It's obviously wrong :)
Now, I filtered tests with the ""security"" keyword(SecurityConfig, security_protocol, etc.) and got the following lists.
Is it enough to check this list with TLS1.3?

{noformat}
[13:45:51]~/src/kafka/tests:[trunk]$ egrep -irl ""security"" . | grep -v pyc
./kafkatest/tests/tools/log_compaction_test.py
./kafkatest/tests/tools/log4j_appender_test.py
./kafkatest/tests/tools/replica_verification_test.py
./kafkatest/tests/core/transactions_test.py
./kafkatest/tests/core/mirror_maker_test.py
./kafkatest/tests/core/security_test.py
./kafkatest/tests/core/security_rolling_upgrade_test.py
./kafkatest/tests/core/delegation_token_test.py
./kafkatest/tests/core/zookeeper_security_upgrade_test.py
./kafkatest/tests/core/consumer_group_command_test.py
./kafkatest/tests/core/log_dir_failure_test.py
./kafkatest/tests/core/throttling_test.py
./kafkatest/tests/core/replication_test.py
./kafkatest/tests/core/upgrade_test.py
./kafkatest/tests/core/get_offset_shell_test.py
./kafkatest/tests/core/downgrade_test.py
./kafkatest/tests/connect/connect_distributed_test.py
./kafkatest/tests/connect/connect_test.py
./kafkatest/tests/client/compression_test.py
./kafkatest/tests/client/quota_test.py
./kafkatest/tests/client/client_compatibility_produce_consume_test.py
{noformat}
","06/Feb/20 10:12;rsivaram;[~nizhikov] Looks good. I think there are some tests under kafkatest/sanity_checks and kafka_test/benchmarks as well. It will be good to run them too.","13/Feb/20 15:16;githubbot;nizhikov commented on pull request #8106: KAFKA-9319: Fix generation of CA certificate for system tests.
URL: https://github.com/apache/kafka/pull/8106
 
 
   I perform system tests check to ensure that we can enable only `TLSv1.3` by default.
   
   I've found two issues:
   
     1. CA certificate that is generated in `security_config.py` can't be validated by the openjdk11, therefore, tests with SSL enabled failed. (Error message is ""TrustAnchor with subject ""CN=SystemTestCA"" is not a CA certificate"")
     2. The actual stack trace of the fail is hidden when the `ConfigException` stack trace printed.
   
   This PR fixes those 2 issues:
      * ` --ext bc=ca:true` param for `keytool` added.
      * SSL Validation exception printed in error log.
   
   [Keytool documentation](https://docs.oracle.com/en/java/javase/11/tools/keytool.html)
   
   >Supported Named Extensions
   > The keytool command supports these named extensions. The names aren't case-sensitive.
   > BC or BasicContraints
   > Values:
   > The full form is ca:{true|false}[,pathlen:len] or len, which is short for ca:true,pathlen:len.
   > When len is omitted, the resulting value is ca:true. 
   
   Command to run tests(openjdk11 used):
   ```
   export tests=""tests/kafkatest/tests/connect/connect_distributed_test.py""
   TC_PATHS=""$tests"" bash tests/docker/run_tests.sh
   ```
   
   java version in a docker container:
   ```
   [nizhikov@sbt-qa-01 kafka]$ docker exec -it ducker04 bash
   ducker@ducker04:/$ java -version
   openjdk version ""11.0.6"" 2020-01-14
   OpenJDK Runtime Environment 18.9 (build 11.0.6+10)
   OpenJDK 64-Bit Server VM 18.9 (build 11.0.6+10, mixed mode)
   ```
   
   Exception in tests *without* new `ext` parameter:
   
   ```
   [2020-02-13 10:17:46,244] DEBUG Created SSL context with keystore SecurityStore(path=/mnt/security/test.keystore.jks, modificationTime=Thu Feb 13 10:17:43 UTC 2020), truststore SecurityStore(path=/mnt/security/test.truststore.jks, modificationTime=Thu Feb 13 10:17:41 UTC 2020), provider SunJSSE. (org.apache.kafka.common.security.ssl.SslEngineBuilder)
   javax.net.ssl.SSLHandshakeException: PKIX path validation failed: sun.security.validator.ValidatorException: TrustAnchor with subject ""CN=SystemTestCA"" is not a CA certificate
           at java.base/sun.security.ssl.Alert.createSSLException(Alert.java:131)
           at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:320)
           at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:263)
           at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:258)
           at java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.checkServerCerts(CertificateMessage.java:1332)
           at java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.onConsumeCertificate(CertificateMessage.java:1207)
           at java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.consume(CertificateMessage.java:1150)
           at java.base/sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:392)
           at java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:443)
           at java.base/sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1061)
           at java.base/sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1048)
           at java.base/java.security.AccessController.doPrivileged(Native Method)
           at java.base/sun.security.ssl.SSLEngineImpl$DelegatedTask.run(SSLEngineImpl.java:995)
           at org.apache.kafka.common.security.ssl.SslFactory$SslEngineValidator.handshake(SslFactory.java:360)
           at org.apache.kafka.common.security.ssl.SslFactory$SslEngineValidator.validate(SslFactory.java:301)
           at org.apache.kafka.common.security.ssl.SslFactory$SslEngineValidator.validate(SslFactory.java:282)
           at org.apache.kafka.common.security.ssl.SslFactory.configure(SslFactory.java:98)
           at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:168)
           at org.apache.kafka.common.network.ChannelBuilders.create(ChannelBuilders.java:157)
           at org.apache.kafka.common.network.ChannelBuilders.serverChannelBuilder(ChannelBuilders.java:97)
           at kafka.network.Processor.<init>(SocketServer.scala:724)
           at kafka.network.SocketServer.newProcessor(SocketServer.scala:367)
           at kafka.network.SocketServer.$anonfun$addDataPlaneProcessors$1(SocketServer.scala:252)
           at kafka.network.SocketServer.addDataPlaneProcessors(SocketServer.scala:251)
           at kafka.network.SocketServer.$anonfun$createDataPlaneAcceptorsAndProcessors$1(SocketServer.scala:214)
           at kafka.network.SocketServer.$anonfun$createDataPlaneAcceptorsAndProcessors$1$adapted(SocketServer.scala:211)
           at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
           at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
           at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
           at kafka.network.SocketServer.createDataPlaneAcceptorsAndProcessors(SocketServer.scala:211)
           at kafka.network.SocketServer.startup(SocketServer.scala:122)
           at kafka.server.KafkaServer.startup(KafkaServer.scala:242)
           at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:44)
           at kafka.Kafka$.main(Kafka.scala:82)
           at kafka.Kafka.main(Kafka.scala)
   Caused by: sun.security.validator.ValidatorException: PKIX path validation failed: sun.security.validator.ValidatorException: TrustAnchor with subject ""CN=SystemTestCA"" is not a CA certificate
           at java.base/sun.security.validator.PKIXValidator.doValidate(PKIXValidator.java:369)
           at java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:263)
           at java.base/sun.security.validator.Validator.validate(Validator.java:264)
           at java.base/sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:313)
           at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:276)
           at java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:141)
           at java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.checkServerCerts(CertificateMessage.java:1310)
           ... 30 more
   Caused by: sun.security.validator.ValidatorException: TrustAnchor with subject ""CN=SystemTestCA"" is not a CA certificate
           at java.base/sun.security.validator.PKIXValidator.verifyTrustAnchor(PKIXValidator.java:393)
           at java.base/sun.security.validator.PKIXValidator.toArray(PKIXValidator.java:333)
           at java.base/sun.security.validator.PKIXValidator.doValidate(PKIXValidator.java:366)
           ... 36 more
   ```
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","13/Feb/20 15:19;nizhikov;Hello, [~rsivaram].

I found two issues with system tests when only TLSv1.3 enabled:

1. CA certificate that is generated in `security_config.py` can't be validated by the openjdk11, therefore, tests with SSL enabled failed. (Error message is ""TrustAnchor with subject ""CN=SystemTestCA"" is not a CA certificate"")
2. The actual stack trace of the fail is hidden when the `ConfigException` stack trace printed.

I fixed both of them and raised a PR - https://github.com/apache/kafka/pull/8106
A detailed explanation of the issues in the PR description.

Can you, please, take a look?

","14/Feb/20 11:25;nizhikov;{noformat}
====================================================================================================
SESSION REPORT (ALL TESTS)
ducktape version: 0.7.6
session_id:       2020-02-13--004
run time:         74 minutes 3.548 seconds
tests run:        60
passed:           43
failed:           17
ignored:          0
====================================================================================================
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_bounce.connect_protocol=compatible.clean=False
status:     FAIL
run time:   1 minute 21.046 seconds


    Kafka Connect failed to start on node: ducker@ducker04 in condition mode: LISTEN
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 364, in test_bounce
    self.cc.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/connect.py"", line 119, in start
    super(ConnectServiceBase, self).start()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/connect.py"", line 359, in start_node
    self.start_and_wait_to_start_listening(node, 'distributed', '')
  File ""/opt/kafka-dev/tests/kafkatest/services/connect.py"", line 137, in start_and_wait_to_start_listening
    (str(node.account), self.startup_mode))
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka Connect failed to start on node: ducker@ducker04 in condition mode: LISTEN

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_bounce.connect_protocol=compatible.clean=True
status:     FAIL
run time:   1 minute 14.514 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 362, in test_bounce
    self.setup_services()
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 95, in setup_services
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_bounce.connect_protocol=eager.clean=False
status:     FAIL
run time:   1 minute 42.762 seconds


    Kafka Connect failed to start on node: ducker@ducker05 in condition mode: LISTEN
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 364, in test_bounce
    self.cc.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/connect.py"", line 119, in start
    super(ConnectServiceBase, self).start()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/connect.py"", line 359, in start_node
    self.start_and_wait_to_start_listening(node, 'distributed', '')
  File ""/opt/kafka-dev/tests/kafkatest/services/connect.py"", line 137, in start_and_wait_to_start_listening
    (str(node.account), self.startup_mode))
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka Connect failed to start on node: ducker@ducker05 in condition mode: LISTEN

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_bounce.connect_protocol=eager.clean=True
status:     PASS
run time:   4 minutes 24.457 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_bounce.connect_protocol=sessioned.clean=False
status:     PASS
run time:   4 minutes 45.534 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_bounce.connect_protocol=sessioned.clean=True
status:     PASS
run time:   4 minutes 34.341 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_file_source_and_sink.security_protocol=PLAINTEXT.connect_protocol=compatible
status:     PASS
run time:   1 minute 54.398 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_file_source_and_sink.security_protocol=PLAINTEXT.connect_protocol=eager
status:     PASS
run time:   59.125 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_file_source_and_sink.security_protocol=PLAINTEXT.connect_protocol=sessioned
status:     PASS
run time:   59.198 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_file_source_and_sink.security_protocol=SASL_SSL.connect_protocol=compatible
status:     PASS
run time:   2 minutes 13.983 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_file_source_and_sink.security_protocol=SASL_SSL.connect_protocol=eager
status:     PASS
run time:   2 minutes 13.432 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_file_source_and_sink.security_protocol=SASL_SSL.connect_protocol=sessioned
status:     PASS
run time:   2 minutes 14.299 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_transformations.connect_protocol=compatible
status:     PASS
run time:   1 minute 1.129 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_transformations.connect_protocol=eager
status:     PASS
run time:   1 minute 3.188 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_transformations.connect_protocol=sessioned
status:     PASS
run time:   59.443 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=0.10.1.1.connect_protocol=compatible.security_protocol=PLAINTEXT
status:     FAIL
run time:   1 minute 12.666 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 554, in test_broker_compatibility
    self.setup_services(broker_version=KafkaVersion(broker_version), auto_create_topics=auto_create_topics, security_protocol=security_protocol)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 95, in setup_services
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=0.10.1.1.connect_protocol=eager.security_protocol=PLAINTEXT
status:     FAIL
run time:   1 minute 12.688 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 554, in test_broker_compatibility
    self.setup_services(broker_version=KafkaVersion(broker_version), auto_create_topics=auto_create_topics, security_protocol=security_protocol)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 95, in setup_services
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=0.10.1.1.connect_protocol=sessioned.security_protocol=PLAINTEXT
status:     FAIL
run time:   1 minute 12.706 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 554, in test_broker_compatibility
    self.setup_services(broker_version=KafkaVersion(broker_version), auto_create_topics=auto_create_topics, security_protocol=security_protocol)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 95, in setup_services
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=0.10.2.2.connect_protocol=compatible.security_protocol=PLAINTEXT
status:     FAIL
run time:   1 minute 12.703 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 554, in test_broker_compatibility
    self.setup_services(broker_version=KafkaVersion(broker_version), auto_create_topics=auto_create_topics, security_protocol=security_protocol)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 95, in setup_services
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=0.10.2.2.connect_protocol=eager.security_protocol=PLAINTEXT
status:     FAIL
run time:   1 minute 12.589 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 554, in test_broker_compatibility
    self.setup_services(broker_version=KafkaVersion(broker_version), auto_create_topics=auto_create_topics, security_protocol=security_protocol)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 95, in setup_services
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=0.10.2.2.connect_protocol=sessioned.security_protocol=PLAINTEXT
status:     FAIL
run time:   1 minute 12.775 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 554, in test_broker_compatibility
    self.setup_services(broker_version=KafkaVersion(broker_version), auto_create_topics=auto_create_topics, security_protocol=security_protocol)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 95, in setup_services
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=0.11.0.3.connect_protocol=compatible.security_protocol=PLAINTEXT
status:     FAIL
run time:   1 minute 12.747 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 554, in test_broker_compatibility
    self.setup_services(broker_version=KafkaVersion(broker_version), auto_create_topics=auto_create_topics, security_protocol=security_protocol)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 95, in setup_services
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=0.11.0.3.connect_protocol=eager.security_protocol=PLAINTEXT
status:     FAIL
run time:   1 minute 12.831 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 554, in test_broker_compatibility
    self.setup_services(broker_version=KafkaVersion(broker_version), auto_create_topics=auto_create_topics, security_protocol=security_protocol)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 95, in setup_services
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=0.11.0.3.connect_protocol=sessioned.security_protocol=PLAINTEXT
status:     FAIL
run time:   1 minute 12.650 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 554, in test_broker_compatibility
    self.setup_services(broker_version=KafkaVersion(broker_version), auto_create_topics=auto_create_topics, security_protocol=security_protocol)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 95, in setup_services
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=1.0.2.connect_protocol=compatible.security_protocol=PLAINTEXT
status:     FAIL
run time:   1 minute 12.642 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 554, in test_broker_compatibility
    self.setup_services(broker_version=KafkaVersion(broker_version), auto_create_topics=auto_create_topics, security_protocol=security_protocol)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 95, in setup_services
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=1.0.2.connect_protocol=eager.security_protocol=PLAINTEXT
status:     FAIL
run time:   1 minute 13.049 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 554, in test_broker_compatibility
    self.setup_services(broker_version=KafkaVersion(broker_version), auto_create_topics=auto_create_topics, security_protocol=security_protocol)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 95, in setup_services
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=1.1.1.connect_protocol=compatible.security_protocol=PLAINTEXT
status:     PASS
run time:   41.845 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=1.1.1.connect_protocol=eager.security_protocol=PLAINTEXT
status:     PASS
run time:   42.772 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=2.0.1.connect_protocol=compatible.security_protocol=PLAINTEXT
status:     PASS
run time:   46.150 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=2.0.1.connect_protocol=eager.security_protocol=PLAINTEXT
status:     PASS
run time:   45.886 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=2.1.1.connect_protocol=compatible.security_protocol=PLAINTEXT
status:     PASS
run time:   46.287 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=2.1.1.connect_protocol=eager.security_protocol=PLAINTEXT
status:     PASS
run time:   43.240 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=2.2.2.connect_protocol=compatible.security_protocol=PLAINTEXT
status:     PASS
run time:   41.715 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=2.2.2.connect_protocol=eager.security_protocol=PLAINTEXT
status:     PASS
run time:   46.272 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=2.3.1.connect_protocol=compatible.security_protocol=PLAINTEXT
status:     PASS
run time:   44.391 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=2.3.1.connect_protocol=eager.security_protocol=PLAINTEXT
status:     PASS
run time:   44.288 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=dev.connect_protocol=compatible.security_protocol=PLAINTEXT
status:     PASS
run time:   42.407 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=dev.connect_protocol=eager.security_protocol=PLAINTEXT
status:     PASS
run time:   46.141 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=False.broker_version=dev.connect_protocol=sessioned.security_protocol=PLAINTEXT
status:     PASS
run time:   40.741 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=True.broker_version=0.10.0.1.connect_protocol=compatible.security_protocol=PLAINTEXT
status:     FAIL
run time:   1 minute 12.698 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 554, in test_broker_compatibility
    self.setup_services(broker_version=KafkaVersion(broker_version), auto_create_topics=auto_create_topics, security_protocol=security_protocol)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 95, in setup_services
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=True.broker_version=0.10.0.1.connect_protocol=eager.security_protocol=PLAINTEXT
status:     FAIL
run time:   1 minute 12.535 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 554, in test_broker_compatibility
    self.setup_services(broker_version=KafkaVersion(broker_version), auto_create_topics=auto_create_topics, security_protocol=security_protocol)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 95, in setup_services
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_broker_compatibility.auto_create_topics=True.broker_version=0.10.0.1.connect_protocol=sessioned.security_protocol=PLAINTEXT
status:     FAIL
run time:   1 minute 12.616 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 554, in test_broker_compatibility
    self.setup_services(broker_version=KafkaVersion(broker_version), auto_create_topics=auto_create_topics, security_protocol=security_protocol)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 95, in setup_services
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_pause_and_resume_sink.connect_protocol=compatible
status:     PASS
run time:   58.382 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_pause_and_resume_sink.connect_protocol=eager
status:     PASS
run time:   56.359 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_pause_and_resume_sink.connect_protocol=sessioned
status:     PASS
run time:   52.496 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_pause_and_resume_source.connect_protocol=compatible
status:     PASS
run time:   51.364 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_pause_and_resume_source.connect_protocol=eager
status:     PASS
run time:   50.820 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_pause_and_resume_source.connect_protocol=sessioned
status:     PASS
run time:   49.994 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_pause_state_persistent.connect_protocol=compatible
status:     PASS
run time:   1 minute 51.728 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_pause_state_persistent.connect_protocol=eager
status:     PASS
run time:   57.258 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_pause_state_persistent.connect_protocol=sessioned
status:     PASS
run time:   54.987 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_restart_failed_connector.connect_protocol=compatible
status:     PASS
run time:   44.422 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_restart_failed_connector.connect_protocol=eager
status:     PASS
run time:   47.049 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_restart_failed_connector.connect_protocol=sessioned
status:     PASS
run time:   43.131 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_restart_failed_task.connector_type=sink.connect_protocol=compatible
status:     PASS
run time:   44.111 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_restart_failed_task.connector_type=sink.connect_protocol=eager
status:     PASS
run time:   47.082 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_restart_failed_task.connector_type=sink.connect_protocol=sessioned
status:     PASS
run time:   46.174 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_restart_failed_task.connector_type=source.connect_protocol=compatible
status:     PASS
run time:   46.173 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_restart_failed_task.connector_type=source.connect_protocol=eager
status:     PASS
run time:   44.354 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_restart_failed_task.connector_type=source.connect_protocol=sessioned
status:     PASS
run time:   44.279 seconds
----------------------------------------------------------------------------------------------------
{noformat}","14/Feb/20 11:26;nizhikov;{noformat}
====================================================================================================
SESSION REPORT (ALL TESTS)
ducktape version: 0.7.6
session_id:       2020-02-13--005
run time:         83 minutes 26.713 seconds
tests run:        29
passed:           22
failed:           7
ignored:          0
====================================================================================================
test_id:    kafkatest.tests.client.client_compatibility_produce_consume_test.ClientCompatibilityProduceConsumeTest.test_produce_consume.broker_version=0.10.0.1
status:     FAIL
run time:   1 minute 34.013 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/client/client_compatibility_produce_consume_test.py"", line 78, in test_produce_consume
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.client_compatibility_produce_consume_test.ClientCompatibilityProduceConsumeTest.test_produce_consume.broker_version=0.10.1.1
status:     FAIL
run time:   1 minute 33.566 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/client/client_compatibility_produce_consume_test.py"", line 78, in test_produce_consume
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.client_compatibility_produce_consume_test.ClientCompatibilityProduceConsumeTest.test_produce_consume.broker_version=0.10.2.2
status:     FAIL
run time:   1 minute 33.735 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/client/client_compatibility_produce_consume_test.py"", line 78, in test_produce_consume
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.client_compatibility_produce_consume_test.ClientCompatibilityProduceConsumeTest.test_produce_consume.broker_version=0.11.0.3
status:     FAIL
run time:   1 minute 33.778 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/client/client_compatibility_produce_consume_test.py"", line 78, in test_produce_consume
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.client_compatibility_produce_consume_test.ClientCompatibilityProduceConsumeTest.test_produce_consume.broker_version=1.0.2
status:     FAIL
run time:   1 minute 33.725 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/client/client_compatibility_produce_consume_test.py"", line 78, in test_produce_consume
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.client_compatibility_produce_consume_test.ClientCompatibilityProduceConsumeTest.test_produce_consume.broker_version=1.1.1
status:     PASS
run time:   2 minutes 39.029 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.client_compatibility_produce_consume_test.ClientCompatibilityProduceConsumeTest.test_produce_consume.broker_version=2.0.1
status:     PASS
run time:   2 minutes 41.430 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.client_compatibility_produce_consume_test.ClientCompatibilityProduceConsumeTest.test_produce_consume.broker_version=2.1.1
status:     PASS
run time:   2 minutes 42.545 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.client_compatibility_produce_consume_test.ClientCompatibilityProduceConsumeTest.test_produce_consume.broker_version=2.2.2
status:     PASS
run time:   2 minutes 40.071 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.client_compatibility_produce_consume_test.ClientCompatibilityProduceConsumeTest.test_produce_consume.broker_version=2.3.1
status:     PASS
run time:   2 minutes 42.532 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.client_compatibility_produce_consume_test.ClientCompatibilityProduceConsumeTest.test_produce_consume.broker_version=2.4.0
status:     PASS
run time:   2 minutes 40.617 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.client_compatibility_produce_consume_test.ClientCompatibilityProduceConsumeTest.test_produce_consume.broker_version=dev
status:     PASS
run time:   2 minutes 41.166 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.compression_test.CompressionTest.test_compressed_topic.compression_types=.snappy.gzip.lz4.zstd.none
status:     PASS
run time:   1 minute 51.993 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_test.ConnectStandaloneFileTest.test_file_source_and_sink.security_protocol=SASL_SSL
status:     PASS
run time:   2 minutes 1.478 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.quota_test.QuotaTest.test_quota.consumer_num=2.quota_type=client-id
status:     PASS
run time:   3 minutes 18.733 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.quota_test.QuotaTest.test_quota.old_broker_throttling_behavior=True.quota_type=client-id
status:     FAIL
run time:   10 minutes 40.414 seconds


    Timed out waiting 600 seconds for service nodes to finish. These nodes are still alive: [u'ProducerPerformanceService-0-140369731991568 node 1 on ducker10']
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/client/quota_test.py"", line 156, in test_quota
    producer.run()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 315, in run
    self.wait()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/background_thread.py"", line 70, in wait
    super(BackgroundThreadService, self).wait(timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 263, in wait
    + ""These nodes are still alive: "" + str(unfinished_nodes))
TimeoutError: Timed out waiting 600 seconds for service nodes to finish. These nodes are still alive: [u'ProducerPerformanceService-0-140369731991568 node 1 on ducker10']

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.quota_test.QuotaTest.test_quota.quota_type=.user.client-id.override_quota=False
status:     PASS
run time:   4 minutes 15.055 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.quota_test.QuotaTest.test_quota.quota_type=.user.client-id.override_quota=True
status:     PASS
run time:   3 minutes 26.379 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.quota_test.QuotaTest.test_quota.quota_type=client-id.old_client_throttling_behavior=True
status:     FAIL
run time:   10 minutes 40.794 seconds


    Timed out waiting 600 seconds for service nodes to finish. These nodes are still alive: [u'ProducerPerformanceService-0-140369732460304 node 1 on ducker12']
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/client/quota_test.py"", line 156, in test_quota
    producer.run()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 315, in run
    self.wait()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/background_thread.py"", line 70, in wait
    super(BackgroundThreadService, self).wait(timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 263, in wait
    + ""These nodes are still alive: "" + str(unfinished_nodes))
TimeoutError: Timed out waiting 600 seconds for service nodes to finish. These nodes are still alive: [u'ProducerPerformanceService-0-140369732460304 node 1 on ducker12']

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.quota_test.QuotaTest.test_quota.quota_type=client-id.override_quota=False
status:     PASS
run time:   4 minutes 8.594 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.quota_test.QuotaTest.test_quota.quota_type=client-id.override_quota=True
status:     PASS
run time:   3 minutes 23.585 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.quota_test.QuotaTest.test_quota.quota_type=user.override_quota=False
status:     PASS
run time:   4 minutes 10.691 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.client.quota_test.QuotaTest.test_quota.quota_type=user.override_quota=True
status:     PASS
run time:   3 minutes 17.743 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_test.ConnectStandaloneFileTest.test_file_source_and_sink.converter=org.apache.kafka.connect.json.JsonConverter.schemas=False
status:     PASS
run time:   56.952 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_test.ConnectStandaloneFileTest.test_file_source_and_sink.converter=org.apache.kafka.connect.json.JsonConverter.schemas=True
status:     PASS
run time:   56.446 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_test.ConnectStandaloneFileTest.test_file_source_and_sink.converter=org.apache.kafka.connect.storage.StringConverter.schemas=None
status:     PASS
run time:   55.679 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_test.ConnectStandaloneFileTest.test_file_source_and_sink.security_protocol=PLAINTEXT
status:     PASS
run time:   55.521 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_test.ConnectStandaloneFileTest.test_skip_and_log_to_dlq.error_tolerance=all
status:     PASS
run time:   48.054 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.connect.connect_test.ConnectStandaloneFileTest.test_skip_and_log_to_dlq.error_tolerance=none
status:     PASS
run time:   1 minute 0.808 seconds
----------------------------------------------------------------------------------------------------
{noformat}","14/Feb/20 11:26;nizhikov;{noformat}
====================================================================================================
SESSION REPORT (ALL TESTS)
ducktape version: 0.7.6
session_id:       2020-02-13--006
run time:         165 minutes 51.003 seconds
tests run:        45
passed:           41
failed:           4
ignored:          0
====================================================================================================
test_id:    kafkatest.tests.core.delegation_token_test.DelegationTokenTest.test_delegation_token_lifecycle
status:     PASS
run time:   1 minute 47.231 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_disable_separate_interbroker_listener
status:     FAIL
run time:   4 minutes 19.073 seconds


    The consumer has terminated, or timed out, on node ducker@ducker08.
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/security_rolling_upgrade_test.py"", line 249, in test_disable_separate_interbroker_listener
    self.remove_separate_broker_listener, client_protocol, client_sasl_mechanism)
  File ""/opt/kafka-dev/tests/kafkatest/tests/produce_consume_validate.py"", line 102, in run_produce_consume_validate
    self.stop_producer_and_consumer()
  File ""/opt/kafka-dev/tests/kafkatest/tests/produce_consume_validate.py"", line 89, in stop_producer_and_consumer
    self.check_alive()
  File ""/opt/kafka-dev/tests/kafkatest/tests/produce_consume_validate.py"", line 81, in check_alive
    raise Exception(msg)
Exception: The consumer has terminated, or timed out, on node ducker@ducker08.

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_enable_separate_interbroker_listener
status:     FAIL
run time:   3 minutes 30.965 seconds


    The consumer has terminated, or timed out, on node ducker@ducker03.
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/security_rolling_upgrade_test.py"", line 225, in test_enable_separate_interbroker_listener
    SecurityConfig.SASL_MECHANISM_PLAIN)
  File ""/opt/kafka-dev/tests/kafkatest/tests/produce_consume_validate.py"", line 102, in run_produce_consume_validate
    self.stop_producer_and_consumer()
  File ""/opt/kafka-dev/tests/kafkatest/tests/produce_consume_validate.py"", line 89, in stop_producer_and_consumer
    self.check_alive()
  File ""/opt/kafka-dev/tests/kafkatest/tests/produce_consume_validate.py"", line 81, in check_alive
    raise Exception(msg)
Exception: The consumer has terminated, or timed out, on node ducker@ducker03.

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_phase_one.client_protocol=SASL_PLAINTEXT
status:     PASS
run time:   4 minutes 46.723 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_phase_one.client_protocol=SASL_SSL
status:     PASS
run time:   5 minutes 15.402 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_sasl_mechanism_phase_one.new_client_sasl_mechanism=PLAIN
status:     PASS
run time:   5 minutes 48.541 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.transactions_test.TransactionsTest.test_transactions.failure_mode=clean_bounce.bounce_target=brokers.check_order=False
status:     PASS
run time:   4 minutes 29.088 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.transactions_test.TransactionsTest.test_transactions.failure_mode=clean_bounce.bounce_target=brokers.check_order=True
status:     PASS
run time:   4 minutes 0.152 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.transactions_test.TransactionsTest.test_transactions.failure_mode=clean_bounce.bounce_target=clients.check_order=False
status:     PASS
run time:   4 minutes 26.723 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.transactions_test.TransactionsTest.test_transactions.failure_mode=clean_bounce.bounce_target=clients.check_order=True
status:     PASS
run time:   3 minutes 44.763 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.transactions_test.TransactionsTest.test_transactions.failure_mode=hard_bounce.bounce_target=brokers.check_order=False
status:     PASS
run time:   4 minutes 51.598 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.transactions_test.TransactionsTest.test_transactions.failure_mode=hard_bounce.bounce_target=brokers.check_order=True
status:     PASS
run time:   4 minutes 15.202 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.transactions_test.TransactionsTest.test_transactions.failure_mode=hard_bounce.bounce_target=clients.check_order=False
status:     FAIL
run time:   4 minutes 26.627 seconds


    copier-1 : Message copier didn't make enough progress in 30s. Current progress: 0
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/transactions_test.py"", line 246, in test_transactions
    num_messages_to_copy=self.num_seed_messages)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/transactions_test.py"", line 189, in copy_messages_transactionally
    self.bounce_copiers(copiers, clean_shutdown)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/transactions_test.py"", line 117, in bounce_copiers
    % (copier.transactional_id, str(copier.progress_percent())))
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: copier-1 : Message copier didn't make enough progress in 30s. Current progress: 0

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.transactions_test.TransactionsTest.test_transactions.failure_mode=hard_bounce.bounce_target=clients.check_order=True
status:     PASS
run time:   3 minutes 46.452 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.mirror_maker_test.TestMirrorMakerService.test_bounce.clean_shutdown=False.security_protocol=SASL_PLAINTEXT
status:     PASS
run time:   2 minutes 55.050 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.mirror_maker_test.TestMirrorMakerService.test_bounce.clean_shutdown=False.security_protocol=SASL_SSL
status:     PASS
run time:   3 minutes 24.344 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.mirror_maker_test.TestMirrorMakerService.test_bounce.clean_shutdown=True.security_protocol=SASL_PLAINTEXT
status:     PASS
run time:   2 minutes 44.703 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.mirror_maker_test.TestMirrorMakerService.test_bounce.clean_shutdown=True.security_protocol=SASL_SSL
status:     PASS
run time:   3 minutes 24.544 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.mirror_maker_test.TestMirrorMakerService.test_simple_end_to_end.security_protocol=SASL_PLAINTEXT
status:     PASS
run time:   2 minutes 20.617 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.mirror_maker_test.TestMirrorMakerService.test_simple_end_to_end.security_protocol=SASL_SSL
status:     PASS
run time:   2 minutes 44.589 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_phase_one.client_protocol=SSL
status:     PASS
run time:   4 minutes 29.670 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_phase_two.broker_protocol=SASL_PLAINTEXT.client_protocol=SASL_PLAINTEXT
status:     PASS
run time:   5 minutes 30.252 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_phase_two.broker_protocol=SASL_PLAINTEXT.client_protocol=SASL_SSL
status:     PASS
run time:   6 minutes 24.268 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_phase_two.broker_protocol=SASL_PLAINTEXT.client_protocol=SSL
status:     PASS
run time:   6 minutes 26.756 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_phase_two.broker_protocol=SASL_SSL.client_protocol=SASL_PLAINTEXT
status:     PASS
run time:   6 minutes 19.379 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_phase_two.broker_protocol=SASL_SSL.client_protocol=SASL_SSL
status:     PASS
run time:   6 minutes 28.369 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_phase_two.broker_protocol=SASL_SSL.client_protocol=SSL
status:     PASS
run time:   6 minutes 34.198 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_phase_two.broker_protocol=SSL.client_protocol=SASL_PLAINTEXT
status:     PASS
run time:   6 minutes 36.551 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_phase_two.broker_protocol=SSL.client_protocol=SASL_SSL
status:     PASS
run time:   6 minutes 44.657 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_phase_two.broker_protocol=SSL.client_protocol=SSL
status:     PASS
run time:   5 minutes 4.645 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_sasl_mechanism_phase_two.new_sasl_mechanism=PLAIN
status:     FAIL
run time:   6 minutes 24.385 seconds


    The consumer has terminated, or timed out, on node ducker@ducker06.
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/security_rolling_upgrade_test.py"", line 208, in test_rolling_upgrade_sasl_mechanism_phase_two
    self.run_produce_consume_validate(self.roll_in_sasl_mechanism, self.kafka.security_protocol, new_sasl_mechanism)
  File ""/opt/kafka-dev/tests/kafkatest/tests/produce_consume_validate.py"", line 102, in run_produce_consume_validate
    self.stop_producer_and_consumer()
  File ""/opt/kafka-dev/tests/kafkatest/tests/produce_consume_validate.py"", line 89, in stop_producer_and_consumer
    self.check_alive()
  File ""/opt/kafka-dev/tests/kafkatest/tests/produce_consume_validate.py"", line 81, in check_alive
    raise Exception(msg)
Exception: The consumer has terminated, or timed out, on node ducker@ducker06.

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.mirror_maker_test.TestMirrorMakerService.test_bounce.clean_shutdown=False.security_protocol=PLAINTEXT
status:     PASS
run time:   2 minutes 26.196 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.mirror_maker_test.TestMirrorMakerService.test_bounce.clean_shutdown=False.security_protocol=SSL
status:     PASS
run time:   2 minutes 44.125 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.mirror_maker_test.TestMirrorMakerService.test_bounce.clean_shutdown=True.security_protocol=PLAINTEXT
status:     PASS
run time:   2 minutes 6.908 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.mirror_maker_test.TestMirrorMakerService.test_bounce.clean_shutdown=True.security_protocol=SSL
status:     PASS
run time:   2 minutes 35.928 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.mirror_maker_test.TestMirrorMakerService.test_simple_end_to_end.security_protocol=PLAINTEXT
status:     PASS
run time:   1 minute 52.046 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.mirror_maker_test.TestMirrorMakerService.test_simple_end_to_end.security_protocol=SSL
status:     PASS
run time:   2 minutes 9.968 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.security_test.SecurityTest.test_client_ssl_endpoint_validation_failure.interbroker_security_protocol=PLAINTEXT.security_protocol=SSL
status:     PASS
run time:   1 minute 22.079 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.security_test.SecurityTest.test_client_ssl_endpoint_validation_failure.interbroker_security_protocol=SSL.security_protocol=PLAINTEXT
status:     PASS
run time:   1 minute 22.838 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.tools.replica_verification_test.ReplicaVerificationToolTest.test_replica_lags
status:     PASS
run time:   56.896 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SASL_PLAINTEXT
status:     PASS
run time:   45.825 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SASL_SSL
status:     PASS
run time:   1 minute 3.470 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=PLAINTEXT
status:     PASS
run time:   29.523 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SSL
status:     PASS
run time:   39.957 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.tools.log_compaction_test.LogCompactionTest.test_log_compaction
status:     PASS
run time:   1 minute 7.593 seconds
----------------------------------------------------------------------------------------------------
{noformat}","14/Feb/20 11:27;nizhikov;{noformat}
====================================================================================================
SESSION REPORT (ALL TESTS)
ducktape version: 0.7.6
session_id:       2020-02-13--007
run time:         232 minutes 34.117 seconds
tests run:        83
passed:           64
failed:           19
ignored:          0
====================================================================================================
test_id:    kafkatest.tests.core.throttling_test.ThrottlingTest.test_throttled_reassignment.bounce_brokers=False
status:     PASS
run time:   8 minutes 4.639 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.throttling_test.ThrottlingTest.test_throttled_reassignment.bounce_brokers=True
status:     PASS
run time:   6 minutes 59.593 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.log_dir_failure_test.LogDirFailureTest.test_replication_with_disk_failure.bounce_broker=False.security_protocol=PLAINTEXT.broker_type=follower
status:     PASS
run time:   4 minutes 17.776 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.log_dir_failure_test.LogDirFailureTest.test_replication_with_disk_failure.bounce_broker=False.security_protocol=PLAINTEXT.broker_type=leader
status:     PASS
run time:   4 minutes 17.052 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.log_dir_failure_test.LogDirFailureTest.test_replication_with_disk_failure.bounce_broker=True.security_protocol=PLAINTEXT.broker_type=follower
status:     PASS
run time:   4 minutes 25.932 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.log_dir_failure_test.LogDirFailureTest.test_replication_with_disk_failure.bounce_broker=True.security_protocol=PLAINTEXT.broker_type=leader
status:     PASS
run time:   4 minutes 22.588 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.zookeeper_security_upgrade_test.ZooKeeperSecurityUpgradeTest.test_zk_security_upgrade.security_protocol=PLAINTEXT
status:     PASS
run time:   4 minutes 42.064 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.zookeeper_security_upgrade_test.ZooKeeperSecurityUpgradeTest.test_zk_security_upgrade.security_protocol=SASL_PLAINTEXT
status:     PASS
run time:   4 minutes 58.661 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.zookeeper_security_upgrade_test.ZooKeeperSecurityUpgradeTest.test_zk_security_upgrade.security_protocol=SASL_SSL
status:     PASS
run time:   6 minutes 5.321 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.zookeeper_security_upgrade_test.ZooKeeperSecurityUpgradeTest.test_zk_security_upgrade.security_protocol=SSL
status:     PASS
run time:   5 minutes 59.970 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.downgrade_test.TestDowngrade.test_upgrade_and_downgrade.version=1.1.1.compression_types=.none
status:     PASS
run time:   2 minutes 6.634 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.downgrade_test.TestDowngrade.test_upgrade_and_downgrade.version=1.1.1.security_protocol=SASL_SSL.compression_types=.lz4
status:     PASS
run time:   4 minutes 29.514 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.downgrade_test.TestDowngrade.test_upgrade_and_downgrade.version=2.0.1.compression_types=.none
status:     PASS
run time:   2 minutes 9.643 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.downgrade_test.TestDowngrade.test_upgrade_and_downgrade.version=2.0.1.security_protocol=SASL_SSL.compression_types=.snappy
status:     PASS
run time:   4 minutes 21.497 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.downgrade_test.TestDowngrade.test_upgrade_and_downgrade.version=2.1.1.compression_types=.none
status:     PASS
run time:   2 minutes 9.006 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.downgrade_test.TestDowngrade.test_upgrade_and_downgrade.version=2.1.1.security_protocol=SASL_SSL.compression_types=.lz4
status:     PASS
run time:   6 minutes 8.769 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.downgrade_test.TestDowngrade.test_upgrade_and_downgrade.version=2.2.2.compression_types=.none
status:     PASS
run time:   2 minutes 12.863 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.downgrade_test.TestDowngrade.test_upgrade_and_downgrade.version=2.2.2.security_protocol=SASL_SSL.compression_types=.zstd
status:     PASS
run time:   6 minutes 3.188 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.downgrade_test.TestDowngrade.test_upgrade_and_downgrade.version=2.3.1.compression_types=.none
status:     PASS
run time:   2 minutes 15.493 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.downgrade_test.TestDowngrade.test_upgrade_and_downgrade.version=2.3.1.security_protocol=SASL_SSL.compression_types=.zstd
status:     PASS
run time:   6 minutes 8.227 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.failure_mode=hard_bounce.client_sasl_mechanism=SCRAM-SHA-256.security_protocol=SASL_SSL.broker_type=leader.interbroker_sasl_mechanism=SCRAM-SHA-512
status:     PASS
run time:   4 minutes 22.813 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=clean_bounce.broker_type=controller
status:     PASS
run time:   2 minutes 24.396 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=clean_bounce.broker_type=leader
status:     PASS
run time:   2 minutes 20.203 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=clean_bounce.broker_type=leader.compression_type=gzip
status:     PASS
run time:   2 minutes 21.933 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=clean_bounce.broker_type=leader.enable_idempotence=True
status:     PASS
run time:   2 minutes 27.064 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=clean_shutdown.broker_type=controller
status:     PASS
run time:   1 minute 10.295 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=clean_shutdown.broker_type=leader
status:     PASS
run time:   1 minute 11.598 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=clean_shutdown.broker_type=leader.compression_type=gzip
status:     PASS
run time:   1 minute 9.889 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=clean_shutdown.broker_type=leader.enable_idempotence=True
status:     PASS
run time:   1 minute 9.385 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=hard_bounce.broker_type=controller
status:     PASS
run time:   2 minutes 40.598 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=hard_bounce.broker_type=leader
status:     PASS
run time:   2 minutes 36.295 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=hard_bounce.broker_type=leader.compression_type=gzip
status:     PASS
run time:   2 minutes 40.614 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=hard_bounce.broker_type=leader.enable_idempotence=True
status:     PASS
run time:   2 minutes 45.557 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=hard_shutdown.broker_type=controller
status:     PASS
run time:   1 minute 16.158 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=hard_shutdown.broker_type=leader
status:     PASS
run time:   1 minute 16.189 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=hard_shutdown.broker_type=leader.compression_type=gzip
status:     PASS
run time:   1 minute 16.132 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=hard_shutdown.broker_type=leader.enable_idempotence=True
status:     PASS
run time:   1 minute 14.936 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.client_sasl_mechanism=PLAIN.failure_mode=hard_bounce.broker_type=leader.interbroker_sasl_mechanism=GSSAPI
status:     PASS
run time:   4 minutes 36.765 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.client_sasl_mechanism=PLAIN.failure_mode=hard_bounce.broker_type=leader.interbroker_sasl_mechanism=PLAIN
status:     PASS
run time:   3 minutes 27.005 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.failure_mode=clean_bounce.broker_type=controller
status:     PASS
run time:   4 minutes 20.256 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.failure_mode=clean_bounce.broker_type=leader
status:     PASS
run time:   4 minutes 2.965 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.failure_mode=clean_shutdown.broker_type=controller
status:     PASS
run time:   1 minute 38.914 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.failure_mode=clean_shutdown.broker_type=leader
status:     PASS
run time:   1 minute 45.185 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.failure_mode=hard_bounce.broker_type=controller
status:     PASS
run time:   4 minutes 38.220 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.failure_mode=hard_bounce.broker_type=leader
status:     PASS
run time:   4 minutes 32.709 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.failure_mode=hard_shutdown.broker_type=controller
status:     PASS
run time:   1 minute 54.670 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.failure_mode=hard_shutdown.broker_type=leader
status:     PASS
run time:   1 minute 49.619 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=0.8.2.2.to_message_format_version=None.compression_types=.none
status:     FAIL
run time:   1 minute 18.053 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=0.8.2.2.to_message_format_version=None.compression_types=.snappy
status:     FAIL
run time:   1 minute 17.940 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=0.9.0.1.to_message_format_version=None.security_protocol=SASL_SSL.compression_types=.none
status:     FAIL
run time:   1 minute 28.387 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=0.10.0.1.to_message_format_version=None.compression_types=.lz4
status:     FAIL
run time:   1 minute 17.979 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=0.10.0.1.to_message_format_version=None.compression_types=.snappy
status:     FAIL
run time:   1 minute 18.046 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=0.10.1.1.to_message_format_version=None.compression_types=.lz4
status:     FAIL
run time:   1 minute 18.145 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=0.10.1.1.to_message_format_version=None.compression_types=.snappy
status:     FAIL
run time:   1 minute 18.210 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=0.10.2.2.to_message_format_version=0.10.2.2.compression_types=.snappy
status:     FAIL
run time:   1 minute 17.892 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=0.10.2.2.to_message_format_version=0.9.0.1.compression_types=.none
status:     FAIL
run time:   1 minute 17.989 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=0.10.2.2.to_message_format_version=None.compression_types=.lz4
status:     FAIL
run time:   1 minute 18.153 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=0.10.2.2.to_message_format_version=None.compression_types=.none
status:     FAIL
run time:   1 minute 18.058 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=0.11.0.3.to_message_format_version=None.compression_types=.gzip
status:     FAIL
run time:   1 minute 18.147 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=0.11.0.3.to_message_format_version=None.compression_types=.lz4
status:     FAIL
run time:   1 minute 18.061 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=0.9.0.1.to_message_format_version=0.9.0.1.compression_types=.lz4
status:     FAIL
run time:   1 minute 18.187 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=0.9.0.1.to_message_format_version=0.9.0.1.compression_types=.none
status:     FAIL
run time:   1 minute 18.098 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=0.9.0.1.to_message_format_version=None.compression_types=.lz4
status:     FAIL
run time:   1 minute 18.075 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=0.9.0.1.to_message_format_version=None.compression_types=.snappy
status:     FAIL
run time:   1 minute 17.867 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=1.0.2.to_message_format_version=None.compression_types=.none
status:     FAIL
run time:   1 minute 17.984 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=1.0.2.to_message_format_version=None.compression_types=.snappy
status:     FAIL
run time:   1 minute 17.998 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=1.1.1.to_message_format_version=None.compression_types=.lz4
status:     PASS
run time:   3 minutes 42.172 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=1.1.1.to_message_format_version=None.compression_types=.none
status:     PASS
run time:   3 minutes 47.181 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=2.0.1.to_message_format_version=None.compression_types=.none
status:     PASS
run time:   3 minutes 45.266 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=2.0.1.to_message_format_version=None.compression_types=.snappy
status:     PASS
run time:   3 minutes 49.620 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=2.1.1.to_message_format_version=None.compression_types=.lz4
status:     PASS
run time:   3 minutes 48.009 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=2.1.1.to_message_format_version=None.compression_types=.none
status:     PASS
run time:   3 minutes 53.952 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=2.2.2.to_message_format_version=None.compression_types=.none
status:     PASS
run time:   3 minutes 51.466 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=2.2.2.to_message_format_version=None.compression_types=.zstd
status:     PASS
run time:   3 minutes 47.676 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=2.3.1.to_message_format_version=None.compression_types=.none
status:     PASS
run time:   3 minutes 57.414 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=2.3.1.to_message_format_version=None.compression_types=.zstd
status:     PASS
run time:   3 minutes 52.892 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=2.4.0.to_message_format_version=None.compression_types=.none
status:     PASS
run time:   3 minutes 41.530 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=2.4.0.to_message_format_version=None.compression_types=.zstd
status:     PASS
run time:   3 minutes 36.466 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.get_offset_shell_test.GetOffsetShellTest.test_get_offset_shell
status:     PASS
run time:   31.698 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.consumer_group_command_test.ConsumerGroupCommandTest.test_describe_consumer_group.security_protocol=PLAINTEXT
status:     PASS
run time:   26.263 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.consumer_group_command_test.ConsumerGroupCommandTest.test_describe_consumer_group.security_protocol=SSL
status:     PASS
run time:   35.221 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.consumer_group_command_test.ConsumerGroupCommandTest.test_list_consumer_groups.security_protocol=PLAINTEXT
status:     PASS
run time:   28.131 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.consumer_group_command_test.ConsumerGroupCommandTest.test_list_consumer_groups.security_protocol=SSL
status:     PASS
run time:   32.893 seconds
----------------------------------------------------------------------------------------------------
{noformat}","14/Feb/20 11:29;nizhikov;{noformat}
[2020-02-13 15:32:12,407] INFO Cluster ID = gPGHZbQnQCifXWdkUfKa8g (kafka.server.KafkaServer)
[2020-02-13 15:32:12,410] WARN No meta.properties file under dir /mnt/kafka/kafka-data-logs-1/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2020-02-13 15:32:12,410] WARN No meta.properties file under dir /mnt/kafka/kafka-data-logs-2/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2020-02-13 15:32:12,424] DEBUG Reading reply sessionid:0x10583def5250000, packet:: clientPath:/config/brokers/<default> serverPath:/config/brokers/<default> finished:false header:: 24,4  replyHeader:: 24,23,-101  request:: '/config/brokers/<default>,F  response::   (org.apache.zookeeper.ClientCnxn)
[2020-02-13 15:32:12,469] INFO KafkaConfig values: 
	advertised.host.name = ducker05
	advertised.listeners = SASL_SSL://ducker05:9095
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 100
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = SASL_SSL
	inter.broker.protocol.version = 2.5-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SASL_SSL:SASL_SSL
	listeners = SASL_SSL://:9095
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /mnt/kafka/kafka-data-logs-1,/mnt/kafka/kafka-data-logs-2
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.5-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = kafka
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 65536
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.3]
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = /mnt/security/test.keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = /mnt/security/test.truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = ducker04:2181
	zookeeper.connection.timeout.ms = 2000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2020-02-13 15:32:12,477] DEBUG Reading reply sessionid:0x10583def5250000, packet:: clientPath:/config/brokers/1 serverPath:/config/brokers/1 finished:false header:: 25,4  replyHeader:: 25,23,-101  request:: '/config/brokers/1,F  response::   (org.apache.zookeeper.ClientCnxn)
[2020-02-13 15:32:12,482] INFO KafkaConfig values: 
	advertised.host.name = ducker05
	advertised.listeners = SASL_SSL://ducker05:9095
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 100
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = SASL_SSL
	inter.broker.protocol.version = 2.5-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SASL_SSL:SASL_SSL
	listeners = SASL_SSL://:9095
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /mnt/kafka/kafka-data-logs-1,/mnt/kafka/kafka-data-logs-2
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.5-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = kafka
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 65536
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.3]
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = /mnt/security/test.keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = /mnt/security/test.truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = ducker04:2181
	zookeeper.connection.timeout.ms = 2000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)

{noformat}","14/Feb/20 11:36;nizhikov;Hello, [~rsivaram].

I ran system tests that use SSL for the TLSv1.3. You can find the results of the tests in the previous messages.
I also sent a part of the Kafka log with the actual properties - so you can see, only TLSv1.3 enabled in the config.

Test environment:

  * openjdk11
  * trunk + changes from my PR [1].

Full system tests results have volume 15gb.
Should I share full logs with you?

What else should be done before we can enable TLSv1.3 by default?
Can you, please, take a look at my changes in PR [1]?

As you can see from the log, tests contain failure, mostly for upgrade cases.
Not sure, If it relates to my changes.
It seems those tests fail in the trunk too.

[1] https://github.com/apache/kafka/pull/8106/files","17/Feb/20 09:49;githubbot;rajinisivaram commented on pull request #8106: KAFKA-9319: Fix generation of CA certificate for system tests.
URL: https://github.com/apache/kafka/pull/8106
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","17/Feb/20 10:03;nizhikov;Hello, [~rsivaram].

I think we are ready to proceed with the KAFKA-9320 and enable TLSv1.3 by default.
What do you think?","17/Feb/20 10:21;rsivaram;Can you rerun the tests that failed on their own and see if those are intermittent failures? If they continue to fail, it will be good to check if they pass with TLSv1.2.","17/Feb/20 19:33;nizhikov;[~rsivaram] TLSv1.3 was added to the java11.

Please, see, JDK issue [1].
If we enabled TLSv1.3 by default then we should require from users that kafka runs on JDK11.
Is it OK?

[1] https://bugs.java.com/bugdatabase/view_bug.do?bug_id=8145252","18/Feb/20 10:34;rsivaram;[~nizhikov] No, we cannot require users to run on Java 11 since we need to continue support for Java 8. For integration tests, we run some tests with TLSv1.3 when running with Java 11. Those tests run with TLSv1.2 on Java 8. We should do something similar for system tests as well. We could convert a subset of the tests to choose TLSv1.3 at runtime if the Java version is 11 or above.","18/Feb/20 10:46;nizhikov;[~rsivaram]

> No, we cannot require users to run on Java 11 since we need to continue support for Java 8

Does this mean that we can't complete KAFKA-9320 for now and enable TLSv1.3 by default and disable TLSv1.2?","18/Feb/20 11:24;rsivaram;[~nizhikov] Yes, that is correct. We can enable TLSv1.3 in `ssl.enabled.protocols`, but we cannot disable TLSv1.2 or make TLSv1.3 the default protocol in `ssl.protocol`.","18/Feb/20 12:02;nizhikov;[~rsivaram] Thanks for the answers.

It seems the results of my tests show that if TLSv1.3 enabled then system tests are still passed.
So I think we can resolve this issue and process with the KAFKA-9320, isn't it?","18/Feb/20 12:40;rsivaram;[~nizhikov] We want to make it possible to optionally run system tests with TLSv1.3 when the tests are run with Java version 11 or above. This is required to make sure there are no regressions in future, without having to change the test code as you did this time. But that change can be done under the KIP for KAFKA-9320.

Just to confirm - have all the system tests using SSL passed with TLSv1.3 in your local run? There were some failures in the output you pasted earlier. Have you rerun those? If all the tests using SSL have passed with TLSv1.3, feel free to close this JIRA.","19/Feb/20 09:25;nizhikov;[~rsivaram]

Double-checked tests results.
Found one test with SSL that failed.
Looks like this test can't be run on Java11.

I created a KAFKA-9573 to fix it and resolve this ticket.
It seems other tests with TLSv1.3 works just fine.

{noformat}
====================================================================================================
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=0.9.0.1.to_message_format_version=None.security_protocol=SASL_SSL.compression_types=.none
status:     FAIL
run time:   1 minute 28.387 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 133, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 242, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 357, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds
{noformat}

Detailed start log:

{noformat}
[0.001s][warning][gc] -Xloggc is deprecated. Will use -Xlog:gc:/opt/kafka-0.9.0.1/bin/../logs/kafkaServer-gc.log instead.
Unrecognized VM option 'PrintGCDateStamps'
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
{noformat}"
Integration tests for KIP-558,KAFKA-9536,13284455,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,kkonstantine,kkonstantine,kkonstantine,11/Feb/20 01:03,14/Feb/20 22:37,12/Jan/21 11:54,14/Feb/20 22:37,,,,,,,,,,,,,,0,,,,"Extend testing coverage for [KIP-558|https://cwiki.apache.org/confluence/display/KAFKA/KIP-558%3A+Track+the+set+of+actively+used+topics+by+connectors+in+Kafka+Connect] with integration tests and additional unit tests. ",,kkonstantine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 14 22:37:05 UTC 2020,,,,,,,"0|z0bcn4:",9223372036854775807,,,,,,,,,,,,,,,,"14/Feb/20 22:37;kkonstantine; 

Fixed with [https://github.com/apache/kafka/pull/8085]",,,,,,,,,,,,,,,,,,,,,
Reduce flakiness of BranchedMultiLevelRepartitionConnectedTopologyTest,KAFKA-9523,13284024,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,bchen225242,bchen225242,07/Feb/20 22:21,10/Feb/20 21:10,12/Jan/21 11:54,10/Feb/20 21:10,,,,,2.5.0,,,,,streams,unit tests,,,0,,,,"KAFKA-9335 introduces an integration test to verify the topology builder itself could survive from building a complex topology. This test gets flaky some time for stream client to broker connection, so we should consider making it less flaky by either converting to a unit test or just focus on making the test logic more robust.",,bchen225242,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-02-10 20:09:41.546,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 10 21:09:31 UTC 2020,,,,,,,"0|z0b9z4:",9223372036854775807,,,,,,,,,,,,,,,,"10/Feb/20 20:09;githubbot;abbccdda commented on pull request #8081: KAFKA-9523: Migrate BranchedMultiLevelRepartitionConnectedTopologyTest into a unit test
URL: https://github.com/apache/kafka/pull/8081
 
 
   Relying on integration test to catch an algorithm bug introduces more flakiness, reduce the test into a unit test to reduce the flakiness until we upgrade Java/Scala libs.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","10/Feb/20 21:09;githubbot;guozhangwang commented on pull request #8081: KAFKA-9523: Migrate BranchedMultiLevelRepartitionConnectedTopologyTest into a unit test
URL: https://github.com/apache/kafka/pull/8081
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,
ConnectDistributedTest is always running broker with dev version,KAFKA-7489,13190035,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rhauch,akatona,akatona,08/Oct/18 08:52,05/Feb/20 15:54,12/Jan/21 11:54,06/Dec/19 19:48,,,,,2.1.2,2.2.3,2.3.2,,,KafkaConnect,system tests,,,0,,,,"h2. Test class
kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest

h2. Details
_test_broker_compatibility_ is +parametrized+ with different types of brokers, yet it is passed as string to _setup_services_ and this way KafkaService is initialised with DEV version in the end.

This is easy to fix, just wrap the _broker_version_ with KafkaVersion
{panel}
self.setup_services(broker_version={color:#FF0000}KafkaVersion{color}(broker_version), auto_create_topics=auto_create_topics, security_protocol=security_protocol)
{panel}

But test is failing with the parameter LATEST_0_9 with the following error message
{noformat}
Kafka Connect failed to start on node: ducker@ducker02 in condition mode: LISTEN
{noformat}",,akatona,githubbot,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-12-06 16:37:53.257,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 05 15:54:06 UTC 2020,,,,,,,"0|i3yxnz:",9223372036854775807,,vvcephei,,,,,,,,,,,,,,"06/Dec/19 16:37;rhauch;This has already been fixed in `2.4` and `trunk` via https://github.com/apache/kafka/pull/7023, but not on earlier branches.","06/Dec/19 16:40;githubbot;rhauch commented on pull request #7791: KAFKA-7489: Backport fix ConnectDistributedTest system test to use KafkaVersion
URL: https://github.com/apache/kafka/pull/7791
 
 
   This was fixed in `2.4` and `trunk` per #7023, but the use of `KafkaVersion` should have been backported. This makes this change on `2.3`, and should be backported to `2.2` and `2.1`.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","06/Dec/19 19:43;githubbot;rhauch commented on pull request #7791: KAFKA-7489: Backport fix ConnectDistributedTest system test to use KafkaVersion
URL: https://github.com/apache/kafka/pull/7791
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","06/Dec/19 19:48;rhauch;Merged this to the `2.3`, `2.2`., and `2.1` branches. As mentioned above, it's already fixed on `2.4` and `trunk`.","03/Feb/20 05:04;githubbot;gharris1727 commented on pull request #8035: KAFKA-7489: Remove 0.9 compatibility checks from ConnectDistributedTest
URL: https://github.com/apache/kafka/pull/8035
 
 
   (#7023) exposed an incompatibility between Kafka <=0.9 and Connect >0.9,
   in which the broker does not recognize a request for ApiVersions.
   For trunk and 2.4, this test case was removed rather than the issue addressed.
   This effectively backports the other half of (#7023) which was left out of (#7791).
   
   Signed-off-by: Greg Harris <gregh@confluent.io>
   
   It is necessary to backport this to all of the branches that #7791 was backported to, namely `2.2` and `2.1`.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","05/Feb/20 15:47;githubbot;rhauch commented on pull request #8035: KAFKA-7489: Remove 0.9 compatibility checks from ConnectDistributedTest
URL: https://github.com/apache/kafka/pull/8035
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","05/Feb/20 15:54;rhauch;Merged [https://github.com/apache/kafka/pull/8035] to the 2.3 branch, and backported to 2.2 and 2.1 to fix broken system tests.",,,,,,,,,,,,,,,
Flaky Test SslAdminClientIntegrationTest.testSynchronousAuthorizerAclUpdatesBlockRequestThreads,KAFKA-9188,13268367,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,bbejeck,bbejeck,14/Nov/19 16:28,10/Jan/20 11:24,12/Jan/21 11:54,10/Jan/20 11:24,,,,,2.5.0,,,,,core,,,,0,flaky-test,,,"Failed in [https://builds.apache.org/job/kafka-pr-jdk11-scala2.12/9373/testReport/junit/kafka.api/SslAdminClientIntegrationTest/testSynchronousAuthorizerAclUpdatesBlockRequestThreads/]

 
{noformat}
Error Messagejava.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Aborted due to timeout.Stacktracejava.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Aborted due to timeout.
	at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)
	at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)
	at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)
	at kafka.api.SslAdminClientIntegrationTest.$anonfun$testSynchronousAuthorizerAclUpdatesBlockRequestThreads$1(SslAdminClientIntegrationTest.scala:201)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at kafka.api.SslAdminClientIntegrationTest.testSynchronousAuthorizerAclUpdatesBlockRequestThreads(SslAdminClientIntegrationTest.scala:201)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:288)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:282)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.common.errors.TimeoutException: Aborted due to timeout.
Standard Output[2019-11-14 15:13:51,489] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition mytopic1-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:13:51,490] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition mytopic1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:14:04,686] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-644, correlationId=4, api=CREATE_ACLS, version=1, body={creations=[{resource_type=2,resource_name=foobar,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:41993-127.0.0.1:34770-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-11-14 15:14:04,689] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-644, correlationId=5, api=DELETE_ACLS, version=1, body={filters=[{resource_type=2,resource_name=foobar,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:41993-127.0.0.1:34770-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-11-14 15:14:04,795] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-644, correlationId=7, api=DESCRIBE_ACLS, version=1, body={resource_type=2,resource_name=*,resource_pattern_type_filter=3,principal=User:*,host=*,operation=2,permission_type=3} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:41993-127.0.0.1:34770-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-11-14 15:14:04,796] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-644, correlationId=8, api=CREATE_ACLS, version=1, body={creations=[{resource_type=2,resource_name=foobar,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:41993-127.0.0.1:34770-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-11-14 15:14:04,798] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-644, correlationId=9, api=DELETE_ACLS, version=1, body={filters=[{resource_type=2,resource_name=foobar,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:41993-127.0.0.1:34770-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-11-14 15:14:04,930] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-644, correlationId=18, api=DESCRIBE_ACLS, version=1, body={resource_type=2,resource_name=*,resource_pattern_type_filter=3,principal=User:*,host=*,operation=2,permission_type=3} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:41993-127.0.0.1:34770-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-11-14 15:14:04,931] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-644, correlationId=19, api=CREATE_ACLS, version=1, body={creations=[{resource_type=2,resource_name=foobar,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:41993-127.0.0.1:34770-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-11-14 15:14:04,933] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-644, correlationId=20, api=DELETE_ACLS, version=1, body={filters=[{resource_type=2,resource_name=foobar,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:41993-127.0.0.1:34770-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-11-14 15:14:04,937] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-644, correlationId=21, api=DESCRIBE_ACLS, version=1, body={resource_type=2,resource_name=*,resource_pattern_type_filter=3,principal=User:*,host=*,operation=2,permission_type=3} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:41993-127.0.0.1:34770-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-11-14 15:14:05,040] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-644, correlationId=23, api=CREATE_ACLS, version=1, body={creations=[{resource_type=2,resource_name=foobar,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:41993-127.0.0.1:34770-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-11-14 15:14:05,042] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-644, correlationId=24, api=DELETE_ACLS, version=1, body={filters=[{resource_type=2,resource_name=foobar,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:41993-127.0.0.1:34770-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-11-14 15:14:53,171] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:15:01,790] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:15:01,792] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:15:09,941] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:15:09,943] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:15:20,683] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition elect-preferred-leaders-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:15:20,686] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition elect-preferred-leaders-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:15:20,865] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition elect-preferred-leaders-topic-2-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:15:20,865] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition elect-preferred-leaders-topic-2-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:15:36,402] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition list-reassignments-no-reassignments-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:15:36,405] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition list-reassignments-no-reassignments-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:15:57,850] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:15:57,851] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:16:56,520] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:16:56,523] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:17:27,098] ERROR [ReplicaManager broker=2] Error while changing replica dir for partition topic-0 (kafka.server.ReplicaManager:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: Partition topic-0 doesn't exist
[2019-11-14 15:17:27,103] ERROR [ReplicaManager broker=1] Error while changing replica dir for partition topic-0 (kafka.server.ReplicaManager:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: Partition topic-0 doesn't exist
[2019-11-14 15:17:27,104] ERROR [ReplicaManager broker=0] Error while changing replica dir for partition topic-0 (kafka.server.ReplicaManager:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: Partition topic-0 doesn't exist
[2019-11-14 15:17:27,148] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:17:27,175] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:17:35,677] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:17:35,677] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:18:05,169] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:18:05,170] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:18:15,414] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:18:21,191] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition create-partitions-topic-2-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:18:21,497] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition create-partitions-topic-2-2 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:18:50,952] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:18:50,953] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:19:00,326] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:19:00,327] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:19:09,623] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:19:32,744] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:19:32,747] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:19:48,147] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:19:48,147] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-11-14 15:19:55,636] ERROR [Consumer instanceId=test_instance_id, clientId=test_client_id, groupId=test_group_id] Offset commit failed on partition test_topic-1 at offset 0: Specified group generation id is not valid. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1108)
[2019-11-14 15:20:01,895] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.{noformat}",,bbejeck,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-11-19 00:30:19.513,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 10 11:23:29 UTC 2020,,,,,,,"0|z08me0:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,"19/Nov/19 00:30;mjsax;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/3444/testReport/junit/kafka.api/SslAdminIntegrationTest/testSynchronousAuthorizerAclUpdatesBlockRequestThreads/]","09/Jan/20 15:59;githubbot;rajinisivaram commented on pull request #7918: KAFKA-9188; Fix flaky test SslAdminClientIntegrationTest.testSynchronousAuthorizerAclUpdatesBlockRequestThreads
URL: https://github.com/apache/kafka/pull/7918
 
 
   From the build failures in the JIRA, it looks like the test occasionally hits request timeout when running from Jenkins (I was able to recreate only with much smaller request timeouts). Since the test blocks requests threads while sending the ACL update requests, updated the test to tolerate timeouts and retry the request for that case. Added an additional check to verify that the requests threads are unblocked when the semaphore is released, ensuring that the timeout is not due to blocked threads.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","10/Jan/20 11:23;githubbot;rajinisivaram commented on pull request #7918: KAFKA-9188; Fix flaky test SslAdminClientIntegrationTest.testSynchronousAuthorizerAclUpdatesBlockRequestThreads
URL: https://github.com/apache/kafka/pull/7918
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,
Add system test covering Foreign Key joins (KIP-213),KAFKA-9138,13266181,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vvcephei,vvcephei,vvcephei,04/Nov/19 20:43,11/Dec/19 17:49,12/Jan/21 11:54,11/Dec/19 17:49,,,,,,,,,,streams,,,,0,,,,"There are unit and integration tests, but we should really have a system test as well.

I plan to create a new test, since this feature is pretty different than the existing topology/data set of smoke test. Although, it might be possible for the new test to subsume smoke test. I'd give the new test a few releases to burn in before considering a merge, though.",,githubbot,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,KAFKA-3705,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-11-08 02:38:49.257,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 11 17:49:45 UTC 2019,,,,,,,"0|z088w8:",9223372036854775807,,,,,,,,,,,,,,,,"08/Nov/19 02:38;githubbot;vvcephei commented on pull request #7664: KAFKA-9138: Add system test for relational joins
URL: https://github.com/apache/kafka/pull/7664
 
 
   Adds a system test to verify the new foreign-key join introduced in KIP-213.
   
   Also, adds a unit test and integration test to verify the test logic itself.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","11/Dec/19 17:48;githubbot;vvcephei commented on pull request #7664: KAFKA-9138: Add system test for relational joins
URL: https://github.com/apache/kafka/pull/7664
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","11/Dec/19 17:49;vvcephei;Added the system test in https://github.com/apache/kafka/commit/717ce42a6d68d3ac8d9478451a31423b5d43234e via https://github.com/apache/kafka/pull/7664",,,,,,,,,,,,,,,,,,,
Add system test with large number of partitions,KAFKA-9123,13265536,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mumrah,mumrah,mumrah,31/Oct/19 13:31,05/Dec/19 02:51,12/Jan/21 11:54,05/Dec/19 02:51,,,,,2.4.0,,,,,system tests,,,,0,,,,Let's add a system test with several thousand replicas and do some validation.,,githubbot,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-10-31 13:35:12.582,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 23 00:32:13 UTC 2019,,,,,,,"0|z084ww:",9223372036854775807,,,,,,,,,,,,,,,,"31/Oct/19 13:35;githubbot;mumrah commented on pull request #7621: KAFKA-9123 Test a large number of replicas
URL: https://github.com/apache/kafka/pull/7621
 
 
   TBD
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","23/Nov/19 00:32;githubbot;mumrah commented on pull request #7621: KAFKA-9123 Test a large number of replicas
URL: https://github.com/apache/kafka/pull/7621
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,
zookeeper service not running ,KAFKA-9147,13266459,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,pgarg00,pgarg00,06/Nov/19 01:16,06/Nov/19 12:19,12/Jan/21 11:54,,2.3.0,,,,,,,,,,,,,0,,,,"i was able to start zookeeper service on stand alone Ubuntu using the command

 

root@N-5CG73531RZ:/# /usr/local/zookeeper/bin/zkServer.sh start
/usr/bin/java
ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED

 

However when i do ps -ef I dont see any zookeeper service running 

 

root@N-5CG73531RZ:/# ps -ef
UID PID PPID C STIME TTY TIME CMD
root 1 0 0 Nov04 ? 00:00:00 /init
root 5 1 0 Nov04 tty1 00:00:00 /init
pgarg00 6 5 0 Nov04 tty1 00:00:00 -bash
root 2861 6 0 Nov04 tty1 00:00:00 sudo -i
root 2862 2861 0 Nov04 tty1 00:00:03 -bash
root 5347 1 0 18:24 ? 00:00:00 /usr/sbin/sshd
root 5367 1 0 18:25 ? 00:00:00 /usr/sbin/inetd
root 8950 2862 0 19:15 tty1 00:00:00 ps -ef

 

Also when I do telnet , connection is refused 

root@N-5CG73531RZ:/# telnet localhost 2181
Trying 127.0.0.1...
telnet: Unable to connect to remote host: Connection refused

 

can you plz help me ?

 ",Ubuntu,mimaison,pgarg00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-11-06 12:19:48.383,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 06 12:19:48 UTC 2019,,,,,,,"0|z08am0:",9223372036854775807,,,,,,,,,,,,,,,,"06/Nov/19 12:19;mimaison;It looks like you want to open an issue against Zookeeper instead of Kafka: https://issues.apache.org/jira/projects/ZOOKEEPER/issues/?filter=allopenissues",,,,,,,,,,,,,,,,,,,,,
Flaky Test kafka.api.SslAdminClientIntegrationTest.testCreateTopicsResponseMetadataAndConfig,KAFKA-9019,13261598,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,bbejeck,bbejeck,10/Oct/19 15:56,17/Oct/19 17:31,12/Jan/21 11:54,,,,,,,,,,,core,,,,0,flaky-test,,,"Seen in [https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/2492/testReport/junit/kafka.api/SslAdminClientIntegrationTest/testCreateTopicsResponseMetadataAndConfig/]
{noformat}
Error Messagejava.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.Stacktracejava.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)
	at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)
	at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)
	at kafka.api.SaslSslAdminClientIntegrationTest.testCreateTopicsResponseMetadataAndConfig(SaslSslAdminClientIntegrationTest.scala:452)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:288)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:282)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Standard Output[2019-10-10 05:41:46,134] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-123, correlationId=4, api=CREATE_ACLS, version=1, body={creations=[{resource_type=2,resource_name=foobar,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:34475-127.0.0.1:42444-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-10-10 05:41:46,136] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-123, correlationId=5, api=DELETE_ACLS, version=1, body={filters=[{resource_type=2,resource_name=foobar,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:34475-127.0.0.1:42444-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-10-10 05:41:46,241] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-123, correlationId=7, api=DESCRIBE_ACLS, version=1, body={resource_type=2,resource_name=*,resource_pattern_type_filter=3,principal=User:*,host=*,operation=2,permission_type=3} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:34475-127.0.0.1:42444-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-10-10 05:41:46,243] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-123, correlationId=8, api=CREATE_ACLS, version=1, body={creations=[{resource_type=2,resource_name=foobar,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:34475-127.0.0.1:42444-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-10-10 05:41:46,244] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-123, correlationId=9, api=DELETE_ACLS, version=1, body={filters=[{resource_type=2,resource_name=foobar,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:34475-127.0.0.1:42444-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-10-10 05:41:46,367] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-123, correlationId=18, api=DESCRIBE_ACLS, version=1, body={resource_type=2,resource_name=*,resource_pattern_type_filter=3,principal=User:*,host=*,operation=2,permission_type=3} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:34475-127.0.0.1:42444-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-10-10 05:41:46,369] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-123, correlationId=19, api=CREATE_ACLS, version=1, body={creations=[{resource_type=2,resource_name=foobar,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:34475-127.0.0.1:42444-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-10-10 05:41:46,370] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-123, correlationId=20, api=DELETE_ACLS, version=1, body={filters=[{resource_type=2,resource_name=foobar,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:34475-127.0.0.1:42444-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-10-10 05:41:46,375] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-123, correlationId=22, api=CREATE_ACLS, version=1, body={creations=[{resource_type=2,resource_name=foobar,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:34475-127.0.0.1:42444-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-10-10 05:41:46,376] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-123, correlationId=23, api=DELETE_ACLS, version=1, body={filters=[{resource_type=2,resource_name=foobar,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:34475-127.0.0.1:42444-0, session=Session(User:ANONYMOUS,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.
[2019-10-10 05:42:20,264] ERROR [Controller id=0] Ignoring request to reassign partition topicA-0 that doesn't exist. (kafka.controller.KafkaController:74)
[2019-10-10 05:42:20,286] ERROR [Controller id=0] Ignoring request to reassign partition alter-reassignments-topic-1-3 that doesn't exist. (kafka.controller.KafkaController:74)
[2019-10-10 05:42:34,736] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:42:44,621] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:42:44,621] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:42:53,326] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:42:53,326] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:43:01,694] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition elect-preferred-leaders-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:43:01,695] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition elect-preferred-leaders-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:43:01,867] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition elect-preferred-leaders-topic-2-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:43:01,867] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition elect-preferred-leaders-topic-2-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:43:15,654] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition list-reassignments-no-reassignments-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:43:15,656] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition list-reassignments-no-reassignments-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:43:33,112] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:44:21,733] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition mytopic2-2 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:44:26,910] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:44:26,918] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:44:49,651] ERROR [ReplicaManager broker=0] Error while changing replica dir for partition topic-0 (kafka.server.ReplicaManager:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: Partition topic-0 doesn't exist
[2019-10-10 05:44:49,654] ERROR [ReplicaManager broker=2] Error while changing replica dir for partition topic-0 (kafka.server.ReplicaManager:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: Partition topic-0 doesn't exist
[2019-10-10 05:44:49,654] ERROR [ReplicaManager broker=1] Error while changing replica dir for partition topic-0 (kafka.server.ReplicaManager:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: Partition topic-0 doesn't exist
[2019-10-10 05:44:49,694] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:44:49,696] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:44:56,790] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:44:56,791] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:45:29,249] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:45:29,250] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:45:38,801] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:45:46,116] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition create-partitions-topic-2-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:45:46,435] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition create-partitions-topic-2-2 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:45:46,465] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition create-partitions-topic-2-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:46:09,864] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:46:09,866] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:46:19,421] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:46:19,422] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:46:27,426] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:46:51,423] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:46:51,424] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:47:05,589] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:47:05,593] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-10 05:47:12,535] ERROR [Consumer instanceId=test_instance_id, clientId=test_client_id, groupId=test_group_id] Offset commit failed on partition test_topic-1 at offset 0: Specified group generation id is not valid. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1095)
[2019-10-10 05:47:20,242] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.{noformat}",,bbejeck,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-10-15 20:05:09.091,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 17 17:31:24 UTC 2019,,,,,,,"0|z07h2w:",9223372036854775807,,,,,,,,,,,,,,,,"15/Oct/19 20:05;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/25846/testReport/junit/kafka.api/SslAdminClientIntegrationTest/testCreateTopicsResponseMetadataAndConfig/]","16/Oct/19 05:36;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/25869/testReport/junit/kafka.api/SslAdminClientIntegrationTest/testCreateTopicsResponseMetadataAndConfig/]","16/Oct/19 18:50;mjsax;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.12/8674/testReport/junit/kafka.api/SslAdminClientIntegrationTest/testCreateTopicsResponseMetadataAndConfig/]","17/Oct/19 17:31;mjsax;Failure in `SaslSslAdminClientIntegrationTest.testCreateTopicsResponseMetadataAndConfig` but seems related.

[https://builds.apache.org/job/kafka-pr-jdk11-scala2.12/8729/testReport/junit/kafka.api/SaslSslAdminClientIntegrationTest/testCreateTopicsResponseMetadataAndConfig/]",,,,,,,,,,,,,,,,,,
Flaky Test KTableKTableForeignKeyJoinIntegrationTest.doLeftJoinFilterOutRapidlyChangingForeignKeyValues,KAFKA-9006,13261358,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,vvcephei,bbejeck,bbejeck,09/Oct/19 15:46,15/Oct/19 03:02,12/Jan/21 11:54,15/Oct/19 03:02,,,,,,,,,,streams,unit tests,,,0,flaky-test,,,"h3.  
{noformat}
Error Message
array lengths differed, expected.length=2 actual.length=1; arrays first differed at element [0]; expected:<KeyValue(3, value1=7.77,value2=70)> but was:<KeyValue(3, value1=1.11,value2=10)>

Stacktrace
array lengths differed, expected.length=2 actual.length=1; arrays first differed at element [0]; expected:<KeyValue(3, value1=7.77,value2=70)> but was:<KeyValue(3, value1=1.11,value2=10)> at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:78) at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:28) at org.junit.Assert.internalArrayEquals(Assert.java:534) at org.junit.Assert.assertArrayEquals(Assert.java:285) at org.junit.Assert.assertArrayEquals(Assert.java:300) at org.apache.kafka.streams.integration.KTableKTableForeignKeyJoinIntegrationTest.doLeftJoinFilterOutRapidlyChangingForeignKeyValues(KTableKTableForeignKeyJoinIntegrationTest.java:585) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:305) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:365) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:330) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:78) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:328) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:65) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:292) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:305) at org.junit.runners.ParentRunner.run(ParentRunner.java:412) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:330) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:78) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:328) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:65) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:292) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:305) at org.junit.runners.ParentRunner.run(ParentRunner.java:412) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38) at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62) at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51) at jdk.internal.reflect.GeneratedMethodAccessor2.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33) at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94) at com.sun.proxy.$Proxy5.processTestClass(Unknown Source) at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118) at jdk.internal.reflect.GeneratedMethodAccessor1.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164) at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412) at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64) at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56) at java.base/java.lang.Thread.run(Thread.java:834) Caused by: java.lang.AssertionError: expected:<KeyValue(3, value1=7.77,value2=70)> but was:<KeyValue(3, value1=1.11,value2=10)> at org.junit.Assert.fail(Assert.java:89) at org.junit.Assert.failNotEquals(Assert.java:835) at org.junit.Assert.assertEquals(Assert.java:120) at org.junit.Assert.assertEquals(Assert.java:146) at org.junit.internal.ExactComparisonCriteria.assertElementsEqual(ExactComparisonCriteria.java:8) at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:76) ... 67 more
{noformat}
 ",,ableegoldman,bbejeck,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,KAFKA-9000,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-10-09 17:19:04.859,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 15 03:02:41 UTC 2019,,,,,,,"0|z07flk:",9223372036854775807,,,,,,,,,,,,,,,,"09/Oct/19 17:19;ableegoldman;Adding this to the high priority list due to incorrect output vs. timing issue","11/Oct/19 22:22;ableegoldman;Not sure if this counts as a failure or not, but: reported as failed on this build [https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/2565/]

...however when you click the test, instead of a stacktrace it just says ""*Passed*"" ","15/Oct/19 03:02;vvcephei;It isn't a true duplicate, but both flaky tests have the same underlying flaw: depending on specific interleavings produced when running 3 Streams instances concurrently.

It might be possible (and desirable) to make such a test work, but the facts of life are that our testing environment doesn't have enough resources to run a test like this reliably.

So, I'm fixing both issues with the same PR and just moving the whole test over to TopologyTestDriver.",,,,,,,,,,,,,,,,,,,
Flaky Test kafka.api.SaslSslAdminClientIntegrationTest.testDescribeConfigsForTopic,KAFKA-9008,13261374,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,bbejeck,bbejeck,09/Oct/19 16:32,09/Oct/19 16:32,12/Jan/21 11:54,,,,,,,,,,,core,,,,0,flaky-test,,,"Failure seen in [https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/25644/testReport/junit/kafka.api/SaslSslAdminClientIntegrationTest/testDescribeConfigsForTopic/]

 
{noformat}
Error Messageorg.junit.runners.model.TestTimedOutException: test timed out after 120000 millisecondsStacktraceorg.junit.runners.model.TestTimedOutException: test timed out after 120000 milliseconds
	at java.io.FileDescriptor.sync(Native Method)
	at jdbm.recman.TransactionManager.sync(TransactionManager.java:385)
	at jdbm.recman.TransactionManager.commit(TransactionManager.java:368)
	at jdbm.recman.RecordFile.commit(RecordFile.java:320)
	at jdbm.recman.PageManager.commit(PageManager.java:289)
	at jdbm.recman.BaseRecordManager.commit(BaseRecordManager.java:419)
	at jdbm.recman.CacheRecordManager.commit(CacheRecordManager.java:350)
	at org.apache.directory.server.core.partition.impl.btree.jdbm.JdbmTable.sync(JdbmTable.java:976)
	at org.apache.directory.server.core.partition.impl.btree.jdbm.JdbmTable.close(JdbmTable.java:961)
	at org.apache.directory.server.core.partition.impl.btree.jdbm.JdbmIndex.close(JdbmIndex.java:571)
	at org.apache.directory.server.core.partition.impl.btree.AbstractBTreePartition.doDestroy(AbstractBTreePartition.java:524)
	at org.apache.directory.server.core.partition.impl.btree.jdbm.JdbmPartition.doDestroy(JdbmPartition.java:744)
	at org.apache.directory.server.core.api.partition.AbstractPartition.destroy(AbstractPartition.java:153)
	at org.apache.directory.server.core.shared.partition.DefaultPartitionNexus.removeContextPartition(DefaultPartitionNexus.java:886)
	at org.apache.directory.server.core.shared.partition.DefaultPartitionNexus.doDestroy(DefaultPartitionNexus.java:287)
	at org.apache.directory.server.core.api.partition.AbstractPartition.destroy(AbstractPartition.java:153)
	at org.apache.directory.server.core.DefaultDirectoryService.shutdown(DefaultDirectoryService.java:1313)
	at kafka.security.minikdc.MiniKdc.stop(MiniKdc.scala:278)
	at kafka.api.SaslSetup$class.closeSasl(SaslSetup.scala:120)
	at kafka.api.SaslSslAdminClientIntegrationTest.closeSasl(SaslSslAdminClientIntegrationTest.scala:38)
	at kafka.api.SaslSslAdminClientIntegrationTest.tearDown(SaslSslAdminClientIntegrationTest.scala:100)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:288)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:282)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
Standard OutputDebug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka5680437396247072679.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka308994997839802486.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka4996638752651080021.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka724961077517167281.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka2497532072326150282.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8629315128259227563.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka1627052457240864846.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka4368738172013102294.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka1518149963208228299.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka431919248028889988.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 03:27:30,751] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-160, correlationId=4, api=CREATE_ACLS, version=1, body={creations=[{resource_type=2,resource_name=foobar,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:39487-127.0.0.1:35930-0, session=Session(User:client2,localhost/127.0.0.1), listenerName=ListenerName(SASL_SSL), securityProtocol=SASL_SSL, buffer=null) is not authorized.
[2019-10-09 03:27:30,754] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-160, correlationId=5, api=DELETE_ACLS, version=1, body={filters=[{resource_type=2,resource_name=foobar,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:39487-127.0.0.1:35930-0, session=Session(User:client2,localhost/127.0.0.1), listenerName=ListenerName(SASL_SSL), securityProtocol=SASL_SSL, buffer=null) is not authorized.
[2019-10-09 03:27:30,757] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-160, correlationId=6, api=DESCRIBE_ACLS, version=1, body={resource_type=2,resource_name=*,resource_pattern_type_filter=3,principal=User:*,host=*,operation=2,permission_type=3} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:39487-127.0.0.1:35930-0, session=Session(User:client2,localhost/127.0.0.1), listenerName=ListenerName(SASL_SSL), securityProtocol=SASL_SSL, buffer=null) is not authorized.
[2019-10-09 03:27:30,759] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-160, correlationId=7, api=CREATE_ACLS, version=1, body={creations=[{resource_type=2,resource_name=foobar,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:39487-127.0.0.1:35930-0, session=Session(User:client2,localhost/127.0.0.1), listenerName=ListenerName(SASL_SSL), securityProtocol=SASL_SSL, buffer=null) is not authorized.
[2019-10-09 03:27:30,760] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-160, correlationId=8, api=DELETE_ACLS, version=1, body={filters=[{resource_type=2,resource_name=foobar,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:39487-127.0.0.1:35930-0, session=Session(User:client2,localhost/127.0.0.1), listenerName=ListenerName(SASL_SSL), securityProtocol=SASL_SSL, buffer=null) is not authorized.
[2019-10-09 03:27:30,883] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-160, correlationId=17, api=DESCRIBE_ACLS, version=1, body={resource_type=2,resource_name=*,resource_pattern_type_filter=3,principal=User:*,host=*,operation=2,permission_type=3} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:39487-127.0.0.1:35930-0, session=Session(User:client2,localhost/127.0.0.1), listenerName=ListenerName(SASL_SSL), securityProtocol=SASL_SSL, buffer=null) is not authorized.
[2019-10-09 03:27:30,885] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-160, correlationId=18, api=CREATE_ACLS, version=1, body={creations=[{resource_type=2,resource_name=foobar,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:39487-127.0.0.1:35930-0, session=Session(User:client2,localhost/127.0.0.1), listenerName=ListenerName(SASL_SSL), securityProtocol=SASL_SSL, buffer=null) is not authorized.
[2019-10-09 03:27:30,886] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-160, correlationId=19, api=DELETE_ACLS, version=1, body={filters=[{resource_type=2,resource_name=foobar,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:39487-127.0.0.1:35930-0, session=Session(User:client2,localhost/127.0.0.1), listenerName=ListenerName(SASL_SSL), securityProtocol=SASL_SSL, buffer=null) is not authorized.
[2019-10-09 03:27:30,889] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-160, correlationId=20, api=DESCRIBE_ACLS, version=1, body={resource_type=2,resource_name=*,resource_pattern_type_filter=3,principal=User:*,host=*,operation=2,permission_type=3} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:39487-127.0.0.1:35930-0, session=Session(User:client2,localhost/127.0.0.1), listenerName=ListenerName(SASL_SSL), securityProtocol=SASL_SSL, buffer=null) is not authorized.
[2019-10-09 03:27:30,992] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-160, correlationId=22, api=CREATE_ACLS, version=1, body={creations=[{resource_type=2,resource_name=foobar,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:39487-127.0.0.1:35930-0, session=Session(User:client2,localhost/127.0.0.1), listenerName=ListenerName(SASL_SSL), securityProtocol=SASL_SSL, buffer=null) is not authorized.
[2019-10-09 03:27:30,993] ERROR [KafkaApi-2] Error when handling request: clientId=adminclient-160, correlationId=23, api=DELETE_ACLS, version=1, body={filters=[{resource_type=2,resource_name=foobar,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,resource_pattern_type_filter=3,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=1, connectionId=127.0.0.1:39487-127.0.0.1:35930-0, session=Session(User:client2,localhost/127.0.0.1), listenerName=ListenerName(SASL_SSL), securityProtocol=SASL_SSL, buffer=null) is not authorized.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka4494612449568468066.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka442906374893016863.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka4921508133374398531.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8921081949129250627.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka4800946551664104351.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka3982161438008570212.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8776113499232750968.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka2576488126024187503.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka5802426984949555715.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka4177555449870714998.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 03:32:19,366] ERROR [Controller id=0] Ignoring request to reassign partition topicA-0 that doesn't exist. (kafka.controller.KafkaController:74)
[2019-10-09 03:32:19,373] ERROR [Controller id=0] Ignoring request to reassign partition alter-reassignments-topic-1-3 that doesn't exist. (kafka.controller.KafkaController:74)
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka5450269149515467558.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8512080914407766374.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka1479318177245981872.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka1200285956371016189.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 03:33:59,631] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka2749421893162098416.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka3034985165070881289.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 03:34:55,412] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 03:34:55,412] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka5500660939840301846.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka5309865643419174523.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 03:35:57,351] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 03:35:57,354] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8401995427865786088.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka342119034747636901.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 03:37:19,318] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition elect-preferred-leaders-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 03:37:19,319] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition elect-preferred-leaders-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 03:37:19,806] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition elect-preferred-leaders-topic-2-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 03:37:19,806] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition elect-preferred-leaders-topic-2-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka7530819319770264536.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka3125512884038499199.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka305054832503570942.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka1342808022763279910.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 03:40:08,628] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition list-reassignments-no-reassignments-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 03:40:08,630] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition list-reassignments-no-reassignments-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka1713774067866350109.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka7495514860241247120.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka4059302399281859947.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka6574994547938359240.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka3065342437446936230.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 03:42:09,782] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 03:42:09,782] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka523740381329832381.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8318552365719462756.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka3720068752835787759.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka9045266023620282952.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka3585159690628331231.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka2365114781284551116.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka1365699454615588058.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka9087667955232865386.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8567390976430786313.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka6385488490747023729.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka1889769144940678292.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka7696635167189303829.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka4363768049608017335.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka9018197047522921827.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka6047649408667795946.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 03:47:01,828] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition mytopic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka7273019373616094056.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 03:47:42,715] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 03:47:42,719] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8552753150738913898.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka3043092930490674036.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka2197728057987554122.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka6617168584137306271.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka4180320228561876773.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8168305056021341060.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka6968831782546500798.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 03:49:53,147] ERROR [ReplicaManager broker=0] Error while changing replica dir for partition topic-0 (kafka.server.ReplicaManager:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: Partition topic-0 doesn't exist
[2019-10-09 03:49:53,150] ERROR [ReplicaManager broker=2] Error while changing replica dir for partition topic-0 (kafka.server.ReplicaManager:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: Partition topic-0 doesn't exist
[2019-10-09 03:49:53,151] ERROR [ReplicaManager broker=1] Error while changing replica dir for partition topic-0 (kafka.server.ReplicaManager:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: Partition topic-0 doesn't exist
[2019-10-09 03:49:53,198] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 03:49:53,198] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka9108846804214688231.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 03:50:42,360] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 03:50:42,361] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8015145437279754818.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka4820681349510036856.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka1244655295830275279.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8363922861068812836.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka7123077206528836075.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka522824697598970654.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka6110069129353578438.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 03:53:37,489] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 03:53:37,490] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka7375248061439919031.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka6796582772206021404.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 03:54:39,564] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka5302075924719142816.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka1246546978760524557.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 03:55:46,268] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition create-partitions-topic-2-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 03:55:47,338] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition create-partitions-topic-2-2 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka150342503211719490.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka5078848257968162978.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka6633030989669624835.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka3913906183853681842.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka3404505020059980963.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka4215300166833523570.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka817808441401459995.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka7492702189473950873.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 03:59:47,231] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 03:59:47,231] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8640081073299117809.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 04:00:49,701] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 04:00:49,709] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka960899380318375346.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka6073812121262391509.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka1520728868576228488.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 04:02:05,910] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka1031301788440863814.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka7735549644433294520.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8616709308883479493.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka2088138529104718124.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka5172182150459152575.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 04:05:20,957] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 04:05:20,961] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka7173714805571693925.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka2982220448573127630.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka5136082085189409345.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka9106507471090241601.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 04:08:32,335] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2019-10-09 04:08:32,338] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka730781745836724592.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka4399630868100982085.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka135208832766166092.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 04:10:12,062] ERROR [Consumer instanceId=test_instance_id, clientId=test_client_id, groupId=test_group_id] Offset commit failed on partition test_topic-1 at offset 0: Specified group generation id is not valid. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1094)
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka6598372433250055567.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka8224297072457207494.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded 

[2019-10-09 04:11:18,141] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition unclean-test-topic-1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka9012021990106498330.tmp refreshKrb5Config is false principal is kafka/localhost@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is kafka/localhost@EXAMPLE.COM
Will use keytab
Commit Succeeded 

Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /tmp/kafka5823614578244622389.tmp refreshKrb5Config is false principal is client2@EXAMPLE.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
principal is client2@EXAMPLE.COM
Will use keytab
Commit Succeeded {noformat}",,bbejeck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-10-09 16:32:59.0,,,,,,,"0|z07fp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaStreamsTest fails in trunk,KAFKA-6215,13118610,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mjsax,yuzhihong@gmail.com,yuzhihong@gmail.com,15/Nov/17 17:44,27/Sep/19 19:09,12/Jan/21 11:54,15/Nov/17 19:41,,,,,1.0.1,1.1.0,,,,,,,,0,,,,"Two subtests fail.

https://builds.apache.org/job/kafka-trunk-jdk9/193/testReport/junit/org.apache.kafka.streams/KafkaStreamsTest/testCannotCleanupWhileRunning/
{code}
org.apache.kafka.streams.errors.StreamsException: org.apache.kafka.streams.errors.ProcessorStateException: state directory [/tmp/kafka-streams/testCannotCleanupWhileRunning] doesn't exist and couldn't be created
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:618)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:505)
	at org.apache.kafka.streams.KafkaStreamsTest.testCannotCleanupWhileRunning(KafkaStreamsTest.java:462)
{code}
testCleanup fails in similar manner.",,githubbot,guozhang,mjsax,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-11-15 17:56:50.173,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 27 19:09:24 UTC 2019,,,,,,,"0|i3mtlz:",9223372036854775807,,,,,,,,,,,,,,,,"15/Nov/17 17:56;guozhang;I think [~mjsax] has already worked on that with a PR.","15/Nov/17 19:20;mjsax;https://github.com/apache/kafka/pull/4221","15/Nov/17 19:41;guozhang;Issue resolved by pull request 4221
[https://github.com/apache/kafka/pull/4221]","15/Nov/17 19:42;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/4221
","27/Sep/19 19:09;guozhang;Should have been fixed via https://github.com/apache/kafka/pull/7382",,,,,,,,,,,,,,,,,
Flaky test: SslTransportLayerTest.testListenerConfigOverride,KAFKA-8322,13231561,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,dhruvilshah,dhruvilshah,03/May/19 23:13,12/Jul/19 19:42,12/Jan/21 11:54,,,,,,,,,,,core,unit tests,,,0,,,,"java.lang.AssertionError: expected:<AUTHENTICATION_FAILED> but was:<AUTHENTICATE> at org.junit.Assert.fail(Assert.java:89) at org.junit.Assert.failNotEquals(Assert.java:835) at org.junit.Assert.assertEquals(Assert.java:120) at org.junit.Assert.assertEquals(Assert.java:146) at org.apache.kafka.common.network.NetworkTestUtils.waitForChannelClose(NetworkTestUtils.java:111)

[https://builds.apache.org/job/kafka-pr-jdk11-scala2.12/4250/testReport/junit/org.apache.kafka.common.network/SslTransportLayerTest/testListenerConfigOverride/]",,dhruvilshah,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-07-12 19:42:04.761,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 12 19:42:04 UTC 2019,,,,,,,"0|z02dy8:",9223372036854775807,,,,,,,,,,,,,,,,"12/Jul/19 19:42;mjsax;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/252/testReport/junit/org.apache.kafka.common.network/SslTransportLayerTest/testListenerConfigOverride/]",,,,,,,,,,,,,,,,,,,,,
Support named listeners in system tests,KAFKA-8557,13240273,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,stanislav.vodetskyi,stanislav.vodetskyi,18/Jun/19 20:12,21/Jun/19 17:20,12/Jan/21 11:54,,,,,,,,,,,system tests,,,,0,system-tests,,,"Kafka currently supports named listeners, where you can have two or more listeners with the same security protocol but different names. Current {{KafkaService}} implementation, however, wouldn't allow that, since listeners in {{port_mappings}} are keyed by {{security_protocol}}, so there's 1-1 relationship. Kafka clients in system tests use {{bootstrap_servers()}} method, which also accepts {{security_protocol}}, as a way to pick a port to talk to kafka.
The scope of this jira is to refactor KafkaService to support named listeners, specifically two things - ability to have custom-named listeners and ability to have several listeners with the same security protocol. ",,githubbot,rsivaram,stanislav.vodetskyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-06-21 16:51:50.875,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 21 17:20:46 UTC 2019,,,,,,,"0|z03vk0:",9223372036854775807,,,,,,,,,,,,,,,,"21/Jun/19 16:51;githubbot;rajinisivaram commented on pull request #6938: KAFKA-8557: system tests - add support for (optional) interbroker listener with the same security protocol as client listeners
URL: https://github.com/apache/kafka/pull/6938
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","21/Jun/19 17:20;rsivaram;Merged [https://github.com/apache/kafka/pull/6938] which adds support for inter-broker listener and client listener with the same security protocol on different listeners. Leaving this Jira open to add full support for named listeners",,,,,,,,,,,,,,,,,,,,
Implement a system test that creates network partitions,KAFKA-5476,13080964,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,cmccabe,cmccabe,cmccabe,19/Jun/17 23:49,20/May/19 19:00,12/Jan/21 11:54,20/May/19 19:00,,,,,,,,,,,,,,0,,,,Implement a system test that creates network partitions,,cmccabe,githubbot,jeromatron,mihbor,zhz,,,,,,,,,,,,,,,,,,KAFKA-1347,KAFKA-5777,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-06-21 18:33:28.032,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 30 21:21:26 UTC 2017,,,,,,,"0|i3gguv:",9223372036854775807,,,,,,,,,,,,,,,,"21/Jun/17 18:33;jeromatron;Perhaps producing distributed tests with jepsen as a starting point would be useful.  I've seen it done other places and it's worked well.

For reference: http://blog.empathybox.com/post/62279088548/a-few-notes-on-kafka-and-jepsen","21/Jun/17 23:41;githubbot;GitHub user cmccabe opened a pull request:

    https://github.com/apache/kafka/pull/3404

    KAFKA-5476: Implement a system test that creates network partitions

    KAFKA-5476: Implement a system test that creates network partitions

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/cmccabe/kafka KAFKA-5476

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3404.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3404
    
----
commit 23b85ccf6c3195be56dbca918dd130de8c68504f
Author: Colin P. Mccabe <cmccabe@confluent.io>
Date:   2017-06-20T21:31:46Z

    KAFKA-5484: Refactor kafkatest docker support

commit 5dd1672a906c55bca800927ce235a0b6989a0559
Author: Colin P. Mccabe <cmccabe@confluent.io>
Date:   2017-06-20T17:33:09Z

    Add PartitionedProduceConsumeTest

----
","21/Jun/17 23:47;cmccabe;Right now, this pull request incorporates KAFKA-5484, since that is needed to test it.  The goal is to get KAFKA-5484 committed first, though. See https://github.com/apache/kafka/pull/3389

The general approach here is to use iptables to create network partitions.  We create DENY rules in iptables which prevent nodes in one partition from talking to nodes in another.  However, it is still possible for the controller node to talk to nodes in any partition, and it is still possible for the nodes to talk to the internet.

There are three tests included here: a unit test of NetworkFracture, a sanity test which tests that NetworkFracture actually does fracture the network, and a producer/consumer test that verifies that we are robust in the face of network partitions.","21/Jun/17 23:49;cmccabe;[~jeromatron]: I think jepsen is a good guide for the sorts of things we should be doing for fault injection.  However, I don't think that jepsen is doing anything fundamentally different than what ducktape is doing in terms of creating clusters, logging into nodes, etc..  Since we have a lot of tests in ducktape right now, it's probably better to start there.  That also ensures that any new tests we add are run as part of our existing test suite.","30/Aug/17 21:21;githubbot;Github user cmccabe closed the pull request at:

    https://github.com/apache/kafka/pull/3404
",,,,,,,,,,,,,,,,,
Add tests for generics in KStream API ,KAFKA-8035,13219265,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,cadonna,cadonna,04/Mar/19 12:25,18/Mar/19 20:35,12/Jan/21 11:54,,,,,,,,,,,streams,,,,0,unit-test,,,"During the work on KAFKA-4217, it was discovered that some combinations of Java generics block the usage of lambda expressions as parameters of the KStream API ([see this discussion|https://github.com/apache/kafka/pull/5273#discussion_r216810275]).

To avoid using those blocking combinations of generics, tests shall be implemented that verify that both lambda expressions as well as classes (anonymous and declared) can be used as parameters with the KStream API. Those tests may also serve as regression tests to ensure that future changes to the generics in the KStream API may not make the API incompatible with previous versions.

Unlike other tests, the tests required here pass if they compile. For example, to verify that the parameter {{mapper}} in
{code:java}
<KR, VR> KStream<KR, VR> flatMap(final KeyValueMapper<? super K, ? super V, ? extends Iterable<? extends KeyValue<? extends KR, ? extends VR>>> mapper);
{code}
accepts a {{KeyValueMapper}} specified as a lambda expression that returns an implementation of the {{Iterable}} interface, the following stream could be specified in the test:
{code:java}
stream
    .flatMap((Integer key, Integer value) -> Arrays.asList(
        KeyValue.pair(key, value),
        KeyValue.pair(key, value),
        KeyValue.pair(key, value)))
    .foreach(action);
{code}
If the test compiles, the test passes.

Other tests for {{flatMap}} need to check the bounds of the generics, e.g., if the {{mapper}} accepts a {{KeyValueMapper}} specified as a lambda expression that takes a super class of K and V as inputs.",,cadonna,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-03-04 12:25:31.0,,,,,,,"0|z00aog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure that tests close ZooKeeper clients since they can impact subsequent tests,KAFKA-8118,13222160,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,17/Mar/19 15:46,18/Mar/19 08:53,12/Jan/21 11:54,18/Mar/19 08:52,,,,,2.2.1,2.3.0,,,,core,,,,0,,,,"A while ago, we added a check to verify that tests are closing ZooKeeper clients in ZooKeeperTestHarness since left over clients can impact subsequent tests (e.g by overwriting static JAAS configuration). But we have changed the ZK client since then and hence the thread name being verified is no longer correct. As a result, we have tests that are leaving ZK clients behind and we need to fix that (as well as the incorrect verification).",,githubbot,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-03-17 16:08:38.44,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 18 08:47:31 UTC 2019,,,,,,,"0|z00sh4:",9223372036854775807,,ijuma,,,,,,,,,,,,,,"17/Mar/19 16:08;githubbot;rajinisivaram commented on pull request #6456: KAFKA-8118; Ensure ZooKeeper clients are closed in tests, fix verification
URL: https://github.com/apache/kafka/pull/6456
 
 
   We verify that ZK clients are closed in tests since these can affect subsequent tests and that makes it hard to debug test failures. But because of changes to ZooKeeper client, we are now checking the wrong thread name. The thread name used now is `<creatorThreadName>-EventThread` where `creatorThreadName` varies depending on the test. Fixed verification `ZooKeeperTestHarness` to check this format and fixed tests which were leaving ZK clients behind. Also added a test to make sure we can detect changes to the thread name when we update ZK clients in future.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","18/Mar/19 08:47;githubbot;rajinisivaram commented on pull request #6456: KAFKA-8118; Ensure ZooKeeper clients are closed in tests, fix verification
URL: https://github.com/apache/kafka/pull/6456
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,
Add system test to detect compatibility issues when requests are updated,KAFKA-8112,13222005,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,rsivaram,rsivaram,15/Mar/19 21:40,15/Mar/19 21:40,12/Jan/21 11:54,,,,,,,,,,,system tests,,,,0,,,,"Both compatibility_test_new_broker_test.py and upgrade_test.py passed with the Metadata version issue in KAFKA-8111. We didn't have a full system test build after the changes, so not sure if there are other tests which may have failed. This is to make sure that we add a test that would fail for similar compatibility issues in future.",,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-03-15 21:40:00.0,,,,,,,"0|z00rio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flaky test RestServerTest.testCORSEnabled,KAFKA-7799,13208413,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,08/Jan/19 22:04,08/Mar/19 18:14,12/Jan/21 11:54,13/Feb/19 01:49,,,,,2.0.2,2.1.2,2.2.0,,,KafkaConnect,,,,0,,,,"Starting to see this failure quite a lot, locally and on jenkins:

{code}

org.apache.kafka.connect.runtime.rest.RestServerTest.testCORSEnabled

Failing for the past 7 builds (Since Failed#18600 )
Took 0.7 sec.
Error Message
java.lang.AssertionError: expected:<http://bar.com> but was:<null>
Stacktrace
java.lang.AssertionError: expected:<http://bar.com> but was:<null>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.kafka.connect.runtime.rest.RestServerTest.checkCORSRequest(RestServerTest.java:221)
	at org.apache.kafka.connect.runtime.rest.RestServerTest.testCORSEnabled(RestServerTest.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:68)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.runTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:326)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:89)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:97)
{code}

If it helps, I see an uncaught exception in the stdout:

{code}
[2019-01-08 19:35:23,664] ERROR Uncaught exception in REST call to /connector-plugins/FileStreamSource/validate (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:61)
javax.ws.rs.NotFoundException: HTTP 404 Not Found
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:274)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:272)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:268)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:316)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:298)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:268)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:289)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:256)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:703)
{code}",,githubbot,hachikuji,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-01-09 01:40:46.95,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 04 12:34:39 UTC 2019,,,,,,,"0|u00n88:",9223372036854775807,,,,,,,,,,,,,,,,"09/Jan/19 01:38;hachikuji;Going to go ahead and pick this up since I think I know the problem. Basically the dependence on the system property is a bit brittle and relies on the order of execution of the tests.","09/Jan/19 01:40;githubbot;hachikuji commented on pull request #6106: KAFKA-7799; Fix flaky test RestServerTest.testCORSEnabled
URL: https://github.com/apache/kafka/pull/6106
 
 
   The test fails 100% of the time if `testOptionsDoesNotIncludeWadlOutput` is executed before `testCORSEnabled`. It seems the problem is the use of the system property. Perhaps there is some static caching somewhere.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","09/Jan/19 07:47;githubbot;hachikuji commented on pull request #6106: KAFKA-7799; Fix flaky test RestServerTest.testCORSEnabled
URL: https://github.com/apache/kafka/pull/6106
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","01/Feb/19 04:07;omkreddy;This test is failing frequently on Jenkins builds. Reopening.","07/Feb/19 05:57;githubbot;avocader commented on pull request #6236: KAFKA-7799: Abstract HttpClient to allow sending restricted headers.
URL: https://github.com/apache/kafka/pull/6236
 
 
   The test `org.apache.kafka.connect.runtime.rest.RestServerTest#testCORSEnabled` assumes Jersey client can send restricted HTTP headers(`Origin`).
   
   Jersey client uses `sun.net.www.protocol.http.HttpURLConnection`.
   `sun.net.www.protocol.http.HttpURLConnection` filters out restricted headers(""Host"", ""Keep-Alive"", ""Origin"", etc) based on static property `allowRestrictedHeaders`.
   This property is initialized in a static block by reading Java system property `sun.net.http.allowRestrictedHeaders`.
   
   So, if classloader loads `HttpURLConnection` before we set `sun.net.http.allowRestrictedHeaders=true`, then all subsequent changes of this system property won't take any effect(which happens if `org.apache.kafka.connect.integration.ExampleConnectIntegrationTest` is executed before `RestServerTest`).
   To prevent this, we have to make sure we set `sun.net.http.allowRestrictedHeaders=true` as early as possible.
   
   This PR abstracts all HTTP calls to `org.apache.kafka.connect.util.clients.HttpClient`, which should be used in tests for HTTP in order to not interfere with `RestServerTest` and restricts imports of all HTTP-related packages in favor of enforcing to use the new client.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","12/Feb/19 20:05;githubbot;gwenshap commented on pull request #6236: KAFKA-7799: Use httpcomponents-client in RestServerTest.
URL: https://github.com/apache/kafka/pull/6236
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","15/Feb/19 18:33;githubbot;avocader commented on pull request #6276: KAFKA-7799; Use httpcomponents-client in RestServerTest.
URL: https://github.com/apache/kafka/pull/6276
 
 
   The test `org.apache.kafka.connect.runtime.rest.RestServerTest#testCORSEnabled` assumes Jersey client can send restricted HTTP headers(`Origin`).
   
   Jersey client uses `sun.net.www.protocol.http.HttpURLConnection`.
   `sun.net.www.protocol.http.HttpURLConnection` drops restricted headers(`Host`, `Keep-Alive`, `Origin`, etc) based on static property `allowRestrictedHeaders`.
   This property is initialized in a static block by reading Java system property `sun.net.http.allowRestrictedHeaders`.
   
   So, if classloader loads `HttpURLConnection` before we set `sun.net.http.allowRestrictedHeaders=true`, then all subsequent changes of this system property won't take any effect(which happens if `org.apache.kafka.connect.integration.ExampleConnectIntegrationTest` is executed before `RestServerTest`).
   To prevent this, we have to either make sure we set `sun.net.http.allowRestrictedHeaders=true` as early as possible or do not rely on this system property at all.
   
   This PR adds test dependency on `httpcomponents-client` which doesn't depend on `sun.net.http.allowRestrictedHeaders` system property. Thus none of existing tests should interfere with `RestServerTest`.
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","15/Feb/19 18:56;githubbot;avocader commented on pull request #6277: KAFKA-7799; Use httpcomponents-client in RestServerTest.
URL: https://github.com/apache/kafka/pull/6277
 
 
   The test `org.apache.kafka.connect.runtime.rest.RestServerTest#testCORSEnabled` assumes Jersey client can send restricted HTTP headers(`Origin`).
   
   Jersey client uses `sun.net.www.protocol.http.HttpURLConnection`.
   `sun.net.www.protocol.http.HttpURLConnection` drops restricted headers(`Host`, `Keep-Alive`, `Origin`, etc) based on static property `allowRestrictedHeaders`.
   This property is initialized in a static block by reading Java system property `sun.net.http.allowRestrictedHeaders`.
   
   So, if classloader loads `HttpURLConnection` before we set `sun.net.http.allowRestrictedHeaders=true`, then all subsequent changes of this system property won't take any effect(which happens if `org.apache.kafka.connect.integration.ExampleConnectIntegrationTest` is executed before `RestServerTest`).
   To prevent this, we have to either make sure we set `sun.net.http.allowRestrictedHeaders=true` as early as possible or do not rely on this system property at all.
   
   This PR adds test dependency on `httpcomponents-client` which doesn't depend on `sun.net.http.allowRestrictedHeaders` system property. Thus none of existing tests should interfere with `RestServerTest`.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","04/Mar/19 12:34;githubbot;ijuma commented on pull request #6277: KAFKA-7799; Use httpcomponents-client in RestServerTest.
URL: https://github.com/apache/kafka/pull/6277
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","04/Mar/19 12:34;githubbot;ijuma commented on pull request #6276: KAFKA-7799; Use httpcomponents-client in RestServerTest.
URL: https://github.com/apache/kafka/pull/6276
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,
Add more unit tests for SslTransportLayer,KAFKA-5964,13104331,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,rsivaram,rsivaram,rsivaram,22/Sep/17 16:10,17/Feb/19 18:55,12/Jan/21 11:54,,,,,,,,,,,,,,,0,,,,"Add unit tests for the  edge cases updated in KAFKA-5920:
1. Test that handshake failures are propagated as SslAuthenticationException even if there are I/O exceptions in any of the read/write operations
2. Test that received data is processed even after an I/O exception

",,mjsax,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-02-17 18:55:36.077,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 17 18:55:36 UTC 2019,,,,,,,"0|i3kf7z:",9223372036854775807,,,,,,,,,,,,,,,,"17/Feb/19 18:55;mjsax;Moving all major/minor/trivial tickets that are not merged yet out of 2.2 release.",,,,,,,,,,,,,,,,,,,,,
Add unit test to verify Kafka-7401 in AK versions >= 2.0,KAFKA-7923,13215312,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,apovzner,apovzner,apovzner,12/Feb/19 19:42,12/Feb/19 19:42,12/Jan/21 11:54,,2.0.1,2.1.0,,,,,,,,,,,,0,,,,"Kafka-7401 affected versions 1.0 and 1.1, which was fixed and the unit test was added. Versions 2.0 did not have that bug, because it was fixed as part of another change. To make sure we don't regress, we need to add a similar unit test that was added as part of Kafka-7401.",,apovzner,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-02-12 19:42:23.0,,,,,,,"0|yi0wr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve testing of idempotent/transactional producer shutdown,KAFKA-7783,13207375,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,hachikuji,hachikuji,02/Jan/19 22:53,02/Jan/19 22:53,12/Jan/21 11:54,,,,,,,,,,,producer ,,,,0,,,,There have been a few issues reported with shutdown of the idempotent/transactional producers (see KAFKA-5503 and KAFKA-6635). Clearly we need to improve our test coverage for shutdown behavior.,,hachikuji,mutdmour,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-01-02 22:53:24.0,,,,,,,"0|u00gtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSL renegotiation code paths need more tests,KAFKA-2609,12902322,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Won't Fix,rsivaram,rsivaram,rsivaram,04/Oct/15 16:58,22/Nov/18 09:47,12/Jan/21 11:54,22/Nov/18 09:47,0.9.0.0,,,,,,,,,,,,,0,,,,"If renegotiation is triggered when read interest is off, at the moment it looks like read interest is never turned back on. More unit tests are required to test different renegotiation scenarios since these are much harder to exercise in system tests.",,ijuma,rsivaram,sriharsha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-10-05 14:09:20.667,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 22 09:47:02 UTC 2018,,,,,,,"0|i2mkp3:",9223372036854775807,,,,,,,,,,,,,,,,"05/Oct/15 13:55;rsivaram;Another issue to test is the inconsistency in the behaviour of Selector.send() between PLAINTEXT and SSL. send() always succeeds with PLAINTEXT, but with SSL, it throws an IllegalStateException if handshake is in progress. This has been working fine with the initial handshake, but not sure if this exception is handled by producers/consumers/brokers if renegotiation is in progress.","05/Oct/15 14:09;ijuma;[~rsivaram], thanks for filing this. One question I have is whether we should be supporting renegotiation at all. It adds a lot of complexity (which increases the probability of bugs) and I don't see the benefit given that we control the protocol between client and server. It seems simpler to close a connection if we want to restart the handshake process for whatever reason. There was also a well publicised vulnerability in the TLS protocol related to renegotiation and the JDK has the `jdk.tls.rejectClientInitiatedRenegotiation` as a protection against DOS attacks.

What am I missing?","05/Oct/15 14:54;rsivaram;[~ijuma] We dont have a requirement to support SSL renegotiation at the moment. We were planning to turn off SSL renegotiation using JVM options until we have a comprehensive set of tests for renegotiation. The JDK issues around renegotiation have been fixed and in the longer term, it is an useful feature to support. But for 0.9.0.0, either simplifying the code by removing renegotiation support or improving testing and fixing bugs would work for us. 

Thoughts?","05/Oct/15 14:59;ijuma;If it is a useful feature to support longer term, then it would make sense to improve testing and fix bugs. I'd be interested in a bit more detail though. Under what scenarios do you see a Kafka client or server making use of renegotiation in the future?","05/Oct/15 15:14;rsivaram;Periodic renegotiation is typically used to refresh encryption keys. This is particularly useful if weak ciphers are used to limit the amount of time keys are in use, making the system more secure. ","05/Oct/15 15:38;ijuma;Interesting, good to know. Regarding the question about producer and consumer handling renegotiation, there are these method in `NetworkClient`:

{code}
    public boolean ready(Node node, long now) {
        if (isReady(node, now))
            return true;

        if (connectionStates.canConnect(node.idString(), now))
            // if we are interested in sending to a node and we don't have a connection to it, initiate one
            initiateConnect(node, now);

        return false;
    }

    public boolean isReady(Node node, long now) {
        // if we need to update our metadata now declare all requests unready to make metadata requests first
        // priority
        return !metadataUpdater.isUpdateDue(now) && canSendRequest(node.idString());
    }

    private boolean canSendRequest(String node) {
        return connectionStates.isConnected(node) && selector.isChannelReady(node) && inFlightRequests.canSendMore(node);
    }
{code}

They are meant to avoid sending a message to a channel that is not ready (eg handshake is taking place). However, as you say, we need a test that triggers renegotiation in the middle of consumption or production to verify that it works as expected.","06/Oct/15 12:24;sriharsha;[~rsivaram] [~ijuma] Do we need to release this as part of 0.9.0? When ssl patch got in we decided to re-visit renegotiation as part of next release. Also lets make this optional i.e turned off by default I don't see many users using weak crypto.","06/Oct/15 12:44;ijuma;[~harsha_ch], my understanding is the same as yours, we don't need renegotiation support for 0.9.0.0. Maybe the thing to do is to turn it off using JDK options as [~rsivaram] said and then target this issue to a subsequent release. Is that what you had in mind?","06/Oct/15 13:02;sriharsha;Yes [~ijuma]. Even after we get this in I would see this as optional rather turn it on by default.","07/Oct/15 13:49;rsivaram;Changing the target version as per discussion above since this can be done after 0.9.0.0","08/Oct/15 16:00;ijuma;Filed KAFKA-2618 for disabling renegotiation for 0.9.0.0.","24/Oct/15 18:25;ijuma;[~rsivaram], one thing to keep in mind is that it looks like renegotiation has been removed in the TLS 1.3 draft spec:

{quote}
1.2. Major Differences from TLS 1.2

draft-10

Remove ClientCertificateTypes field from CertificateRequest and add extensions.
Merge client and server key shares into a single extension.
draft-09

Change to RSA-PSS signatures for handshake messages.
Remove support for DSA.
Update key schedule per suggestions by Hugo, Hoeteck, and Bjoern Tackmann.
Add support for per-record padding.
Switch to encrypted record ContentType.
Change HKDF labeling to include protocol version and value lengths.
Shift the final decision to abort a handshake due to incompatible certificates to the client rather than having servers abort early.
Deprecate SHA-1 with signatures.
Add MTI algorithms.
draft-08

Remove support for weak and lesser used named curves.
Remove support for MD5 and SHA-224 hashes with signatures.
Update lists of available AEAD cipher suites and error alerts.
Reduce maximum permitted record expansion for AEAD from 2048 to 256 octets.
Require digital signatures even when a previous configuration is used.
Merge EarlyDataIndication and KnownConfiguration.
Change code point for server_configuration to avoid collision with server_hello_done.
Relax certificate_list ordering requirement to match current practice.
draft-07

Integration of semi-ephemeral DH proposal.
Add initial 0-RTT support.
Remove resumption and replace with PSK + tickets.
Move ClientKeyShare into an extension.
Move to HKDF.
draft-06

Prohibit RC4 negotiation for backwards compatibility.
Freeze & deprecate record layer version field.
Update format of signatures with context.
Remove explicit IV.
draft-05

Prohibit SSL negotiation for backwards compatibility.
Fix which MS is used for exporters.
draft-04

Modify key computations to include session hash.
Remove ChangeCipherSpec.
Renumber the new handshake messages to be somewhat more consistent with existing convention and to remove a duplicate registration.
Remove renegotiation.
Remove point format negotiation.
draft-03

Remove GMT time.
Merge in support for ECC from RFC 4492 but without explicit curves.
Remove the unnecessary length field from the AD input to AEAD ciphers.
Rename {Client,Server}KeyExchange to {Client,Server}KeyShare.
Add an explicit HelloRetryRequest to reject the client’s.
draft-02

Increment version number.
Rework handshake to provide 1-RTT mode.
Remove custom DHE groups.
Remove support for compression.
Remove support for static RSA and DH key exchange.
Remove support for non-AEAD ciphers.
{quote}
https://tlswg.github.io/tls13-spec/","22/Nov/18 09:47;rsivaram;Since renegotiation is currently disabled in Kafka and renegotiation is forbidden in TLS 1.3, closing this JIRA.",,,,,,,,,
Consumer rolling upgrade test case,KAFKA-2931,12917678,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,02/Dec/15 19:08,21/Nov/18 22:38,12/Jan/21 11:54,21/Nov/18 22:38,,,,,,,,,,,,,,0,,,,"We need a system test which covers the rolling upgrade process for the new consumer. The idea is to start the consumers with a ""range"" assignment strategy and then upgrade to ""round-robin"" without any down-time. This validates the coordinator's protocol selection process.",,githubbot,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-02 22:48:59.01,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 21 22:38:57 UTC 2018,,,,,,,"0|i2p6tz:",9223372036854775807,,,,,,,,,,,,,,,,"02/Dec/15 22:48;githubbot;GitHub user hachikuji opened a pull request:

    https://github.com/apache/kafka/pull/619

    KAFKA-2931: add system test for consumer rolling upgrades

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/hachikuji/kafka KAFKA-2931

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/619.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #619
    
----
commit 41e4ac43ed008b1043292cfc879992de3b5098ac
Author: Jason Gustafson <jason@confluent.io>
Date:   2015-12-02T22:48:14Z

    KAFKA-2931: add system test for consumer rolling upgrades

----
","04/Dec/15 05:18;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/619
","21/Nov/18 22:38;hachikuji;This was resolved by https://github.com/apache/kafka/pull/619.",,,,,,,,,,,,,,,,,,,
"ConnectionStressSpec: add ""action"", allow multiple clients",KAFKA-7428,13186500,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cmccabe,cmccabe,cmccabe,20/Sep/18 23:09,21/Nov/18 22:22,12/Jan/21 11:54,21/Nov/18 22:22,,,,,,,,,,,,,,0,,,,"ConnectionStressSpec: add ""action"", allow multiple clients",,cmccabe,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-09-20 23:09:59.648,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 26 09:08:51 UTC 2018,,,,,,,"0|i3ybxj:",9223372036854775807,,,,,,,,,,,,,,,,"20/Sep/18 23:09;githubbot;cmccabe opened a new pull request #5668: KAFKA-7428: ConnectionStressSpec: add ""action"", allow multiple clients
URL: https://github.com/apache/kafka/pull/5668
 
 
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","26/Sep/18 09:08;githubbot;rajinisivaram closed pull request #5668: KAFKA-7428: ConnectionStressSpec: add ""action"", allow multiple clients
URL: https://github.com/apache/kafka/pull/5668
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/tools/src/main/java/org/apache/kafka/trogdor/workload/ConnectionStressSpec.java b/tools/src/main/java/org/apache/kafka/trogdor/workload/ConnectionStressSpec.java
index 4195f9b1977..c22396f7a22 100644
--- a/tools/src/main/java/org/apache/kafka/trogdor/workload/ConnectionStressSpec.java
+++ b/tools/src/main/java/org/apache/kafka/trogdor/workload/ConnectionStressSpec.java
@@ -24,40 +24,52 @@
 import org.apache.kafka.trogdor.task.TaskSpec;
 import org.apache.kafka.trogdor.task.TaskWorker;
 
+import java.util.ArrayList;
 import java.util.Collections;
+import java.util.List;
 import java.util.Map;
 import java.util.Set;
+import java.util.TreeSet;
 
 /**
  * The specification for a task which connects and disconnects many times a
  * second to stress the broker.
  */
 public class ConnectionStressSpec extends TaskSpec {
-    private final String clientNode;
+    private final List<String> clientNodes;
     private final String bootstrapServers;
     private final Map<String, String> commonClientConf;
     private final int targetConnectionsPerSec;
     private final int numThreads;
+    private final ConnectionStressAction action;
+
+    enum ConnectionStressAction {
+        CONNECT,
+        FETCH_METADATA
+    }
 
     @JsonCreator
     public ConnectionStressSpec(@JsonProperty(""startMs"") long startMs,
             @JsonProperty(""durationMs"") long durationMs,
-            @JsonProperty(""clientNode"") String clientNode,
+            @JsonProperty(""clientNode"") List<String> clientNodes,
             @JsonProperty(""bootstrapServers"") String bootstrapServers,
             @JsonProperty(""commonClientConf"") Map<String, String> commonClientConf,
             @JsonProperty(""targetConnectionsPerSec"") int targetConnectionsPerSec,
-            @JsonProperty(""numThreads"") int numThreads) {
+            @JsonProperty(""numThreads"") int numThreads,
+            @JsonProperty(""action"") ConnectionStressAction action) {
         super(startMs, durationMs);
-        this.clientNode = (clientNode == null) ? """" : clientNode;
+        this.clientNodes = (clientNodes == null) ? Collections.emptyList() :
+            Collections.unmodifiableList(new ArrayList<>(clientNodes));
         this.bootstrapServers = (bootstrapServers == null) ? """" : bootstrapServers;
         this.commonClientConf = configOrEmptyMap(commonClientConf);
         this.targetConnectionsPerSec = targetConnectionsPerSec;
         this.numThreads = numThreads < 1 ? 1 : numThreads;
+        this.action = (action == null) ? ConnectionStressAction.CONNECT : action;
     }
 
     @JsonProperty
-    public String clientNode() {
-        return clientNode;
+    public List<String> clientNode() {
+        return clientNodes;
     }
 
     @JsonProperty
@@ -80,11 +92,16 @@ public int numThreads() {
         return numThreads;
     }
 
+    @JsonProperty
+    public ConnectionStressAction action() {
+        return action;
+    }
+
     public TaskController newController(String id) {
         return new TaskController() {
             @Override
             public Set<String> targetNodes(Topology topology) {
-                return Collections.singleton(clientNode);
+                return new TreeSet<>(clientNodes);
             }
         };
     }
diff --git a/tools/src/main/java/org/apache/kafka/trogdor/workload/ConnectionStressWorker.java b/tools/src/main/java/org/apache/kafka/trogdor/workload/ConnectionStressWorker.java
index 5d78de8f6fa..82d1d6c49ac 100644
--- a/tools/src/main/java/org/apache/kafka/trogdor/workload/ConnectionStressWorker.java
+++ b/tools/src/main/java/org/apache/kafka/trogdor/workload/ConnectionStressWorker.java
@@ -24,6 +24,7 @@
 import org.apache.kafka.clients.ManualMetadataUpdater;
 import org.apache.kafka.clients.NetworkClient;
 import org.apache.kafka.clients.NetworkClientUtils;
+import org.apache.kafka.clients.admin.AdminClient;
 import org.apache.kafka.clients.admin.AdminClientConfig;
 import org.apache.kafka.clients.producer.ProducerConfig;
 import org.apache.kafka.common.Cluster;
@@ -132,7 +133,15 @@ public void run() {
                     }
                     throttle.increment();
                     long lastTimeMs = throttle.lastTimeMs();
-                    boolean success = attemptConnection(conf, updater);
+                    boolean success = false;
+                    switch (spec.action()) {
+                        case CONNECT:
+                            success = attemptConnection(conf, updater);
+                            break;
+                        case FETCH_METADATA:
+                            success = attemptMetadataFetch(props);
+                            break;
+                    }
                     synchronized (ConnectionStressWorker.this) {
                         totalConnections++;
                         if (!success) {
@@ -185,6 +194,17 @@ private boolean attemptConnection(AdminClientConfig conf,
                 return false;
             }
         }
+
+        private boolean attemptMetadataFetch(Properties conf) {
+            try (AdminClient client = AdminClient.create(conf)) {
+                client.describeCluster().nodes().get();
+            } catch (RuntimeException e) {
+                return false;
+            } catch (Exception e) {
+                return false;
+            }
+            return true;
+        }
     }
 
     public static class StatusData {


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,
Add a trogdor test that creates many connections to brokers,KAFKA-7183,13173056,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cmccabe,cmccabe,cmccabe,18/Jul/18 22:31,21/Nov/18 22:21,12/Jan/21 11:54,21/Nov/18 22:21,,,,,,,,,,system tests,,,,0,,,,Add a trogdor test that creates many connections to brokers,,cmccabe,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-07-18 22:34:47.243,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 06 07:47:29 UTC 2018,,,,,,,"0|i3w1nr:",9223372036854775807,,,,,,,,,,,,,,,,"18/Jul/18 22:34;githubbot;cmccabe opened a new pull request #5393: KAFKA-7183: Add a trogdor test that creates many connections to brokers
URL: https://github.com/apache/kafka/pull/5393
 
 
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","18/Jul/18 22:49;cmccabe;https://github.com/apache/kafka/pull/5393","06/Aug/18 07:47;githubbot;rajinisivaram closed pull request #5393: KAFKA-7183: Add a trogdor test that creates many connections to brokers
URL: https://github.com/apache/kafka/pull/5393
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/checkstyle/import-control.xml b/checkstyle/import-control.xml
index 106ad0a8c70..0d7639bd698 100644
--- a/checkstyle/import-control.xml
+++ b/checkstyle/import-control.xml
@@ -194,6 +194,7 @@
     <allow pkg=""javax.servlet"" />
     <allow pkg=""javax.ws.rs"" />
     <allow pkg=""net.sourceforge.argparse4j"" />
+    <allow pkg=""org.apache.kafka.clients"" />
     <allow pkg=""org.apache.kafka.clients.admin"" />
     <allow pkg=""org.apache.kafka.clients.consumer"" exact-match=""true""/>
     <allow pkg=""org.apache.kafka.clients.producer"" exact-match=""true""/>
diff --git a/tools/src/main/java/org/apache/kafka/trogdor/workload/ConnectionStressSpec.java b/tools/src/main/java/org/apache/kafka/trogdor/workload/ConnectionStressSpec.java
new file mode 100644
index 00000000000..4195f9b1977
--- /dev/null
+++ b/tools/src/main/java/org/apache/kafka/trogdor/workload/ConnectionStressSpec.java
@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.kafka.trogdor.workload;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import org.apache.kafka.trogdor.common.Topology;
+import org.apache.kafka.trogdor.task.TaskController;
+import org.apache.kafka.trogdor.task.TaskSpec;
+import org.apache.kafka.trogdor.task.TaskWorker;
+
+import java.util.Collections;
+import java.util.Map;
+import java.util.Set;
+
+/**
+ * The specification for a task which connects and disconnects many times a
+ * second to stress the broker.
+ */
+public class ConnectionStressSpec extends TaskSpec {
+    private final String clientNode;
+    private final String bootstrapServers;
+    private final Map<String, String> commonClientConf;
+    private final int targetConnectionsPerSec;
+    private final int numThreads;
+
+    @JsonCreator
+    public ConnectionStressSpec(@JsonProperty(""startMs"") long startMs,
+            @JsonProperty(""durationMs"") long durationMs,
+            @JsonProperty(""clientNode"") String clientNode,
+            @JsonProperty(""bootstrapServers"") String bootstrapServers,
+            @JsonProperty(""commonClientConf"") Map<String, String> commonClientConf,
+            @JsonProperty(""targetConnectionsPerSec"") int targetConnectionsPerSec,
+            @JsonProperty(""numThreads"") int numThreads) {
+        super(startMs, durationMs);
+        this.clientNode = (clientNode == null) ? """" : clientNode;
+        this.bootstrapServers = (bootstrapServers == null) ? """" : bootstrapServers;
+        this.commonClientConf = configOrEmptyMap(commonClientConf);
+        this.targetConnectionsPerSec = targetConnectionsPerSec;
+        this.numThreads = numThreads < 1 ? 1 : numThreads;
+    }
+
+    @JsonProperty
+    public String clientNode() {
+        return clientNode;
+    }
+
+    @JsonProperty
+    public String bootstrapServers() {
+        return bootstrapServers;
+    }
+
+    @JsonProperty
+    public Map<String, String> commonClientConf() {
+        return commonClientConf;
+    }
+
+    @JsonProperty
+    public int targetConnectionsPerSec() {
+        return targetConnectionsPerSec;
+    }
+
+    @JsonProperty
+    public int numThreads() {
+        return numThreads;
+    }
+
+    public TaskController newController(String id) {
+        return new TaskController() {
+            @Override
+            public Set<String> targetNodes(Topology topology) {
+                return Collections.singleton(clientNode);
+            }
+        };
+    }
+
+    @Override
+    public TaskWorker newTaskWorker(String id) {
+        return new ConnectionStressWorker(id, this);
+    }
+}
diff --git a/tools/src/main/java/org/apache/kafka/trogdor/workload/ConnectionStressWorker.java b/tools/src/main/java/org/apache/kafka/trogdor/workload/ConnectionStressWorker.java
new file mode 100644
index 00000000000..5d78de8f6fa
--- /dev/null
+++ b/tools/src/main/java/org/apache/kafka/trogdor/workload/ConnectionStressWorker.java
@@ -0,0 +1,232 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.kafka.trogdor.workload;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import org.apache.kafka.clients.ApiVersions;
+import org.apache.kafka.clients.ClientUtils;
+import org.apache.kafka.clients.ManualMetadataUpdater;
+import org.apache.kafka.clients.NetworkClient;
+import org.apache.kafka.clients.NetworkClientUtils;
+import org.apache.kafka.clients.admin.AdminClientConfig;
+import org.apache.kafka.clients.producer.ProducerConfig;
+import org.apache.kafka.common.Cluster;
+import org.apache.kafka.common.Node;
+import org.apache.kafka.common.internals.KafkaFutureImpl;
+import org.apache.kafka.common.metrics.Metrics;
+import org.apache.kafka.common.network.ChannelBuilder;
+import org.apache.kafka.common.network.Selector;
+import org.apache.kafka.common.utils.LogContext;
+import org.apache.kafka.common.utils.Time;
+import org.apache.kafka.trogdor.common.JsonUtil;
+import org.apache.kafka.trogdor.common.Platform;
+import org.apache.kafka.trogdor.common.ThreadUtils;
+import org.apache.kafka.trogdor.common.WorkerUtils;
+import org.apache.kafka.trogdor.task.TaskWorker;
+import org.apache.kafka.trogdor.task.WorkerStatusTracker;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.net.InetSocketAddress;
+import java.util.List;
+import java.util.Properties;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.ThreadLocalRandom;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+public class ConnectionStressWorker implements TaskWorker {
+    private static final Logger log = LoggerFactory.getLogger(ConnectionStressWorker.class);
+
+    private static final int THROTTLE_PERIOD_MS = 100;
+
+    private static final int REPORT_INTERVAL_MS = 20000;
+
+    private final String id;
+
+    private final ConnectionStressSpec spec;
+
+    private final AtomicBoolean running = new AtomicBoolean(false);
+
+    private KafkaFutureImpl<String> doneFuture;
+
+    private WorkerStatusTracker status;
+
+    private long totalConnections;
+
+    private long totalFailedConnections;
+
+    private long startTimeMs;
+
+    private Throttle throttle;
+
+    private long nextReportTime;
+
+    private ExecutorService workerExecutor;
+
+    public ConnectionStressWorker(String id, ConnectionStressSpec spec) {
+        this.id = id;
+        this.spec = spec;
+    }
+
+    @Override
+    public void start(Platform platform, WorkerStatusTracker status,
+                      KafkaFutureImpl<String> doneFuture) throws Exception {
+        if (!running.compareAndSet(false, true)) {
+            throw new IllegalStateException(""ConnectionStressWorker is already running."");
+        }
+        log.info(""{}: Activating ConnectionStressWorker with {}"", id, spec);
+        this.doneFuture = doneFuture;
+        this.status = status;
+        this.totalConnections = 0;
+        this.totalFailedConnections  = 0;
+        this.startTimeMs = Time.SYSTEM.milliseconds();
+        this.throttle = new ConnectStressThrottle(WorkerUtils.
+            perSecToPerPeriod(spec.targetConnectionsPerSec(), THROTTLE_PERIOD_MS));
+        this.nextReportTime = 0;
+        this.workerExecutor = Executors.newFixedThreadPool(spec.numThreads(),
+            ThreadUtils.createThreadFactory(""ConnectionStressWorkerThread%d"", false));
+        for (int i = 0; i < spec.numThreads(); i++) {
+            this.workerExecutor.submit(new ConnectLoop());
+        }
+    }
+
+    private static class ConnectStressThrottle extends Throttle {
+        ConnectStressThrottle(int maxPerPeriod) {
+            super(maxPerPeriod, THROTTLE_PERIOD_MS);
+        }
+    }
+
+    public class ConnectLoop implements Runnable {
+        @Override
+        public void run() {
+            try {
+                Properties props = new Properties();
+                props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, spec.bootstrapServers());
+                WorkerUtils.addConfigsToProperties(props, spec.commonClientConf(), spec.commonClientConf());
+                AdminClientConfig conf = new AdminClientConfig(props);
+                List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(
+                    conf.getList(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG));
+                ManualMetadataUpdater updater = new ManualMetadataUpdater(Cluster.bootstrap(addresses).nodes());
+                while (true) {
+                    if (doneFuture.isDone()) {
+                        break;
+                    }
+                    throttle.increment();
+                    long lastTimeMs = throttle.lastTimeMs();
+                    boolean success = attemptConnection(conf, updater);
+                    synchronized (ConnectionStressWorker.this) {
+                        totalConnections++;
+                        if (!success) {
+                            totalFailedConnections++;
+                        }
+                        if (lastTimeMs > nextReportTime) {
+                            status.update(JsonUtil.JSON_SERDE.valueToTree(
+                                new StatusData(totalConnections,
+                                    totalFailedConnections,
+                                    (totalConnections * 1000.0) / (lastTimeMs - startTimeMs))));
+                            nextReportTime = lastTimeMs + REPORT_INTERVAL_MS;
+                        }
+                    }
+                }
+            } catch (Exception e) {
+                WorkerUtils.abort(log, ""ConnectionStressRunnable"", e, doneFuture);
+            }
+        }
+
+        private boolean attemptConnection(AdminClientConfig conf,
+                                          ManualMetadataUpdater updater) throws Exception {
+            try {
+                List<Node> nodes = updater.fetchNodes();
+                Node targetNode = nodes.get(ThreadLocalRandom.current().nextInt(nodes.size()));
+                try (ChannelBuilder channelBuilder = ClientUtils.createChannelBuilder(conf)) {
+                    try (Metrics metrics = new Metrics()) {
+                        LogContext logContext = new LogContext();
+                        try (Selector selector = new Selector(conf.getLong(AdminClientConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG),
+                            metrics, Time.SYSTEM, """", channelBuilder, logContext)) {
+                            try (NetworkClient client = new NetworkClient(selector,
+                                    updater,
+                                    ""ConnectionStressWorker"",
+                                    1,
+                                    1000,
+                                    1000,
+                                    4096,
+                                    4096,
+                                    1000,
+                                    Time.SYSTEM,
+                                    false,
+                                    new ApiVersions(),
+                                    logContext)) {
+                                NetworkClientUtils.awaitReady(client, targetNode, Time.SYSTEM, 100);
+                            }
+                        }
+                    }
+                }
+                return true;
+            } catch (IOException e) {
+                return false;
+            }
+        }
+    }
+
+    public static class StatusData {
+        private final long totalConnections;
+        private final long totalFailedConnections;
+        private final double connectsPerSec;
+
+        @JsonCreator
+        StatusData(@JsonProperty(""totalConnections"") long totalConnections,
+                   @JsonProperty(""totalFailedConnections"") long totalFailedConnections,
+                   @JsonProperty(""connectsPerSec"") double connectsPerSec) {
+            this.totalConnections = totalConnections;
+            this.totalFailedConnections = totalFailedConnections;
+            this.connectsPerSec = connectsPerSec;
+        }
+
+        @JsonProperty
+        public long totalConnections() {
+            return totalConnections;
+        }
+
+        @JsonProperty
+        public long totalFailedConnections() {
+            return totalFailedConnections;
+        }
+
+        @JsonProperty
+        public double connectsPerSec() {
+            return connectsPerSec;
+        }
+    }
+
+    @Override
+    public void stop(Platform platform) throws Exception {
+        if (!running.compareAndSet(true, false)) {
+            throw new IllegalStateException(""ConnectionStressWorker is not running."");
+        }
+        log.info(""{}: Deactivating ConnectionStressWorker."", id);
+        doneFuture.complete("""");
+        workerExecutor.shutdownNow();
+        workerExecutor.awaitTermination(1, TimeUnit.DAYS);
+        this.workerExecutor = null;
+        this.status = null;
+    }
+}
diff --git a/tools/src/main/java/org/apache/kafka/trogdor/workload/Throttle.java b/tools/src/main/java/org/apache/kafka/trogdor/workload/Throttle.java
index 41f9d022793..6a99c02a5dc 100644
--- a/tools/src/main/java/org/apache/kafka/trogdor/workload/Throttle.java
+++ b/tools/src/main/java/org/apache/kafka/trogdor/workload/Throttle.java
@@ -24,12 +24,14 @@
     private final int periodMs;
     private int count;
     private long prevPeriod;
+    private long lastTimeMs;
 
     Throttle(int maxPerPeriod, int periodMs) {
         this.maxPerPeriod = maxPerPeriod;
         this.periodMs = periodMs;
         this.count = maxPerPeriod;
         this.prevPeriod = -1;
+        this.lastTimeMs = 0;
     }
 
     synchronized public boolean increment() throws InterruptedException {
@@ -39,11 +41,11 @@ synchronized public boolean increment() throws InterruptedException {
                 count++;
                 return throttled;
             }
-            long now = time().milliseconds();
-            long curPeriod = now / periodMs;
+            lastTimeMs = time().milliseconds();
+            long curPeriod = lastTimeMs / periodMs;
             if (curPeriod <= prevPeriod) {
                 long nextPeriodMs = (curPeriod + 1) * periodMs;
-                delay(nextPeriodMs - now);
+                delay(nextPeriodMs - lastTimeMs);
                 throttled = true;
             } else {
                 prevPeriod = curPeriod;
@@ -52,6 +54,10 @@ synchronized public boolean increment() throws InterruptedException {
         }
     }
 
+    public synchronized long lastTimeMs() {
+        return lastTimeMs;
+    }
+
     protected Time time() {
         return Time.SYSTEM;
     }


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,
HOW TO FILETER NESTED THROUGH KAFKA CONNECT,KAFKA-7624,13198286,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,lucky786,lucky786,14/Nov/18 10:36,14/Nov/18 10:36,12/Jan/21 11:54,,,,,,,,,,,KafkaConnect,,,,0,,,,"Hi Team

please help

 

{""messageID"":""ID:COPTW_B_SIT.1D815BC7447C2A20:3880"",""messageType"":""text"",""timestamp"":1542191173487,""deliveryMode"":2,""correlationID"":null,""replyTo"":null,""destination"":\{""destinationType"":""queue"",""name"":""test.queue""},""redelivered"":false,""type"":null,""expiration"":0,""priority"":4,""properties"":{},""bytes"":null,""map"":null,{color:#f6c342}""text"":""helo{color}""}

 

 

we need to get only ""text"":""helo"" message while consuming , so that i can send direclty to mango DB.",,lucky786,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-11-14 10:36:51.0,,,,,,,"0|s00h3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SMT STRUCT to MASK or FILTER,KAFKA-7623,13198242,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,lucky786,lucky786,14/Nov/18 06:59,14/Nov/18 10:34,12/Jan/21 11:54,14/Nov/18 10:34,,,,,,,,,,KafkaConnect,,,,0,,,,"{
 ""schema"": {
 ""type"": ""struct"",
 ""fields"": [{
 ""type"": ""string"",
 ""optional"": false,
 ""doc"": ""This field stores the value of `Message.getJMSMessageID() <http://docs.oracle.com/javaee/6/api/javax/jms/Message.html#getJMSMessageID()>`_."",
 ""field"": ""messageID""
 }, {
 ""type"": ""string"",
 ""optional"": false,
 ""doc"": ""This field stores the type of message that was received. This corresponds to the subinterfaces of `Message <http://docs.oracle.com/javaee/6/api/javax/jms/Message.html>`_. `BytesMessage <http://docs.oracle.com/javaee/6/api/javax/jms/BytesMessage.html>`_ = `bytes`, `MapMessage <http://docs.oracle.com/javaee/6/api/javax/jms/MapMessage.html>`_ = `map`, `ObjectMessage <http://docs.oracle.com/javaee/6/api/javax/jms/ObjectMessage.html>`_ = `object`, `StreamMessage <http://docs.oracle.com/javaee/6/api/javax/jms/StreamMessage.html>`_ = `stream` and `TextMessage <http://docs.oracle.com/javaee/6/api/javax/jms/TextMessage.html>`_ = `text`. The corresponding field will be populated with the values from the respective Message subinterface."",
 ""field"": ""messageType""
 }, {
 ""type"": ""int64"",
 ""optional"": false,
 ""doc"": ""Data from the `getJMSTimestamp() <http://docs.oracle.com/javaee/6/api/javax/jms/Message.html#getJMSTimestamp()>`_ method."",
 ""field"": ""timestamp""
 }, {
 ""type"": ""int32"",
 ""optional"": false,
 ""doc"": ""This field stores the value of `Message.getJMSDeliveryMode() <http://docs.oracle.com/javaee/6/api/javax/jms/Message.html#getJMSDeliveryMode()>`_."",
 ""field"": ""deliveryMode""
 }, {
 ""type"": ""string"",
 ""optional"": true,
 ""doc"": ""This field stores the value of `Message.getJMSCorrelationID() <http://docs.oracle.com/javaee/6/api/javax/jms/Message.html#getJMSCorrelationID()>`_."",
 ""field"": ""correlationID""
 }, {
 ""type"": ""struct"",
 ""fields"": [{
 ""type"": ""string"",
 ""optional"": false,
 ""doc"": ""The type of JMS Destination, and either ``queue`` or ``topic``."",
 ""field"": ""destinationType""
 }, {
 ""type"": ""string"",
 ""optional"": false,
 ""doc"": ""The name of the destination. This will be the value of `Queue.getQueueName() <http://docs.oracle.com/javaee/6/api/javax/jms/Queue.html#getQueueName()>`_ or `Topic.getTopicName() <http://docs.oracle.com/javaee/6/api/javax/jms/Topic.html#getTopicName()>`_."",
 ""field"": ""name""
 }],
 ""optional"": true,
 ""name"": ""io.confluent.connect.jms.Destination"",
 ""doc"": ""This schema is used to represent a JMS Destination, and is either `queue <http://docs.oracle.com/javaee/6/api/javax/jms/Queue.html>`_ or `topic <http://docs.oracle.com/javaee/6/api/javax/jms/Topic.html>`_."",
 ""field"": ""replyTo""
 }, {
 ""type"": ""struct"",
 ""fields"": [{
 ""type"": ""string"",
 ""optional"": false,
 ""doc"": ""The type of JMS Destination, and either ``queue`` or ``topic``."",
 ""field"": ""destinationType""
 }, {
 ""type"": ""string"",
 ""optional"": false,
 ""doc"": ""The name of the destination. This will be the value of `Queue.getQueueName() <http://docs.oracle.com/javaee/6/api/javax/jms/Queue.html#getQueueName()>`_ or `Topic.getTopicName() <http://docs.oracle.com/javaee/6/api/javax/jms/Topic.html#getTopicName()>`_."",
 ""field"": ""name""
 }],
 ""optional"": true,
 ""name"": ""io.confluent.connect.jms.Destination"",
 ""doc"": ""This schema is used to represent a JMS Destination, and is either `queue <http://docs.oracle.com/javaee/6/api/javax/jms/Queue.html>`_ or `topic <http://docs.oracle.com/javaee/6/api/javax/jms/Topic.html>`_."",
 ""field"": ""destination""
 }, {
 ""type"": ""boolean"",
 ""optional"": false,
 ""doc"": ""This field stores the value of `Message.getJMSRedelivered() <http://docs.oracle.com/javaee/6/api/javax/jms/Message.html#getJMSRedelivered()>`_."",
 ""field"": ""redelivered""
 }, {
 ""type"": ""string"",
 ""optional"": true,
 ""doc"": ""This field stores the value of `Message.getJMSType() <http://docs.oracle.com/javaee/6/api/javax/jms/Message.html#getJMSType()>`_."",
 ""field"": ""type""
 }, {
 ""type"": ""int64"",
 ""optional"": false,
 ""doc"": ""This field stores the value of `Message.getJMSExpiration() <http://docs.oracle.com/javaee/6/api/javax/jms/Message.html#getJMSExpiration()>`_."",
 ""field"": ""expiration""
 }, {
 ""type"": ""int32"",
 ""optional"": false,
 ""doc"": ""This field stores the value of `Message.getJMSPriority() <http://docs.oracle.com/javaee/6/api/javax/jms/Message.html#getJMSPriority()>`_."",
 ""field"": ""priority""
 }, {
 ""type"": ""map"",
 ""keys"": {
 ""type"": ""string"",
 ""optional"": false
 },
 ""values"": {
 ""type"": ""struct"",
 ""fields"": [{
 ""type"": ""string"",
 ""optional"": false,
 ""doc"": ""The java type of the property on the Message. One of ``boolean``, ``byte``, ``short``, ``integer``, ``long``, ``float``, ``double``, or ``string``."",
 ""field"": ""propertyType""
 }, {
 ""type"": ""boolean"",
 ""optional"": true,
 ""doc"": ""The value stored as a boolean. Null unless ``propertyType`` is set to ``boolean``."",
 ""field"": ""boolean""
 }, {
 ""type"": ""int8"",
 ""optional"": true,
 ""doc"": ""The value stored as a byte. Null unless ``propertyType`` is set to ``byte``."",
 ""field"": ""byte""
 }, {
 ""type"": ""int16"",
 ""optional"": true,
 ""doc"": ""The value stored as a short. Null unless ``propertyType`` is set to ``short``."",
 ""field"": ""short""
 }, {
 ""type"": ""int32"",
 ""optional"": true,
 ""doc"": ""The value stored as a integer. Null unless ``propertyType`` is set to ``integer``."",
 ""field"": ""integer""
 }, {
 ""type"": ""int64"",
 ""optional"": true,
 ""doc"": ""The value stored as a long. Null unless ``propertyType`` is set to ``long``."",
 ""field"": ""long""
 }, {
 ""type"": ""float"",
 ""optional"": true,
 ""doc"": ""The value stored as a float. Null unless ``propertyType`` is set to ``float``."",
 ""field"": ""float""
 }, {
 ""type"": ""double"",
 ""optional"": true,
 ""doc"": ""The value stored as a double. Null unless ``propertyType`` is set to ``double``."",
 ""field"": ""double""
 }, {
 ""type"": ""string"",
 ""optional"": true,
 ""doc"": ""The value stored as a string. Null unless ``propertyType`` is set to ``string``."",
 ""field"": ""string""
 }],
 ""optional"": false,
 ""name"": ""io.confluent.connect.jms.PropertyValue"",
 ""doc"": ""This schema is used to store the data that is found in the properties of the message. To ensure that the proper type mappings are preserved field ``propertyType`` stores the value type for the field. The corresponding field in the schema will contain the data for the property. This ensures that the data is retrievable as the type returned by `Message.getObjectProperty() <http://docs.oracle.com/javaee/6/api/javax/jms/Message.html#getObjectProperty(java.lang.String)>`_.""
 },
 ""optional"": false,
 ""doc"": ""This field stores the data from all of the properties for the Message indexed by their propertyName."",
 ""field"": ""properties""
 }, {
 ""type"": ""bytes"",
 ""optional"": true,
 ""doc"": ""This field stores the value from `BytesMessage.html.readBytes(byte[]) <http://docs.oracle.com/javaee/6/api/javax/jms/BytesMessage.html#readBytes(byte[])>`_."",
 ""field"": ""bytes""
 }, {
 ""type"": ""map"",
 ""keys"": {
 ""type"": ""string"",
 ""optional"": false
 },
 ""values"": {
 ""type"": ""struct"",
 ""fields"": [{
 ""type"": ""string"",
 ""optional"": false,
 ""doc"": ""The java type of the property on the Message. One of ``boolean``, ``byte``, ``short``, ``integer``, ``long``, ``float``, ``double``, or ``string``."",
 ""field"": ""propertyType""
 }, {
 ""type"": ""boolean"",
 ""optional"": true,
 ""doc"": ""The value stored as a boolean. Null unless ``propertyType`` is set to ``boolean``."",
 ""field"": ""boolean""
 }, {
 ""type"": ""int8"",
 ""optional"": true,
 ""doc"": ""The value stored as a byte. Null unless ``propertyType`` is set to ``byte``."",
 ""field"": ""byte""
 }, {
 ""type"": ""int16"",
 ""optional"": true,
 ""doc"": ""The value stored as a short. Null unless ``propertyType`` is set to ``short``."",
 ""field"": ""short""
 }, {
 ""type"": ""int32"",
 ""optional"": true,
 ""doc"": ""The value stored as a integer. Null unless ``propertyType`` is set to ``integer``."",
 ""field"": ""integer""
 }, {
 ""type"": ""int64"",
 ""optional"": true,
 ""doc"": ""The value stored as a long. Null unless ``propertyType`` is set to ``long``."",
 ""field"": ""long""
 }, {
 ""type"": ""float"",
 ""optional"": true,
 ""doc"": ""The value stored as a float. Null unless ``propertyType`` is set to ``float``."",
 ""field"": ""float""
 }, {
 ""type"": ""double"",
 ""optional"": true,
 ""doc"": ""The value stored as a double. Null unless ``propertyType`` is set to ``double``."",
 ""field"": ""double""
 }, {
 ""type"": ""string"",
 ""optional"": true,
 ""doc"": ""The value stored as a string. Null unless ``propertyType`` is set to ``string``."",
 ""field"": ""string""
 }],
 ""optional"": false,
 ""name"": ""io.confluent.connect.jms.PropertyValue"",
 ""doc"": ""This schema is used to store the data that is found in the properties of the message. To ensure that the proper type mappings are preserved field ``propertyType`` stores the value type for the field. The corresponding field in the schema will contain the data for the property. This ensures that the data is retrievable as the type returned by `Message.getObjectProperty() <http://docs.oracle.com/javaee/6/api/javax/jms/Message.html#getObjectProperty(java.lang.String)>`_.""
 },
 ""optional"": true,
 ""doc"": ""This field stores the data from all of the map entries returned from `MapMessage.getMapNames() <http://docs.oracle.com/javaee/6/api/javax/jms/MapMessage.html#getMapNames()>`_ for the Message indexed by their key."",
 ""field"": ""map""
 }, {
 ""type"": ""string"",
 ""optional"": true,
 ""doc"": ""This field stores the value from `TextMessage.html.getText() <http://docs.oracle.com/javaee/6/api/javax/jms/TextMessage.html#getText()>`_."",
 ""field"": ""text""
 }],
 ""optional"": false,
 ""name"": ""io.confluent.connect.jms.Value"",
 ""doc"": ""This schema is used to store the value of the JMS message.""
 },
 ""payload"": {
 ""messageID"": ""ID:COPTW_B_SIT.1D815BC7447C2A24:2308"",
 ""messageType"": ""text"",
 ""timestamp"": 1542167593548,
 ""deliveryMode"": 2,
 ""correlationID"": null,
 ""replyTo"": null,
 ""destination"": {
 ""destinationType"": ""queue"",
 ""name"": ""test.queue""
 },
 ""redelivered"": false,
 ""type"": null,
 ""expiration"": 0,
 ""priority"": 4,
 ""properties"": {},
 ""bytes"": null,
 ""map"": null,
 ""text"": ""helo""
 }
}",,lucky786,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-11-14 06:59:47.0,,,,,,,"0|s00gtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add additional tests for validating store restoration completes before Topology is intitalized,KAFKA-6509,13135109,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,bbejeck,bbejeck,31/Jan/18 15:15,03/Oct/18 18:34,12/Jan/21 11:54,,,,,,,,,,,streams,,,,0,newbie,,,Right now ,,bbejeck,gopinath-langote,mjsax,wgaus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-04-20 16:17:10.946,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 20 16:17:10 UTC 2018,,,,,,,"0|i3pllz:",9223372036854775807,,,,,,,,,,,,,,,,"20/Apr/18 16:17;gopinath-langote;Hey [~bbejeck] ,

 

More comments (explanation) will help to understand the issue",,,,,,,,,,,,,,,,,,,,,
System test for ZooKeeper quorum failure scenarios,KAFKA-1918,12772373,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,omid,omid,04/Feb/15 14:02,03/Aug/18 00:28,12/Jan/21 11:54,,,,,,,,,,,system tests,,,,1,,,,"Following up on the [conversation on the mailing list|http://mail-archives.apache.org/mod_mbox/kafka-users/201502.mbox/%3CCAHwHRrX3SAWDUGF5LjU4rrMUsqv%3DtJcyjX7OENeL5C_V5o3tCw%40mail.gmail.com%3E], the FAQ writes:

{quote}
Once the Zookeeper quorum is down, brokers could result in a bad state and could not normally serve client requests, etc. Although when Zookeeper quorum recovers, the Kafka brokers should be able to resume to normal state automatically, _there are still a few +corner cases+ the they cannot and a hard kill-and-recovery is required to bring it back to normal_. Hence it is recommended to closely monitor your zookeeper cluster and provision it so that it is performant.
{quote}

As ZK quorum failures are inevitable (due to rolling upgrades of ZK, leader hardware failure, etc), it would be great to identify the corner cases (if they still exist) and fix them if necessary.",,granders,omid,tsenart,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-05-13 00:48:30.839,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 13 00:48:30 UTC 2015,,,,,,,"0|i256kv:",9223372036854775807,,,,,,,,,,,,,,,,"13/May/15 00:48;granders;Just making sure you're aware of work we're doing at Confluent on system tests. I'll be posting a KIP for this soon, but here's some info:

The original plan is sketched here:
https://cwiki.apache.org/confluence/display/KAFKA/System+Test+Improvements

This is the core library/test framework (WIP) which aids in writing and running the tests
https://github.com/confluentinc/ducktape/

This has system tests we've written to date for the Confluent Platform
https://github.com/confluentinc/muckrake",,,,,,,,,,,,,,,,,,,,,
Add successful acks verification to ProduceConsumeValidateTest,KAFKA-3320,12946266,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,apovzner,apovzner,02/Mar/16 19:39,02/Aug/18 22:12,12/Jan/21 11:54,,,,,,,,,,,unit tests,,,,0,,,,"Currently ProduceConsumeValidateTest only validates that each acked message was consumed. Some tests may want an additional verification that all acks were successful.

This JIRA is to add an addition optional verification that all acks were successful and use it in couple of tests that need that verification. Example is compression test.",,apovzner,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-24 11:03:09.386,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 28 00:05:26 UTC 2016,,,,,,,"0|i2u1tr:",9223372036854775807,,,,,,,,,,,,,,,,"24/Mar/16 11:03;mimaison;What sort of validation did you have in mind ? Should we pass all the acked messages through the message_validator method again even though the consumer should have caught any invalid ones already ?","28/Mar/16 00:05;apovzner;If you look at verifiable_producer.py, it collects all successfully produced messages into acked_values. If producer send() was unsuccessful, those messages are collected into not_acked_values. However, our tests do not check whether any produce send() got an error. Suppose the test tried to produce 100 messages, and only 50 were successfully produced. If the consumer successfully consumed 50 messages, then the test is considered a success. It would be good to verify that we also did not get any produce errors for some tests.",,,,,,,,,,,,,,,,,,,,
Add regression check for KAFKA-4073,KAFKA-4088,13000201,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,granders,granders,25/Aug/16 22:48,02/Aug/18 19:54,12/Jan/21 11:54,,,,,,,,,,,mirrormaker,,,,0,,,,"The patch for KAFKA-4073 fixed an issue introduced in 0.10.0.1, it may be worth adding a regression test

Ideally this would be a unit test, but it's not immediately clear how to do so since it's hard to produce a pre-0.10.0 message through the producer api in 0.10.0

If we were to write a system test, it would look something like:

Setup: 
- Two small kafka clusters, 0.9.0.X (source), and 0.10.0.X (destination)
- 0.9.0.X mirror maker from source to destination

Produce messages with 0.9.X producer to source brokers.
upgrade source brokers to 0.10.0.1
upgrade mirror maker to 0.10.0.1 (don't upgrade 0.9 producer)

This should reveal the problem if run without the fix introduced in KAFKA-4073",,granders,ijuma,junrao,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-08-25 22:54:37.044,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 25 22:56:44 UTC 2016,,,,,,,"0|i32tfr:",9223372036854775807,,,,,,,,,,,,,,,,"25/Aug/16 22:54;ijuma;Thanks for filing this. I think what we should do is basically extend all of our upgrade tests to include MirrorMaker. Would that make sense?","25/Aug/16 22:56;junrao;It may be possible to cover this in a unit test. ByteBufferMessageSetTest.getMessages() allows us to generate message sets in 0.9 format. So, we can start a broker, use the log.append() api to insert some 0.9 message sets. Then, run MirrorMaker to mirror the data to another cluster.",,,,,,,,,,,,,,,,,,,,
StreamStreamJoinIntegrationTest fails in 2.0 Jenkins,KAFKA-7195,13173922,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,omkreddy,yuzhihong@gmail.com,yuzhihong@gmail.com,23/Jul/18 21:12,24/Jul/18 19:04,12/Jan/21 11:54,24/Jul/18 19:04,2.0.0,,,,2.0.0,,,,,,,,,0,,,,"From https://builds.apache.org/job/kafka-2.0-jdk8/87/testReport/junit/org.apache.kafka.streams.integration/StreamStreamJoinIntegrationTest/testOuter_caching_enabled___false_/ :
{code}
java.lang.AssertionError: 
Expected: is <[A-null]>
     but: was <[A-a, A-b, A-c, A-d]>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
	at org.apache.kafka.streams.integration.AbstractJoinIntegrationTest.checkResult(AbstractJoinIntegrationTest.java:171)
	at org.apache.kafka.streams.integration.AbstractJoinIntegrationTest.runTest(AbstractJoinIntegrationTest.java:212)
	at org.apache.kafka.streams.integration.AbstractJoinIntegrationTest.runTest(AbstractJoinIntegrationTest.java:184)
	at org.apache.kafka.streams.integration.StreamStreamJoinIntegrationTest.testOuter(StreamStreamJoinIntegrationTest.java:198)
{code}
However, some test output was missing:
{code}
[2018-07-23 20:51:36,363] INFO Socket c
...[truncated 1627692 chars]...
671)
{code}
I ran the test locally which passed.",,githubbot,mjsax,rsivaram,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-07-24 01:08:44.333,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 24 09:29:59 UTC 2018,,,,,,,"0|i3w6zr:",9223372036854775807,,,,,,,,,,,,,,,,"24/Jul/18 01:08;githubbot;omkreddy opened a new pull request #5418: KAFKA-7195: Fix StreamStreamJoinIntegrationTest test failures
URL: https://github.com/apache/kafka/pull/5418
 
 
   
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","24/Jul/18 09:29;githubbot;rajinisivaram closed pull request #5418: KAFKA-7195: Fix StreamStreamJoinIntegrationTest test failures
URL: https://github.com/apache/kafka/pull/5418
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/AbstractJoinIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/AbstractJoinIntegrationTest.java
index 80ab60647ad..3e29fc2a29b 100644
--- a/streams/src/test/java/org/apache/kafka/streams/integration/AbstractJoinIntegrationTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/AbstractJoinIntegrationTest.java
@@ -163,7 +163,7 @@ void prepareEnvironment() throws InterruptedException {
 
     @After
     public void cleanup() throws InterruptedException {
-        CLUSTER.deleteTopicsAndWait(120000, INPUT_TOPIC_LEFT, INPUT_TOPIC_RIGHT, OUTPUT_TOPIC);
+        CLUSTER.deleteAllTopicsAndWait(120000);
     }
 
     private void checkResult(final String outputTopic, final List<String> expectedResult) throws InterruptedException {
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/utils/EmbeddedKafkaCluster.java b/streams/src/test/java/org/apache/kafka/streams/integration/utils/EmbeddedKafkaCluster.java
index ce6324df617..ab52649dee4 100644
--- a/streams/src/test/java/org/apache/kafka/streams/integration/utils/EmbeddedKafkaCluster.java
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/utils/EmbeddedKafkaCluster.java
@@ -274,6 +274,24 @@ public void deleteTopicsAndWait(final long timeoutMs, final String... topics) th
         }
     }
 
+    /**
+     * Deletes all topics and blocks until all topics got deleted.
+     *
+     * @param timeoutMs the max time to wait for the topics to be deleted (does not block if {@code <= 0})
+     */
+    public void deleteAllTopicsAndWait(final long timeoutMs) throws InterruptedException {
+        final Set<String> topics = new HashSet<>(JavaConverters.seqAsJavaListConverter(zkUtils.getAllTopics()).asJava());
+        for (final String topic : topics) {
+            try {
+                brokers[0].deleteTopic(topic);
+            } catch (final UnknownTopicOrPartitionException e) { }
+        }
+
+        if (timeoutMs > 0) {
+            TestUtils.waitForCondition(new TopicsDeletedCondition(topics), timeoutMs, ""Topics not deleted after "" + timeoutMs + "" milli seconds."");
+        }
+    }
+
     public void deleteAndRecreateTopics(final String... topics) throws InterruptedException {
         deleteTopicsAndWait(TOPIC_DELETION_TIMEOUT, topics);
         createTopics(topics);
@@ -295,6 +313,10 @@ private TopicsDeletedCondition(final String... topics) {
             Collections.addAll(deletedTopics, topics);
         }
 
+        public TopicsDeletedCondition(final Set<String> topics) {
+            deletedTopics.addAll(topics);
+        }
+
         @Override
         public boolean conditionMet() {
             final Set<String> allTopics = new HashSet<>(


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","24/Jul/18 09:29;rsivaram;[~omkreddy] I have merged the PR to 2.0 branch, but leaving the JIRA open for now to address review comments.",,,,,,,,,,,,,,,,,,,
Add system tests testing the new throttling behavior using older clients/brokers,KAFKA-6944,13161847,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jonlee2,jonlee2,jonlee2,24/May/18 18:05,20/Jul/18 22:22,12/Jan/21 11:54,28/Jun/18 23:46,2.0.0,,,,,,,,,system tests,,,,0,,,,"KAFKA-6028 (KIP-219) changes the throttling behavior on quota violation as follows:
 * the broker will send out a response with throttle time to the client immediately and mute the channel
 * upon receiving a response with a non-zero throttle time, the client will also block sending further requests to the broker until the throttle time is over.

The current system tests assume that both clients and brokers are of the same version. We'll need an additional set of quota tests that test throttling behavior between older clients and newer brokers and between newer clients and older brokers. ",,githubbot,jonlee2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-06-26 07:32:53.064,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 27 23:51:01 UTC 2018,,,,,,,"0|i3u4tz:",9223372036854775807,,,,,,,,,,,,,,,,"26/Jun/18 07:32;githubbot;jonlee2 opened a new pull request #5294: KAFKA-6944: Add system tests testing the new throttling behavior usin…
URL: https://github.com/apache/kafka/pull/5294
 
 
   …g older clients/brokers
   
   Added two additional test cases to quota_test.py, which run between brokers and clients with different throttling behaviors. More specifically,
   1. clients with new throttling behavior (i.e., post-KIP-219) and brokers with old throttling behavior (i.e., pre-KIP-219)
   2. clients with old throttling behavior and brokers with new throttling behavior
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","27/Jun/18 23:51;githubbot;lindong28 closed pull request #5294: KAFKA-6944: Add system tests testing the new throttling behavior using older clients/brokers
URL: https://github.com/apache/kafka/pull/5294
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/tests/kafkatest/tests/client/quota_test.py b/tests/kafkatest/tests/client/quota_test.py
index 47a6a96b020..c084e08ab65 100644
--- a/tests/kafkatest/tests/client/quota_test.py
+++ b/tests/kafkatest/tests/client/quota_test.py
@@ -21,6 +21,7 @@
 from kafkatest.services.kafka import KafkaService
 from kafkatest.services.performance import ProducerPerformanceService
 from kafkatest.services.console_consumer import ConsoleConsumer
+from kafkatest.version import DEV_BRANCH, V_1_1_0
 
 class QuotaConfig(object):
     CLIENT_ID = 'client-id'
@@ -119,7 +120,6 @@ def __init__(self, test_context):
 
     def setUp(self):
         self.zk.start()
-        self.kafka.start()
 
     def min_cluster_size(self):
         """"""Override this since we're adding services outside of the constructor""""""
@@ -128,15 +128,30 @@ def min_cluster_size(self):
     @cluster(num_nodes=5)
     @matrix(quota_type=[QuotaConfig.CLIENT_ID, QuotaConfig.USER, QuotaConfig.USER_CLIENT], override_quota=[True, False])
     @parametrize(quota_type=QuotaConfig.CLIENT_ID, consumer_num=2)
-    def test_quota(self, quota_type, override_quota=True, producer_num=1, consumer_num=1):
+    @parametrize(quota_type=QuotaConfig.CLIENT_ID, old_broker_throttling_behavior=True)
+    @parametrize(quota_type=QuotaConfig.CLIENT_ID, old_client_throttling_behavior=True)
+    def test_quota(self, quota_type, override_quota=True, producer_num=1, consumer_num=1,
+                   old_broker_throttling_behavior=False, old_client_throttling_behavior=False):
+        # Old (pre-2.0) throttling behavior for broker throttles before sending a response to the client.
+        if old_broker_throttling_behavior:
+            self.kafka.set_version(V_1_1_0)
+        self.kafka.start()
+
         self.quota_config = QuotaConfig(quota_type, override_quota, self.kafka)
         producer_client_id = self.quota_config.client_id
         consumer_client_id = self.quota_config.client_id
 
+        # Old (pre-2.0) throttling behavior for client does not throttle upon receiving a response with a non-zero throttle time.
+        if old_client_throttling_behavior:
+            client_version = V_1_1_0
+        else:
+            client_version = DEV_BRANCH
+
         # Produce all messages
         producer = ProducerPerformanceService(
             self.test_context, producer_num, self.kafka,
-            topic=self.topic, num_records=self.num_records, record_size=self.record_size, throughput=-1, client_id=producer_client_id)
+            topic=self.topic, num_records=self.num_records, record_size=self.record_size, throughput=-1,
+            client_id=producer_client_id, version=client_version)
 
         producer.run()
 
@@ -144,7 +159,7 @@ def test_quota(self, quota_type, override_quota=True, producer_num=1, consumer_n
         consumer = ConsoleConsumer(self.test_context, consumer_num, self.kafka, self.topic,
             consumer_timeout_ms=60000, client_id=consumer_client_id,
             jmx_object_names=['kafka.consumer:type=consumer-fetch-manager-metrics,client-id=%s' % consumer_client_id],
-            jmx_attributes=['bytes-consumed-rate'])
+            jmx_attributes=['bytes-consumed-rate'], version=client_version)
         consumer.run()
 
         for idx, messages in consumer.messages_consumed.iteritems():


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,
SimpleAclAuthorizerTest#testHighConcurrencyModificationOfResourceAcls fails intermittently,KAFKA-6335,13123839,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Cannot Reproduce,omkreddy,yuzhihong@gmail.com,yuzhihong@gmail.com,08/Dec/17 23:45,08/Jul/18 13:01,12/Jan/21 11:54,08/Jul/18 13:01,,,,,,,,,,,,,,0,,,,"From https://builds.apache.org/job/kafka-pr-jdk9-scala2.12/3045/testReport/junit/kafka.security.auth/SimpleAclAuthorizerTest/testHighConcurrencyModificationOfResourceAcls/ :
{code}
java.lang.AssertionError: expected acls Set(User:36 has Allow permission for operations: Read from hosts: *, User:7 has Allow permission for operations: Read from hosts: *, User:21 has Allow permission for operations: Read from hosts: *, User:39 has Allow permission for operations: Read from hosts: *, User:43 has Allow permission for operations: Read from hosts: *, User:3 has Allow permission for operations: Read from hosts: *, User:35 has Allow permission for operations: Read from hosts: *, User:15 has Allow permission for operations: Read from hosts: *, User:16 has Allow permission for operations: Read from hosts: *, User:22 has Allow permission for operations: Read from hosts: *, User:26 has Allow permission for operations: Read from hosts: *, User:11 has Allow permission for operations: Read from hosts: *, User:38 has Allow permission for operations: Read from hosts: *, User:8 has Allow permission for operations: Read from hosts: *, User:28 has Allow permission for operations: Read from hosts: *, User:32 has Allow permission for operations: Read from hosts: *, User:25 has Allow permission for operations: Read from hosts: *, User:41 has Allow permission for operations: Read from hosts: *, User:44 has Allow permission for operations: Read from hosts: *, User:48 has Allow permission for operations: Read from hosts: *, User:2 has Allow permission for operations: Read from hosts: *, User:9 has Allow permission for operations: Read from hosts: *, User:14 has Allow permission for operations: Read from hosts: *, User:46 has Allow permission for operations: Read from hosts: *, User:13 has Allow permission for operations: Read from hosts: *, User:5 has Allow permission for operations: Read from hosts: *, User:29 has Allow permission for operations: Read from hosts: *, User:45 has Allow permission for operations: Read from hosts: *, User:6 has Allow permission for operations: Read from hosts: *, User:37 has Allow permission for operations: Read from hosts: *, User:23 has Allow permission for operations: Read from hosts: *, User:19 has Allow permission for operations: Read from hosts: *, User:24 has Allow permission for operations: Read from hosts: *, User:17 has Allow permission for operations: Read from hosts: *, User:34 has Allow permission for operations: Read from hosts: *, User:12 has Allow permission for operations: Read from hosts: *, User:42 has Allow permission for operations: Read from hosts: *, User:4 has Allow permission for operations: Read from hosts: *, User:47 has Allow permission for operations: Read from hosts: *, User:18 has Allow permission for operations: Read from hosts: *, User:31 has Allow permission for operations: Read from hosts: *, User:49 has Allow permission for operations: Read from hosts: *, User:33 has Allow permission for operations: Read from hosts: *, User:1 has Allow permission for operations: Read from hosts: *, User:27 has Allow permission for operations: Read from hosts: *) but got Set(User:36 has Allow permission for operations: Read from hosts: *, User:7 has Allow permission for operations: Read from hosts: *, User:21 has Allow permission for operations: Read from hosts: *, User:39 has Allow permission for operations: Read from hosts: *, User:43 has Allow permission for operations: Read from hosts: *, User:3 has Allow permission for operations: Read from hosts: *, User:35 has Allow permission for operations: Read from hosts: *, User:15 has Allow permission for operations: Read from hosts: *, User:16 has Allow permission for operations: Read from hosts: *, User:22 has Allow permission for operations: Read from hosts: *, User:26 has Allow permission for operations: Read from hosts: *, User:11 has Allow permission for operations: Read from hosts: *, User:38 has Allow permission for operations: Read from hosts: *, User:8 has Allow permission for operations: Read from hosts: *, User:28 has Allow permission for operations: Read from hosts: *, User:32 has Allow permission for operations: Read from hosts: *, User:25 has Allow permission for operations: Read from hosts: *, User:41 has Allow permission for operations: Read from hosts: *, User:44 has Allow permission for operations: Read from hosts: *, User:48 has Allow permission for operations: Read from hosts: *, User:2 has Allow permission for operations: Read from hosts: *, User:9 has Allow permission for operations: Read from hosts: *, User:14 has Allow permission for operations: Read from hosts: *, User:46 has Allow permission for operations: Read from hosts: *, User:13 has Allow permission for operations: Read from hosts: *, User:5 has Allow permission for operations: Read from hosts: *, User:29 has Allow permission for operations: Read from hosts: *, User:45 has Allow permission for operations: Read from hosts: *, User:6 has Allow permission for operations: Read from hosts: *, User:37 has Allow permission for operations: Read from hosts: *, User:23 has Allow permission for operations: Read from hosts: *, User:19 has Allow permission for operations: Read from hosts: *, User:24 has Allow permission for operations: Read from hosts: *, User:17 has Allow permission for operations: Read from hosts: *, User:34 has Allow permission for operations: Read from hosts: *, User:12 has Allow permission for operations: Read from hosts: *, User:42 has Allow permission for operations: Read from hosts: *, User:4 has Allow permission for operations: Read from hosts: *, User:47 has Allow permission for operations: Read from hosts: *, User:18 has Allow permission for operations: Read from hosts: *, User:31 has Allow permission for operations: Read from hosts: *, User:49 has Allow permission for operations: Read from hosts: *, User:33 has Allow permission for operations: Read from hosts: *, User:27 has Allow permission for operations: Read from hosts: *)
{code}
After initial check, one user in the expected Set() was missing.",,guozhang,ijuma,omkreddy,smurakozi,Sonia,vserrao,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-12-12 17:01:23.603,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 19 03:35:52 UTC 2018,,,,,,,"0|i3npqf:",9223372036854775807,,,,,,,,,,,,,,,,"09/Dec/17 17:53;yuzhihong@gmail.com;Diff shows that the following was missing in actual Set:
{code}
User:1 has Allow permission for operations: Read from hosts: *,
{code}","11/Dec/17 21:38;yuzhihong@gmail.com;Same error:
https://builds.apache.org/job/kafka-trunk-jdk8/2267/testReport/junit/kafka.security.auth/SimpleAclAuthorizerTest/testHighConcurrencyModificationOfResourceAcls/

User:1 was missing.","12/Dec/17 17:01;ijuma;This is failing more often since the code was changed to use the async zk client so it's possible that there is a regression. We need to investigate this before 1.1.0 is released. cc [~omkreddy] [~junrao]","13/Dec/17 17:13;yuzhihong@gmail.com;SimpleAclAuthorizer#updateResourceAcls() returns boolean, indicating whether the update succeeds or not.
However, the return value is not checked by testHighConcurrencyModificationOfResourceAcls().

In highly contended scenario, the test should expect few of the Acl request not going thru.","13/Dec/17 18:46;guozhang;Encountered this test failure again on Jenkins: https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/9984/testReport/junit/kafka.security.auth/SimpleAclAuthorizerTest/testHighConcurrencyModificationOfResourceAcls/","13/Dec/17 19:42;omkreddy;SimpleAclAuthorizer#updateResourceAcls() will retry incase of any update failure/version mismatch.  test case configures Int.Max number of retries to avoid any transient errors. test failures are always missing first acl in the expected list.  looks like a synchronization issue/corner case. Will look into this later this week.","21/Dec/17 00:22;yuzhihong@gmail.com;If needed, we can add logging into codebase so that it is easier to figure out the cause.","25/Jan/18 02:21;yuzhihong@gmail.com;Haven't seen this for a while.","28/Feb/18 06:10;smurakozi;Seems to be the same error in https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/11457/testReport/junit/kafka.security.auth/SimpleAclAuthorizerTest/testHighConcurrencyModificationOfResourceAcls/","09/Mar/18 05:06;yuzhihong@gmail.com;Happened again:

https://builds.apache.org/job/kafka-trunk-jdk9/480/testReport/junit/kafka.security.auth/SimpleAclAuthorizerTest/testHighConcurrencyModificationOfResourceAcls/","13/Apr/18 01:47;yuzhihong@gmail.com;Haven't seen this test fail lately.","18/Apr/18 12:14;Sonia;Hi, I encountered the same error. Is there any update on this?","18/Apr/18 13:28;omkreddy;[~Sonia] Can you give more details? Are you seeing test failure or an issue with SimpleAclAuthorizer? Can you also confirm the Kafka version? ","19/Apr/18 12:14;Sonia;[~omkreddy] I got this as a test failure.Its an intermittent failure. I am running tests for the trunk branch of Kafka(1.2.0-SNAPSHOT). ","29/May/18 03:18;yuzhihong@gmail.com;[~Sonia]:
Does this test still fail on your computer ?

Thanks","04/Jun/18 04:29;vserrao;Hi [~yuzhihong@gmail.com],

I've been tracking this Jira for quite some time now, the failure is no longer occurring on my computer.","19/Jun/18 03:35;Sonia;[~yuzhihong@gmail.com] No this test is no longer failing on my computer. Thanks.",,,,,
FileStreamSink is very slow,KAFKA-6831,13155578,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Invalid,,vrmprabhat,vrmprabhat,27/Apr/18 01:19,29/May/18 18:18,12/Jan/21 11:54,29/May/18 18:17,1.1.0,,,,,,,,,KafkaConnect,,,,0,,,,"Hi Team,

 

I am very new in kafka. My project requirement is fetch data from source location and place it in other other location (consumer location). I am using FileStreamSink class to perform above action.

I am using Linux machine having memory of 32 GB. 

When i start FIleStreamSink , It is syncing to consumer location very very slowly. Not sure why it is taking 2000 message at a time and then sync it. After that it wait for few second then sync again. This waiting time increases per run .

 

I am processing 600K message but it took 1 hrs to process only 60K message.

 

Below are my config details : 

 

connect-file-sink.property

Name = local-file

Connector.class = FileStreamSource

task.max=20

file=/d/d1/kafka/destination/outfile.txt

topic=abc_partion_20

connect-file-source.property

Name = local-file

Connector.class = FileStreamSource

task.max=20

file=/d/d1/kafka/source/infile.txt

topic=abc_partion_20

 

Can you please help ?

 

 ",,rhauch,vrmprabhat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-05-29 18:17:57.977,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 29 18:17:57 UTC 2018,,,,,,,"0|i3t34f:",9223372036854775807,,,,,,,,,,,,,,,,"01/May/18 05:50;vrmprabhat;Any help please ...","29/May/18 18:17;rhauch;[~vrmprabhat], first of all, these kinds of questions are better asked through the user mailing list or other online resources. But to get you started, Kafka Connect uses normal producers and consumers under the covers, so be sure that you set the consumer settings in the Connect worker configuration to handle traffic like yours. It's often a balancing act of batch and message sizes to handle your throughputs. A few commonly used consumer properties include {{fetch.min.bytes}}, {{max.partition.fetch.bytes}}, {{fetch.max.bytes}}, and {{fetch.message.max.bytes}}; these might be too small given the message size, # of partitions, throughput, etc., and might be instructing Connect to consume too many small batches.
",,,,,,,,,,,,,,,,,,,,
Additional authorization test cases,KAFKA-2951,12919635,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,fpj,fpj,fpj,07/Dec/15 10:14,26/May/18 06:49,12/Jan/21 11:54,26/May/18 06:49,0.9.0.0,,,,2.0.0,,,,,,,,,0,,,,"There are a few test cases that are worth adding. I've run them manually, but it sounds like a good idea to have them in:

# Test incorrect topic name (authorization failure)
# Test topic wildcard

The first one is covered by checking access to a topic with no authorization, which could happen for example if the user as a typo in the topic name. This case is somewhat covered by the test case testProduceWithNoTopicAccess in AuthorizerIntegrationTest, but not in EndToEndAuthorizationTest. The second case consists of testing that using the topic wildcard works. This wildcard might end up being commonly used and it is worth checking the functionality. At the moment, I believe none of AuthorizerIntegrationTest or EndToEndAuthorizationTest.",,fpj,githubbot,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-07 10:45:49.589,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 26 06:49:34 UTC 2018,,,,,,,"0|i2pivz:",9223372036854775807,,,,,,,,,,,,,,,,"07/Dec/15 10:45;ijuma;[~fpj], have you checked if `AuthorizerIntegrationTest` already covers these?","07/Dec/15 12:11;fpj;[~ijuma] thanks for the feedback, I have updated the description to give more context.","21/May/18 18:08;githubbot;omkreddy opened a new pull request #5054: KAFKA-2951: Add a testcase to verify produce, consume with acls for topic/group wildcard resources.
URL: https://github.com/apache/kafka/pull/5054
 
 
   
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","26/May/18 06:49;githubbot;hachikuji closed pull request #5054: KAFKA-2951: Add a testcase to verify produce, consume with acls for topic/group wildcard resources.
URL: https://github.com/apache/kafka/pull/5054
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/core/src/test/scala/integration/kafka/api/EndToEndAuthorizationTest.scala b/core/src/test/scala/integration/kafka/api/EndToEndAuthorizationTest.scala
index 4500d49e337..a2e5fd9d2b3 100644
--- a/core/src/test/scala/integration/kafka/api/EndToEndAuthorizationTest.scala
+++ b/core/src/test/scala/integration/kafka/api/EndToEndAuthorizationTest.scala
@@ -67,7 +67,7 @@ abstract class EndToEndAuthorizationTest extends IntegrationTestHarness with Sas
   val numRecords = 1
   val group = ""group""
   val topic = ""e2etopic""
-  val topicWildcard = ""*""
+  val wildcard = ""*""
   val part = 0
   val tp = new TopicPartition(topic, part)
   val topicAndPartition = TopicAndPartition(topic, part)
@@ -79,6 +79,8 @@ abstract class EndToEndAuthorizationTest extends IntegrationTestHarness with Sas
   val topicResource = new Resource(Topic, topic)
   val groupResource = new Resource(Group, group)
   val clusterResource = Resource.ClusterResource
+  val wildcardTopicResource = new Resource(Topic, wildcard)
+  val wildcardGroupResource = new Resource(Group, wildcard)
 
   // Arguments to AclCommand to set ACLs. There are three definitions here:
   // 1- Provides read and write access to topic
@@ -93,7 +95,7 @@ abstract class EndToEndAuthorizationTest extends IntegrationTestHarness with Sas
   def topicBrokerReadAclArgs: Array[String] = Array(""--authorizer-properties"",
                                                     s""zookeeper.connect=$zkConnect"",
                                                     s""--add"",
-                                                    s""--topic=$topicWildcard"",
+                                                    s""--topic=$wildcard"",
                                                     s""--operation=Read"",
                                                     s""--allow-principal=$kafkaPrincipalType:$kafkaPrincipal"")
   def produceAclArgs: Array[String] = Array(""--authorizer-properties"",
@@ -135,6 +137,15 @@ abstract class EndToEndAuthorizationTest extends IntegrationTestHarness with Sas
                                           s""--group=$group"",
                                           s""--operation=Read"",
                                           s""--allow-principal=$kafkaPrincipalType:$clientPrincipal"")
+  def produceConsumeWildcardAclArgs: Array[String] = Array(""--authorizer-properties"",
+                                            s""zookeeper.connect=$zkConnect"",
+                                            s""--add"",
+                                            s""--topic=$wildcard"",
+                                            s""--group=$wildcard"",
+                                            s""--consumer"",
+                                            s""--producer"",
+                                            s""--allow-principal=$kafkaPrincipalType:$clientPrincipal"")
+
   def ClusterActionAcl = Set(new Acl(new KafkaPrincipal(kafkaPrincipalType, kafkaPrincipal), Allow, Acl.WildCardHost, ClusterAction))
   def TopicBrokerReadAcl = Set(new Acl(new KafkaPrincipal(kafkaPrincipalType, kafkaPrincipal), Allow, Acl.WildCardHost, Read))
   def GroupReadAcl = Set(new Acl(new KafkaPrincipal(kafkaPrincipalType, clientPrincipal), Allow, Acl.WildCardHost, Read))
@@ -173,7 +184,7 @@ abstract class EndToEndAuthorizationTest extends IntegrationTestHarness with Sas
                                 saslProperties = this.clientSaslProperties,
                                 props = Some(producerConfig))
   }
-  
+
   /**
     * Closes MiniKDC last when tearing down.
     */
@@ -201,6 +212,22 @@ abstract class EndToEndAuthorizationTest extends IntegrationTestHarness with Sas
     consumeRecords(this.consumers.head, numRecords)
   }
 
+  @Test
+  def testProduceConsumeWithWildcardAcls(): Unit = {
+    setWildcardResourceAcls()
+    sendRecords(numRecords, tp)
+    consumers.head.subscribe(List(topic).asJava)
+    consumeRecords(this.consumers.head, numRecords)
+  }
+
+  private def setWildcardResourceAcls() {
+    AclCommand.main(produceConsumeWildcardAclArgs)
+    servers.foreach { s =>
+      TestUtils.waitAndVerifyAcls(TopicReadAcl ++ TopicWriteAcl ++ TopicDescribeAcl ++ TopicBrokerReadAcl, s.apis.authorizer.get, wildcardTopicResource)
+      TestUtils.waitAndVerifyAcls(GroupReadAcl, s.apis.authorizer.get, wildcardGroupResource)
+    }
+  }
+
   protected def setAclsAndProduce() {
     AclCommand.main(produceAclArgs)
     AclCommand.main(consumeAclArgs)


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,
test toolchain,KAFKA-6862,13157122,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Invalid,,baviskar,baviskar,04/May/18 11:40,24/May/18 17:48,12/Jan/21 11:54,24/May/18 17:48,,,,,,,,,,build,,,,0,,,,test toolchain,,baviskar,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-05-24 17:48:38.336,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 24 17:48:38 UTC 2018,,,,,,,"0|i3tchr:",9223372036854775807,,,,,,,,,,,,,,,,"24/May/18 17:48;omkreddy;Please reopen the Jira with more details.",,,,,,,,,,,,,,,,,,,,,
Add unit tests for handling of authentication failures in clients,KAFKA-5944,13103691,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vahid,rsivaram,rsivaram,20/Sep/17 18:31,25/Apr/18 09:42,12/Jan/21 11:54,12/Feb/18 22:30,,,,,2.0.0,,,,,clients,,,,0,,,,"KAFKA-5854 improves authentication failures in clients and has added integration tests and some basic client-side tests that create actual connections to a mock server. It will be good to add a set of tests for producers, consumers etc. that use MockClient to add more extensive tests for various scenarios.

cc [~hachikuji] [~vahid]",,githubbot,rsivaram,vahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-09-26 20:33:06.761,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 12 22:29:47 UTC 2018,,,,,,,"0|i3kb9z:",9223372036854775807,,,,,,,,,,,,,,,,"26/Sep/17 20:33;githubbot;GitHub user vahidhashemian opened a pull request:

    https://github.com/apache/kafka/pull/3965

    KAFKA-5944: Unit tests for handling SASL authentication failures in clients

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/vahidhashemian/kafka KAFKA-5944

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3965.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3965
    
----
commit 448ba5bdd4d25cb2cee53f2e161938f6f71e65bd
Author: Vahid Hashemian <vahidhashemian@us.ibm.com>
Date:   2017-09-21T23:22:34Z

    KAFKA-5944: Unit tests for handling authentication failures in clients

----
","12/Feb/18 22:29;githubbot;hachikuji closed pull request #3965: KAFKA-5944: Unit tests for handling SASL authentication failures in clients
URL: https://github.com/apache/kafka/pull/3965
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/clients/src/test/java/org/apache/kafka/clients/MockClient.java b/clients/src/test/java/org/apache/kafka/clients/MockClient.java
index d843414fd7a..65255fe9648 100644
--- a/clients/src/test/java/org/apache/kafka/clients/MockClient.java
+++ b/clients/src/test/java/org/apache/kafka/clients/MockClient.java
@@ -20,6 +20,7 @@
 import org.apache.kafka.common.Node;
 import org.apache.kafka.common.errors.AuthenticationException;
 import org.apache.kafka.common.errors.UnsupportedVersionException;
+import org.apache.kafka.common.protocol.Errors;
 import org.apache.kafka.common.requests.AbstractRequest;
 import org.apache.kafka.common.requests.AbstractResponse;
 import org.apache.kafka.common.utils.Time;
@@ -77,6 +78,7 @@ public FutureResponse(Node node,
     private Node node = null;
     private final Set<String> ready = new HashSet<>();
     private final Map<Node, Long> blackedOut = new HashMap<>();
+    private final Map<Node, AuthenticationException> authenticationException = new HashMap<>();
     // Use concurrent queue for requests so that requests may be queried from a different thread
     private final Queue<ClientRequest> requests = new ConcurrentLinkedDeque<>();
     // Use concurrent queue for responses so that responses may be updated during poll() from a different thread.
@@ -102,7 +104,7 @@ public boolean isReady(Node node, long now) {
 
     @Override
     public boolean ready(Node node, long now) {
-        if (isBlackedOut(node))
+        if (isBlackedOut(node) || authenticationException(node) != null)
             return false;
         ready.add(node.idString());
         return true;
@@ -117,6 +119,12 @@ public void blackout(Node node, long duration) {
         blackedOut.put(node, time.milliseconds() + duration);
     }
 
+    public void authenticationFailed(Node node, long duration) {
+        authenticationException.put(node, (AuthenticationException) Errors.SASL_AUTHENTICATION_FAILED.exception());
+        disconnect(node.idString());
+        blackout(node, duration);
+    }
+
     private boolean isBlackedOut(Node node) {
         if (blackedOut.containsKey(node)) {
             long expiration = blackedOut.get(node);
@@ -137,7 +145,7 @@ public boolean connectionFailed(Node node) {
 
     @Override
     public AuthenticationException authenticationException(Node node) {
-        return null;
+        return authenticationException.get(node);
     }
 
     @Override
@@ -347,6 +355,7 @@ public void reset() {
         responses.clear();
         futureResponses.clear();
         metadataUpdates.clear();
+        authenticationException.clear();
     }
 
     public void prepareMetadataUpdate(Cluster cluster, Set<String> unavailableTopics) {
diff --git a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java
index 186ccf06cb5..f08a99b6ddc 100644
--- a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java
+++ b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java
@@ -30,6 +30,7 @@
 import org.apache.kafka.common.acl.AclOperation;
 import org.apache.kafka.common.acl.AclPermissionType;
 import org.apache.kafka.common.config.ConfigResource;
+import org.apache.kafka.common.errors.AuthenticationException;
 import org.apache.kafka.common.errors.InvalidTopicException;
 import org.apache.kafka.common.errors.LeaderNotAvailableException;
 import org.apache.kafka.common.errors.NotLeaderForPartitionException;
@@ -248,6 +249,75 @@ public void testInvalidTopicNames() throws Exception {
         }
     }
 
+    @Test
+    public void testAdminClientApisWithinBlackoutPeriodAfterAuthenticationFailure() throws Exception {
+        AdminClientUnitTestEnv env = mockClientEnv();
+        Node node = env.cluster().controller();
+        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());
+        env.kafkaClient().setNode(node);
+        env.kafkaClient().authenticationFailed(node, 300);
+
+        callAdminClientApisAndExpectAnAuthenticationError(env);
+
+        // wait less than the blackout period, the connection should fail and the authentication error should remain
+        env.time().sleep(30);
+        assertTrue(env.kafkaClient().connectionFailed(node));
+        callAdminClientApisAndExpectAnAuthenticationError(env);
+
+        env.close();
+    }
+
+    private void callAdminClientApisAndExpectAnAuthenticationError(AdminClientUnitTestEnv env) throws InterruptedException {
+        env.kafkaClient().prepareMetadataUpdate(env.cluster(), Collections.<String>emptySet());
+
+        try {
+            env.adminClient().createTopics(
+                    Collections.singleton(new NewTopic(""myTopic"", Collections.singletonMap(Integer.valueOf(0), asList(new Integer[]{0, 1, 2})))),
+                    new CreateTopicsOptions().timeoutMs(10000)).all().get();
+            fail(""Expected an authentication error."");
+        } catch (ExecutionException e) {
+            assertTrue(""Expected only an authentication error."", e.getCause() instanceof AuthenticationException);
+        }
+
+        try {
+            Map<String, NewPartitions> counts = new HashMap<>();
+            counts.put(""my_topic"", NewPartitions.increaseTo(3));
+            counts.put(""other_topic"", NewPartitions.increaseTo(3, asList(asList(2), asList(3))));
+            env.adminClient().createPartitions(counts).all().get();
+            fail(""Expected an authentication error."");
+        } catch (ExecutionException e) {
+            assertTrue(""Expected only an authentication error."", e.getCause() instanceof AuthenticationException);
+        }
+
+        try {
+            env.adminClient().createAcls(asList(ACL1, ACL2)).all().get();
+            fail(""Expected an authentication error."");
+        } catch (ExecutionException e) {
+            assertTrue(""Expected only an authentication error."", e.getCause() instanceof AuthenticationException);
+        }
+
+        try {
+            env.adminClient().describeAcls(FILTER1).values().get();
+            fail(""Expected an authentication error."");
+        } catch (ExecutionException e) {
+            assertTrue(""Expected only an authentication error."", e.getCause() instanceof AuthenticationException);
+        }
+
+        try {
+            env.adminClient().deleteAcls(asList(FILTER1, FILTER2)).all().get();
+            fail(""Expected an authentication error."");
+        } catch (ExecutionException e) {
+            assertTrue(""Expected only an authentication error."", e.getCause() instanceof AuthenticationException);
+        }
+
+        try {
+            env.adminClient().describeConfigs(Collections.singleton(new ConfigResource(ConfigResource.Type.BROKER, ""0""))).all().get();
+            fail(""Expected an authentication error."");
+        } catch (ExecutionException e) {
+            assertTrue(""Expected only an authentication error."", e.getCause() instanceof AuthenticationException);
+        }
+    }
+
     private static final AclBinding ACL1 = new AclBinding(new Resource(ResourceType.TOPIC, ""mytopic3""),
         new AccessControlEntry(""User:ANONYMOUS"", ""*"", AclOperation.DESCRIBE, AclPermissionType.ALLOW));
     private static final AclBinding ACL2 = new AclBinding(new Resource(ResourceType.TOPIC, ""mytopic4""),
@@ -579,7 +649,7 @@ public static KafkaAdminClient createInternal(AdminClientConfig config, KafkaAdm
         private int numTries = 0;
 
         private int failuresInjected = 0;
-        
+
         @Override
         public KafkaAdminClient.TimeoutProcessor create(long now) {
             return new FailureInjectingTimeoutProcessor(now);
diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java
index d47124f8f3e..be8db2bf55c 100644
--- a/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java
@@ -32,6 +32,7 @@
 import org.apache.kafka.common.KafkaException;
 import org.apache.kafka.common.Node;
 import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.common.errors.AuthenticationException;
 import org.apache.kafka.common.errors.InterruptException;
 import org.apache.kafka.common.errors.WakeupException;
 import org.apache.kafka.common.metrics.Metrics;
@@ -1431,6 +1432,88 @@ public boolean conditionMet() {
         }
     }
 
+    @Test
+    public void testConsumerWithinBlackoutPeriodAfterAuthenticationFailure() {
+        int rebalanceTimeoutMs = 60000;
+        int sessionTimeoutMs = 30000;
+        int heartbeatIntervalMs = 3000;
+        int autoCommitIntervalMs = 1000;
+
+        Time time = new MockTime();
+        Map<String, Integer> tpCounts = new HashMap<>();
+        tpCounts.put(topic, 1);
+        Cluster cluster = TestUtils.singletonCluster(tpCounts);
+        Node node = cluster.nodes().get(0);
+
+        Metadata metadata = createMetadata();
+        metadata.update(cluster, Collections.<String>emptySet(), time.milliseconds());
+
+        MockClient client = new MockClient(time, metadata);
+        client.setNode(node);
+        client.authenticationFailed(node, 300);
+        PartitionAssignor assignor = new RangeAssignor();
+
+        final KafkaConsumer<String, String> consumer = newConsumer(time, client, metadata, assignor,
+                rebalanceTimeoutMs, sessionTimeoutMs, heartbeatIntervalMs, true, autoCommitIntervalMs);
+
+        consumer.subscribe(Collections.singleton(topic));
+        callConsumerApisAndExpectAnAuthenticationError(consumer, tp0);
+
+        time.sleep(30); // wait less than the blackout period
+        assertTrue(client.connectionFailed(node));
+        callConsumerApisAndExpectAnAuthenticationError(consumer, tp0);
+
+        client.requests().clear();
+        consumer.close(0, TimeUnit.MILLISECONDS);
+    }
+
+    private void callConsumerApisAndExpectAnAuthenticationError(KafkaConsumer<?, ?> consumer, TopicPartition partition) {
+        try {
+            consumer.partitionsFor(""some other topic"");
+            fail(""Expected an authentication error!"");
+        } catch (AuthenticationException e) {
+            // OK
+        }
+
+        try {
+            consumer.beginningOffsets(Collections.singleton(partition));
+            fail(""Expected an authentication error!"");
+        } catch (AuthenticationException e) {
+            // OK
+        }
+
+        try {
+            consumer.endOffsets(Collections.singleton(partition));
+            fail(""Expected an authentication error!"");
+        } catch (AuthenticationException e) {
+            // OK
+        }
+
+        try {
+            consumer.poll(10);
+            fail(""Expected an authentication error!"");
+        } catch (AuthenticationException e) {
+            // OK
+        }
+
+        Map<TopicPartition, OffsetAndMetadata> offset = new HashMap<>();
+        offset.put(partition, new OffsetAndMetadata(10L));
+
+        try {
+            consumer.commitSync(offset);
+            fail(""Expected an authentication error!"");
+        } catch (AuthenticationException e) {
+            // OK
+        }
+
+        try {
+            consumer.committed(partition);
+            fail(""Expected an authentication error!"");
+        } catch (AuthenticationException e) {
+            // OK
+        }
+    }
+
     private ConsumerRebalanceListener getConsumerRebalanceListener(final KafkaConsumer<String, String> consumer) {
         return new ConsumerRebalanceListener() {
             @Override
diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinatorTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinatorTest.java
index 7eaca9826d9..1c88803e26c 100644
--- a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinatorTest.java
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinatorTest.java
@@ -20,6 +20,7 @@
 import org.apache.kafka.clients.MockClient;
 import org.apache.kafka.common.Cluster;
 import org.apache.kafka.common.Node;
+import org.apache.kafka.common.errors.AuthenticationException;
 import org.apache.kafka.common.errors.WakeupException;
 import org.apache.kafka.common.metrics.Metrics;
 import org.apache.kafka.common.protocol.Errors;
@@ -519,6 +520,30 @@ public void testWakeupInOnJoinComplete() throws Exception {
         awaitFirstHeartbeat(heartbeatReceived);
     }
 
+    @Test
+    public void testEnsureCoordinatorReadyWithinBlackoutPeriodAfterAuthenticationFailure() {
+        setupCoordinator(RETRY_BACKOFF_MS);
+
+        mockClient.authenticationFailed(node, 300);
+
+        try {
+            coordinator.ensureCoordinatorReady();
+            fail(""Expected an authentication error."");
+        } catch (AuthenticationException e) {
+            // OK
+        }
+
+        mockTime.sleep(30); // wait less than the blackout period
+        assertTrue(mockClient.connectionFailed(node));
+
+        try {
+            coordinator.ensureCoordinatorReady();
+            fail(""Expected an authentication error."");
+        } catch (AuthenticationException e) {
+            // OK
+        }
+    }
+
     private AtomicBoolean prepareFirstHeartbeat() {
         final AtomicBoolean heartbeatReceived = new AtomicBoolean(false);
         mockClient.prepareResponse(new MockClient.RequestMatcher() {
diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java
index c49339b6525..fdaa6b3f522 100644
--- a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java
@@ -33,6 +33,7 @@
 import org.apache.kafka.common.Node;
 import org.apache.kafka.common.TopicPartition;
 import org.apache.kafka.common.errors.ApiException;
+import org.apache.kafka.common.errors.AuthenticationException;
 import org.apache.kafka.common.errors.DisconnectException;
 import org.apache.kafka.common.errors.GroupAuthorizationException;
 import org.apache.kafka.common.errors.OffsetMetadataTooLarge;
@@ -1503,6 +1504,28 @@ public void testRefreshOffsetWithNoFetchableOffsets() {
         assertEquals(null, subscriptions.committed(t1p));
     }
 
+    @Test
+    public void testEnsureActiveGroupWithinBlackoutPeriodAfterAuthenticationFailure() {
+        client.authenticationFailed(node, 300);
+
+        try {
+            coordinator.ensureActiveGroup();
+            fail(""Expected an authentication error."");
+        } catch (AuthenticationException e) {
+            // OK
+        }
+
+        time.sleep(30); // wait less than the blackout period
+        assertTrue(client.connectionFailed(node));
+
+        try {
+            coordinator.ensureActiveGroup();
+            fail(""Expected an authentication error."");
+        } catch (AuthenticationException e) {
+            // OK
+        }
+    }
+
     @Test
     public void testProtocolMetadataOrder() {
         RoundRobinAssignor roundRobin = new RoundRobinAssignor();
diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClientTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClientTest.java
index 93c6acd9c40..904270ec489 100644
--- a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClientTest.java
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClientTest.java
@@ -22,6 +22,7 @@
 import org.apache.kafka.clients.NetworkClient;
 import org.apache.kafka.common.Cluster;
 import org.apache.kafka.common.Node;
+import org.apache.kafka.common.errors.AuthenticationException;
 import org.apache.kafka.common.errors.DisconnectException;
 import org.apache.kafka.common.errors.WakeupException;
 import org.apache.kafka.common.protocol.Errors;
@@ -69,6 +70,24 @@ public void send() {
         assertEquals(Errors.NONE, response.error());
     }
 
+    @Test
+    public void sendWithinBlackoutPeriodAfterAuthenticationFailure() throws InterruptedException {
+        client.authenticationFailed(node, 300);
+        client.prepareResponse(heartbeatResponse(Errors.NONE));
+        final RequestFuture<ClientResponse> future = consumerClient.send(node, heartbeat());
+        consumerClient.poll(future);
+        assertTrue(future.failed());
+        assertTrue(""Expected only an authentication error."", future.exception() instanceof AuthenticationException);
+
+        time.sleep(30); // wait less than the blackout period
+        assertTrue(client.connectionFailed(node));
+
+        final RequestFuture<ClientResponse> future2 = consumerClient.send(node, heartbeat());
+        consumerClient.poll(future2);
+        assertTrue(future2.failed());
+        assertTrue(""Expected only an authentication error."", future2.exception() instanceof AuthenticationException);
+    }
+
     @Test
     public void multiSend() {
         client.prepareResponse(heartbeatResponse(Errors.NONE));


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,
Add a unit test for disconnecting idle socket connections ,KAFKA-2661,12905397,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,junrao,junrao,16/Oct/15 01:53,13/Apr/18 17:41,12/Jan/21 11:54,13/Apr/18 17:41,,,,,,,,,,core,,,,0,,,,The logic for disconnecting idle connections is now moved to Selector. We just need to add a unit test to verify that it works.,,junrao,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-04-13 17:41:36.461,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 13 17:41:36 UTC 2018,,,,,,,"0|i2n3gv:",9223372036854775807,,,,,,,,,,,,,,,,"13/Apr/18 17:41;omkreddy;SelectorTest.testCloseConnectionInClosingState/testCloseOldestConnection tests covers unit test case for idle connections.",,,,,,,,,,,,,,,,,,,,,
Add a system test for dynamic broker config update,KAFKA-6695,13146669,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,rsivaram,rsivaram,rsivaram,20/Mar/18 21:41,20/Mar/18 21:41,12/Jan/21 11:54,,,,,,,,,,,core,,,,0,,,,Add a system test that does some basic validation of dynamic broker configs.,,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-03-20 21:41:39.0,,,,,,,"0|i3rkev:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SaslSslAdminClientIntegrationTest sometimes fails,KAFKA-6232,13119415,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Cannot Reproduce,,yuzhihong@gmail.com,yuzhihong@gmail.com,18/Nov/17 10:09,29/Jan/18 04:22,12/Jan/21 11:54,29/Jan/18 04:22,,,,,,,,,,,,,,0,security,,,"Here was one recent occurrence:

https://builds.apache.org/job/kafka-trunk-jdk9/203/testReport/kafka.api/SaslSslAdminClientIntegrationTest/testLogStartOffsetCheckpoint/
{code}
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.LeaderNotAvailableException: There is no leader for this topic-partition as we are in the middle of a leadership election.
	at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)
	at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)
	at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:104)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:225)
	at kafka.api.AdminClientIntegrationTest.$anonfun$testLogStartOffsetCheckpoint$3(AdminClientIntegrationTest.scala:762)
{code}
In the test output, I saw:
{code}
[2017-11-17 23:15:45,593] ERROR [KafkaApi-1] Error when handling request {filters=[{resource_type=2,resource_name=foobar,principal=User:ANONYMOUS,host=*,operation=3,permission_type=3},{resource_type=5,resource_name=transactional_id,principal=User:ANONYMOUS,host=*,operation=4,permission_type=3}]} (kafka.server.KafkaApis:107)
org.apache.kafka.common.errors.ClusterAuthorizationException: Request Request(processor=2, connectionId=127.0.0.1:36295-127.0.0.1:58183-0, session=Session(User:client2,localhost/127.0.0.1), listenerName=ListenerName(SASL_SSL), securityProtocol=SASL_SSL, buffer=null) is not authorized.
{code}",,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/17 10:11;yuzhihong@gmail.com;saslSslAdminClientIntegrationTest-203.out;https://issues.apache.org/jira/secure/attachment/12898335/saslSslAdminClientIntegrationTest-203.out",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 19 01:58:42 UTC 2018,,,,,,,"0|i3mykn:",9223372036854775807,,,,,,,,,,,,,,,,"21/Nov/17 15:58;yuzhihong@gmail.com;From https://builds.apache.org/job/kafka-trunk-jdk8/2225/testReport/ :

Failed 4 times in the last 11 runs. Flakiness: 40%, Stability: 63%","19/Jan/18 01:58;yuzhihong@gmail.com;Hasn't seen this failure lately.",,,,,,,,,,,,,,,,,,,,
Add concurrent tests to exercise all paths in group/transaction managers,KAFKA-6096,13110973,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,20/Oct/17 13:07,09/Jan/18 22:39,12/Jan/21 11:54,09/Jan/18 22:39,,,,,1.1.0,,,,,core,,,,0,,,,"We don't have enough tests to test locking/deadlocks in GroupMetadataManager and TransactionManager. Since we have had a lot of deadlocks (KAFKA-5970, KAFKA-6042 etc.) which were not detected during testing, we should add more mock tests with concurrency to verify the locking.",,githubbot,mjuarez,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-10-23 22:06:29.045,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 09 00:15:38 UTC 2018,,,,,,,"0|i3lilj:",9223372036854775807,,,,,,,,,,,,,,,,"23/Oct/17 22:06;githubbot;GitHub user rajinisivaram opened a pull request:

    https://github.com/apache/kafka/pull/4122

    KAFKA-6096: Add multi-threaded tests for group coordinator, txn manager

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rajinisivaram/kafka KAFKA-6096-deadlock-test

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/4122.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #4122
    
----
commit 5e39e73da84c0057ae4815b066cbc6e9113bc608
Author: Rajini Sivaram <rajinisivaram@googlemail.com>
Date:   2017-10-23T20:59:04Z

    KAFKA-6096: Add multi-threaded tests for group coordinator, txn manager

----
","09/Jan/18 00:15;githubbot;hachikuji closed pull request #4122: KAFKA-6096: Add multi-threaded tests for group coordinator, txn manager
URL: https://github.com/apache/kafka/pull/4122
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala b/core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala
new file mode 100644
index 00000000000..0ecc3f538b1
--- /dev/null
+++ b/core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala
@@ -0,0 +1,226 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package kafka.coordinator
+
+import java.util.{ Collections, Random }
+import java.util.concurrent.{ ConcurrentHashMap, Executors }
+import java.util.concurrent.atomic.AtomicInteger
+import java.util.concurrent.locks.Lock
+
+import kafka.coordinator.AbstractCoordinatorConcurrencyTest._
+import kafka.log.Log
+import kafka.server._
+import kafka.utils._
+import kafka.utils.timer.MockTimer
+import kafka.zk.KafkaZkClient
+import org.apache.kafka.common.TopicPartition
+import org.apache.kafka.common.protocol.Errors
+import org.apache.kafka.common.record.{ MemoryRecords, RecordBatch, RecordsProcessingStats }
+import org.apache.kafka.common.requests.ProduceResponse.PartitionResponse
+import org.easymock.EasyMock
+import org.junit.{ After, Before }
+
+import scala.collection._
+import scala.collection.JavaConverters._
+
+abstract class AbstractCoordinatorConcurrencyTest[M <: CoordinatorMember] {
+
+  val nThreads = 5
+
+  val time = new MockTime
+  val timer = new MockTimer
+  val executor = Executors.newFixedThreadPool(nThreads)
+  val scheduler = new MockScheduler(time)
+  var replicaManager: TestReplicaManager = _
+  var zkClient: KafkaZkClient = _
+  val serverProps = TestUtils.createBrokerConfig(nodeId = 0, zkConnect = """")
+  val random = new Random
+
+  @Before
+  def setUp() {
+
+    replicaManager = EasyMock.partialMockBuilder(classOf[TestReplicaManager]).createMock()
+    replicaManager.createDelayedProducePurgatory(timer)
+
+    zkClient = EasyMock.createNiceMock(classOf[KafkaZkClient])
+  }
+
+  @After
+  def tearDown() {
+    EasyMock.reset(replicaManager)
+    if (executor != null)
+      executor.shutdownNow()
+  }
+
+  /**
+    * Verify that concurrent operations run in the normal sequence produce the expected results.
+    */
+  def verifyConcurrentOperations(createMembers: String => Set[M], operations: Seq[Operation]) {
+    OrderedOperationSequence(createMembers(""verifyConcurrentOperations""), operations).run()
+  }
+
+  /**
+    * Verify that arbitrary operations run in some random sequence don't leave the coordinator
+    * in a bad state. Operations in the normal sequence should continue to work as expected.
+    */
+  def verifyConcurrentRandomSequences(createMembers: String => Set[M], operations: Seq[Operation]) {
+    EasyMock.reset(replicaManager)
+    for (i <- 0 to 10) {
+      // Run some random operations
+      RandomOperationSequence(createMembers(s""random$i""), operations).run()
+
+      // Check that proper sequences still work correctly
+      OrderedOperationSequence(createMembers(s""ordered$i""), operations).run()
+    }
+  }
+
+  def verifyConcurrentActions(actions: Set[Action]) {
+    val futures = actions.map(executor.submit)
+    futures.map(_.get)
+    enableCompletion()
+    actions.foreach(_.await())
+  }
+
+  def enableCompletion(): Unit = {
+    replicaManager.tryCompleteDelayedRequests()
+    scheduler.tick()
+  }
+
+  abstract class OperationSequence(members: Set[M], operations: Seq[Operation]) {
+    def actionSequence: Seq[Set[Action]]
+    def run(): Unit = {
+      actionSequence.foreach(verifyConcurrentActions)
+    }
+  }
+
+  case class OrderedOperationSequence(members: Set[M], operations: Seq[Operation])
+    extends OperationSequence(members, operations) {
+    override def actionSequence: Seq[Set[Action]] = {
+      operations.map { op =>
+        members.map(op.actionWithVerify)
+      }
+    }
+  }
+
+  case class RandomOperationSequence(members: Set[M], operations: Seq[Operation])
+    extends OperationSequence(members, operations) {
+    val opCount = operations.length
+    def actionSequence: Seq[Set[Action]] = {
+      (0 to opCount).map { _ =>
+        members.map { member =>
+          val op = operations(random.nextInt(opCount))
+          op.actionNoVerify(member) // Don't wait or verify since these operations may block
+        }
+      }
+    }
+  }
+
+  abstract class Operation {
+    def run(member: M): Unit
+    def awaitAndVerify(member: M): Unit
+    def actionWithVerify(member: M): Action = {
+      new Action() {
+        def run(): Unit = Operation.this.run(member)
+        def await(): Unit = awaitAndVerify(member)
+      }
+    }
+    def actionNoVerify(member: M): Action = {
+      new Action() {
+        def run(): Unit = Operation.this.run(member)
+        def await(): Unit = timer.advanceClock(100) // Don't wait since operation may block
+      }
+    }
+  }
+}
+
+object AbstractCoordinatorConcurrencyTest {
+
+  trait Action extends Runnable {
+    def await(): Unit
+  }
+
+  trait CoordinatorMember {
+  }
+
+  class TestReplicaManager extends ReplicaManager(
+    null, null, null, null, null, null, null, null, null, null, null, null, null, null, None) {
+
+    var producePurgatory: DelayedOperationPurgatory[DelayedProduce] = _
+    var watchKeys: mutable.Set[TopicPartitionOperationKey] = _
+    def createDelayedProducePurgatory(timer: MockTimer): Unit = {
+      producePurgatory = new DelayedOperationPurgatory[DelayedProduce](""Produce"", timer, 1, reaperEnabled = false)
+      watchKeys = Collections.newSetFromMap(new ConcurrentHashMap[TopicPartitionOperationKey, java.lang.Boolean]()).asScala
+    }
+    def tryCompleteDelayedRequests(): Unit = {
+      watchKeys.map(producePurgatory.checkAndComplete)
+    }
+
+    override def appendRecords(timeout: Long,
+                               requiredAcks: Short,
+                               internalTopicsAllowed: Boolean,
+                               isFromClient: Boolean,
+                               entriesPerPartition: Map[TopicPartition, MemoryRecords],
+                               responseCallback: Map[TopicPartition, PartitionResponse] => Unit,
+                               delayedProduceLock: Option[Lock] = None,
+                               processingStatsCallback: Map[TopicPartition, RecordsProcessingStats] => Unit = _ => ()) {
+
+      if (entriesPerPartition.isEmpty)
+        return
+      val produceMetadata = ProduceMetadata(1, entriesPerPartition.map {
+        case (tp, _) =>
+          (tp, ProducePartitionStatus(0L, new PartitionResponse(Errors.NONE, 0L, RecordBatch.NO_TIMESTAMP, 0L)))
+      })
+      val delayedProduce = new DelayedProduce(5, produceMetadata, this, responseCallback, delayedProduceLock) {
+        // Complete produce requests after a few attempts to trigger delayed produce from different threads
+        val completeAttempts = new AtomicInteger
+        override def tryComplete(): Boolean = {
+          if (completeAttempts.incrementAndGet() >= 3)
+            forceComplete()
+          else
+            false
+        }
+        override def onComplete() {
+          responseCallback(entriesPerPartition.map {
+            case (tp, _) =>
+              (tp, new PartitionResponse(Errors.NONE, 0L, RecordBatch.NO_TIMESTAMP, 0L))
+          })
+        }
+      }
+      val producerRequestKeys = entriesPerPartition.keys.map(new TopicPartitionOperationKey(_)).toSeq
+      watchKeys ++= producerRequestKeys
+      producePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)
+      tryCompleteDelayedRequests()
+    }
+    override def getMagic(topicPartition: TopicPartition): Option[Byte] = {
+      Some(RecordBatch.MAGIC_VALUE_V2)
+    }
+    @volatile var logs: mutable.Map[TopicPartition, (Log, Long)] = _
+    def getOrCreateLogs(): mutable.Map[TopicPartition, (Log, Long)] = {
+      if (logs == null)
+        logs = mutable.Map[TopicPartition, (Log, Long)]()
+      logs
+    }
+    def updateLog(topicPartition: TopicPartition, log: Log, endOffset: Long): Unit = {
+      getOrCreateLogs().put(topicPartition, (log, endOffset))
+    }
+    override def getLog(topicPartition: TopicPartition): Option[Log] =
+      getOrCreateLogs().get(topicPartition).map(l => l._1)
+    override def getLogEndOffset(topicPartition: TopicPartition): Option[Long] =
+      getOrCreateLogs().get(topicPartition).map(l => l._2)
+  }
+}
diff --git a/core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala b/core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala
new file mode 100644
index 00000000000..44e13560b00
--- /dev/null
+++ b/core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala
@@ -0,0 +1,310 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package kafka.coordinator.group
+
+import java.util.concurrent.{ ConcurrentHashMap, TimeUnit }
+
+import kafka.common.OffsetAndMetadata
+import kafka.coordinator.AbstractCoordinatorConcurrencyTest
+import kafka.coordinator.AbstractCoordinatorConcurrencyTest._
+import kafka.coordinator.group.GroupCoordinatorConcurrencyTest._
+import kafka.server.{ DelayedOperationPurgatory, KafkaConfig }
+import org.apache.kafka.common.TopicPartition
+import org.apache.kafka.common.internals.Topic
+import org.apache.kafka.common.protocol.Errors
+import org.apache.kafka.common.requests.{ JoinGroupRequest, TransactionResult }
+import org.easymock.EasyMock
+import org.junit.Assert._
+import org.junit.{ After, Before, Test }
+
+import scala.collection._
+import scala.concurrent.duration.Duration
+import scala.concurrent.{ Await, Future, Promise, TimeoutException }
+
+class GroupCoordinatorConcurrencyTest extends AbstractCoordinatorConcurrencyTest[GroupMember] {
+
+  private val protocolType = ""consumer""
+  private val metadata = Array[Byte]()
+  private val protocols = List((""range"", metadata))
+
+  private val nGroups = nThreads * 10
+  private val nMembersPerGroup = nThreads * 5
+  private val numPartitions = 2
+
+  private val allOperations = Seq(
+      new JoinGroupOperation,
+      new SyncGroupOperation,
+      new CommitOffsetsOperation,
+      new HeartbeatOperation,
+      new LeaveGroupOperation
+    )
+  private val allOperationsWithTxn = Seq(
+    new JoinGroupOperation,
+    new SyncGroupOperation,
+    new CommitTxnOffsetsOperation,
+    new CompleteTxnOperation,
+    new HeartbeatOperation,
+    new LeaveGroupOperation
+  )
+
+  var groupCoordinator: GroupCoordinator = _
+
+  @Before
+  override def setUp() {
+    super.setUp()
+
+    EasyMock.expect(zkClient.getTopicPartitionCount(Topic.GROUP_METADATA_TOPIC_NAME))
+      .andReturn(Some(numPartitions))
+      .anyTimes()
+    EasyMock.replay(zkClient)
+
+    serverProps.setProperty(KafkaConfig.GroupMinSessionTimeoutMsProp, ConsumerMinSessionTimeout.toString)
+    serverProps.setProperty(KafkaConfig.GroupMaxSessionTimeoutMsProp, ConsumerMaxSessionTimeout.toString)
+    serverProps.setProperty(KafkaConfig.GroupInitialRebalanceDelayMsProp, GroupInitialRebalanceDelay.toString)
+
+    val config = KafkaConfig.fromProps(serverProps)
+
+    val heartbeatPurgatory = new DelayedOperationPurgatory[DelayedHeartbeat](""Heartbeat"", timer, config.brokerId, reaperEnabled = false)
+    val joinPurgatory = new DelayedOperationPurgatory[DelayedJoin](""Rebalance"", timer, config.brokerId, reaperEnabled = false)
+
+    groupCoordinator = GroupCoordinator(config, zkClient, replicaManager, heartbeatPurgatory, joinPurgatory, timer.time)
+    groupCoordinator.startup(false)
+  }
+
+  @After
+  override def tearDown() {
+    try {
+      if (groupCoordinator != null)
+        groupCoordinator.shutdown()
+    } finally {
+      super.tearDown()
+    }
+  }
+
+  def createGroupMembers(groupPrefix: String): Set[GroupMember] = {
+    (0 until nGroups).flatMap { i =>
+      new Group(s""$groupPrefix$i"", nMembersPerGroup, groupCoordinator, replicaManager).members
+    }.toSet
+  }
+
+  @Test
+  def testConcurrentGoodPathSequence() {
+    verifyConcurrentOperations(createGroupMembers, allOperations)
+  }
+
+  @Test
+  def testConcurrentTxnGoodPathSequence() {
+    verifyConcurrentOperations(createGroupMembers, allOperationsWithTxn)
+  }
+
+  @Test
+  def testConcurrentRandomSequence() {
+    verifyConcurrentRandomSequences(createGroupMembers, allOperationsWithTxn)
+  }
+
+
+  abstract class GroupOperation[R, C] extends Operation {
+    val responseFutures = new ConcurrentHashMap[GroupMember, Future[R]]()
+
+    def setUpCallback(member: GroupMember): C = {
+      val responsePromise = Promise[R]
+      val responseFuture = responsePromise.future
+      responseFutures.put(member, responseFuture)
+      responseCallback(responsePromise)
+    }
+    def responseCallback(responsePromise: Promise[R]): C
+
+    override def run(member: GroupMember): Unit = {
+      val responseCallback = setUpCallback(member)
+      runWithCallback(member, responseCallback)
+    }
+
+    def runWithCallback(member: GroupMember, responseCallback: C): Unit
+
+    def await(member: GroupMember, timeoutMs: Long): R = {
+      var retries = (timeoutMs + 10) / 10
+      val responseFuture = responseFutures.get(member)
+      while (retries > 0) {
+        timer.advanceClock(10)
+        try {
+          return Await.result(responseFuture, Duration(10, TimeUnit.MILLISECONDS))
+        } catch {
+          case _: TimeoutException =>
+        }
+        retries -= 1
+      }
+      throw new TimeoutException(s""Operation did not complete within $timeoutMs millis"")
+    }
+  }
+
+
+  class JoinGroupOperation extends GroupOperation[JoinGroupResult, JoinGroupCallback] {
+    override def responseCallback(responsePromise: Promise[JoinGroupResult]): JoinGroupCallback = {
+      val callback: JoinGroupCallback = responsePromise.success(_)
+      callback
+    }
+    override def runWithCallback(member: GroupMember, responseCallback: JoinGroupCallback): Unit = {
+      groupCoordinator.handleJoinGroup(member.groupId, member.memberId, ""clientId"", ""clientHost"",
+       DefaultRebalanceTimeout, DefaultSessionTimeout,
+       protocolType, protocols, responseCallback)
+    }
+    override def awaitAndVerify(member: GroupMember): Unit = {
+       val joinGroupResult = await(member, DefaultRebalanceTimeout)
+       assertEquals(Errors.NONE, joinGroupResult.error)
+       member.memberId = joinGroupResult.memberId
+       member.generationId = joinGroupResult.generationId
+    }
+  }
+
+  class SyncGroupOperation extends GroupOperation[SyncGroupCallbackParams, SyncGroupCallback] {
+    override def responseCallback(responsePromise: Promise[SyncGroupCallbackParams]): SyncGroupCallback = {
+      val callback: SyncGroupCallback = (assignment, error) =>
+        responsePromise.success((assignment, error))
+      callback
+    }
+    override def runWithCallback(member: GroupMember, responseCallback: SyncGroupCallback): Unit = {
+      if (member.leader) {
+        groupCoordinator.handleSyncGroup(member.groupId, member.generationId, member.memberId,
+            member.group.assignment, responseCallback)
+      } else {
+         groupCoordinator.handleSyncGroup(member.groupId, member.generationId, member.memberId,
+             Map.empty[String, Array[Byte]], responseCallback)
+      }
+    }
+    override def awaitAndVerify(member: GroupMember): Unit = {
+       val result = await(member, DefaultSessionTimeout)
+       assertEquals(Errors.NONE, result._2)
+    }
+  }
+
+  class HeartbeatOperation extends GroupOperation[HeartbeatCallbackParams, HeartbeatCallback] {
+    override def responseCallback(responsePromise: Promise[HeartbeatCallbackParams]): HeartbeatCallback = {
+      val callback: HeartbeatCallback = error => responsePromise.success(error)
+      callback
+    }
+    override def runWithCallback(member: GroupMember, responseCallback: HeartbeatCallback): Unit = {
+      groupCoordinator.handleHeartbeat( member.groupId, member.memberId,  member.generationId, responseCallback)
+    }
+    override def awaitAndVerify(member: GroupMember): Unit = {
+       val error = await(member, DefaultSessionTimeout)
+       assertEquals(Errors.NONE, error)
+    }
+  }
+  class CommitOffsetsOperation extends GroupOperation[CommitOffsetCallbackParams, CommitOffsetCallback] {
+    override def responseCallback(responsePromise: Promise[CommitOffsetCallbackParams]): CommitOffsetCallback = {
+      val callback: CommitOffsetCallback = offsets => responsePromise.success(offsets)
+      callback
+    }
+    override def runWithCallback(member: GroupMember, responseCallback: CommitOffsetCallback): Unit = {
+      val tp = new TopicPartition(""topic"", 0)
+      val offsets = immutable.Map(tp -> OffsetAndMetadata(1))
+      groupCoordinator.handleCommitOffsets(member.groupId, member.memberId, member.generationId,
+          offsets, responseCallback)
+    }
+    override def awaitAndVerify(member: GroupMember): Unit = {
+       val offsets = await(member, 500)
+       offsets.foreach { case (_, error) => assertEquals(Errors.NONE, error) }
+    }
+  }
+
+  class CommitTxnOffsetsOperation extends CommitOffsetsOperation {
+    override def runWithCallback(member: GroupMember, responseCallback: CommitOffsetCallback): Unit = {
+      val tp = new TopicPartition(""topic"", 0)
+      val offsets = immutable.Map(tp -> OffsetAndMetadata(1))
+      val producerId = 1000L
+      val producerEpoch : Short = 2
+      groupCoordinator.handleTxnCommitOffsets(member.group.groupId,
+          producerId, producerEpoch, offsets, responseCallback)
+    }
+  }
+
+  class CompleteTxnOperation extends GroupOperation[CompleteTxnCallbackParams, CompleteTxnCallback] {
+    override def responseCallback(responsePromise: Promise[CompleteTxnCallbackParams]): CompleteTxnCallback = {
+      val callback: CompleteTxnCallback = error => responsePromise.success(error)
+      callback
+    }
+    override def runWithCallback(member: GroupMember, responseCallback: CompleteTxnCallback): Unit = {
+      val producerId = 1000L
+      val offsetsPartitions = (0 to numPartitions).map(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, _))
+      groupCoordinator.handleTxnCompletion(producerId, offsetsPartitions, transactionResult(member.group.groupId))
+      responseCallback(Errors.NONE)
+    }
+    override def awaitAndVerify(member: GroupMember): Unit = {
+      val error = await(member, 500)
+      assertEquals(Errors.NONE, error)
+    }
+    // Test both commit and abort. Group ids used in the test have the format <prefix><index>
+    // Use the last digit of the index to decide between commit and abort.
+    private def transactionResult(groupId: String): TransactionResult = {
+      val lastDigit = groupId(groupId.length - 1).toInt
+      if (lastDigit % 2 == 0) TransactionResult.COMMIT else TransactionResult.ABORT
+    }
+  }
+
+  class LeaveGroupOperation extends GroupOperation[LeaveGroupCallbackParams, LeaveGroupCallback] {
+    override def responseCallback(responsePromise: Promise[LeaveGroupCallbackParams]): LeaveGroupCallback = {
+      val callback: LeaveGroupCallback = error => responsePromise.success(error)
+      callback
+    }
+    override def runWithCallback(member: GroupMember, responseCallback: LeaveGroupCallback): Unit = {
+      groupCoordinator.handleLeaveGroup(member.group.groupId, member.memberId, responseCallback)
+    }
+    override def awaitAndVerify(member: GroupMember): Unit = {
+       val error = await(member, DefaultSessionTimeout)
+       assertEquals(Errors.NONE, error)
+    }
+  }
+}
+
+object GroupCoordinatorConcurrencyTest {
+
+
+  type JoinGroupCallback = JoinGroupResult => Unit
+  type SyncGroupCallbackParams = (Array[Byte], Errors)
+  type SyncGroupCallback = (Array[Byte], Errors) => Unit
+  type HeartbeatCallbackParams = Errors
+  type HeartbeatCallback = Errors => Unit
+  type CommitOffsetCallbackParams = Map[TopicPartition, Errors]
+  type CommitOffsetCallback = Map[TopicPartition, Errors] => Unit
+  type LeaveGroupCallbackParams = Errors
+  type LeaveGroupCallback = Errors => Unit
+  type CompleteTxnCallbackParams = Errors
+  type CompleteTxnCallback = Errors => Unit
+
+  private val ConsumerMinSessionTimeout = 10
+  private val ConsumerMaxSessionTimeout = 120 * 1000
+  private val DefaultRebalanceTimeout = 60 * 1000
+  private val DefaultSessionTimeout = 60 * 1000
+  private val GroupInitialRebalanceDelay = 50
+
+  class Group(val groupId: String, nMembers: Int,
+      groupCoordinator: GroupCoordinator, replicaManager: TestReplicaManager) {
+    val groupPartitionId = groupCoordinator.partitionFor(groupId)
+    groupCoordinator.groupManager.addPartitionOwnership(groupPartitionId)
+    val members = (0 until nMembers).map { i =>
+      new GroupMember(this, groupPartitionId, i == 0)
+    }
+    def assignment = members.map { m => (m.memberId, Array[Byte]()) }.toMap
+  }
+
+  class GroupMember(val group: Group, val groupPartitionId: Int, val leader: Boolean) extends CoordinatorMember {
+    @volatile var memberId: String = JoinGroupRequest.UNKNOWN_MEMBER_ID
+    @volatile var generationId: Int = -1
+    def groupId: String = group.groupId
+  }
+}
diff --git a/core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorConcurrencyTest.scala b/core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorConcurrencyTest.scala
new file mode 100644
index 00000000000..046741afa1e
--- /dev/null
+++ b/core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorConcurrencyTest.scala
@@ -0,0 +1,388 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package kafka.coordinator.transaction
+
+import java.nio.ByteBuffer
+
+import kafka.coordinator.AbstractCoordinatorConcurrencyTest
+import kafka.coordinator.AbstractCoordinatorConcurrencyTest._
+import kafka.coordinator.transaction.TransactionCoordinatorConcurrencyTest._
+import kafka.log.Log
+import kafka.server.{ DelayedOperationPurgatory, FetchDataInfo, KafkaConfig, LogOffsetMetadata, MetadataCache }
+import kafka.utils.timer.MockTimer
+import kafka.utils.{ Pool, TestUtils}
+
+import org.apache.kafka.clients.{ ClientResponse, NetworkClient }
+import org.apache.kafka.common.{ Node, TopicPartition }
+import org.apache.kafka.common.internals.Topic.TRANSACTION_STATE_TOPIC_NAME
+import org.apache.kafka.common.protocol.{ ApiKeys, Errors }
+import org.apache.kafka.common.record.{ CompressionType, FileRecords, MemoryRecords, SimpleRecord }
+import org.apache.kafka.common.requests._
+import org.apache.kafka.common.utils.{ LogContext, MockTime }
+
+import org.easymock.EasyMock
+import org.junit.Assert._
+import org.junit.{ After, Before, Test }
+
+import scala.collection.Map
+import scala.collection.mutable
+import scala.collection.JavaConverters._
+
+class TransactionCoordinatorConcurrencyTest extends AbstractCoordinatorConcurrencyTest[Transaction] {
+  private val nTransactions = nThreads * 10
+  private val coordinatorEpoch = 10
+  private val numPartitions = nThreads * 5
+
+  private val txnConfig = TransactionConfig()
+  private var transactionCoordinator: TransactionCoordinator = _
+  private var txnStateManager: TransactionStateManager = _
+  private var txnMarkerChannelManager: TransactionMarkerChannelManager = _
+
+  private val allOperations = Seq(
+      new InitProducerIdOperation,
+      new AddPartitionsToTxnOperation(Set(new TopicPartition(""topic"", 0))),
+      new EndTxnOperation)
+
+  private val allTransactions = mutable.Set[Transaction]()
+  private val txnRecordsByPartition: Map[Int, mutable.ArrayBuffer[SimpleRecord]] =
+    (0 until numPartitions).map { i => (i, mutable.ArrayBuffer[SimpleRecord]()) }.toMap
+
+  @Before
+  override def setUp() {
+    super.setUp()
+
+    EasyMock.expect(zkClient.getTopicPartitionCount(TRANSACTION_STATE_TOPIC_NAME))
+      .andReturn(Some(numPartitions))
+      .anyTimes()
+    EasyMock.replay(zkClient)
+
+    txnStateManager = new TransactionStateManager(0, zkClient, scheduler, replicaManager, txnConfig, time)
+    for (i <- 0 until numPartitions)
+      txnStateManager.addLoadedTransactionsToCache(i, coordinatorEpoch, new Pool[String, TransactionMetadata]())
+
+    val producerId = 11
+    val pidManager: ProducerIdManager = EasyMock.createNiceMock(classOf[ProducerIdManager])
+    EasyMock.expect(pidManager.generateProducerId())
+      .andReturn(producerId)
+      .anyTimes()
+    val txnMarkerPurgatory = new DelayedOperationPurgatory[DelayedTxnMarker](""txn-purgatory-name"",
+      new MockTimer,
+      reaperEnabled = false)
+    val brokerNode = new Node(0, ""host"", 10)
+    val metadataCache = EasyMock.createNiceMock(classOf[MetadataCache])
+    EasyMock.expect(metadataCache.getPartitionLeaderEndpoint(
+      EasyMock.anyString(),
+      EasyMock.anyInt(),
+      EasyMock.anyObject())
+    ).andReturn(Some(brokerNode)).anyTimes()
+    val networkClient = EasyMock.createNiceMock(classOf[NetworkClient])
+    txnMarkerChannelManager = new TransactionMarkerChannelManager(
+      KafkaConfig.fromProps(serverProps),
+      metadataCache,
+      networkClient,
+      txnStateManager,
+      txnMarkerPurgatory,
+      time) {
+        override def shutdown(): Unit = {
+          txnMarkerPurgatory.shutdown()
+        }
+    }
+
+    transactionCoordinator = new TransactionCoordinator(brokerId = 0,
+      txnConfig,
+      scheduler,
+      pidManager,
+      txnStateManager,
+      txnMarkerChannelManager,
+      time,
+      new LogContext)
+    EasyMock.replay(pidManager)
+    EasyMock.replay(metadataCache)
+    EasyMock.replay(networkClient)
+  }
+
+  @After
+  override def tearDown() {
+    try {
+      EasyMock.reset(zkClient, replicaManager)
+      transactionCoordinator.shutdown()
+    } finally {
+      super.tearDown()
+    }
+  }
+
+  @Test
+  def testConcurrentGoodPathSequence(): Unit = {
+    verifyConcurrentOperations(createTransactions, allOperations)
+  }
+
+  @Test
+  def testConcurrentRandomSequences(): Unit = {
+    verifyConcurrentRandomSequences(createTransactions, allOperations)
+  }
+
+  /**
+    * Concurrently load one set of transaction state topic partitions and unload another
+    * set of partitions. This tests partition leader changes of transaction state topic
+    * that are handled by different threads concurrently. Verifies that the metadata of
+    * unloaded partitions are removed from the transaction manager and that the transactions
+    * from the newly loaded partitions are loaded correctly.
+    */
+  @Test
+  def testConcurrentLoadUnloadPartitions(): Unit = {
+    val partitionsToLoad = (0 until numPartitions / 2).toSet
+    val partitionsToUnload = (numPartitions / 2 until numPartitions).toSet
+    verifyConcurrentActions(loadUnloadActions(partitionsToLoad, partitionsToUnload))
+  }
+
+  /**
+    * Concurrently load one set of transaction state topic partitions, unload a second set
+    * of partitions and expire transactions on a third set of partitions. This tests partition
+    * leader changes of transaction state topic that are handled by different threads concurrently
+    * while expiry is performed on another thread. Verifies the state of transactions on all the partitions.
+    */
+  @Test
+  def testConcurrentTransactionExpiration(): Unit = {
+    val partitionsToLoad = (0 until numPartitions / 3).toSet
+    val partitionsToUnload = (numPartitions / 3 until numPartitions * 2 / 3).toSet
+    val partitionsWithExpiringTxn = (numPartitions * 2 / 3 until numPartitions).toSet
+    val expiringTransactions = allTransactions.filter { txn =>
+      partitionsWithExpiringTxn.contains(txnStateManager.partitionFor(txn.transactionalId))
+    }.toSet
+    val expireAction = new ExpireTransactionsAction(expiringTransactions)
+    verifyConcurrentActions(loadUnloadActions(partitionsToLoad, partitionsToUnload) + expireAction)
+  }
+
+  override def enableCompletion(): Unit = {
+    super.enableCompletion()
+
+    def createResponse(request: WriteTxnMarkersRequest): WriteTxnMarkersResponse  = {
+      val pidErrorMap = request.markers.asScala.map { marker =>
+        (marker.producerId.asInstanceOf[java.lang.Long], marker.partitions.asScala.map { tp => (tp, Errors.NONE) }.toMap.asJava)
+      }.toMap.asJava
+      new WriteTxnMarkersResponse(pidErrorMap)
+    }
+    synchronized {
+      txnMarkerChannelManager.generateRequests().foreach { requestAndHandler =>
+        val request = requestAndHandler.request.asInstanceOf[WriteTxnMarkersRequest.Builder].build()
+        val response = createResponse(request)
+        requestAndHandler.handler.onComplete(new ClientResponse(new RequestHeader(ApiKeys.PRODUCE, 0, ""client"", 1),
+          null, null, 0, 0, false, null, response))
+      }
+    }
+  }
+
+  /**
+    * Concurrently load `partitionsToLoad` and unload `partitionsToUnload`. Before the concurrent operations
+    * are run `partitionsToLoad` must be unloaded first since all partitions were loaded during setUp.
+    */
+  private def loadUnloadActions(partitionsToLoad: Set[Int], partitionsToUnload: Set[Int]): Set[Action] = {
+    val transactions = (1 to 10).flatMap(i => createTransactions(s""testConcurrentLoadUnloadPartitions$i-"")).toSet
+    transactions.foreach(txn => prepareTransaction(txn))
+    val unload = partitionsToLoad.map(new UnloadTxnPartitionAction(_))
+    unload.foreach(_.run())
+    unload.foreach(_.await())
+    partitionsToLoad.map(new LoadTxnPartitionAction(_)) ++ partitionsToUnload.map(new UnloadTxnPartitionAction(_))
+  }
+
+  private def createTransactions(txnPrefix: String): Set[Transaction] = {
+    val transactions = (0 until nTransactions).map { i => new Transaction(s""$txnPrefix$i"", i, time) }
+    allTransactions ++= transactions
+    transactions.toSet
+  }
+
+  private def verifyTransaction(txn: Transaction, expectedState: TransactionState): Unit = {
+    val (metadata, success) = TestUtils.computeUntilTrue({
+      enableCompletion()
+      transactionMetadata(txn)
+    })(metadata => metadata.nonEmpty && metadata.forall(m => m.state == expectedState && m.pendingState.isEmpty))
+    assertTrue(s""Invalid metadata state $metadata"", success)
+  }
+
+  private def transactionMetadata(txn: Transaction): Option[TransactionMetadata] = {
+    txnStateManager.getTransactionState(txn.transactionalId) match {
+      case Left(error) =>
+        if (error == Errors.NOT_COORDINATOR)
+          None
+        else
+          throw new AssertionError(s""Unexpected transaction error $error for $txn"")
+      case Right(Some(metadata)) =>
+        Some(metadata.transactionMetadata)
+      case Right(None) =>
+        None
+    }
+  }
+
+  private def prepareTransaction(txn: Transaction): Unit = {
+    val partitionId = txnStateManager.partitionFor(txn.transactionalId)
+    val txnRecords = txnRecordsByPartition(partitionId)
+    val initPidOp = new InitProducerIdOperation()
+    val addPartitionsOp = new AddPartitionsToTxnOperation(Set(new TopicPartition(""topic"", 0)))
+      initPidOp.run(txn)
+      initPidOp.awaitAndVerify(txn)
+      addPartitionsOp.run(txn)
+      addPartitionsOp.awaitAndVerify(txn)
+
+      val txnMetadata = transactionMetadata(txn).getOrElse(throw new IllegalStateException(s""Transaction not found $txn""))
+      txnRecords += new SimpleRecord(txn.txnMessageKeyBytes, TransactionLog.valueToBytes(txnMetadata.prepareNoTransit()))
+
+      txnMetadata.state = PrepareCommit
+      txnRecords += new SimpleRecord(txn.txnMessageKeyBytes, TransactionLog.valueToBytes(txnMetadata.prepareNoTransit()))
+
+      prepareTxnLog(partitionId)
+  }
+
+  private def prepareTxnLog(partitionId: Int): Unit = {
+
+    val logMock =  EasyMock.mock(classOf[Log])
+    val fileRecordsMock = EasyMock.mock(classOf[FileRecords])
+
+    val topicPartition = new TopicPartition(TRANSACTION_STATE_TOPIC_NAME, partitionId)
+    val startOffset = replicaManager.getLogEndOffset(topicPartition).getOrElse(20L)
+    val records = MemoryRecords.withRecords(startOffset, CompressionType.NONE, txnRecordsByPartition(partitionId): _*)
+    val endOffset = startOffset + records.records.asScala.size
+
+    EasyMock.expect(logMock.logStartOffset).andStubReturn(startOffset)
+    EasyMock.expect(logMock.read(EasyMock.eq(startOffset), EasyMock.anyInt(), EasyMock.eq(None),
+      EasyMock.eq(true), EasyMock.eq(IsolationLevel.READ_UNCOMMITTED)))
+      .andReturn(FetchDataInfo(LogOffsetMetadata(startOffset), fileRecordsMock))
+    EasyMock.expect(fileRecordsMock.readInto(EasyMock.anyObject(classOf[ByteBuffer]), EasyMock.anyInt()))
+      .andReturn(records.buffer)
+
+    EasyMock.replay(logMock, fileRecordsMock)
+    synchronized {
+      replicaManager.updateLog(topicPartition, logMock, endOffset)
+    }
+  }
+
+  abstract class TxnOperation[R] extends Operation {
+    @volatile var result: Option[R] = None
+    def resultCallback(r: R): Unit = this.result = Some(r)
+  }
+
+  class InitProducerIdOperation extends TxnOperation[InitProducerIdResult] {
+    override def run(txn: Transaction): Unit = {
+      transactionCoordinator.handleInitProducerId(txn.transactionalId, 60000, resultCallback)
+    }
+    override def awaitAndVerify(txn: Transaction): Unit = {
+      val initPidResult = result.getOrElse(throw new IllegalStateException(""InitProducerId has not completed""))
+      assertEquals(Errors.NONE, initPidResult.error)
+      verifyTransaction(txn, Empty)
+    }
+  }
+
+  class AddPartitionsToTxnOperation(partitions: Set[TopicPartition]) extends TxnOperation[Errors] {
+    override def run(txn: Transaction): Unit = {
+      transactionMetadata(txn).foreach { txnMetadata =>
+        transactionCoordinator.handleAddPartitionsToTransaction(txn.transactionalId,
+            txnMetadata.producerId,
+            txnMetadata.producerEpoch,
+            partitions,
+            resultCallback)
+      }
+    }
+    override def awaitAndVerify(txn: Transaction): Unit = {
+      val error = result.getOrElse(throw new IllegalStateException(""AddPartitionsToTransaction has not completed""))
+      assertEquals(Errors.NONE, error)
+      verifyTransaction(txn, Ongoing)
+    }
+  }
+
+  class EndTxnOperation extends TxnOperation[Errors] {
+    override def run(txn: Transaction): Unit = {
+      transactionMetadata(txn).foreach { txnMetadata =>
+        transactionCoordinator.handleEndTransaction(txn.transactionalId,
+          txnMetadata.producerId,
+          txnMetadata.producerEpoch,
+          transactionResult(txn),
+          resultCallback)
+      }
+    }
+    override def awaitAndVerify(txn: Transaction): Unit = {
+      val error = result.getOrElse(throw new IllegalStateException(""EndTransaction has not completed""))
+      if (!txn.ended) {
+        txn.ended = true
+        assertEquals(Errors.NONE, error)
+        val expectedState = if (transactionResult(txn) == TransactionResult.COMMIT) CompleteCommit else CompleteAbort
+        verifyTransaction(txn, expectedState)
+      } else
+        assertEquals(Errors.INVALID_TXN_STATE, error)
+    }
+    // Test both commit and abort. Transactional ids used in the test have the format <prefix><index>
+    // Use the last digit of the index to decide between commit and abort.
+    private def transactionResult(txn: Transaction): TransactionResult = {
+      val txnId = txn.transactionalId
+      val lastDigit = txnId(txnId.length - 1).toInt
+      if (lastDigit % 2 == 0) TransactionResult.COMMIT else TransactionResult.ABORT
+    }
+  }
+
+  class LoadTxnPartitionAction(txnTopicPartitionId: Int) extends Action {
+    override def run(): Unit = {
+      transactionCoordinator.handleTxnImmigration(txnTopicPartitionId, coordinatorEpoch)
+    }
+    override def await(): Unit = {
+      allTransactions.foreach { txn =>
+        if (txnStateManager.partitionFor(txn.transactionalId) == txnTopicPartitionId) {
+          verifyTransaction(txn, CompleteCommit)
+        }
+      }
+    }
+  }
+
+  class UnloadTxnPartitionAction(txnTopicPartitionId: Int) extends Action {
+    val txnRecords: mutable.ArrayBuffer[SimpleRecord] = mutable.ArrayBuffer[SimpleRecord]()
+    override def run(): Unit = {
+      transactionCoordinator.handleTxnEmigration(txnTopicPartitionId, coordinatorEpoch)
+    }
+    override def await(): Unit = {
+      allTransactions.foreach { txn =>
+        if (txnStateManager.partitionFor(txn.transactionalId) == txnTopicPartitionId)
+          assertTrue(""Transaction metadata not removed"", transactionMetadata(txn).isEmpty)
+      }
+    }
+  }
+
+  class ExpireTransactionsAction(transactions: Set[Transaction]) extends Action {
+    override def run(): Unit = {
+      transactions.foreach { txn =>
+        transactionMetadata(txn).foreach { txnMetadata =>
+          txnMetadata.txnLastUpdateTimestamp = time.milliseconds() - txnConfig.transactionalIdExpirationMs
+        }
+      }
+      txnStateManager.enableTransactionalIdExpiration()
+      time.sleep(txnConfig.removeExpiredTransactionalIdsIntervalMs + 1)
+    }
+
+    override def await(): Unit = {
+      val (_, success) = TestUtils.computeUntilTrue({
+        replicaManager.tryCompleteDelayedRequests()
+        transactions.forall(txn => transactionMetadata(txn).isEmpty)
+      })(identity)
+      assertTrue(""Transaction not expired"", success)
+    }
+  }
+}
+
+object TransactionCoordinatorConcurrencyTest {
+
+  class Transaction(val transactionalId: String, producerId: Long, time: MockTime) extends CoordinatorMember {
+    val txnMessageKeyBytes: Array[Byte] = TransactionLog.keyToBytes(transactionalId)
+    @volatile var ended = false
+    override def toString: String = transactionalId
+  }
+}
diff --git a/core/src/test/scala/unit/kafka/utils/timer/MockTimer.scala b/core/src/test/scala/unit/kafka/utils/timer/MockTimer.scala
index e4ac4fa1ec7..17ee578f6a5 100644
--- a/core/src/test/scala/unit/kafka/utils/timer/MockTimer.scala
+++ b/core/src/test/scala/unit/kafka/utils/timer/MockTimer.scala
@@ -28,8 +28,11 @@ class MockTimer extends Timer {
   def add(timerTask: TimerTask) {
     if (timerTask.delayMs <= 0)
       timerTask.run()
-    else
-      taskQueue.enqueue(new TimerTaskEntry(timerTask, timerTask.delayMs + time.milliseconds))
+    else {
+      taskQueue synchronized {
+        taskQueue.enqueue(new TimerTaskEntry(timerTask, timerTask.delayMs + time.milliseconds))
+      }
+    }
   }
 
   def advanceClock(timeoutMs: Long): Boolean = {
@@ -38,15 +41,25 @@ class MockTimer extends Timer {
     var executed = false
     val now = time.milliseconds
 
-    while (taskQueue.nonEmpty && now > taskQueue.head.expirationMs) {
-      val taskEntry = taskQueue.dequeue()
-      if (!taskEntry.cancelled) {
-        val task = taskEntry.timerTask
-        task.run()
-        executed = true
+    var hasMore = true
+    while (hasMore) {
+      hasMore = false
+      val head = taskQueue synchronized {
+        if (taskQueue.nonEmpty && now > taskQueue.head.expirationMs) {
+          val entry = Some(taskQueue.dequeue())
+          hasMore = taskQueue.nonEmpty
+          entry
+        } else
+          None
+      }
+      head.foreach { taskEntry =>
+        if (!taskEntry.cancelled) {
+          val task = taskEntry.timerTask
+          task.run()
+          executed = true
+        }
       }
     }
-
     executed
   }
 


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,
Update system test(s) to use multiple listeners for the same security protocol,KAFKA-4637,13035245,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,ijuma,ijuma,16/Jan/17 15:10,07/Dec/17 09:14,12/Jan/21 11:54,,,,,,,,,,,,,,,0,newbie,,,"Even though this is tested via the JUnit tests introduced by KAFKA-4565, it would be good to have at least one system test exercising this functionality.",,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-01-16 15:10:47.0,,,,,,,"0|i38rmn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Only delete reassign_partitions znode after reassignment is complete,KAFKA-6193,13117314,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ijuma,yuzhihong@gmail.com,yuzhihong@gmail.com,09/Nov/17 15:56,06/Dec/17 20:27,12/Jan/21 11:54,06/Dec/17 20:26,,,,,1.1.0,,,,,,,,,0,,,,"From https://builds.apache.org/job/kafka-trunk-jdk8/2198/testReport/junit/kafka.admin/ReassignPartitionsClusterTest/shouldPerformMultipleReassignmentOperationsOverVariousTopics/ :
{code}
java.lang.AssertionError: expected:<List(3, 1, 2)> but was:<Vector(0, 1, 2)>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at kafka.admin.ReassignPartitionsClusterTest.shouldPerformMultipleReassignmentOperationsOverVariousTopics(ReassignPartitionsClusterTest.scala:524)
{code}",,githubbot,ijuma,rsivaram,vahid,wicknicks,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/17 15:57;yuzhihong@gmail.com;6193.out;https://issues.apache.org/jira/secure/attachment/12896889/6193.out",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2017-11-09 15:57:51.894,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 06 20:27:59 UTC 2017,,,,,,,"0|i3mlmv:",9223372036854775807,,,,,,,,,,,,,,,,"09/Nov/17 15:57;ijuma;PR for this https://github.com/apache/kafka/pull/4187","09/Nov/17 19:08;rsivaram;In some failing cases, there were exceptions while registering error metrics. But having looked into it, those exceptions were generated during shutdown initiated in tearDown and hence is not the cause of the failures. Will submit separate PR to fix that.","16/Nov/17 17:52;wicknicks;This test is failing for a PR I created. The Jenkins build is [here|https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/9469/testReport/kafka.admin/ReassignPartitionsClusterTest/shouldPerformMultipleReassignmentOperationsOverVariousTopics/]. ","21/Nov/17 18:31;vahid;Another instance of this failure that happened yesterday is [here|https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/9556/testReport/kafka.admin/ReassignPartitionsClusterTest/shouldPerformMultipleReassignmentOperationsOverVariousTopics/].","30/Nov/17 10:41;ijuma;Some of the log errors seen when the test fails have been fixed by https://github.com/apache/kafka/pull/4219/. If anyone sees this failure in a branch that includes PR #4219, please post the link to the failure to this JIRA.","30/Nov/17 14:22;ijuma;Another failure:

https://builds.apache.org/job/kafka-trunk-jdk9/lastCompletedBuild/testReport/kafka.admin/ReassignPartitionsClusterTest/shouldPerformMultipleReassignmentOperationsOverVariousTopics/

Looking into it.","01/Dec/17 15:26;githubbot;GitHub user ijuma opened a pull request:

    https://github.com/apache/kafka/pull/4283

    (WIP) KAFKA-6193: Flaky shouldPerformMultipleReassignmentOperationsOverVariousTopics

    I have been unable to reproduce it locally, so enabled additional
    logging while running it in Jenkins.
    
    ### Committer Checklist (excluded from commit message)
    - [ ] Verify design and implementation 
    - [ ] Verify test coverage and CI build status
    - [ ] Verify documentation (including upgrade notes)


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ijuma/kafka kafka-6193-flaky-shouldPerformMultipleReassignmentOperationsOverVariousTopics

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/4283.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #4283
    
----
commit 3bbbae786432275ed690733f69289fff76f78a9e
Author: Ismael Juma <ismael@juma.me.uk>
Date:   2017-12-01T15:21:56Z

    KAFKA-6193: Flaky shouldPerformMultipleReassignmentOperationsOverVariousTopics

----
","03/Dec/17 11:28;ijuma;Found the reason for the failures and updated the PR to fix them.","03/Dec/17 16:04;yuzhihong@gmail.com;Updated JIRA title according to Ismael's discovery.","06/Dec/17 20:27;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/4283
",,,,,,,,,,,,
"""Server not found in kerberos database"" issue while starting a Kafka server in a secured mode",KAFKA-4097,13000934,Test,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,syaamb4u,syaamb4u,29/Aug/16 11:34,10/Nov/17 08:31,12/Jan/21 11:54,30/Aug/16 06:38,0.10.0.1,,,,,,,,,,,,,0,,,,,,Ronald van de Kuil,syaamb4u,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-11-10 08:31:11.89,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 10 08:31:11 UTC 2017,,,,,,,"0|i32xxz:",9223372036854775807,,,,,,,,,,,,,,,,"29/Aug/16 11:45;syaamb4u;Hi,
 
 Zookeeper was started well in a secured mode (as I can see TGT starting time and expiry time) with the following properties:
 
 zookeeper properties:
 =====================
 dataDir=/tmp/zookeeper2
# the port at which the clients will connect
clientPort=2182
# disable the per-ip limit on the number of connections since this is a non-production config
maxClientCnxns=0
authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider
requireClientAuthScheme=sasl

jaasLoginRenew=3600000

 zookeeper_jas.conf:
 ==================
 Server {
    com.sun.security.auth.module.Krb5LoginModule required
    useKeyTab=true
    keyTab=""/home/dsadm/syam/zookeeper-service.keytab""
    storeKey=true
    serviceName=""zookeeper""
    debug=true
    useTicketCache=false
    principal=""zookeeper/archimedes.in.ibm.com@HADOOPBI.COM"";
};

When I started the kafka server, with the following properties:

server.properties:
==================
listeners=SASL_PLAINTEXT://archimedes.in.ibm.com:9093
security.inter.broker.protocol=SASL_PLAINTEXT
sasl.enabled.mechanisms=GSSAPI
sasl.mechanism.inter.broker.protocol=GSSAPI
sasl.kerberos.service.name=kafka
zookeeper.set.acl=true
authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer

kafka_broker_jass.conf:
======================
    KafkaServer {
        com.sun.security.auth.module.Krb5LoginModule required
        useKeyTab=true
        storeKey=true
        serviceName=""kafka""
        keyTab=""/home/dsadm/syam/kafka_service.keytab""
        principal=""kafka/archimedes.in.ibm.com@HADOOPBI.COM"";
    };

    // Zookeeper client authentication
    Client {
       com.sun.security.auth.module.Krb5LoginModule required
       useKeyTab=true
       storeKey=true
       debug=true
       serviceName=""zookeeper""
       keyTab=""/home/dsadm/syam/kafka_service.keytab""
       principal=""kafka/archimedes.in.ibm.com@HADOOPBI.COM"";
    };

	krb5 and jaas files are specified via exporting KAFKA_OPTS:
	=================================================
	
export KAFKA_OPTS=""-Djava.security.krb5.conf=/home/dsadm/syam/krb5.conf -Djava.security.auth.login.config=/home/dsadm/syam/kafka_broker_jaas.conf""

export KAFKA_OPTS=""-Djava.security.krb5.conf=/home/dsadm/syam/krb5.conf -Djava.security.auth.login.config=/home/dsadm/syam/zookeeper_jaas.conf""
	

I was seeing the following issue,while starting a kafka server (./bin/kafka_server_start.sh config/server.properties):
	
[2016-08-29 16:51:27,375] INFO Socket connection established to archimedes/9.124.101.5:2182, initiating session (org.apache.zookeeper.ClientCnxn)
[2016-08-29 16:51:27,467] INFO Session establishment complete on server archimedes/9.124.101.5:2182, sessionid = 0x156d5ffea8a0001, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2016-08-29 16:51:27,492] INFO zookeeper state changed (SyncConnected) (org.I0Itec.zkclient.ZkClient)
[2016-08-29 16:51:27,614] ERROR An error: (java.security.PrivilegedActionException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - UNKNOWN_SERVER)]) occurred when evaluating Zookeeper Quorum Member's  received SASL token. This may be caused by Java's being unable to resolve the Zookeeper Quorum Member's hostname correctly. You may want to try to adding '-Dsun.net.spi.nameservice.provider.1=dns,sun' to your client's JVMFLAGS environment. Zookeeper Client will go to AUTH_FAILED state. (org.apache.zookeeper.client.ZooKeeperSaslClient)
[2016-08-29 16:51:27,615] ERROR SASL authentication with Zookeeper Quorum member failed: javax.security.sasl.SaslException: An error: (java.security.PrivilegedActionException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - UNKNOWN_SERVER)]) occurred when evaluating Zookeeper Quorum Member's  received SASL token. This may be caused by Java's being unable to resolve the Zookeeper Quorum Member's hostname correctly. You may want to try to adding '-Dsun.net.spi.nameservice.provider.1=dns,sun' to your client's JVMFLAGS environment. Zookeeper Client will go to AUTH_FAILED state. (org.apache.zookeeper.ClientCnxn)
[2016-08-29 16:51:27,617] INFO zookeeper state changed (AuthFailed) (org.I0Itec.zkclient.ZkClient)
[2016-08-29 16:51:27,621] INFO Terminate ZkClient event thread. (org.I0Itec.zkclient.ZkEventThread)
[2016-08-29 16:51:27,646] FATAL Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)
org.I0Itec.zkclient.exception.ZkAuthFailedException: Authentication failure


","29/Aug/16 11:49;syaamb4u;Sorry. I have missed adding this property in my last update under server.properties:

""zookeeper.connect=archimedes.in.ibm.com:2182""
","29/Aug/16 11:52;syaamb4u;By list_principals command in kadmin window, I was able to see zookeeper and kafka SPNs under the mentioned KDC.","30/Aug/16 06:38;syaamb4u;I was able to identify the root cause for this issue. the SPN for zookeeper was not constructed in a correct way. This can be seen by enabling kerberos logs (-Dsun.security.krb5.debug=true). As NSLOOKUP command returns correct hostname in FQDN format, I have never suspected /etc/hosts file. But there is some problem with /etc/hosts file.

The conventional format of hosts file should be some thing like below:
< IP >	<FQDN>	<ShortName>

Example:
9.124.101.5	archimedes.in.ibm.com	archimedes

But in my hosts file, shortname precedes FQDN. Hence the issue.

Thanks for your time.
","10/Nov/17 08:31;Ronald van de Kuil;Syam,

Thank you for your comment. That helped me out!",,,,,,,,,,,,,,,,,
Transient failure in ConnectDistributedTest.test_pause_and_resume_sink in consuming messages after resuming sink connector,KAFKA-4575,13031130,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,kkonstantine,shikhar,shikhar,29/Dec/16 18:10,13/Oct/17 15:04,12/Jan/21 11:54,13/Oct/17 15:04,,,,,0.10.2.0,,,,,KafkaConnect,system tests,,,1,,,,"http://confluent-kafka-system-test-results.s3-us-west-2.amazonaws.com/2016-12-29--001.1483003056--apache--trunk--dc55025/report.html

{noformat}
[INFO  - 2016-12-29 08:56:23,050 - runner_client - log - lineno:221]: RunnerClient: kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_pause_and_resume_sink: Summary: Failed to consume messages after resuming source connector
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.6.0-py2.7.egg/ducktape/tests/runner_client.py"", line 123, in run
    data = self.run_test()
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.6.0-py2.7.egg/ducktape/tests/runner_client.py"", line 176, in run_test
    return self.test_context.function(self.test)
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 267, in test_pause_and_resume_sink
    err_msg=""Failed to consume messages after resuming source connector"")
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.6.0-py2.7.egg/ducktape/utils/util.py"", line 36, in wait_until
    raise TimeoutError(err_msg)
TimeoutError: Failed to consume messages after resuming source connector
{noformat}

We recently fixed KAFKA-4527 and this is a new kind of failure in the same test.",,ewencp,githubbot,rhauch,sergioaparicio,shikhar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-05 00:28:49.856,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 13 15:04:23 UTC 2017,,,,,,,"0|i3841j:",9223372036854775807,,,,,,,,,,,,,,,,"05/Jan/17 00:21;shikhar;By the way, the error message is misleading, it should read 'after resuming _sink_ connector'.","05/Jan/17 00:28;githubbot;GitHub user shikhar opened a pull request:

    https://github.com/apache/kafka/pull/2313

    KAFKA-4575: ensure topic created before starting sink for ConnectDistributedTest.test_pause_resume_sink

    Otherwise in this test the sink task goes through the pause/resume cycle with 0 assigned partitions, since the default metadata refresh interval is quite long

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/shikhar/kafka kafka-4575

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2313.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2313
    
----
commit fdc2bb353de995ba09398d0851b934d3aee4570c
Author: Shikhar Bhushan <shikhar@confluent.io>
Date:   2017-01-05T00:28:14Z

    KAFKA-4575: ensure topic created before starting sink for ConnectDistributedTest.test_pause_resume_sink
    
    Otherwise in this test the sink task goes through the pause/resume cycle with 0 assigned partitions, since the default metadata refresh interval is quite long

----
","05/Jan/17 23:23;ewencp;Issue resolved by pull request 2313
[https://github.com/apache/kafka/pull/2313]","05/Jan/17 23:23;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2313
","17/Apr/17 19:57;ewencp;The test failed again.
{code}
Report : http://confluent-kafka-system-test-results.s3-us-west-2.amazonaws.com/2017-04-14--001.1492171087--apache--trunk--1e93c3b/report.txt
test_id:    kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_pause_and_resume_sink
status:     FAIL
run time:   1 minute 13.778 seconds


    Failed to consume messages after resuming sink connector
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.6.0-py2.7.egg/ducktape/tests/runner_client.py"", line 123, in run
    data = self.run_test()
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.6.0-py2.7.egg/ducktape/tests/runner_client.py"", line 176, in run_test
    return self.test_context.function(self.test)
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/tests/kafkatest/tests/connect/connect_distributed_test.py"", line 274, in test_pause_and_resume_sink
    err_msg=""Failed to consume messages after resuming sink connector"")
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.6.0-py2.7.egg/ducktape/utils/util.py"", line 36, in wait_until
    raise TimeoutError(err_msg)
TimeoutError: Failed to consume messages after resuming sink connector
{code}","13/Oct/17 15:04;rhauch;This appears to no longer be an issue, so I'm closing this as fixed.",,,,,,,,,,,,,,,,
"Transient failure in ZooKeeperSecurityUpgradeTest.test_zk_security_upgrade with security_protocol = SASL_PLAINTEXT, SSL",KAFKA-4574,13031129,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,apurva,shikhar,shikhar,29/Dec/16 18:05,09/Oct/17 21:03,12/Jan/21 11:54,07/Apr/17 00:09,,,,,0.11.0.0,,,,,system tests,,,,0,kip-101,,,"http://confluent-kafka-system-test-results.s3-us-west-2.amazonaws.com/2016-12-29--001.1483003056--apache--trunk--dc55025/report.html

{{ZooKeeperSecurityUpgradeTest.test_zk_security_upgrade}} failed with these {{security_protocol}} parameters 

{noformat}
====================================================================================================
test_id:    kafkatest.tests.core.zookeeper_security_upgrade_test.ZooKeeperSecurityUpgradeTest.test_zk_security_upgrade.security_protocol=SASL_PLAINTEXT
status:     FAIL
run time:   3 minutes 44.094 seconds


    1 acked message did not make it to the Consumer. They are: [5076]. We validated that the first 1 of these missing messages correctly made it into Kafka's data files. This suggests they were lost on their way to the consumer.
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.6.0-py2.7.egg/ducktape/tests/runner_client.py"", line 123, in run
    data = self.run_test()
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.6.0-py2.7.egg/ducktape/tests/runner_client.py"", line 176, in run_test
    return self.test_context.function(self.test)
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.6.0-py2.7.egg/ducktape/mark/_mark.py"", line 321, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/tests/kafkatest/tests/core/zookeeper_security_upgrade_test.py"", line 117, in test_zk_security_upgrade
    self.run_produce_consume_validate(self.run_zk_migration)
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/tests/kafkatest/tests/produce_consume_validate.py"", line 101, in run_produce_consume_validate
    self.validate()
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/tests/kafkatest/tests/produce_consume_validate.py"", line 163, in validate
    assert success, msg
AssertionError: 1 acked message did not make it to the Consumer. They are: [5076]. We validated that the first 1 of these missing messages correctly made it into Kafka's data files. This suggests they were lost on their way to the consumer.
{noformat}

{noformat}
====================================================================================================
test_id:    kafkatest.tests.core.zookeeper_security_upgrade_test.ZooKeeperSecurityUpgradeTest.test_zk_security_upgrade.security_protocol=SSL
status:     FAIL
run time:   3 minutes 50.578 seconds


    1 acked message did not make it to the Consumer. They are: [3559]. We validated that the first 1 of these missing messages correctly made it into Kafka's data files. This suggests they were lost on their way to the consumer.
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.6.0-py2.7.egg/ducktape/tests/runner_client.py"", line 123, in run
    data = self.run_test()
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.6.0-py2.7.egg/ducktape/tests/runner_client.py"", line 176, in run_test
    return self.test_context.function(self.test)
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.6.0-py2.7.egg/ducktape/mark/_mark.py"", line 321, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/tests/kafkatest/tests/core/zookeeper_security_upgrade_test.py"", line 117, in test_zk_security_upgrade
    self.run_produce_consume_validate(self.run_zk_migration)
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/tests/kafkatest/tests/produce_consume_validate.py"", line 101, in run_produce_consume_validate
    self.validate()
  File ""/var/lib/jenkins/workspace/system-test-kafka/kafka/tests/kafkatest/tests/produce_consume_validate.py"", line 163, in validate
    assert success, msg
AssertionError: 1 acked message did not make it to the Consumer. They are: [3559]. We validated that the first 1 of these missing messages correctly made it into Kafka's data files. This suggests they were lost on their way to the consumer.
{noformat}

Previously: KAFKA-3985",,apurva,ewencp,githubbot,ijuma,shikhar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-04 03:34:47.806,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 09 21:03:10 UTC 2017,,,,,,,"0|i3841b:",9223372036854775807,,,,,,,,,,,,,,,,"04/Jan/17 03:34;ewencp;This is very likely the same as https://issues.apache.org/jira/browse/KAFKA-4558 since ProduceConsumeValidate tests all currently are at risk of hitting this issue where a consumer doesn't actually start consuming before we start the producer.","10/Mar/17 01:14;apurva;Another occurrence: http://confluent-kafka-system-test-results.s3-us-west-2.amazonaws.com/2017-03-09--001.1489057062--apache--trunk--537f98a/ZooKeeperSecurityUpgradeTest/test_zk_security_upgrade/security_protocol=SASL_SSL/50.tgz

{noformat}
AssertionError: 1 acked message did not make it to the Consumer. They are: [5614]. We validated that the first 1 of these missing messages correctly made it into Kafka's data files. This suggests they were lost on their way to the consumer.
{noformat}","10/Mar/17 01:33;apurva;I don't think this is due to https://issues.apache.org/jira/browse/KAFKA-4558, since the missing message is somewhere in the middle of the stream. If it were an instance of that ticket, then we should see missing messages at the beginning of the range. Also, the only tests which would suffer any problems due to KAFKA-4558 were those where the topic being verified already had data, which is not the case here.

Whatever the issue is, it is something else. I am looking at it.","10/Mar/17 02:19;apurva;So, some interesting findings. 

I dumped the log segments of that test, and found that the record with value 5614 was in partition 2 at offset 1496. In the `console_consumer.log`, I see this.

{noformat}
[2017-03-09 05:21:02,510] TRACE Adding fetched record for partition test_topic-2 with offset 1496 to buffered record list (org.apache.kafka.clients.consumer.internals.Fetcher)
[2017-03-09 05:21:02,510] TRACE Received 1 records in fetch response for partition test_topic-2 with offset 1496 (org.apache.kafka.clients.consumer.internals.Fetcher)
[2017-03-09 05:21:02,510] TRACE Returning fetched records at offset 1496 for assigned partition test_topic-2 and update position to 1497 (org.apache.kafka.clients.consumer.internals.Fetcher)
[2017-03-09 05:21:02,510] DEBUG Ignoring fetched records for test_topic-2 at offset 1496 since the current position is 1497 (org.apache.kafka.clients.consumer.internals.Fetcher)
[2017-03-09 05:21:02,510] TRACE Added fetch request for partition test_topic-2 at offset 1497 to node worker2:9095 (id: 2 rack: null) (org.apache.kafka.clients.consumer.internals.Fetcher)
[2017-03-09 05:21:02,510] DEBUG Sending fetch for partitions [test_topic-2] to broker worker2:9095 (id: 2 rack: null) (org.apache.kafka.clients.consumer.internals.Fetcher)
[2017-03-09 05:21:02,510] TRACE Skipping fetch for partition test_topic-2 because there is an in-flight request to worker2:9095 (id: 2 rack: null) (org.apache.kafka.clients.consumer.internals.Fetcher)
{noformat}

This suggests that the consumer actually received the record, but somehow it didn't make it to the python consumer. Will continue to dig. ","10/Mar/17 22:11;apurva;It appears than different messages wound up at the same offset: 

{noformat}
amehta-macbook-pro:50 apurva$ grep 4738 VerifiableProducer-0-140193439058000/worker5/verifiable_producer.stdout | grep producer_send_success
{""topic"":""test_topic"",""partition"":2,""name"":""producer_send_success"",""value"":""4738"",""time_ms"":1489036862544,""offset"":1496,""key"":null}
amehta-macbook-pro:50 apurva$ grep 5614 VerifiableProducer-0-140193439058000/worker5/verifiable_producer.stdout | grep producer_send_success
{""topic"":""test_topic"",""partition"":2,""name"":""producer_send_success"",""value"":""5614"",""time_ms"":1489036872096,""offset"":1496,""key"":null}
{noformat}

Note that two unique messages were acked at offset 1496 in partition 2. In the log, only the second message (by timestamp) persists. So the first message was lost. What happened during the test is that the consumer read offset 1496 which had the first value at that time, and then moved on. So the second message was never read, and was reported missing. 

This may be a scenario where KIP-101 would solve the problem? The brokers are being continually bounced in the test, so it is possible that once in a while the wrong truncation happens and messages are lost. 

Another interesting finding: the producer received acks for 6425 messages. But there are 6427 unique messages in the log, and the consumer also read 6427 unique messages. So some unacked messages seem to have made it into the log.

There were three messages which were not acked, but are in the log, and all of them have this error: 

{noformat}
amehta-macbook-pro:50 apurva$ grep 4728 VerifiableProducer-0-140193439058000/worker5/verifiable_producer.stdout | grep producer_send_error
{""topic"":""test_topic"",""message"":""This server is not the leader for that topic-partition."",""exception"":""class org.apache.kafka.common.errors.NotLeaderForPartitionException"",""name"":""producer_send_error"",""value"":""4728"",""time_ms"":1489036862629,""key"":null}
amehta-macbook-pro:50 apurva$ grep 2844 VerifiableProducer-0-140193439058000/worker5/verifiable_producer.stdout | grep producer_send_error
{""topic"":""test_topic"",""message"":""Messages are written to the log, but to fewer in-sync replicas than required."",""exception"":""class org.apache.kafka.common.errors.NotEnoughReplicasAfterAppendException"",""name"":""producer_send_error"",""value"":""2844"",""time_ms"":1489036843590,""key"":null}
amehta-macbook-pro:50 apurva$ grep 5588 VerifiableProducer-0-140193439058000/worker5/verifiable_producer.stdout | grep producer_send_error
{""topic"":""test_topic"",""message"":""Messages are written to the log, but to fewer in-sync replicas than required."",""exception"":""class org.apache.kafka.common.errors.NotEnoughReplicasAfterAppendException"",""name"":""producer_send_error"",""value"":""5588"",""time_ms"":1489036871054,""key"":null}
{noformat}

This also seems problematic, since the producer config shows acks=-1, and retries=0. So the producer considered this a failure, but the messages wound up in the log. This also seems like a problem, but it is not the cause of this test failure.","10/Mar/17 23:01;apurva;From the server logs, we see that the logs were truncated to offset 1497 on two separate brokers within 10ms of each other. Would this affect offset 1496?

{noformat}
amehta-macbook-pro:KafkaService-0-140193561885648 apurva$ for i in `find . -name ""server.log""`; do grep -Hni ""Truncating log"" $i | grep test_topic-2; done | grep ""/info/""
./worker2/info/server.log:649:[2017-03-09 05:20:43,539] INFO Truncating log test_topic-2 to offset 946. (kafka.log.Log)
./worker2/info/server.log:968:[2017-03-09 05:20:51,132] INFO Truncating log test_topic-2 to offset 946. (kafka.log.Log)
./worker2/info/server.log:1121:[2017-03-09 05:20:51,639] INFO Truncating log test_topic-2 to offset 946. (kafka.log.Log)
./worker2/info/server.log:1488:[2017-03-09 05:21:11,054] INFO Truncating log test_topic-2 to offset 1497. (kafka.log.Log)
./worker2/info/server.log:1807:[2017-03-09 05:21:18,212] INFO Truncating log test_topic-2 to offset 1497. (kafka.log.Log)
./worker6/info/server.log:192:[2017-03-09 05:20:07,587] INFO Truncating log test_topic-2 to offset 0. (kafka.log.Log)
./worker6/info/server.log:1201:[2017-03-09 05:20:59,970] INFO Truncating log test_topic-2 to offset 1165. (kafka.log.Log)
./worker6/info/server.log:1473:[2017-03-09 05:21:11,066] INFO Truncating log test_topic-2 to offset 1497. (kafka.log.Log)
./worker6/info/server.log:1990:[2017-03-09 05:21:25,433] INFO Truncating log test_topic-2 to offset 1722. (kafka.log.Log)
./worker6/info/server.log:2049:[2017-03-09 05:21:25,684] INFO Truncating log test_topic-2 to offset 1722. (kafka.log.Log)
./worker8/info/server.log:196:[2017-03-09 05:20:07,581] INFO Truncating log test_topic-2 to offset 0. (kafka.log.Log)
./worker8/info/server.log:873:[2017-03-09 05:20:43,768] INFO Truncating log test_topic-2 to offset 669. (kafka.log.Log)
./worker8/info/server.log:1007:[2017-03-09 05:20:44,397] INFO Truncating log test_topic-2 to offset 669. (kafka.log.Log)
./worker8/info/server.log:1435:[2017-03-09 05:20:59,003] INFO Truncating log test_topic-2 to offset 1379. (kafka.log.Log)
./worker8/info/server.log:1900:[2017-03-09 05:21:11,134] INFO Truncating log test_topic-2 to offset 1496. (kafka.log.Log)
./worker8/info/server.log:2441:[2017-03-09 05:21:25,751] INFO Truncating log test_topic-2 to offset 1948. (kafka.log.Log)
{noformat}","11/Mar/17 00:41;apurva;Here are all the leader changes for this partition across all the controller logs: 

{noformat}
amehta-macbook-pro:KafkaService-0-140193561885648 apurva$ for i in `find . -name controller.log`; do echo $i;  perl -lne  'if ($_ =~ /^\[(.*)\] DEBUG.*After leader election.*(\[test_topic,2\] -> \(Leader:\d+,ISR:\d+,LeaderEpoch:\d+,ControllerEpoch:\d+\))/) { print ""$1 -> $2""; }' $i; done
./worker2/debug/controller.log
2017-03-09 05:20:43,538 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:2,ControllerEpoch:2)
2017-03-09 05:20:43,544 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:2,ControllerEpoch:2)
2017-03-09 05:20:43,573 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:2,ControllerEpoch:2)
2017-03-09 05:20:43,579 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:2,ControllerEpoch:2)
2017-03-09 05:20:43,593 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:2,ControllerEpoch:2)
2017-03-09 05:20:43,608 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:2,ControllerEpoch:2)
2017-03-09 05:20:43,621 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:2,ControllerEpoch:2)
2017-03-09 05:20:43,629 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:2,ControllerEpoch:2)
2017-03-09 05:20:43,635 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:2,ControllerEpoch:2)
2017-03-09 05:20:43,641 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:2,ControllerEpoch:2)
2017-03-09 05:20:43,660 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:2,ControllerEpoch:2)
./worker2/info/controller.log
./worker6/debug/controller.log
2017-03-09 05:20:51,053 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:3,ControllerEpoch:3)
2017-03-09 05:20:51,059 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:3,ControllerEpoch:3)
2017-03-09 05:20:51,065 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:3,ControllerEpoch:3)
2017-03-09 05:20:51,072 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:3,ControllerEpoch:3)
2017-03-09 05:20:51,079 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:3,ControllerEpoch:3)
2017-03-09 05:20:51,086 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:3,ControllerEpoch:3)
2017-03-09 05:20:51,092 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:3,ControllerEpoch:3)
2017-03-09 05:20:51,098 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:3,ControllerEpoch:3)
2017-03-09 05:20:51,104 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:3,ControllerEpoch:3)
2017-03-09 05:20:51,112 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:3,ControllerEpoch:3)
2017-03-09 05:20:51,118 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:3,ControllerEpoch:3)
2017-03-09 05:20:51,130 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:3,ControllerEpoch:3)
2017-03-09 05:20:51,137 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:3,ControllerEpoch:3)
2017-03-09 05:20:51,146 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:3,ControllerEpoch:3)
2017-03-09 05:20:51,153 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:3,ControllerEpoch:3)
2017-03-09 05:20:51,164 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:3,ControllerEpoch:3)
2017-03-09 05:20:51,189 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:3,ControllerEpoch:3)
2017-03-09 05:20:51,196 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:3,ControllerEpoch:3)
2017-03-09 05:21:18,092 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:7,ControllerEpoch:6)
2017-03-09 05:21:18,101 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:7,ControllerEpoch:6)
2017-03-09 05:21:18,108 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:7,ControllerEpoch:6)
2017-03-09 05:21:18,116 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:7,ControllerEpoch:6)
2017-03-09 05:21:18,126 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:7,ControllerEpoch:6)
2017-03-09 05:21:18,133 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:7,ControllerEpoch:6)
2017-03-09 05:21:18,145 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:7,ControllerEpoch:6)
2017-03-09 05:21:18,153 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:7,ControllerEpoch:6)
2017-03-09 05:21:18,164 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:7,ControllerEpoch:6)
2017-03-09 05:21:18,171 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:7,ControllerEpoch:6)
2017-03-09 05:21:18,177 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:7,ControllerEpoch:6)
2017-03-09 05:21:18,184 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:7,ControllerEpoch:6)
2017-03-09 05:21:18,192 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:7,ControllerEpoch:6)
2017-03-09 05:21:18,199 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:7,ControllerEpoch:6)
2017-03-09 05:21:18,207 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:7,ControllerEpoch:6)
2017-03-09 05:21:18,218 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:7,ControllerEpoch:6)
2017-03-09 05:21:18,225 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:7,ControllerEpoch:6)
2017-03-09 05:21:18,230 -> [test_topic,2] -> (Leader:1,ISR:1,LeaderEpoch:7,ControllerEpoch:6)
2017-03-09 05:22:59,462 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:10,ControllerEpoch:9)
2017-03-09 05:22:59,472 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:10,ControllerEpoch:9)
2017-03-09 05:22:59,480 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:10,ControllerEpoch:9)
2017-03-09 05:22:59,489 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:10,ControllerEpoch:9)
2017-03-09 05:22:59,499 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:10,ControllerEpoch:9)
2017-03-09 05:22:59,508 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:10,ControllerEpoch:9)
2017-03-09 05:22:59,517 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:10,ControllerEpoch:9)
2017-03-09 05:22:59,526 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:10,ControllerEpoch:9)
2017-03-09 05:22:59,544 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:10,ControllerEpoch:9)
2017-03-09 05:22:59,554 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:10,ControllerEpoch:9)
2017-03-09 05:22:59,562 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:10,ControllerEpoch:9)
2017-03-09 05:22:59,570 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:10,ControllerEpoch:9)
2017-03-09 05:22:59,578 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:10,ControllerEpoch:9)
2017-03-09 05:22:59,586 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:10,ControllerEpoch:9)
2017-03-09 05:22:59,594 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:10,ControllerEpoch:9)
2017-03-09 05:22:59,602 -> [test_topic,2] -> (Leader:3,ISR:3,LeaderEpoch:10,ControllerEpoch:9)
./worker6/info/controller.log
./worker8/debug/controller.log
./worker8/info/controller.log
amehta-macbook-pro:KafkaService-0-140193561885648 apurva$

{noformat}","11/Mar/17 01:29;apurva;Here is everything from the state change logs.

{noformat}
amehta-macbook-pro:KafkaService-0-140193561885648 apurva$ for i in `find . -name state-change.log`; do grep -Hni ""test_topic,2"" $i; done
./worker2/debug/state-change.log:81:[2017-03-09 05:20:37,788] WARN Broker 2 ignoring LeaderAndIsr request from controller 2 with correlation id 1 epoch 2 for partition [test_topic,2] since its associated leader epoch 1 is not higher than the current leader epoch 1 (state.change.logger)
./worker2/debug/state-change.log:135:[2017-03-09 05:20:53,677] WARN Broker 2 ignoring LeaderAndIsr request from controller 1 with correlation id 1 epoch 4 for partition [test_topic,2] since its associated leader epoch 3 is not higher than the current leader epoch 3 (state.change.logger)
./worker2/debug/state-change.log:206:[2017-03-09 05:21:05,555] WARN Broker 2 ignoring LeaderAndIsr request from controller 2 with correlation id 1 epoch 5 for partition [test_topic,2] since its associated leader epoch 5 is not higher than the current leader epoch 5 (state.change.logger)
./worker2/debug/state-change.log:927:[2017-03-09 05:21:20,303] WARN Broker 2 ignoring LeaderAndIsr request from controller 1 with correlation id 1 epoch 7 for partition [test_topic,2] since its associated leader epoch 7 is not higher than the current leader epoch 7 (state.change.logger)
./worker2/info/state-change.log:81:[2017-03-09 05:20:37,788] WARN Broker 2 ignoring LeaderAndIsr request from controller 2 with correlation id 1 epoch 2 for partition [test_topic,2] since its associated leader epoch 1 is not higher than the current leader epoch 1 (state.change.logger)
./worker2/info/state-change.log:135:[2017-03-09 05:20:53,677] WARN Broker 2 ignoring LeaderAndIsr request from controller 1 with correlation id 1 epoch 4 for partition [test_topic,2] since its associated leader epoch 3 is not higher than the current leader epoch 3 (state.change.logger)
./worker2/info/state-change.log:206:[2017-03-09 05:21:05,555] WARN Broker 2 ignoring LeaderAndIsr request from controller 2 with correlation id 1 epoch 5 for partition [test_topic,2] since its associated leader epoch 5 is not higher than the current leader epoch 5 (state.change.logger)
./worker2/info/state-change.log:927:[2017-03-09 05:21:20,303] WARN Broker 2 ignoring LeaderAndIsr request from controller 1 with correlation id 1 epoch 7 for partition [test_topic,2] since its associated leader epoch 7 is not higher than the current leader epoch 7 (state.change.logger)
./worker6/debug/state-change.log:72:[2017-03-09 05:20:37,759] WARN Broker 3 ignoring LeaderAndIsr request from controller 2 with correlation id 1 epoch 2 for partition [test_topic,2] since its associated leader epoch 1 is not higher than the current leader epoch 1 (state.change.logger)
./worker6/debug/state-change.log:152:[2017-03-09 05:20:46,152] WARN Broker 3 ignoring LeaderAndIsr request from controller 3 with correlation id 1 epoch 3 for partition [test_topic,2] since its associated leader epoch 2 is not higher than the current leader epoch 2 (state.change.logger)
./worker6/debug/state-change.log:404:[2017-03-09 05:20:51,253] ERROR Controller 3 epoch 3 encountered error while electing leader for partition [test_topic,2] due to: Preferred replica 2 for partition [test_topic,2] is either not alive or not in the isr. Current leader and ISR: [{""leader"":1,""leader_epoch"":3,""isr"":[1]}]. (state.change.logger)
./worker6/debug/state-change.log:405:[2017-03-09 05:20:51,253] ERROR Controller 3 epoch 3 initiated state change for partition [test_topic,2] from OnlinePartition to OnlinePartition failed (state.change.logger)
./worker6/debug/state-change.log:406:kafka.common.StateChangeFailedException: encountered error while electing leader for partition [test_topic,2] due to: Preferred replica 2 for partition [test_topic,2] is either not alive or not in the isr. Current leader and ISR: [{""leader"":1,""leader_epoch"":3,""isr"":[1]}].
./worker6/debug/state-change.log:439:Caused by: kafka.common.StateChangeFailedException: Preferred replica 2 for partition [test_topic,2] is either not alive or not in the isr. Current leader and ISR: [{""leader"":1,""leader_epoch"":3,""isr"":[1]}]
./worker6/debug/state-change.log:925:[2017-03-09 05:21:05,541] WARN Broker 3 ignoring LeaderAndIsr request from controller 2 with correlation id 1 epoch 5 for partition [test_topic,2] since its associated leader epoch 5 is not higher than the current leader epoch 5 (state.change.logger)
./worker6/debug/state-change.log:1005:[2017-03-09 05:21:13,306] WARN Broker 3 ignoring LeaderAndIsr request from controller 3 with correlation id 1 epoch 6 for partition [test_topic,2] since its associated leader epoch 6 is not higher than the current leader epoch 6 (state.change.logger)
./worker6/debug/state-change.log:1257:[2017-03-09 05:21:18,342] ERROR Controller 3 epoch 6 encountered error while electing leader for partition [test_topic,2] due to: Preferred replica 2 for partition [test_topic,2] is either not alive or not in the isr. Current leader and ISR: [{""leader"":1,""leader_epoch"":7,""isr"":[1]}]. (state.change.logger)
./worker6/debug/state-change.log:1258:[2017-03-09 05:21:18,342] ERROR Controller 3 epoch 6 initiated state change for partition [test_topic,2] from OnlinePartition to OnlinePartition failed (state.change.logger)
./worker6/debug/state-change.log:1259:kafka.common.StateChangeFailedException: encountered error while electing leader for partition [test_topic,2] due to: Preferred replica 2 for partition [test_topic,2] is either not alive or not in the isr. Current leader and ISR: [{""leader"":1,""leader_epoch"":7,""isr"":[1]}].
./worker6/debug/state-change.log:1292:Caused by: kafka.common.StateChangeFailedException: Preferred replica 2 for partition [test_topic,2] is either not alive or not in the isr. Current leader and ISR: [{""leader"":1,""leader_epoch"":7,""isr"":[1]}]
./worker6/info/state-change.log:72:[2017-03-09 05:20:37,759] WARN Broker 3 ignoring LeaderAndIsr request from controller 2 with correlation id 1 epoch 2 for partition [test_topic,2] since its associated leader epoch 1 is not higher than the current leader epoch 1 (state.change.logger)
./worker6/info/state-change.log:152:[2017-03-09 05:20:46,152] WARN Broker 3 ignoring LeaderAndIsr request from controller 3 with correlation id 1 epoch 3 for partition [test_topic,2] since its associated leader epoch 2 is not higher than the current leader epoch 2 (state.change.logger)
./worker6/info/state-change.log:404:[2017-03-09 05:20:51,253] ERROR Controller 3 epoch 3 encountered error while electing leader for partition [test_topic,2] due to: Preferred replica 2 for partition [test_topic,2] is either not alive or not in the isr. Current leader and ISR: [{""leader"":1,""leader_epoch"":3,""isr"":[1]}]. (state.change.logger)
./worker6/info/state-change.log:405:[2017-03-09 05:20:51,253] ERROR Controller 3 epoch 3 initiated state change for partition [test_topic,2] from OnlinePartition to OnlinePartition failed (state.change.logger)
./worker6/info/state-change.log:406:kafka.common.StateChangeFailedException: encountered error while electing leader for partition [test_topic,2] due to: Preferred replica 2 for partition [test_topic,2] is either not alive or not in the isr. Current leader and ISR: [{""leader"":1,""leader_epoch"":3,""isr"":[1]}].
./worker6/info/state-change.log:439:Caused by: kafka.common.StateChangeFailedException: Preferred replica 2 for partition [test_topic,2] is either not alive or not in the isr. Current leader and ISR: [{""leader"":1,""leader_epoch"":3,""isr"":[1]}]
./worker6/info/state-change.log:925:[2017-03-09 05:21:05,541] WARN Broker 3 ignoring LeaderAndIsr request from controller 2 with correlation id 1 epoch 5 for partition [test_topic,2] since its associated leader epoch 5 is not higher than the current leader epoch 5 (state.change.logger)
./worker6/info/state-change.log:1005:[2017-03-09 05:21:13,306] WARN Broker 3 ignoring LeaderAndIsr request from controller 3 with correlation id 1 epoch 6 for partition [test_topic,2] since its associated leader epoch 6 is not higher than the current leader epoch 6 (state.change.logger)
./worker6/info/state-change.log:1257:[2017-03-09 05:21:18,342] ERROR Controller 3 epoch 6 encountered error while electing leader for partition [test_topic,2] due to: Preferred replica 2 for partition [test_topic,2] is either not alive or not in the isr. Current leader and ISR: [{""leader"":1,""leader_epoch"":7,""isr"":[1]}]. (state.change.logger)
./worker6/info/state-change.log:1258:[2017-03-09 05:21:18,342] ERROR Controller 3 epoch 6 initiated state change for partition [test_topic,2] from OnlinePartition to OnlinePartition failed (state.change.logger)
./worker6/info/state-change.log:1259:kafka.common.StateChangeFailedException: encountered error while electing leader for partition [test_topic,2] due to: Preferred replica 2 for partition [test_topic,2] is either not alive or not in the isr. Current leader and ISR: [{""leader"":1,""leader_epoch"":7,""isr"":[1]}].
./worker6/info/state-change.log:1292:Caused by: kafka.common.StateChangeFailedException: Preferred replica 2 for partition [test_topic,2] is either not alive or not in the isr. Current leader and ISR: [{""leader"":1,""leader_epoch"":7,""isr"":[1]}]
./worker8/debug/state-change.log:63:[2017-03-09 05:20:46,125] WARN Broker 1 ignoring LeaderAndIsr request from controller 3 with correlation id 1 epoch 3 for partition [test_topic,2] since its associated leader epoch 2 is not higher than the current leader epoch 2 (state.change.logger)
./worker8/debug/state-change.log:117:[2017-03-09 05:20:53,689] WARN Broker 1 ignoring LeaderAndIsr request from controller 1 with correlation id 1 epoch 4 for partition [test_topic,2] since its associated leader epoch 3 is not higher than the current leader epoch 3 (state.change.logger)
./worker8/debug/state-change.log:197:[2017-03-09 05:21:13,291] WARN Broker 1 ignoring LeaderAndIsr request from controller 3 with correlation id 1 epoch 6 for partition [test_topic,2] since its associated leader epoch 6 is not higher than the current leader epoch 6 (state.change.logger)
./worker8/debug/state-change.log:255:[2017-03-09 05:21:20,314] WARN Broker 1 ignoring LeaderAndIsr request from controller 1 with correlation id 1 epoch 7 for partition [test_topic,2] since its associated leader epoch 7 is not higher than the current leader epoch 7 (state.change.logger)
./worker8/info/state-change.log:63:[2017-03-09 05:20:46,125] WARN Broker 1 ignoring LeaderAndIsr request from controller 3 with correlation id 1 epoch 3 for partition [test_topic,2] since its associated leader epoch 2 is not higher than the current leader epoch 2 (state.change.logger)
./worker8/info/state-change.log:117:[2017-03-09 05:20:53,689] WARN Broker 1 ignoring LeaderAndIsr request from controller 1 with correlation id 1 epoch 4 for partition [test_topic,2] since its associated leader epoch 3 is not higher than the current leader epoch 3 (state.change.logger)
./worker8/info/state-change.log:197:[2017-03-09 05:21:13,291] WARN Broker 1 ignoring LeaderAndIsr request from controller 3 with correlation id 1 epoch 6 for partition [test_topic,2] since its associated leader epoch 6 is not higher than the current leader epoch 6 (state.change.logger)
./worker8/info/state-change.log:255:[2017-03-09 05:21:20,314] WARN Broker 1 ignoring LeaderAndIsr request from controller 1 with correlation id 1 epoch 7 for partition [test_topic,2] since its associated leader epoch 7 is not higher than the current leader epoch 7 (state.change.logger)
{noformat}","11/Mar/17 01:42;apurva;I posted all the information I could glean from the logs. The tl;dr is :

This seems like something similar to KAFKA-1211, but the current logs offer no definitive proof. We see log truncation to offset 1496, but this is not accompanied by a quick succession of leader elections in that timeframe. Nor are there any 'reset offset' messages from the replicas in the log. So it is hard to explain the log truncations being observed in this test. 

It may make sense to enable trace logging for the controller and the state change log for this test. Then the next time we see this, we would atleast be sure that we are seeing _all_ the leader elections, and will give us a clear idea of whether it is KAFKA-1211 or not.

","13/Mar/17 17:33;githubbot;GitHub user apurvam opened a pull request:

    https://github.com/apache/kafka/pull/2677

    MINOR: set trace logging for zookeeper upgrade test

    This adds logging which will hopefully help root cause https://issues.apache.org/jira/browse/KAFKA-4574.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apurvam/kafka minor-set-trace-logging-for-zookeeper-upgrade-test

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2677.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2677
    
----
commit f643d8ddc2b9c9952fa2cd62ba5e3dadac092562
Author: Apurva Mehta <apurva@confluent.io>
Date:   2017-03-11T23:43:48Z

    MINOR: configure TRACE logging for zookeeper upgrade test
    
    We need trace logging for the controller and state change log to debug
    KAFKA-4574.

commit f9c8c51cfdf7eae1842c441ece56d65738cfdfc8
Author: Apurva Mehta <apurva@confluent.io>
Date:   2017-03-12T00:22:52Z

    More experiments

commit 88314b97a774d1c629409ca0e46f596e8f4e7a11
Author: Apurva Mehta <apurva@confluent.io>
Date:   2017-03-12T00:29:25Z

    Add the trace directory for collection

----
","27/Mar/17 12:53;githubbot;GitHub user ijuma opened a pull request:

    https://github.com/apache/kafka/pull/2742

    KAFKA-4574: Ignore test_zk_security_upgrade until KIP-101 lands

    The transient failures make it harder to spot real failures and we can live without what is being tested (adding security to ZK via a rolling upgrade) until KIP-101 lands.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ijuma/kafka disable-zk-upgrade-test

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2742.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2742
    
----
commit 79f0a5561fef0e3ed06907c3f39f3f36a60e3917
Author: Ismael Juma <ismael@juma.me.uk>
Date:   2017-03-27T12:51:52Z

    KAFKA-4574: Ignore test_zk_security_upgrade until KIP-101 lands

----
","29/Mar/17 01:31;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2742
","07/Apr/17 00:09;ijuma;I ran this test with --repeat=8 (which works out to 24 executions) on the KIP-101 branch (which has now been merged) and all of them passed. I will close it for now, but we can reopen if we see the issue in trunk again.","09/Oct/17 21:03;githubbot;Github user apurvam closed the pull request at:

    https://github.com/apache/kafka/pull/2677
",,,,,,,,
TransactionsTest#testBasicTransactions sometimes hangs,KAFKA-5840,13099992,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Cannot Reproduce,,yuzhihong@gmail.com,yuzhihong@gmail.com,05/Sep/17 23:50,22/Sep/17 17:33,12/Jan/21 11:54,22/Sep/17 17:33,,,,,,,,,,,,,,0,,,,"While testing 0.11.0.1 RC0 , I found TransactionsTest hanging.

Here is part of the stack trace:
{code}
""Test worker"" #20 prio=5 os_prio=0 tid=0x00007feb449fc000 nid=0x5f69 waiting on condition [0x00007feb05f8c000]
   java.lang.Thread.State: WAITING (parking)
  at sun.misc.Unsafe.park(Native Method)
  - parking to wait for  <0x0000000081272ec0> (a java.util.concurrent.CountDownLatch$Sync)
  at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
  at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
  at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
  at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
  at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
  at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
  at org.apache.kafka.clients.producer.internals.RecordAccumulator.awaitFlushCompletion(RecordAccumulator.java:573)
  at org.apache.kafka.clients.producer.KafkaProducer.flush(KafkaProducer.java:948)
  at kafka.api.TransactionsTest.testBasicTransactions(TransactionsTest.scala:93)
{code}
{code}
Apache Maven 3.5.0 (ff8f5e7444045639af65f6095c62210b5713f426; 2017-04-03T19:39:06Z)
Maven home: /apache-maven-3.5.0
Java version: 1.8.0_131, vendor: Oracle Corporation
Java home: /jdk1.8.0_131/jre
Default locale: en_US, platform encoding: UTF-8
OS name: ""linux"", version: ""3.10.0-327.28.3.el7.x86_64"", arch: ""amd64"", family: ""unix""
{code}",,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/17 23:51;yuzhihong@gmail.com;5840.stack;https://issues.apache.org/jira/secure/attachment/12885481/5840.stack",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-09-05 23:50:06.0,,,,,,,"0|i3jol3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support passing ZK chroot in system tests,KAFKA-5742,13095148,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,xvrl,xvrl,xvrl,16/Aug/17 21:02,08/Sep/17 18:07,12/Jan/21 11:54,17/Aug/17 22:16,,,,,0.11.0.1,1.0.0,,,,system tests,,,,0,,,,"Currently spinning up multiple Kafka clusters in a system tests requires at least one ZK node per Kafka cluster, which wastes a lot of resources. We currently also don't test anything outside of the ZK root path.",,githubbot,xvrl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-08-16 21:18:08.939,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 08 18:07:13 UTC 2017,,,,,,,"0|i3ivfr:",9223372036854775807,,,,,,,,,,,,,,,,"16/Aug/17 21:18;githubbot;GitHub user xvrl opened a pull request:

    https://github.com/apache/kafka/pull/3677

    KAFKA-5742 support ZK chroot in system tests

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/xvrl/kafka support-zk-chroot-in-tests

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3677.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3677
    
----
commit e2c6ed282c1d6d0a373def99fa0243e654cce37d
Author: Xavier Léauté <xavier@confluent.io>
Date:   2017-08-16T20:58:58Z

    KAFKA-5742 support ZK chroot in system tests

----
","17/Aug/17 22:16;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/3677
","17/Aug/17 22:24;githubbot;GitHub user xvrl opened a pull request:

    https://github.com/apache/kafka/pull/3690

    Backport KAFKA-5742 support ZK chroot in system tests

    backport of #3677

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/xvrl/kafka zk-chroot-0.11.0

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3690.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3690
    
----
commit 7847b5343410d5a8d8991e74bd03fb423a95686e
Author: Xavier Léauté <xavier@confluent.io>
Date:   2017-08-16T20:58:58Z

    KAFKA-5742 support ZK chroot in system tests

----
","21/Aug/17 04:58;githubbot;Github user xvrl closed the pull request at:

    https://github.com/apache/kafka/pull/3690
","08/Sep/17 17:40;githubbot;GitHub user xvrl opened a pull request:

    https://github.com/apache/kafka/pull/3818

    MINOR: KAFKA-5742 follow-up, fix incorrect method name

    @ewencp @ijuma 

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/xvrl/kafka fix-startswith

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3818.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3818
    
----
commit 091281596f2a4ac1ccebc0240aba4313f7348eec
Author: Xavier Léauté <xvrl@users.noreply.github.com>
Date:   2017-09-08T17:38:33Z

    MINOR: KAFKA-5742 follow-up, fix incorrect method name

----
","08/Sep/17 18:07;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/3818
",,,,,,,,,,,,,,,,
Create a system test for network partitions,KAFKA-1347,12704281,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,,jkreps,jkreps,28/Mar/14 15:57,31/Aug/17 06:50,12/Jan/21 11:54,31/Aug/17 06:50,,,,,,,,,,,,,,0,,,,"We got some free and rather public QA here:
http://aphyr.com/posts/293-call-me-maybe-kafka

We have since added a configuration to disable unclean leader election which allows you to prefer consistency over availability when all brokers fail.

This has some unit tests, but ultimately there is no reason to believe this works unless we have a fairly aggressive system test case for it.
https://cwiki.apache.org/confluence/display/KAFKA/Kafka+System+Tests

It would be good to add support for network partitions. I don't think we actually need to try to use the jepsen stuff directly, we can just us the underlying tools it uses--iptables and tc. These are linux specific, but that is prolly okay. You can see these at work here:
https://github.com/aphyr/jepsen/blob/master/src/jepsen/control/net.clj

Having this would help provide better evidence that this works now, and would keep it working in the future.",,Andrew.Kostousov@gmail.com,cagatayk,eidi,granders,hongyu.bi,jghoman,jkreps,kieren,noslowerdna,omkreddy,vpernin,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-05-13 00:48:19.684,,,false,,,,,,,,,,,,,,,,,382615,,,Thu Aug 31 06:50:31 UTC 2017,,,,,,,"0|i1tx1r:",382883,,,,,,,,,,,,,,,,"13/May/15 00:48;granders;Just making sure you're aware of work we're doing at Confluent on system tests. I'll be posting a KIP for this soon, but here's some info:

The original plan is sketched here:
https://cwiki.apache.org/confluence/display/KAFKA/System+Test+Improvements

This is the core library/test framework (WIP) which aids in writing and running the tests
https://github.com/confluentinc/ducktape/

This has system tests we've written to date for the Confluent Platform
https://github.com/confluentinc/muckrake","31/Aug/17 06:50;omkreddy;This is being fixed in KAFKA-5476.",,,,,,,,,,,,,,,,,,,,
Additional failure testing for streams with bouncing brokers,KAFKA-5725,13094102,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,enothereska,enothereska,enothereska,11/Aug/17 13:54,18/Aug/17 17:14,12/Jan/21 11:54,18/Aug/17 17:14,0.11.0.0,,,,0.11.0.2,,,,,streams,,,,0,,,,"We have tests/streams/streams_broker_bounce_test.py that tests streams' robustness when some brokers quit or are terminated. We do not have coverage for transient failures, such as when brokers come back. Add such tests.",,enothereska,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-08-18 17:14:35.958,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 18 17:14:35 UTC 2017,,,,,,,"0|i3ip1b:",9223372036854775807,,,,,,,,,,,,,,,,"18/Aug/17 17:14;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/3656
",,,,,,,,,,,,,,,,,,,,,
SyncProducerTest.testReachableServer has become flaky,KAFKA-5371,13077001,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ijuma,apurva,apurva,02/Jun/17 21:48,04/Jun/17 14:36,12/Jan/21 11:54,04/Jun/17 14:36,0.11.0.0,,,,0.11.0.0,,,,,,,,,0,,,,"This test has started failing recently on jenkins with the following 

{noformat}
org.scalatest.junit.JUnitTestFailedError: Unexpected failure sending message to broker. null
	at org.scalatest.junit.AssertionsForJUnit$class.newAssertionFailedException(AssertionsForJUnit.scala:100)
	at org.scalatest.junit.JUnitSuite.newAssertionFailedException(JUnitSuite.scala:71)
	at org.scalatest.Assertions$class.fail(Assertions.scala:1089)
	at org.scalatest.junit.JUnitSuite.fail(JUnitSuite.scala:71)
	at kafka.producer.SyncProducerTest.testReachableServer(SyncProducerTest.scala:71)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.runTestClass(JUnitTestClassExecuter.java:114)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.execute(JUnitTestClassExecuter.java:57)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassProcessor.processTestClass(JUnitTestClassProcessor.java:66)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:109)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:147)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:129)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:404)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)
	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

{noformat}",,apurva,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-06-03 08:18:55.554,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 04 14:32:47 UTC 2017,,,,,,,"0|i3fu1z:",9223372036854775807,,,,,,,,,,,,,,,,"03/Jun/17 08:18;githubbot;GitHub user ijuma opened a pull request:

    https://github.com/apache/kafka/pull/3225

    KAFKA-5371: Increase request timeout for producer used by testReachableServer

    500ms is low for a shared Jenkins environment.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ijuma/kafka kafka-5371-flaky-testReachableServer

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3225.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3225
    
----
commit 5f9b0df4c296d80d64091da3b4b0106816643114
Author: Ismael Juma <ismael@juma.me.uk>
Date:   2017-06-03T08:18:21Z

    KAFKA-5371: Increase request timeout for producer used by testReachableServer
    
    500ms is low for a shared Jenkins environment.

----
","04/Jun/17 14:32;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/3225
",,,,,,,,,,,,,,,,,,,,
Implement KIP-98 transactional methods in the MockProducer,KAFKA-5126,13066785,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mjsax,apurva,apurva,25/Apr/17 18:54,09/May/17 17:16,12/Jan/21 11:54,09/May/17 17:16,,,,,0.11.0.0,,,,,,,,,0,,,,"The initial code for the transactional producer leaves the implementation of `initTransactions`, `beginTransaction`, `sendOffsetsToTransaction`, `commitTransaction`, and `abortTransaction` empty in the MockProducer. We need have some implementation there so that our mocks stay healthy.",,apurva,githubbot,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-05-01 23:28:34.262,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 09 17:16:48 UTC 2017,,,,,,,"0|i3e32n:",9223372036854775807,,,,,,,,,,,,,,,,"01/May/17 23:28;githubbot;GitHub user mjsax opened a pull request:

    https://github.com/apache/kafka/pull/2951

    KAFKA-5126: Implement KIP-98 transactional methods in the MockProducer

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/mjsax/kafka kafka-5126-add-transactions-to-mock-producer

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2951.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2951
    
----
commit 0ee76ea931d09a592091fffe621fb897cfd12fda
Author: Matthias J. Sax <matthias@confluent.io>
Date:   2017-05-01T22:20:11Z

    KAFKA-5126: Implement KIP-98 transactional methods in the MockProducer

----
","09/May/17 17:16;guozhang;Issue resolved by pull request 2951
[https://github.com/apache/kafka/pull/2951]","09/May/17 17:16;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2951
",,,,,,,,,,,,,,,,,,,
Test with two SASL_SSL listeners with different JAAS contexts,KAFKA-4703,13038302,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,baluchicken,ijuma,ijuma,26/Jan/17 22:38,03/May/17 20:10,12/Jan/21 11:54,03/May/17 20:09,,,,,0.11.0.0,,,,,,,,,0,newbie,,,"[~rsivaram] suggested the following in https://github.com/apache/kafka/pull/2406

{quote}
I think this feature allows two SASL_SSL listeners, one for external and one for internal and the two can use different mechanisms and different JAAS contexts. That makes the multi-mechanism configuration neater. I think it will be useful to have an integration test for this, perhaps change SaslMultiMechanismConsumerTest.
{quote}

And my reply:

{quote}
I think it's a bit tricky to support multiple listeners in KafkaServerTestHarness. Maybe it's easier to do the test you suggest in MultipleListenersWithSameSecurityProtocolTest.
{quote}",,baluchicken,githubbot,ijuma,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-31 12:39:26.135,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 03 20:10:11 UTC 2017,,,,,,,"0|i399xb:",9223372036854775807,,,,,,,,,,,,,,,,"31/Jan/17 12:39;baluchicken;[~ijuma] should I create a new test for this in class MultipleListenersWithSameSecurityProtocolTest or is it enough to modify the existing one to use SASL_SSL and different jaas context instead of SSL? ","31/Jan/17 12:44;ijuma;[~baluchicken], if it's not too hard to add another test (while reusing shared code), I think that would be better.","06/Feb/17 14:57;githubbot;GitHub user baluchicken opened a pull request:

    https://github.com/apache/kafka/pull/2506

    KAFKA-4703 Test with two SASL_SSL listeners with different JAAS contexts

    @ijuma plz review

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/baluchicken/kafka-1 KAFKA-4703

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2506.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2506
    
----
commit ecbf27736e746aceb368744c170a7a7fe2bbea09
Author: Balint Molnar <balintmolnar91@gmail.com>
Date:   2017-02-06T14:55:08Z

    KAFKA-4703 Test with two SASL_SSL listeners with different JAAS contexts

----
","03/May/17 20:09;rsivaram;Issue resolved by pull request 2506
[https://github.com/apache/kafka/pull/2506]","03/May/17 20:10;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2506
",,,,,,,,,,,,,,,,,
Improve test coverage for broker CRC validation,KAFKA-5106,13065946,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,hachikuji,hachikuji,21/Apr/17 17:42,21/Apr/17 17:42,12/Jan/21 11:54,,,,,,,,,,,,,,,0,,,,"Test cases should ensure that CRC errors are detected across all message formats and compression types, and they should ensure that CRC validation is only done once on the broker to protect ourselves from performance regression in the future.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-04-21 17:42:10.0,,,,,,,"0|i3dxwn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Why is there no difference between kafka benchmark tests on SSD and HDD? ,KAFKA-4971,13059935,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,Dasol Kim,Dasol Kim,29/Mar/17 06:27,05/Apr/17 02:29,12/Jan/21 11:54,,0.10.0.0,,,,,,,,,,,,,0,,,,"I installed OS and kafka in the two SSD and two HDDs  to perform the kafka benchmark test based on the disc difference. As expected, the SSD should show faster results, but according to my experimental results, there is no big difference between SSD and HDD. why? Ohter settings have been set to default.

*test settings

zookeeper node  : 1, producer node : 2, broker node : 2(SSD 1, HDD 1)
test scenario : Two producers send messages to the broker and compare the throughtput per second of kafka installed on SSD and kafka on HDD

command : ./bin/kafka-producer-perf-test.sh --num-records 1000000 --record-size 2000 --topic test --throughput 100000 --producer-props bootstrap.servers=SN02:9092


 ","Oracle VM VirtualBox
OS : CentOs 7
Memory : 1G
Disk : 8GB",Dasol Kim,mihbor,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-04-03 14:50:39.423,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 05 02:29:10 UTC 2017,,,,,,,"0|i3cxfj:",9223372036854775807,,,,,,,,,,,,,,,,"03/Apr/17 14:50;mihbor;I think your question would be easier to respond to if you quantified it by providing your test results and the drive specs.
Kafka IO access patterns are designed to be sequential for good reason. Spinning disks and OS level buffering are optimised for such IO patterns, but I don't know if that alone can account for the miss-match between your expectations and the results you are getting on your hardware.","04/Apr/17 14:55;Dasol Kim;I was experimenting with the kafka producer installed on the SSD by sending messages to one broker installed on another ssd server in throughput / sec. In the same way, the experiment was conducted using the kafka of the HDD installed server. According to the known facts, SSD experiments should show more throughput per second.

test settings
OS : CentOs 7, Memory : 1G, Disk : 8G

it's my test result and command
VM - SSD(producer 1, broker 1, partition 1)

./bin/kafka-producer-perf-test.sh --num-records 1000000 --record-size 2000 --topic test --throughput 100000 --producer-props bootstrap.servers=SN02:9092
9169 records sent, 1833.8 records/sec (3.50 MB/sec), 2393.7 ms avg latency, 3621.0 max latency.
17840 records sent, 3566.6 records/sec (6.80 MB/sec), 4412.3 ms avg latency, 5282.0 max latency.
23568 records sent, 4704.2 records/sec (8.97 MB/sec), 4391.3 ms avg latency, 5132.0 max latency.
32872 records sent, 6574.4 records/sec (12.54 MB/sec), 2612.5 ms avg latency, 3055.0 max latency.
39352 records sent, 7870.4 records/sec (15.01 MB/sec), 2001.6 ms avg latency, 2644.0 max latency.
41168 records sent, 8228.7 records/sec (15.69 MB/sec), 2003.0 ms avg latency, 2585.0 max latency.
40568 records sent, 8105.5 records/sec (15.46 MB/sec), 2113.5 ms avg latency, 2374.0 max latency.
65528 records sent, 13105.6 records/sec (25.00 MB/sec), 1351.3 ms avg latency, 1778.0 max latency.
108096 records sent, 21619.2 records/sec (41.24 MB/sec), 780.2 ms avg latency, 1026.0 max latency.
79992 records sent, 15988.8 records/sec (30.50 MB/sec), 855.3 ms avg latency, 2238.0 max latency.
31152 records sent, 6230.4 records/sec (11.88 MB/sec), 2651.9 ms avg latency, 3180.0 max latency.
39520 records sent, 7899.3 records/sec (15.07 MB/sec), 1820.1 ms avg latency, 2536.0 max latency.
52824 records sent, 10564.8 records/sec (20.15 MB/sec), 1942.5 ms avg latency, 3243.0 max latency.
68912 records sent, 13760.4 records/sec (26.25 MB/sec), 1122.5 ms avg latency, 1678.0 max latency.
93024 records sent, 18601.1 records/sec (35.48 MB/sec), 954.2 ms avg latency, 1624.0 max latency.
80200 records sent, 16040.0 records/sec (30.59 MB/sec), 981.6 ms avg latency, 1417.0 max latency.
101720 records sent, 20344.0 records/sec (38.80 MB/sec), 829.8 ms avg latency, 1201.0 max latency.
73272 records sent, 14654.4 records/sec (27.95 MB/sec), 1076.5 ms avg latency, 1487.0 max latency.
1000000 records sent, 11094.223238 records/sec (21.16 MB/sec), 1444.94 ms avg latency, 5282.00 ms max latency, 1181 ms 50th, 3116 ms 95th, 4794 ms 99th, 5158 ms 99.9th.

VM - HDD(producer 1, broker 1, partition 1)

./bin/kafka-producer-perf-test.sh --num-records 1000000 --record-size 2000 --topic test --throughput 100000 --producer-props bootstrap.servers=SN03:9092
11145 records sent, 2228.6 records/sec (4.25 MB/sec), 2209.9 ms avg latency, 3442.0 max latency.
19592 records sent, 3912.9 records/sec (7.46 MB/sec), 4165.9 ms avg latency, 4661.0 max latency.
18472 records sent, 3694.4 records/sec (7.05 MB/sec), 4416.4 ms avg latency, 4598.0 max latency.
33312 records sent, 6662.4 records/sec (12.71 MB/sec), 2862.6 ms avg latency, 4366.0 max latency.
49000 records sent, 9782.4 records/sec (18.66 MB/sec), 1923.6 ms avg latency, 2673.0 max latency.
41856 records sent, 8357.8 records/sec (15.94 MB/sec), 1760.9 ms avg latency, 2241.0 max latency.
48032 records sent, 9602.6 records/sec (18.32 MB/sec), 1863.5 ms avg latency, 2283.0 max latency.
78032 records sent, 15606.4 records/sec (29.77 MB/sec), 1096.7 ms avg latency, 1364.0 max latency.
93440 records sent, 18688.0 records/sec (35.64 MB/sec), 833.6 ms avg latency, 1299.0 max latency.
72184 records sent, 14436.8 records/sec (27.54 MB/sec), 1185.9 ms avg latency, 1421.0 max latency.
80352 records sent, 16070.4 records/sec (30.65 MB/sec), 955.9 ms avg latency, 1896.0 max latency.
64200 records sent, 12840.0 records/sec (24.49 MB/sec), 1319.0 ms avg latency, 1652.0 max latency.
86400 records sent, 17280.0 records/sec (32.96 MB/sec), 972.3 ms avg latency, 1292.0 max latency.
74472 records sent, 14894.4 records/sec (28.41 MB/sec), 1073.2 ms avg latency, 1224.0 max latency.
75912 records sent, 15182.4 records/sec (28.96 MB/sec), 961.6 ms avg latency, 1901.0 max latency.
30088 records sent, 6017.6 records/sec (11.48 MB/sec), 2746.6 ms avg latency, 3482.0 max latency.
70368 records sent, 14073.6 records/sec (26.84 MB/sec), 1257.5 ms avg latency, 2300.0 max latency.
1000000 records sent, 11269.129347 records/sec (21.49 MB/sec), 1423.68 ms avg latency, 4661.00 ms max latency, 1179 ms 50th, 3467 ms 95th, 4460 ms 99th, 4597 ms 99.9th.


Comparing the two experimental results, we can see that the results of the two experiments show little difference. I do not know what this is about.
please let me know what happened.

","04/Apr/17 15:34;mihbor;I'd venture a guess that you are limited by something else than your hdd/ssd performance.
Is 1g your total memory in the VM? How much of it is allocated to the kafka jvm process?
Some things I can think of:
Is there a lot of activity in the gc.log?
Is the OS not swapping ferociously due to over-allocation of memory by any chance?

Hope that helps.","05/Apr/17 02:29;Dasol Kim;It might be a silly question, If kafka uses the page cache of the OS, if the OS gets bottleneck, the OS is installed in the SSD on the server having both the SSD and the HDD, and the kafka is installed on each of the SSD and HDD separately Is not it an accurate experiment? I experimented with 9 servers I had before experimenting with VMs, but the results of SSDs and HDDs were similar, so I divided VMs into HDDs and SSDs and experimented. ",,,,,,,,,,,,,,,,,,
Test failure: kafka.api.ConsumerBounceTest.testSubscribeWhenTopicUnavailable,KAFKA-4983,13060383,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,edenhill,edenhill,30/Mar/17 14:36,30/Mar/17 14:36,12/Jan/21 11:54,,,,,,,,,,,,,,,0,,,,"The PR builder encountered this test failure:
{{kafka.api.ConsumerBounceTest.testSubscribeWhenTopicUnavailable}}

https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2532/testReport/junit/kafka.api/ConsumerBounceTest/testSubscribeWhenTopicUnavailable/

{noformat}
[2017-03-30 13:42:40,875] ERROR ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes (org.apache.zookeeper.server.ZooKeeperServer:472)
[2017-03-30 13:42:40,884] ERROR ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes (org.apache.zookeeper.server.ZooKeeperServer:472)
[2017-03-30 13:42:41,221] FATAL [Kafka Server 1], Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer:118)
kafka.common.KafkaException: Socket server failed to bind to localhost:42198: Address already in use.
	at kafka.network.Acceptor.openServerSocket(SocketServer.scala:330)
	at kafka.network.Acceptor.<init>(SocketServer.scala:255)
	at kafka.network.SocketServer.$anonfun$startup$1(SocketServer.scala:99)
	at kafka.network.SocketServer.$anonfun$startup$1$adapted(SocketServer.scala:90)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at kafka.network.SocketServer.startup(SocketServer.scala:90)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:215)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:122)
	at kafka.integration.KafkaServerTestHarness.$anonfun$setUp$1(KafkaServerTestHarness.scala:91)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.IterableLike.foreach(IterableLike.scala:71)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:70)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike.map(TraversableLike.scala:234)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:227)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at kafka.integration.KafkaServerTestHarness.setUp(KafkaServerTestHarness.scala:91)
	at kafka.api.IntegrationTestHarness.setUp(IntegrationTestHarness.scala:65)
	at kafka.api.ConsumerBounceTest.setUp(ConsumerBounceTest.scala:67)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.runTestClass(JUnitTestClassExecuter.java:114)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.execute(JUnitTestClassExecuter.java:57)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassProcessor.processTestClass(JUnitTestClassProcessor.java:66)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:109)
	at sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54)
	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:67)
	at kafka.network.Acceptor.openServerSocket(SocketServer.scala:326)
	... 65 more
{noformat}

",,edenhill,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-03-30 14:36:25.0,,,,,,,"0|i3d073:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System Test for Transaction Management,KAFKA-1604,12735219,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Abandoned,lindong,lindong,lindong,20/Aug/14 00:13,28/Feb/17 05:48,12/Jan/21 11:54,28/Feb/17 05:48,,,,,,,,,,,,,,0,transactions,,,"Perform end-to-end transaction management test in the following steps:
1) Start Zookeeper.
2) Start multiple brokers.
3) Create topic.
4) Start transaction-aware ProducerPerformance to generate transactional messages to topic.
5) Start transaction-aware ConsoleConsumer to read messages from topic.
6) Bounce brokers (optional).
7) Verify that same number of messages are sent and received.

This patch depends on KAFKA-1524, KAFKA-1526 and KAFKA-1601.",,granders,hachikuji,lindong,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/14 00:31;lindong;KAFKA-1604_2014-08-19_17:31:16.patch;https://issues.apache.org/jira/secure/attachment/12662929/KAFKA-1604_2014-08-19_17%3A31%3A16.patch","20/Aug/14 04:07;lindong;KAFKA-1604_2014-08-19_21:07:35.patch;https://issues.apache.org/jira/secure/attachment/12662974/KAFKA-1604_2014-08-19_21%3A07%3A35.patch",,,,,,2.0,,,,,,,,,,,,,,,,,,,,2015-05-13 00:47:48.089,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 28 05:48:21 UTC 2017,,,,,,,"0|i1z2qn:",9223372036854775807,,,,,,,,,,,,,,,,"20/Aug/14 00:27;lindong;Created reviewboard https://reviews.apache.org/r/24874/diff
against branch origin/transactional_messaging","20/Aug/14 00:28;lindong;Updated reviewboard https://reviews.apache.org/r/24874/diff/
 against branch origin/transactional_messaging","20/Aug/14 00:31;lindong;Updated reviewboard https://reviews.apache.org/r/24874/diff/
 against branch origin/transactional_messaging","20/Aug/14 04:07;lindong;Updated reviewboard https://reviews.apache.org/r/24874/diff/
 against branch origin/transactional_messaging","13/May/15 00:47;granders;Just making sure you're aware of work we're doing at Confluent on system tests. I'll be posting a KIP for this soon, but here's some info:

The original plan is sketched here:
https://cwiki.apache.org/confluence/display/KAFKA/System+Test+Improvements

This is the core library/test framework (WIP) which aids in writing and running the tests
https://github.com/confluentinc/ducktape/

This has system tests we've written to date for the Confluent Platform
https://github.com/confluentinc/muckrake","28/Feb/17 05:48;hachikuji;This work has been superseded by KIP-98: https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging.",,,,,,,,,,,,,,,,
consumer block,KAFKA-4026,12995831,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Not A Problem,ewencp,imperio,imperio,09/Aug/16 04:34,26/Jan/17 03:43,12/Jan/21 11:54,26/Jan/17 03:43,0.8.1.1,,,,,,,,,consumer,,,,0,,,,"when i use high level api make consumer. it is a block consumer,how can i know the time of blocked?I put messages into a buffer.It did not reach the buffer length the consumer  blocked,the buffer can not be handled.How can i deal with this problem?The buffer did not reach the buffer length,I can handled the buffer.",ubuntu 14.04,dpnchl,ewencp,imperio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-08-26 21:57:59.373,,,false,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Thu Jan 26 03:43:50 UTC 2017,,,,,,,"0|i322hb:",9223372036854775807,,,,,,,,,,,,,,,,"26/Aug/16 21:57;dpnchl;[~imperio] 
So far I've tried setting up a 0.8.1.1 environment and used the high level api to create a consumer. Using the console producer I published a few messages and as expected they were immediately picked up by my consumer, so I am unable to reproduce your behavior as described.

Please provide sample code that reproduces your problem along with any broker/client setting overrides you use.","26/Jan/17 03:43;ewencp;[~imperio] For the old, high-level consumer, there is a configuration parameter consumer.timeout.ms that you can use to control how long the consumer blocks waiting for new messages.

That said, we'd recommend moving to the new consumer if possible (which I realize might not be an option given the version you are on). It has a different API based on single-threaded polling for data -- in that API the timeout is explicit since it is provided to every poll() API call.

(I'm resolving this as it seemed to just be a question, and removed the fix version.)",,,,,,,,,,,,,,,,,,,,
Failure in security rolling upgrade phase 2 system test,KAFKA-3374,12948727,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,fpj,ijuma,ijuma,10/Mar/16 10:01,06/Oct/16 01:51,12/Jan/21 11:54,06/Oct/16 01:51,,,,,0.10.1.0,,,,,system tests,,,,0,,,,"[~geoffra] reported the following a few days ago.

Seeing fairly consistent failures in
""Module: kafkatest.tests.security_rolling_upgrade_test
Class:  TestSecurityRollingUpgrade
Method: test_rolling_upgrade_phase_two
Arguments:
{
  ""broker_protocol"": ""SASL_PLAINTEXT"",
  ""client_protocol"": ""SASL_SSL""
}
Last successful run (git hash): 2a58ba9
First failure: f7887bd
(note failures are not 100% consistent, so there's non-zero chance the commit that introduced the failure is prior to 2a58ba9)

See for example:
http://confluent-kafka-system-test-results.s3-us-west-2.amazonaws.com/2016-03-08--001.1457454171--apache--trunk--f6e35de/report.html",,ewencp,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-08 17:33:24.339,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 06 01:51:17 UTC 2016,,,,,,,"0|i2uggv:",9223372036854775807,,,,,,,,,,,,,,,,"08/Apr/16 17:33;ewencp;Another case of this here: http://confluent-kafka-system-test-results.s3-us-west-2.amazonaws.com/2016-03-08--001.1457454171--apache--trunk--f6e35de/report.html

In both of the test_log.debug files, right before the failure this is logged:

{quote}
Time delta between successively acked messages is large: delta_t_sec: 8.61418795586, prev_message: {u'name': u'producer_send_success', u'time_ms': 14601149
70682, u'partition': 0, u'value': u'15721', u'topic': u'test_topic', u'key': None, u'offset': 2958, u'class': u'class org.apache.kafka.tools.VerifiableProducer'}, current_message: {u'name': u'producer_send_success', u'time_ms': 1460114
979295, u'partition': 2, u'value': u'15716', u'topic': u'test_topic', u'key': None, u'offset': 2774, u'class': u'class org.apache.kafka.tools.VerifiableProducer'}
{quote}

And the consumer times out. It looks like there's some kind of stall on the producer side that then causes the consumer to hit the limit we put in place. It's 8s in last night's run, but was 21s in the older failure. This could just be a matter of timeouts (I don't know the details of this test well enough to say), but there might also be some underlying cause for the stall in the producer.","26/Jul/16 14:59;ewencp;There's also a few cases of this on the 0.10.0 branch, so I'm going to add 0.10.0.1 to the fix version.","06/Oct/16 01:51;ijuma;We think the underlying cause for these failures is the same as KAFKA-3985, which has now been fixed. Closing this.",,,,,,,,,,,,,,,,,,,
Transient system test failure ZooKeeperSecurityUpgradeTest.test_zk_security_upgrade.security_protocol,KAFKA-3985,12991879,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,fpj,hachikuji,hachikuji,22/Jul/16 16:35,05/Oct/16 22:05,12/Jan/21 11:54,05/Oct/16 22:03,0.10.0.0,,,,0.10.1.0,,,,,system tests,,,,0,,,,"Found this in the nightly build on the 0.10.0 branch. Full details here: http://testing.confluent.io/confluent-kafka-0-10-0-system-test-results/?prefix=2016-07-22--001.1469199875--apache--0.10.0--71a598a/.  

{code}
test_id:    2016-07-22--001.kafkatest.tests.core.zookeeper_security_upgrade_test.ZooKeeperSecurityUpgradeTest.test_zk_security_upgrade.security_protocol=SSL
status:     FAIL
run time:   5 minutes 14.067 seconds


    292 acked message did not make it to the Consumer. They are: 11264, 11265, 11266, 11267, 11268, 11269, 11270, 11271, 11272, 11273, 11274, 11275, 11276, 11277, 11278, 11279, 11280, 11281, 11282, 11283, ...plus 252 more. Total Acked: 11343, Total Consumed: 11054. We validated that the first 272 of these missing messages correctly made it into Kafka's data files. This suggests they were lost on their way to the consumer.
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/system-test-kafka-0.10.0/kafka/venv/local/lib/python2.7/site-packages/ducktape/tests/runner.py"", line 106, in run_all_tests
    data = self.run_single_test()
  File ""/var/lib/jenkins/workspace/system-test-kafka-0.10.0/kafka/venv/local/lib/python2.7/site-packages/ducktape/tests/runner.py"", line 162, in run_single_test
    return self.current_test_context.function(self.current_test)
  File ""/var/lib/jenkins/workspace/system-test-kafka-0.10.0/kafka/venv/local/lib/python2.7/site-packages/ducktape/mark/_mark.py"", line 331, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/var/lib/jenkins/workspace/system-test-kafka-0.10.0/kafka/tests/kafkatest/tests/core/zookeeper_security_upgrade_test.py"", line 115, in test_zk_security_upgrade
    self.run_produce_consume_validate(self.run_zk_migration)
  File ""/var/lib/jenkins/workspace/system-test-kafka-0.10.0/kafka/tests/kafkatest/tests/produce_consume_validate.py"", line 79, in run_produce_consume_validate
    raise e
AssertionError: 292 acked message did not make it to the Consumer. They are: 11264, 11265, 11266, 11267, 11268, 11269, 11270, 11271, 11272, 11273, 11274, 11275, 11276, 11277, 11278, 11279, 11280, 11281, 11282, 11283, ...plus 252 more. Total Acked: 11343, Total Consumed: 11054. We validated that the first 272 of these missing messages correctly made it into Kafka's data files. This suggests they were lost on their way to the consumer.
{code}",,fpj,githubbot,granders,hachikuji,ijuma,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-10-04 16:04:57.86,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 05 22:05:04 UTC 2016,,,,,,,"0|i31e3b:",9223372036854775807,,,,,,,,,,,,,,,,"04/Oct/16 16:04;fpj;We have been observing in our system test runs that test cases with SSL enabled fail occasionally. I did some digging with SSL debug enabled and and here is a fairly detailed description of my findings. 

The test case that failed in this particular run is this one:

{noformat}
Module: kafkatest.tests.core.security_rolling_upgrade_test
Class:  TestSecurityRollingUpgrade
Method: test_rolling_upgrade_phase_one
Arguments:
{
  ""client_protocol"": ""SASL_SSL""
}
{noformat}

First, I noticed that that the server (worker10) was failing to complete a connection request from the producer:

{noformat}
kafka-network-thread-1-SASL_SSL-3, WRITE: TLSv1.2 Handshake, length = 1925
kafka-network-thread-1-SASL_SSL-3, READ: TLSv1.2 Handshake, length = 193
kafka-network-thread-1-SASL_SSL-3, fatal error: 80: problem unwrapping net record
javax.net.ssl.SSLProtocolException: Handshake message sequence violation, state = 1, type = 1
{noformat}

The exception indicated an error in the unwrapping of the incoming message:

{noformat}
javax.net.ssl.SSLProtocolException: Handshake message sequence violation, state = 1, type = 1
	at sun.security.ssl.Handshaker.checkThrown(Handshaker.java:1357)
	at sun.security.ssl.SSLEngineImpl.checkTaskThrown(SSLEngineImpl.java:519)
	at sun.security.ssl.SSLEngineImpl.readNetRecord(SSLEngineImpl.java:796)
	at sun.security.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:764)
	at javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:624)
	at org.apache.kafka.common.network.SslTransportLayer.handshakeUnwrap(SslTransportLayer.java:409)
	at org.apache.kafka.common.network.SslTransportLayer.handshake(SslTransportLayer.java:270)
	at org.apache.kafka.common.network.KafkaChannel.prepare(KafkaChannel.java:62)
	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:338)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:291)
	at kafka.network.Processor.poll(SocketServer.scala:476)
	at kafka.network.Processor.run(SocketServer.scala:416)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

Inspecting the producer output, I found the following:

{noformat}
kafka-producer-network-thread | producer-1, fatal error: 46: General SSLEngine problem
sun.security.validator.ValidatorException: PKIX path validation failed: java.security.cert.CertPathValidatorException: 
basic constraints check failed: this is not a CA certificate
%% Invalidated:  [Session-361, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256]
kafka-producer-network-thread | producer-1, SEND TLSv1.2 ALERT:  fatal, description = certificate_unknown
{noformat}

which indicates that the certificate of the CA is incorrect, and it turns out that it is incorrect in this run. The way I observed this is the following. In the {{ServerHello}} response the producer receives from the server, we have as the second element of the certificate chain the following:

{noformat}
chain [1] = [
[
  Version: V3
  Subject: CN=SystemTestCA
  Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11

  Key:  Sun RSA public key, 2048 bits
  modulus: 2421083435994698547407383995133450358448219871045998555961244700979330643844364183298673326405969256709602016
1158944052724951660368237891673420098874800950540649610593248597413809483851739528155298853162531155745397359
0767317213764937849600679327937089735954501718065324256197703519689803978997662398590165914517169395637524460
3378903708218606621761213913048196427063541851690381274617210042171263186499322961280259599484330528665445123
8302313616171177770753250228263123742100612605694409578300489940094647906623975819080502840638336757802617437
298013918600386417877347815394687707255367254823636003381423029812181649
  public exponent: 65537
  Validity: [From: Mon Oct 03 09:35:36 UTC 2016,
               To: Sun Jan 01 09:35:36 UTC 2017]
  Issuer: CN=SystemTestCA
  SerialNumber: [    1f5f5e9e]

Certificate Extensions: 1
[1]: ObjectId: 2.5.29.14 Criticality=false
SubjectKeyIdentifier [
KeyIdentifier [
0000: A5 66 37 91 4F D4 A7 78   DD 33 48 A8 92 0E 62 9C  .f7.O..x.3H...b.
0010: 5E A5 31 EE                                        ^.1.
]
]

]
  Algorithm: [SHA256withRSA]
  Signature:
0000: 4F BC ED FB DC B7 96 D4   30 0E 56 47 3A 17 7C 98  O.......0.VG:...
0010: C7 7A 07 0D 29 DC E7 52   B1 E3 6A B6 E8 30 A5 20  .z..)..R..j..0. 
0020: B5 19 75 41 ED 8E A5 F8   BC D7 E8 B8 B4 31 43 BA  ..uA.........1C.
0030: C2 38 05 19 05 1C 45 49   F3 D6 E3 C1 E4 40 C8 1B  .8....EI.....@..
0040: 4E E2 30 CE A8 03 BC DC   F4 70 A7 86 D8 49 AB 50  N.0......p...I.P
0050: 93 98 64 84 8F FB 0A 61   F5 42 18 C8 98 BB 85 5C  ..d....a.B.....\
0060: E9 AB 15 B1 1A E2 E8 8A   54 1E B6 C0 8A B1 9F D9  ........T.......
0070: FE 31 F3 30 BB EC A1 E8   8B 62 79 78 BD 30 85 6F  .1.0.....byx.0.o
0080: 99 43 DB 0C 8F 53 91 B8   67 01 2F D6 11 5D D1 DD  .C...S..g./..]..
0090: AF E2 A8 32 CB 49 68 28   2C 2F 56 B4 C0 49 0B A9  ...2.Ih(,/V..I..
00A0: 65 30 0D 6A 59 88 89 AD   0A A2 EA 88 7F 2B 5E 2B  e0.jY........+^+
00B0: 5C 2E 31 7E 3A 8E 7E DD   2F 8D 69 12 4C DE E5 32  \.1.:.../.i.L..2
00C0: 0E B7 27 6B 31 4C FA 03   45 6C BB 22 8B 49 E7 5E  ..'k1L..El."".I.^
00D0: 10 5C 52 72 88 F1 D0 7C   B9 E7 EA EC 67 08 2A EF  .\Rr........g.*.
00E0: 3E 32 2C 66 0E 86 5B C5   DD 68 AF B7 CD 90 F0 B7  >2,f..[..h......
00F0: D1 41 37 BE C0 2E EA D2   C1 7C 00 B0 38 08 FD 8E  .A7.........8...

]
***
{noformat}

The certificate, however, should be this one instead, which I could find in the logs corresponding to the producer loading the key entry {{kafka}} from the keystore:

{noformat}
chain [1] = [
[
  Version: V3
  Subject: CN=SystemTestCA
  Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11

  Key:  Sun RSA public key, 2048 bits
  modulus: 21764805780610680096899849598570065114366691738099560028265285678634635260326628439310540620474404121894
28837320034498591154935495719586811871677117856001504917455871043545701765773999211491735917750683751734778415465
6808368130560745100828027834306469567092980787103490844983058203701005538036639256533164454027786830984177893100
1665292571714406816927375223448787431887985822089549650745336073720648023043110616617877325717470252466628447540
7469913561906182551997803303452644603273770169060856283566998276859728657509348800308679584434046601202424040524
2779560327146084301434080054974452408698040373821098393838004417
  public exponent: 65537
  Validity: [From: Mon Oct 03 09:55:15 UTC 2016,
               To: Sun Jan 01 09:55:15 UTC 2017]
  Issuer: CN=SystemTestCA
  SerialNumber: [    313ef821]

Certificate Extensions: 1
[1]: ObjectId: 2.5.29.14 Criticality=false
SubjectKeyIdentifier [
KeyIdentifier [
0000: 3D FA EA B1 6A E6 E3 D4   CF CB 60 8F EB F3 5B 6B  =...j.....`...[k
0010: 7D 77 ED A9                                        .w..
]
]

]
  Algorithm: [SHA256withRSA]
  Signature:
0000: 6E B0 98 B3 70 04 C8 96   63 7C 01 4B CE DF 86 E7  n...p...c..K....
0010: 13 0A 9F 72 8A 5E A5 36   00 02 1F 80 AB 8F E1 AB  ...r.^.6........
0020: A9 61 69 28 76 32 84 73   A9 45 E9 97 3A 8A 92 DF  .ai(v2.s.E..:...
0030: 6F B3 07 E7 D5 DD A6 B7   C9 CD 03 0C DF 58 B6 F4  o............X..
0040: 8B 77 18 69 66 D9 90 18   79 8E 63 AD 91 20 1B C9  .w.if...y.c.. ..
0050: D8 EB 75 4F 6D 4A CE 44   5E 39 7E 82 D2 9D E4 08  ..uOmJ.D^9......
0060: C4 ED 6E 84 17 BE BC 52   C5 61 29 E1 3A 54 58 5E  ..n....R.a).:TX^
0070: B7 1F 2F 2D C9 52 C3 3A   8B 27 14 BD 8F 86 DE 57  ../-.R.:.'.....W
0080: BC E1 F3 B7 5C CA 4B 06   BA B9 CD 6B 4C AF 53 1A  ....\.K....kL.S.
0090: 02 16 0D 1C 3E 5B 14 55   9B 53 26 49 DE E2 6E E4  ....>[.U.S&I..n.
00A0: 20 BE FB BE 64 B6 E9 C9   6B 36 1E 9D F2 24 C8 33   ...d...k6...$.3
00B0: 8A 16 C9 F1 FE 56 EA 4B   F3 11 02 7A 29 CD 0F 9F  .....V.K...z)...
00C0: 6F F1 07 EB 04 73 C0 8F   82 DE FB 34 40 85 BB 87  o....s.....4@...
00D0: 5C C9 43 40 99 EA 6C 63   84 5A 28 E5 3D 38 0A 1A  \.C@..lc.Z(.=8..
00E0: C6 9B 35 41 C3 CC 21 88   6E 76 86 DB C3 1A 1A C3  ..5A..!.nv......
00F0: C9 5F 63 0B 3F A0 10 83   E3 63 D1 47 2E AA 00 56  ._c.?....c.G...V

]
{noformat}


Interestingly, earlier in the logs, I can see that the producer is able to connect to worker2 successfully, and the certificate chain includes precisely this certificate.

Another interesting point is that date in the certificate validity field maps roughly to the start time of the test. The test case that failed started roughly at 10:55 (we use a lag of 1 hour to avoid issues with clock drift), which maps perfectly to the certificate that worked. The broken certificate starts at 9:35, and the test that ran around that time had exactly that CA certificate, this is what I got:

{noformat}
Module: kafkatest.tests.core.consumer_group_command_test
Class:  ConsumerGroupCommandTest
Method: test_describe_consumer_group
Arguments:
{
  ""security_protocol"": ""SSL""
}


===========================================================


CA Certificate

chain [1] = [
[
  Version: V3
  Subject: CN=SystemTestCA
  Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11

  Key:  Sun RSA public key, 2048 bits
  modulus: 2421083435994698547407383995133450358448219871045998555961244700979330643844364183298673326405969256709602016115
8944052724951660368237891673420098874800950540649610593248597413809483851739528155298853162531155745397359076731
7213764937849600679327937089735954501718065324256197703519689803978997662398590165914517169395637524460337890370
8218606621761213913048196427063541851690381274617210042171263186499322961280259599484330528665445123830231361617
1177770753250228263123742100612605694409578300489940094647906623975819080502840638336757802617437298013918600386
417877347815394687707255367254823636003381423029812181649
  public exponent: 65537
  Validity: [From: Mon Oct 03 09:35:36 UTC 2016,
               To: Sun Jan 01 09:35:36 UTC 2017]
  Issuer: CN=SystemTestCA
  SerialNumber: [    1f5f5e9e]

Certificate Extensions: 1
[1]: ObjectId: 2.5.29.14 Criticality=false
SubjectKeyIdentifier [
KeyIdentifier [
0000: A5 66 37 91 4F D4 A7 78   DD 33 48 A8 92 0E 62 9C  .f7.O..x.3H...b.
0010: 5E A5 31 EE                                        ^.1.
]
]

]
  Algorithm: [SHA256withRSA]
  Signature:
0000: 4F BC ED FB DC B7 96 D4   30 0E 56 47 3A 17 7C 98  O.......0.VG:...
0010: C7 7A 07 0D 29 DC E7 52   B1 E3 6A B6 E8 30 A5 20  .z..)..R..j..0. 
0020: B5 19 75 41 ED 8E A5 F8   BC D7 E8 B8 B4 31 43 BA  ..uA.........1C.
0030: C2 38 05 19 05 1C 45 49   F3 D6 E3 C1 E4 40 C8 1B  .8....EI.....@..
0040: 4E E2 30 CE A8 03 BC DC   F4 70 A7 86 D8 49 AB 50  N.0......p...I.P
0050: 93 98 64 84 8F FB 0A 61   F5 42 18 C8 98 BB 85 5C  ..d....a.B.....\
0060: E9 AB 15 B1 1A E2 E8 8A   54 1E B6 C0 8A B1 9F D9  ........T.......
0070: FE 31 F3 30 BB EC A1 E8   8B 62 79 78 BD 30 85 6F  .1.0.....byx.0.o
0080: 99 43 DB 0C 8F 53 91 B8   67 01 2F D6 11 5D D1 DD  .C...S..g./..]..
0090: AF E2 A8 32 CB 49 68 28   2C 2F 56 B4 C0 49 0B A9  ...2.Ih(,/V..I..
00A0: 65 30 0D 6A 59 88 89 AD   0A A2 EA 88 7F 2B 5E 2B  e0.jY........+^+
00B0: 5C 2E 31 7E 3A 8E 7E DD   2F 8D 69 12 4C DE E5 32  \.1.:.../.i.L..2
00C0: 0E B7 27 6B 31 4C FA 03   45 6C BB 22 8B 49 E7 5E  ..'k1L..El."".I.^
00D0: 10 5C 52 72 88 F1 D0 7C   B9 E7 EA EC 67 08 2A EF  .\Rr........g.*.
00E0: 3E 32 2C 66 0E 86 5B C5   DD 68 AF B7 CD 90 F0 B7  >2,f..[..h......
00F0: D1 41 37 BE C0 2E EA D2   C1 7C 00 B0 38 08 FD 8E  .A7.........8...

]
{noformat}

My conclusion is that for some reason worker10 picked an old certificate, while everybody else picked the new one. Inspecting the code, I can’t tell immediately what the problem is. There seem to be a race between deleting the old files in {{SslStores.__init__}} and the call to {{SslStores.generate_and_copy_keystore}}, which is called from {{SecurityConfig.setup_ssl}} and is called per node. Is that at all possible?

cc [~rsivaram], would you have any insight here?","04/Oct/16 21:22;rsivaram;[~fpj] Would it be possible to attach the full logs of the failing test to the JIRA? Thanks.","05/Oct/16 08:46;rsivaram;[~fpj] I would not have expected each test to create its own CA. Do the logs above correspond to https://jenkins.confluent.io/job/system-test-kafka-branch-builder-2/124? The timings seem to match. From the console logs there, CA with start time 09:35:36 would correspond to the CA explicitly generated by {{kafkatest.tests.core.security_test.SecurityTest.test_client_ssl_endpoint_validation_failure}}. I wouldn't have expected to see a CA with start time 09:55:15 at all. Is it possible that there was another system test running on that same host (the tests use a fixed file name for CA and truststore, so the tests would fail if there are multiple instances of system tests or any command that loads the security config class that is run on that host).","05/Oct/16 09:02;fpj;[~rsivaram]

bq. I wouldn't have expected to see a CA with start time 09:55:15 at all. Is it possible that there was another system test running on that same host (the tests use a fixed file name for CA and truststore, so the tests would fail if there are multiple instances of system tests or any command that loads the security config class that is run on that host).

That's a very good point, I'm thinking that the problem is that the CA files are in {{/tmp}}: 

{noformat}
        self.ca_crt_path = ""/tmp/test.ca.crt""
        self.ca_jks_path = ""/tmp/test.ca.jks""
{noformat}

and perhaps we should use {{mkdtemp}} like in {{generate_and_copy_keystore}}. Does it make sense?
","05/Oct/16 09:14;rsivaram;[~fpj] At the moment they use a fixed filename since the file never gets deleted. But I think [~geoffra] is fixing this under KAFKA-4140.","05/Oct/16 16:10;githubbot;GitHub user fpj opened a pull request:

    https://github.com/apache/kafka/pull/1973

    KAFKA-3985: Transient system test failure ZooKeeperSecurityUpgradeTest.test_zk_security_upgrade.security_protocol

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/fpj/kafka KAFKA-3985

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1973.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1973
    
----
commit 28ea6c105fbdd81f0cc5b8687bea4a356223915e
Author: fpj <fpj@apache.org>
Date:   2016-10-05T09:25:46Z

    Adding temporary directory for CA files.

commit e1f8275ac4fcffaaa35310501dfa1821b7c270bf
Author: fpj <fpj@apache.org>
Date:   2016-10-05T09:48:40Z

    Adding self ref to rmtree to avoid compilation error.

commit a673e863ec44ddda357f4e8b34331a2a2e284022
Author: fpj <fpj@apache.org>
Date:   2016-10-05T09:51:46Z

    Fixed another compilation error.

commit 2c2b2258b616c576d86075a61201deb61a7f90ca
Author: fpj <fpj@apache.org>
Date:   2016-10-05T09:55:20Z

    Back with rmtree reference.

commit 210084f50352448cdb52d938301ffa21720967e2
Author: fpj <fpj@apache.org>
Date:   2016-10-05T11:29:51Z

    Switching to use atexit

----
","05/Oct/16 16:26;fpj;I've created a pull request for this, focused on this issue. KAFKA-4140 is a broader set of changes, and it could incorporate the changes I'm proposing in the PR. In fact, it seems to change it in a similar way. 

It is still WIP, though, and might be better for the sake of seeing more green in our builds to check this one in while we wait for KAFKA-4140, but I'm happy with whatever folks prefer.","05/Oct/16 16:57;granders;Yes, using canonical location in /tmp is absolutely problematic if multiple ducktape processes are running. 

[~fpj] getting this fix in for greener builds will be great.","05/Oct/16 22:03;ijuma;Issue resolved by pull request 1973
[https://github.com/apache/kafka/pull/1973]","05/Oct/16 22:05;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1973
",,,,,,,,,,,,
Add system tests for replication throttling (KIP-73),KAFKA-4213,13007300,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,apurva,apurva,apurva,23/Sep/16 23:29,30/Sep/16 19:02,12/Jan/21 11:54,30/Sep/16 19:02,,,,,0.10.1.0,,,,,,,,,0,,,,"Add system tests for replication throttling. The two main things to test are: 

1. kafka-reassign-partitions: in this use case, a new broker is added to a cluster, and we are testing throttling of the partitions being replicated to this cluster. The '--throttle' option of the reassign partitions tool is what we want to test. we will invoke the tool with this option, and assert that the replication takes a minimum amount of time, based on the throttle and the amount of data being replicated.

2. kafka-configs: in this use case, we lost a broker of an existing cluster for whatever reason, and want to re-replicate data to it from some point in time. We want this re-replicated data to be throttled. Again, we will check that the re-replication took at least a certain amount of time based on the value of the throttle and the amount of data being replicated.",,apurva,githubbot,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-09-23 23:32:21.521,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 30 19:02:56 UTC 2016,,,,,,,"0|i3415z:",9223372036854775807,,,,,,,,,,,,,,,,"23/Sep/16 23:32;githubbot;GitHub user apurvam opened a pull request:

    https://github.com/apache/kafka/pull/1903

    KAFKA-4213: First set of system tests for replication throttling 

    Added the first set of system tests for replication quotas. These tests validate throttling behavior during partition reassigment.
    
    Along with this patch are fixes to the test framework which include:
    
    1. KakfaService.verify_replica_reassignment: this method was a no-op and would always return success, as explained in KAFKA-4204. This patch adds a workaround to the problems mentioned there, by grepping correctly for success, failure, and 'in progress' states of partition reassignment.
    2.ProduceConsumeValidateTest.annotate_missing_messages would call missing.pop() to enumerate the first 20 missing messages. This meant that all future counts of what is actually missing would be off by 20, leading to the impression of data loss.
    


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apurvam/kafka throttling-tests

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1903.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1903
    
----
commit 372db72c2da55dd2aba70019b258429855832804
Author: Apurva Mehta <apurva.1618@gmail.com>
Date:   2016-09-09T18:53:42Z

    Merge remote-tracking branch 'apache/trunk' into trunk

commit ae912d444d3fb63c2e5487f88949408e0b1207e9
Author: Jason Gustafson <jason@confluent.io>
Date:   2016-09-09T20:44:55Z

    KAFKA-3807; Fix transient test failure caused by race on future completion
    
    Author: Jason Gustafson <jason@confluent.io>
    
    Reviewers: Dan Norwood <norwood@confluent.io>, Ismael Juma <ismael@juma.me.uk>
    
    Closes #1821 from hachikuji/KAFKA-3807

commit d0a86ffdec330f6e7213a370287a2d81bb93e2bc
Author: Vahid Hashemian <vahidhashemian@us.ibm.com>
Date:   2016-09-10T07:16:23Z

    KAFKA-4145; Avoid redundant integration testing in ProducerSendTests
    
    Author: Vahid Hashemian <vahidhashemian@us.ibm.com>
    
    Reviewers: Ismael Juma <ismael@juma.me.uk>
    
    Closes #1842 from vahidhashemian/KAFKA-4145

commit 42b5583561895e308063ed9e2186d83c83ca35d8
Author: Jason Gustafson <jason@confluent.io>
Date:   2016-09-11T07:46:20Z

    KAFKA-4147; Fix transient failure in ConsumerCoordinatorTest.testAutoCommitDynamicAssignment
    
    Author: Jason Gustafson <jason@confluent.io>
    
    Reviewers: Ismael Juma <ismael@juma.me.uk>
    
    Closes #1841 from hachikuji/KAFKA-4147

commit e7697ad0ab0f292ad1e29d9a159d113574bfcf67
Author: Eric Wasserman <eric.wasserman@gmail.com>
Date:   2016-09-12T01:45:05Z

    KAFKA-1981; Make log compaction point configurable
    
    Now uses LogSegment.largestTimestamp to determine age of segment's messages.
    
    Author: Eric Wasserman <eric.wasserman@gmail.com>
    
    Reviewers: Jun Rao <junrao@gmail.com>
    
    Closes #1794 from ewasserman/feat-1981

commit b36034eaa4eb284fafddb1a7507a2cf187993e62
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-09-12T04:00:32Z

    MINOR: catch InvalidStateStoreException in QueryableStateIntegrationTest
    
    A couple of the tests may transiently fail in QueryableStateIntegrationTest as they are not catching InvalidStateStoreException. This exception is expected during rebalance.
    
    Author: Damian Guy <damian.guy@gmail.com>
    
    Reviewers: Eno Thereska, Guozhang Wang
    
    Closes #1840 from dguy/minor-fix

commit 642b709f919a02379f9d0c9313586b02d179ca78
Author: Tim Brooks <tim@uncontended.net>
Date:   2016-09-13T03:28:01Z

    KAFKA-2311; Make KafkaConsumer's ensureNotClosed method thread-safe
    
    Here is the patch on github ijuma.
    
    Acquiring the consumer lock (the single thread access controls) requires that the consumer be open. I changed the closed variable to be volatile so that another thread's writes will visible to the reading thread.
    
    Additionally, there was an additional check if the consumer was closed after the lock was acquired. This check is no longer necessary.
    
    This is my original work and I license it to the project under the project's open source license.
    
    Author: Tim Brooks <tim@uncontended.net>
    
    Reviewers: Jason Gustafson <jason@confluent.io>
    
    Closes #1637 from tbrooks8/KAFKA-2311

commit ca539df5887bdfdbe86ba45f5514ed54b3b648d4
Author: Dong Lin <lindong28@gmail.com>
Date:   2016-09-14T00:33:54Z

    KAFKA-4158; Reset quota to default value if quota override is deleted
    
    Author: Dong Lin <lindong28@gmail.com>
    
    Reviewers: Joel Koshy <jjkoshy.w@gmail.com>, Jiangjie Qin <becket.qin@gmail.com>
    
    Closes #1851 from lindong28/KAFKA-4158

commit ba712d29eb2880fbf1709b5d0921028735a09f68
Author: Ismael Juma <ismael@juma.me.uk>
Date:   2016-09-14T16:16:29Z

    MINOR: Give a name to the coordinator heartbeat thread
    
    Followed the same naming pattern as the producer sender thread.
    
    Author: Ismael Juma <ismael@juma.me.uk>
    
    Reviewers: Jason Gustafson
    
    Closes #1854 from ijuma/heartbeat-thread-name

commit 7c0f9b70e4e1ff643d953af50ca70e2d448ef431
Author: David Chen <mvjome@gmail.com>
Date:   2016-09-14T17:38:40Z

    KAFKA-4162: Fixed typo ""rebalance""
    
    Author: David Chen <mvjome@gmail.com>
    
    Reviewers: Ewen Cheslack-Postava <ewen@confluent.io>
    
    Closes #1853 from mvj3/KAFKA-4162

commit 5ec0ffb32ef556ff22b24ad239f9aa546ac9783a
Author: Jason Gustafson <jason@confluent.io>
Date:   2016-09-15T01:04:58Z

    KAFKA-4172; Ensure fetch responses contain the requested partitions
    
    Author: Jason Gustafson <jason@confluent.io>
    
    Reviewers: Ismael Juma <ismael@juma.me.uk>
    
    Closes #1857 from hachikuji/KAFKA-4172

commit 8792ef05dc6253dfb1b673832cc0030e8bd3f075
Author: Jason Gustafson <jason@confluent.io>
Date:   2016-09-15T05:31:52Z

    KAFKA-4160: Ensure rebalance listener not called with coordinator lock
    
    Author: Jason Gustafson <jason@confluent.io>
    
    Reviewers: Guozhang Wang <wangguoz@gmail.com>
    
    Closes #1855 from hachikuji/KAFKA-4160

commit 57a69d9cab6884e0ed67f2a91320fc918034bfd2
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-09-15T15:57:48Z

    HOTFIX: fix KafkaStreams SmokeTest
    
    Set the NUM_STREAM_THREADS_CONFIG = 1 in SmokeTestClient as we get locking issues when we have NUM_STREAM_THREADS_CONFIG > 1 and we have Standby Tasks, i.e., replicas. This is because the Standby Tasks can be assigned to the same KafkaStreams instance as the active task, hence the directory is locked
    
    Author: Damian Guy <damian.guy@gmail.com>
    
    Reviewers: Eno Thereska, Guozhang Wang
    
    Closes #1861 from dguy/fix-smoketest

commit 5f555091bdd04cb49acf3da40ca1de84c731f6cb
Author: Bill Bejeck <bbejeck@gmail.com>
Date:   2016-09-16T00:08:00Z

    KAFKA-4131; Multiple Regex KStream-Consumers cause Null pointer exception
    
    Fix for bug outlined in KAFKA-4131
    
    Author: bbejeck <bbejeck@gmail.com>
    
    Reviewers: Damian Guy, Guozhang Wang
    
    Closes #1843 from bbejeck/KAFKA-4131_mulitple_regex_consumers_cause_npe

commit bed93e182a52909a108849a3960123b592937653
Author: Ben Stopford <benstopford@gmail.com>
Date:   2016-09-16T05:25:56Z

    KAFKA-1464; Add a throttling option to the Kafka replication
    
    This applies to Replication Quotas
    based on KIP-73 [(link)](https://cwiki.apache.org/confluence/display/KAFKA/KIP-73+Replication+Quotas) originally motivated by KAFKA-1464.
    
    System Tests Run: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/544/
    
    **This first PR demonstrates the approach**.
    
    **_Overview of Change_**
    The guts of this change are relatively small. Throttling occurs on both leader and follower sides. A single class tracks the throttled throughput in and out of each broker (**_ReplicationQuotaManager_**).
    
    On the follower side, the Follower Throttled Rate is calculated as fetch responses arrive. Then, before the next fetch request is sent, we check to see if the quota is violated, removing throttled partitions from the request if it is. This is all encapsulated in a few lines of code in the **_ReplicaFetcherThread_**. There is existing code to handle temporal back off, if the request ends up being empty.
    
    On the leader side it's a little more complex. When a fetch request arrives in the leader, it is built, partition by partition, in **_ReplicaManager.readFromLocalLog_**. As we put each partition into the fetch response, we check if the total size fits in the current quota. If the quota is exceeded, the partition will not be added to the fetch response. Importantly, we don't increase the quota at this point, we just check to see if the bytes will fit.
    
    Now, if there aren't enough bytes to send the response immediately, which is common if we're catching up and throttled, then the request will be put in purgatory. I've added some simple code to **_DelayedFetch_** to handle throttled partitions (throttled partitions are checked against the quota, rather than the messages available in the log).
    
    When the delayed fetch completes, and exits purgatory, _**ReplicaManager.readFromLocalLog**_ will be called again. This is why _**ReplicaManager.readFromLocalLog**_ does not actually increase the quota, it just checks whether enough bytes are available for a partition.
    
    Finally, when there are enough bytes to be sent, or the delayed fetch times out, the response will be sent. Before it is sent the throttled-outbound-rate is increased, based on the size of throttled partitions being sent. This is at the end of _**KafkaApis.handleFetchRequest**_, exactly where client quotas are recorded.
    
    There is an acceptance test which asserts the whole throttling process stabilises on the desired value. This covers a number of use cases including many-to-many replication. See **_ReplicationQuotaTest_**.
    
    Note:
    It should be noted that this protocol can over-request. The request is built, based on the quota at time t1 (_ReplicaManager.readFromLocalLog_). The bytes in the response are recorded at time t2 (end of _KafkaApis.handleFetchRequest_), where t2 > t1. For this reason I originally included an OverRequestedRate as a JMX metric, but testing has not seen revealed any obvious issue. Over-requesting is quickly compensated by subsequent requests, stabilising close to the quota value.
    
    _**Main stuff left to do:**_
    - The fetch size is currently unbounded. This will be addressed in KIP-74, but we need to ensure this ensures requests don’t go beyond the throttle window.
    - There are two failures showing up in the system tests on this branch:  StreamsSmokeTest.test_streams (which looks like it fails regularly) and OffsetValidationTest.test_broker_rolling_bounce (which I need to look into)
    
    _**Stuff left to do that could be deferred:**_
    - Add the extra metrics specified in the KIP.
    - There are no system tests.
    - There is no validation for the cluster size / throttle combination that could lead to ISR dropouts
    
    Author: Ben Stopford <benstopford@gmail.com>
    
    Reviewers: Ismael Juma <ismael@juma.me.uk>, Apurva Mehta <apurva@confluent.io>, Jun Rao <junrao@gmail.com>
    
    Closes #1776 from benstopford/rep-quotas-v2

commit 7f3f0b1e511cc2579e8c99596b206e51feeb078a
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-09-16T16:58:36Z

    KAFKA-3776: Unify store and downstream caching in streams
    
    This is joint work between dguy and enothereska. The work implements KIP-63. Overview of main changes:
    
    - New byte-based cache that acts as a buffer for any persistent store and for forwarding changes downstream.
    - Forwarding record path changes: previously a record in a task completed end-to-end. Now it may be buffered in a processor node while other records complete in the task.
    - Cleanup and state stores and decoupling of cache from state store and forwarding.
    - More than 80 new unit and integration tests.
    
    Author: Damian Guy <damian.guy@gmail.com>
    Author: Eno Thereska <eno.thereska@gmail.com>
    
    Reviewers: Matthias J. Sax, Guozhang Wang
    
    Closes #1752 from enothereska/KAFKA-3776-poc

commit 5ea969a0d67c7df8344df819076baf6f54570cdb
Author: Randall Hauch <rhauch@gmail.com>
Date:   2016-09-16T21:55:46Z

    KAFKA-4183; Corrected Kafka Connect's JSON Converter to properly convert from null to logical values
    
    The `JsonConverter` class has `LogicalTypeConverter` implementations for Date, Time, Timestamp, and Decimal, but these implementations fail when the input literal value (deserialized from the message) is null.
    
    Test cases were added to check for these cases, and these failed before the `LogicalTypeConverter` implementations were fixed to consider whether the schema has a default value or is optional, similarly to how the `JsonToConnectTypeConverter` implementations do this. Once the fixes were made, the new tests pass.
    
    Author: Randall Hauch <rhauch@gmail.com>
    
    Reviewers: Shikhar Bhushan <shikhar@confluent.io>, Jason Gustafson <jason@confluent.io>
    
    Closes #1867 from rhauch/kafka-4183

commit 8b549e8f6485256cc586ba01ac5178871af21a65
Author: Shikhar Bhushan <shikhar@confluent.io>
Date:   2016-09-16T22:54:33Z

    KAFKA-4173; SchemaProjector should successfully project missing Struct field when target field is optional
    
    Author: Shikhar Bhushan <shikhar@confluent.io>
    
    Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Jason Gustafson <jason@confluent.io>
    
    Closes #1865 from shikhar/kafka-4173

commit c9cec1bdc6abf0058319fae0a944790f3ad4fc7d
Author: Sumit Arrawatia <sumit.arrawatia@gmail.com>
Date:   2016-09-17T03:10:13Z

    KAFKA-4093; Cluster Id (KIP-78)
    
    This PR implements  KIP-78:Cluster Identifiers [(link)](https://cwiki.apache.org/confluence/display/KAFKA/KIP-78%3A+Cluster+Id#KIP-78:ClusterId-Overview) and includes the following changes:
    
    1. Changes to broker code
    	- generate cluster id and store it in Zookeeper
    	- update protocol to add cluster id to metadata request and response
    	- add ClusterResourceListener interface, ClusterResource class and ClusterMetadataListeners utility class
    	- send ClusterResource events to the metric reporters
    2. Changes to client code
    	- update Cluster and Metadata code to support cluster id
    	- update clients for sending ClusterResource events to interceptors, (de)serializers and metric reporters
    3. Integration tests for interceptors, (de)serializers and metric reporters for clients and for protocol changes and metric reporters for broker.
    4. System tests for upgrading from previous versions.
    
    Author: Sumit Arrawatia <sumit.arrawatia@gmail.com>
    Author: Ismael Juma <ismael@juma.me.uk>
    
    Reviewers: Jun Rao <junrao@gmail.com>, Ismael Juma <ismael@juma.me.uk>
    
    Closes #1830 from arrawatia/kip-78

commit 378e30c677c4f9d734ecd6858a925ff309c2ceb1
Author: Rajini Sivaram <rajinisivaram@googlemail.com>
Date:   2016-09-17T17:06:05Z

    KAFKA-3492; Secure quotas for authenticated users
    
    Implementation and tests for secure quotas at <user> and <user, client-id> levels as described in KIP-55. Also adds dynamic default quotas for <client-id>, <user> and <user-client-id>. For each client connection, the most specific quota matching the connection is used, with user quota taking precedence over client-id quota.
    
    Author: Rajini Sivaram <rajinisivaram@googlemail.com>
    
    Reviewers: Jun Rao <junrao@gmail.com>
    
    Closes #1753 from rajinisivaram/KAFKA-3492

commit a78dabbc5a70e0be829d0143258c0c719ebfda74
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-09-17T21:43:43Z

    HOTFIX: Increase timeout for bounce test
    
    Author: Eno Thereska <eno.thereska@gmail.com>
    
    Reviewers: Ismael Juma <ismael@juma.me.uk>
    
    Closes #1874 from enothereska/hotfix-bounce-test

commit ce6f8a6ef73a54989e95d6ad6e4022e192392a8c
Author: Matthias J. Sax <matthias@confluent.io>
Date:   2016-09-17T21:45:29Z

    HOTFIX: changed quickstart donwload from 0.10.0.0 to 0.10.0.1
    
    Author: Matthias J. Sax <matthias@confluent.io>
    
    Reviewers: Ismael Juma <ismael@juma.me.uk>
    
    Closes #1869 from mjsax/hotfix-doc

commit 751622eceaf4eed5532b1e44f1738cbd81ae359d
Author: Grant Henke <granthenke@gmail.com>
Date:   2016-09-17T21:47:56Z

    KAFKA-4157; Transient system test failure in replica_verification_test.test_replica_lags
    
    …t.test_replica_lags
    
    Author: Grant Henke <granthenke@gmail.com>
    
    Reviewers: Ashish Singh <asingh@cloudera.com>, Ismael Juma <ismael@juma.me.uk>
    
    Closes #1849 from granthenke/replica-verification-fix

commit 639b7cd133d62275f6ada3344579ca7f755ef2fb
Author: Jaikiran Pai <jaikiran.pai@gmail.com>
Date:   2016-09-17T22:01:32Z

    MINOR: Update the README.md to include a note about GRADLE_USER_HOME
    
    Trying to build the source and publish it to internal Maven repo, I ran into an issue that I explain in the mailing list discussion here https://www.mail-archive.com/devkafka.apache.org/msg56359.html.
    
    The commit here updates the README.md to make a note that the GRADLE_USER_HOME environment variable plays a role in deciding which file to add the maven configs to.
    
    Author: Jaikiran Pai <jaikiran.pai@gmail.com>
    
    Reviewers: Ismael Juma <ismael@juma.me.uk>
    
    Closes #1837 from jaikiran/readme-update-grade-user-home

commit 6965270a84eee075adf466bd949ae7f1ff41e579
Author: Andrey Neporada <neporada@gmail.com>
Date:   2016-09-18T16:12:53Z

    KAFKA-2063; Bound fetch response size (KIP-74)
    
    This PR is implementation of [KIP-74](https://cwiki.apache.org/confluence/display/KAFKA/KIP-74%3A+Add+Fetch+Response+Size+Limit+in+Bytes) which is originally motivated by [KAFKA-2063](https://issues.apache.org/jira/browse/KAFKA-2063).
    
    Author: Andrey Neporada <neporada@gmail.com>
    Author: Ismael Juma <ismael@juma.me.uk>
    
    Reviewers: Jun Rao <junrao@gmail.com>, Jiangjie Qin <becket.qin@gmail.com>, Jason Gustafson <jason@confluent.io>
    
    Closes #1812 from nepal/kip-74

commit bdc95ea18c01374d26f77eaa157c58f8f30e6edf
Author: Luke Zaparaniuk <luke.zaparaniuk@gmail.com>
Date:   2016-09-18T21:59:00Z

    MINOR: Fix reference to argument in `LogSegment.translateOffset`
    
    Changed the lowerBound argument reference in the summary comment of the translateOffset method to match the actual argument name: startingFilePosition.
    
    Author: Luke Zaparaniuk <luke.zaparaniuk@gmail.com>
    
    Reviewers: Ismael Juma <ismael@juma.me.uk>
    
    Closes #1876 from lukezaparaniuk/patch-1

commit 2a23e81f0d14d55684f6df4d3a717affcb40ab52
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-09-19T17:28:58Z

    HOTFIX: logic in QuerybaleStateIntegrationTest.shouldBeAbleToQueryState incorrect
    
    The logic in `verifyCanGetByKey` was incorrect. It was
    ```
    windowState.size() < keys.length &&
    countState.size() < keys.length &&
    System.currentTimeMillis() < timeout
    ```
    but should be:
    ```
    (windowState.size() < keys.length || countState.size() < keys.length) && System.currentTimeMillis() < timeout
    ```
    
    Author: Damian Guy <damian.guy@gmail.com>
    
    Reviewers: Guozhang Wang <wangguoz@gmail.com>
    
    Closes #1879 from dguy/minor-fix-test

commit 21ab564f4dd8d3d7a443d74f444cee3b6eecc664
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-09-19T17:30:58Z

    KAFKA-4175: Can't have StandbyTasks in KafkaStreams where NUM_STREAM_THREADS_CONFIG > 1
    
    standby tasks should be assigned per consumer not per process
    
    Author: Damian Guy <damian.guy@gmail.com>
    
    Reviewers: Eno Thereska, Guozhang Wang
    
    Closes #1862 from dguy/kafka-4175

commit 184c0a6c04784491055033c747c13633df4ba1c0
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-09-19T18:00:53Z

    KAFKA-4163: NPE in StreamsMetadataState during re-balance operations
    
    During rebalance operations the Cluster object gets set to Cluster.empty(). This can result in NPEs when doing certain operation on StreamsMetadataState. This should throw a StreamsException if the Cluster is empty as it is not yet (re-)initialized
    
    Author: Damian Guy <damian.guy@gmail.com>
    
    Reviewers: Eno Thereska, Guozhang Wang
    
    Closes #1845 from dguy/streams-meta-hotfix

commit 3e983af14c138c555a0443c94c3d05631073c2fd
Author: Rajini Sivaram <rajinisivaram@googlemail.com>
Date:   2016-09-19T19:16:45Z

    KAFKA-4079; Documentation for secure quotas
    
    Details in KIP-55.
    
    Author: Rajini Sivaram <rajinisivaram@googlemail.com>
    
    Reviewers: Jun Rao <junrao@gmail.com>
    
    Closes #1847 from rajinisivaram/KAFKA-4079

----
","23/Sep/16 23:34;githubbot;Github user apurvam closed the pull request at:

    https://github.com/apache/kafka/pull/1903
","23/Sep/16 23:47;githubbot;GitHub user apurvam opened a pull request:

    https://github.com/apache/kafka/pull/1904

    KAFKA-4213: First set of system tests for replication throttling, KIP-73.

    This patch also fixes the following:
    
      1. KafkaService.verify_reassign_partitions did not check whether
    partition reassignment actually completed successfully (KAFKA-4204).
    This patch works around those shortcomings so that we get the right
    signal from this method.
    
      2. ProduceConsumeValidateTest.annotate_missing_messages would call
    `pop' on the list of missing messages, causing downstream methods to get
    incomplete data. We fix that in this patch as well.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apurvam/kafka throttling-tests

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1904.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1904
    
----
commit fe4a0b1070f25e687fb8075210da9c5a356fa1c8
Author: Apurva Mehta <apurva.1618@gmail.com>
Date:   2016-09-23T23:41:02Z

    Initial commit of system tests for replication throttling, KIP-73.
    
    This patch also fixes the following:
    
      1. KafkaService.verify_reassign_partitions did not check whether
    partition reassignment actually completed successfully (KAFKA-4204).
    This patch works around those shortcomings so that we get the right
    signal from this method.
    
      2. ProduceConsumeValidateTest.annotate_missing_messages would call
    `pop' on the list of missing messages, causing downstream methods to get
    incomplete data. We fix that in this patch as well.

----
","30/Sep/16 18:59;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1904
","30/Sep/16 19:02;ijuma;Issue resolved by pull request 1904
[https://github.com/apache/kafka/pull/1904]",,,,,,,,,,,,,,,,,
Reduce time taken to run quota integration tests,KAFKA-4209,13007104,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,23/Sep/16 09:34,29/Sep/16 00:19,12/Jan/21 11:54,29/Sep/16 00:19,0.10.1.0,,,,0.10.2.0,,,,,unit tests,,,,0,,,,"Quota integration tests take over a minute to run each class. Since there are three versions of the test now for client-id, user and <user, client-id>, the total time for these tests has a big impact on the total test run time. ",,githubbot,ijuma,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-09-23 11:09:12.784,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 29 00:19:23 UTC 2016,,,,,,,"0|i33zy7:",9223372036854775807,,,,,,,,,,,,,,,,"23/Sep/16 11:09;githubbot;GitHub user rajinisivaram opened a pull request:

    https://github.com/apache/kafka/pull/1902

    KAFKA-4209: Reduce run time for quota integration tests

    Run quota tests which expect throttling only until the first produce/consume request is throttled.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rajinisivaram/kafka KAFKA-4209

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1902.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1902
    
----
commit 8425363159d5237a89e7ce738f9409daa1429f60
Author: Rajini Sivaram <rajinisivaram@googlemail.com>
Date:   2016-09-23T09:38:31Z

    KAFKA-4209: Reduce run time for quota integration tests

----
","29/Sep/16 00:19;ijuma;Issue resolved by pull request 1902
[https://github.com/apache/kafka/pull/1902]","29/Sep/16 00:19;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1902
",,,,,,,,,,,,,,,,,,,
Add ducktape test for secure->unsecure ZK migration ,KAFKA-2952,12919637,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,fpj,fpj,fpj,07/Dec/15 10:17,28/Sep/16 03:44,12/Jan/21 11:54,,0.9.0.0,,,,,,,,,,,,,0,,,,"We have test cases for the unsecure -> secure path, but not the other way around, We should add it.",,fpj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-12-07 10:17:21.0,,,,,,,"0|i2piwf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add system tests for secure quotas,KAFKA-4055,12997878,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,17/Aug/16 11:04,26/Sep/16 00:04,12/Jan/21 11:54,26/Sep/16 00:04,,,,,0.10.1.0,,,,,system tests,,,,0,,,,"Add system tests for quotas for authenticated users and <user, client> (corresponding to KIP-55). Implementation is being done under KAFKA-3492.",,githubbot,junrao,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-09-15 10:55:50.693,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 26 00:04:24 UTC 2016,,,,,,,"0|i32f3r:",9223372036854775807,,,,,,,,,,,,,,,,"15/Sep/16 10:55;githubbot;GitHub user rajinisivaram opened a pull request:

    https://github.com/apache/kafka/pull/1860

    KAFKA-4055: System tests for secure quotas

    Fix existing client-id quota test which currently don't configure quota overrides correctly. Add new tests for user and (user, client-id) quota overrides and default quotas.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rajinisivaram/kafka KAFKA-4055

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1860.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1860
    
----
commit 01497fdbf6e3883423eb752fbb02ce75f840727c
Author: Rajini Sivaram <rajinisivaram@googlemail.com>
Date:   2016-09-15T08:40:38Z

    KAFKA-4055: System tests for secure quotas

----
","26/Sep/16 00:02;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1860
","26/Sep/16 00:04;junrao;Issue resolved by pull request 1860
[https://github.com/apache/kafka/pull/1860]",,,,,,,,,,,,,,,,,,,
Avoid redundant integration testing in ProducerSendTests,KAFKA-4145,13003707,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vahid,hachikuji,hachikuji,08/Sep/16 22:25,10/Sep/16 07:18,12/Jan/21 11:54,10/Sep/16 07:18,,,,,0.10.1.0,,,,,unit tests,,,,0,,,,"We have a few test cases in {{BaseProducerSendTest}} which probably have little value being tested for both Plaintext and SSL. We can move them to {{PlaintextProducerSendTest}} and save a little bit on the build time. The following tests seem like possible candidates:

1. testSendCompressedMessageWithCreateTime
2. testSendNonCompressedMessageWithCreateTime
3. testSendCompressedMessageWithLogAppendTime
4. testSendNonCompressedMessageWithLogApendTime
5. testAutoCreateTopic
6. testFlush
7. testSendWithInvalidCreateTime
8. testCloseWithZeroTimeoutFromCallerThread
9. testCloseWithZeroTimeoutFromSenderThread",,githubbot,hachikuji,ijuma,singhashish,vahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-09-09 20:42:00.071,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 10 07:17:15 UTC 2016,,,,,,,"0|i33f1b:",9223372036854775807,,ijuma,,,,,,,,,,,,,,"09/Sep/16 20:42;ijuma;I am not sure about the `testClose` ones. It seems like it would be good to test that with SSL as well. We should have a few `send` tests that are tested with multiple protocols.","09/Sep/16 20:56;vahid;How about reducing the above list to:

# testSendCompressedMessageWithLogAppendTime
# testSendNonCompressedMessageWithLogApendTime
# testAutoCreateTopic
# testFlush
# testSendWithInvalidCreateTime
","09/Sep/16 21:03;ijuma;I'd probably remove `testFlush`. The rest sounds good.","09/Sep/16 22:03;singhashish;+1, testFlush has been one of the most flaky tests as well.","09/Sep/16 23:17;githubbot;GitHub user vahidhashemian opened a pull request:

    https://github.com/apache/kafka/pull/1842

    KAFKA-4145: Avoid redundant integration testing in ProducerSendTests

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/vahidhashemian/kafka KAFKA-4145

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1842.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1842
    
----
commit 101ab22d130c9b03ca914edd14f41ada2e7f374b
Author: Vahid Hashemian <vahidhashemian@us.ibm.com>
Date:   2016-09-09T20:43:50Z

    KAFKA-4145: Avoid redundant integration testing in ProducerSendTests

----
","10/Sep/16 07:17;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1842
",,,,,,,,,,,,,,,,
Turn on endpoint validation in SSL system tests,KAFKA-3799,12976351,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,07/Jun/16 08:35,31/Aug/16 16:15,12/Jan/21 11:54,31/Aug/16 16:15,0.10.0.0,,,,0.10.1.0,,,,,system tests,,,,0,,,,Endpoint validation is off by default and currently system tests are run without endpoint validation. It will be better to run system tests with endpoint validation turned on. KAFKA-3665 will be enabling validation by default as well.,,ewencp,githubbot,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-09 07:02:13.575,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 31 16:15:47 UTC 2016,,,,,,,"0|i2z2mn:",9223372036854775807,,,,,,,,,,,,,,,,"09/Jun/16 07:02;githubbot;GitHub user rajinisivaram opened a pull request:

    https://github.com/apache/kafka/pull/1483

    KAFKA-3799: Enable SSL endpoint validation in system tests

    Generate certificates with hostname in SubjectAlternativeName and enable hostname validation.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rajinisivaram/kafka KAFKA-3799

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1483.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1483
    
----
commit 0c86afa6f0c1fb69b5fb700ff7b27c352e3ab3a8
Author: Rajini Sivaram <rajinisivaram@googlemail.com>
Date:   2016-06-09T06:53:54Z

    KAFKA-3799: Enable SSL endpoint validation in system tests

----
","31/Aug/16 16:15;ewencp;Issue resolved by pull request 1483
[https://github.com/apache/kafka/pull/1483]","31/Aug/16 16:15;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1483
",,,,,,,,,,,,,,,,,,,
Add system test for connector failure/restart,KAFKA-3863,12980316,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,17/Jun/16 21:37,23/Jun/16 00:07,12/Jan/21 11:54,23/Jun/16 00:07,,,,,0.10.0.1,0.10.1.0,,,,KafkaConnect,system tests,,,0,,,,We should have system tests covering connector/task failure and the ability to restart through the REST API.,,ewencp,githubbot,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-17 21:43:54.284,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 23 00:07:23 UTC 2016,,,,,,,"0|i2znev:",9223372036854775807,,,,,,,,,,,,,,,,"17/Jun/16 21:43;githubbot;GitHub user hachikuji opened a pull request:

    https://github.com/apache/kafka/pull/1519

    KAFKA-3863: System tests covering connector/task failure and restart

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/hachikuji/kafka KAFKA-3863

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1519.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1519
    
----
commit 5d615cf54319e703e841486844d7b690a84ab1fe
Author: Jason Gustafson <jason@confluent.io>
Date:   2016-06-16T23:48:01Z

    KAFKA-3863: System tests covering connector/task failure and restart

----
","23/Jun/16 00:07;ewencp;Issue resolved by pull request 1519
[https://github.com/apache/kafka/pull/1519]","23/Jun/16 00:07;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1519
",,,,,,,,,,,,,,,,,,,
System tests of config validate and list connectors REST APIs,KAFKA-3520,12956671,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,liquanpei,liquanpei,liquanpei,06/Apr/16 22:00,17/May/16 14:21,12/Jan/21 11:54,13/May/16 01:19,0.10.0.0,,,,0.10.0.0,,,,,KafkaConnect,,,,0,test,,,,,ewencp,githubbot,liquanpei,,,,,,,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-06 22:01:27.762,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 13 01:19:41 UTC 2016,,,,,,,"0|i2vr1z:",9223372036854775807,,,,,,,,,,,,,,,,"06/Apr/16 22:01;githubbot;GitHub user Ishiihara opened a pull request:

    https://github.com/apache/kafka/pull/1195

    KAFKA-3520: Add system tests for REST APIs of list connector plugins and config validation

    @ewen @granders Ready for review. 

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/Ishiihara/kafka system-test

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1195.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1195
    
----
commit f5fb2911c0dc5353d360c9df75ab3cfb44a2ab67
Author: Liquan Pei <liquanpei@gmail.com>
Date:   2016-04-06T21:50:38Z

    Add system tests for REST APIs of list connector plugins and config validation

----
","13/May/16 01:19;ewencp;Issue resolved by pull request 1195
[https://github.com/apache/kafka/pull/1195]",,,,,,,,,,,,,,,,,,,,
Enable query ConsoleConsumer and VerifiableProducer if they shutdown cleanly,KAFKA-3597,12960528,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,apovzner,apovzner,apovzner,20/Apr/16 23:08,17/May/16 14:06,12/Jan/21 11:54,29/Apr/16 17:51,,,,,0.10.0.0,,,,,,,,,0,,,,"It would be useful for some tests to check if ConsoleConsumer and VerifiableProducer shutdown cleanly or not. 

Add methods to ConsoleConsumer and VerifiableProducer that return true if all producers/consumes shutdown cleanly; otherwise false. ",,apovzner,githubbot,gwenshap,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-27 23:01:23.994,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 29 17:51:57 UTC 2016,,,,,,,"0|i2wetz:",9223372036854775807,,,,,,,,,,,,,,,,"27/Apr/16 23:01;githubbot;GitHub user apovzner opened a pull request:

    https://github.com/apache/kafka/pull/1278

    KAFKA-3597: Query ConsoleConsumer and VerifiableProducer if they shutdown cleanly

    Even if a test calls stop() on console_consumer or verifiable_producer, it is still possible that producer/consumer will not shutdown cleanly, and will be killed forcefully after a timeout. It will be useful for some tests to know whether a clean shutdown happened or not. This PR adds methods to console_consumer and verifiable_producer to query whether clean shutdown happened or not.
    
    @hachikuji and/or @granders Please review.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apovzner/kafka kafka-3597

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1278.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1278
    
----
commit 501ae1ad97b6b77a2078ed234bbbbafe9f828ecd
Author: Anna Povzner <anna@confluent.io>
Date:   2016-04-27T22:56:43Z

    KAFKA-3597: Enable query ConsoleConsumer and VerifiableProducer if they shutdown cleanly

----
","29/Apr/16 17:51;gwenshap;Issue resolved by pull request 1278
[https://github.com/apache/kafka/pull/1278]","29/Apr/16 17:51;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1278
",,,,,,,,,,,,,,,,,,,
Add lz4 to parametrized `test_upgrade` system test,KAFKA-3675,12965847,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ijuma,ijuma,ijuma,07/May/16 20:28,17/May/16 14:04,12/Jan/21 11:54,09/May/16 07:22,,,,,0.10.0.0,,,,,,,,,0,,,,KIP-57 fixes the LZ4 framing in message format 0.10.0 and we should verify that this works correctly during upgrades.,,ewencp,githubbot,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-08 07:27:36.545,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 09 07:22:56 UTC 2016,,,,,,,"0|i2xb2f:",9223372036854775807,,ewencp,,,,,,,,,,,,,,"08/May/16 07:27;githubbot;GitHub user ijuma opened a pull request:

    https://github.com/apache/kafka/pull/1343

    KAFKA-3675; Add lz4 to parametrized `test_upgrade` system test

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ijuma/kafka kafka-3675-lz4-test-upgrade

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1343.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1343
    
----
commit 895fc2e77f90278a64bfecbec7732e5bdc42d183
Author: Ismael Juma <ismael@juma.me.uk>
Date:   2016-05-07T20:29:40Z

    Add lz4 to parametrized test_upgrade

----
","09/May/16 07:22;ewencp;Issue resolved by pull request 1343
[https://github.com/apache/kafka/pull/1343]","09/May/16 07:22;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1343
",,,,,,,,,,,,,,,,,,,
Add system tests for connector pause/resume,KAFKA-3676,12965939,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,08/May/16 23:53,17/May/16 14:03,12/Jan/21 11:54,09/May/16 23:56,,,,,0.10.0.0,,,,,KafkaConnect,,,,0,,,,We're missing system test cases for connector pause/resume from KIP-52.,,ewencp,githubbot,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-09 03:34:16.976,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 09 23:57:01 UTC 2016,,,,,,,"0|i2xbmv:",9223372036854775807,,,,,,,,,,,,,,,,"09/May/16 03:34;githubbot;GitHub user hachikuji opened a pull request:

    https://github.com/apache/kafka/pull/1345

    KAFKA-3676: system tests for connector pause/resume

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/hachikuji/kafka KAFKA-3676

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1345.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1345
    
----
commit 1645983c2bad01730a515d274ddfdabcd19b056a
Author: Jason Gustafson <jason@confluent.io>
Date:   2016-05-06T19:24:32Z

    KAFKA-3676: system tests for connector pause/resume

----
","09/May/16 23:56;ewencp;Issue resolved by pull request 1345
[https://github.com/apache/kafka/pull/1345]","09/May/16 23:57;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1345
",,,,,,,,,,,,,,,,,,,
Add ducktape tests for upgrade with SASL,KAFKA-3634,12963438,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,28/Apr/16 09:52,17/May/16 13:48,12/Jan/21 11:54,09/May/16 22:47,,,,,0.10.0.0,,,,,,,,,0,,,,"Add SASL upgrade tests (moved out of KAFKA-2693):
  - 0.9.0.x to 0.10.0 with GSSAPI as inter-broker SASL mechanism
  - Rolling upgrade with change in SASL mechanism",,githubbot,ijuma,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-28 21:33:23.547,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 09 22:47:47 UTC 2016,,,,,,,"0|i2ww73:",9223372036854775807,,,,,,,,,,,,,,,,"28/Apr/16 21:33;ijuma;I think we may already have the first case described above, but please double-check.","29/Apr/16 17:58;githubbot;GitHub user rajinisivaram opened a pull request:

    https://github.com/apache/kafka/pull/1290

    KAFKA-3634: Upgrade tests for SASL authentication

    Add a test for changing SASL mechanism using rolling upgrade and a test for rolling upgrade from 0.9.0.x to 0.10.0 with SASL/GSSAPI.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rajinisivaram/kafka KAFKA-3634

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1290.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1290
    
----
commit def35d33525d7fb5384b6a7af2757f6afcd7428c
Author: Rajini Sivaram <rajinisivaram@googlemail.com>
Date:   2016-04-28T14:56:48Z

    KAFKA-3634: Upgrade tests for SASL authentication

----
","29/Apr/16 18:01;rsivaram;[~ijuma] Upgrade test from 0.9.0.x was being run only with PLAINTEXT. I have added SASL/GSSAPI (don't think this upgrade is being tested elsewhere).","09/May/16 22:47;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1290
","09/May/16 22:47;ijuma;Issue resolved by pull request 1290
[https://github.com/apache/kafka/pull/1290]",,,,,,,,,,,,,,,,,
Add unit tests for SASL authentication,KAFKA-3617,12962299,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,25/Apr/16 17:07,28/Apr/16 20:40,12/Jan/21 11:54,28/Apr/16 20:39,,,,,0.10.0.0,,,,,security,,,,0,,,,"Add unit tests for SASL in the clients project using SASL/PLAIN implementation. Include tests for authentication failures, handshake request flow etc.",,ecomar,githubbot,ijuma,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-27 11:12:26.791,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 28 20:40:01 UTC 2016,,,,,,,"0|i2wp7j:",9223372036854775807,,,,,,,,,,,,,,,,"27/Apr/16 11:12;githubbot;GitHub user rajinisivaram opened a pull request:

    https://github.com/apache/kafka/pull/1273

    KAFKA-3617: Unit tests for SASL authenticator

    Unit tests for SASL authenticator, tests for SASL/PLAIN and multiple mechanisms, authorization test for SASL/PLAIN

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rajinisivaram/kafka KAFKA-3617

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1273.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1273
    
----
commit 732ee449077dcafba694a0c5578704d427fe71b6
Author: Rajini Sivaram <rajinisivaram@googlemail.com>
Date:   2016-04-27T08:26:58Z

    KAFKA-3617: Unit tests for SASL authenticator

----
","28/Apr/16 20:39;ijuma;Issue resolved by pull request 1273
[https://github.com/apache/kafka/pull/1273]","28/Apr/16 20:40;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1273
",,,,,,,,,,,,,,,,,,,
Enable VerifiableProducer and ConsoleConsumer to run with interceptors,KAFKA-3566,12959310,Test,In Progress,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,apovzner,apovzner,apovzner,15/Apr/16 22:56,28/Apr/16 00:08,12/Jan/21 11:54,,0.10.0.0,,,,,,,,,,,,,0,test,,,Add interceptor class list and export path list params to VerifiableProducer and ConsoleConsumer constructors. This is to allow running VerifiableProducer and ConsoleConsumer with interceptors.,,apovzner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2016-04-15 22:56:33.0,,,,,,,"0|i2w7bj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add controller failover to existing replication tests,KAFKA-2825,12912649,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,apovzner,apovzner,apovzner,12/Nov/15 23:01,25/Apr/16 23:25,12/Jan/21 11:54,03/Dec/15 18:45,,,,,0.10.0.0,,,,,,,,,0,,,,"Extend existing replication tests to include controller failover:
* clean/hard shutdown
* clean/hard bounce
",,apovzner,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-11-12 23:01:44.267,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 25 23:25:58 UTC 2016,,,,,,,"0|i2obv3:",9223372036854775807,,guozhang,,,,,,,,,,,,,,"12/Nov/15 23:01;githubbot;GitHub user apovzner opened a pull request:

    https://github.com/apache/kafka/pull/518

    KAFKA-2825: Extended existing ducktape replication tests to include controller failover

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apovzner/kafka cpkafka-86

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/518.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #518
    
----
commit b084c876bfd5d2214640aa75767c6935519ae2db
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-11T22:43:49Z

    Extended existing ducktape replication tests to include controller failover

commit 3bf3b88256c7fb9e13776ed7ab17c61c017b58d7
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-12T04:12:43Z

    Using random dir under /temp for local kdc files to avoid conflicts.

commit 8f210d479bdcf71fb5cb7a3bdebb6a969a7a453e
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-12T21:40:32Z

    Fixed usage of random dir under local /tmp for miniKdc files

commit 7bb75427709810736afe213e83c33fb98d2f6c5a
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-12T21:48:47Z

    Fix to make sure that temp dir for mini kcd files is removed after test finishes.

----
","19/Nov/15 00:14;apovzner;The PR also contains the fix for KAFKA-2851. Will assign the same reviewer.","20/Nov/15 22:05;githubbot;GitHub user apovzner opened a pull request:

    https://github.com/apache/kafka/pull/570

    KAFKA-2825, KAFKA-2852: Controller failover tests added to ducktape replication tests and fix to temp dir

    I closed an original pull request that contained previous comments by Geoff (which are already addressed here), because I got into bad rebase situation. So, I created a new branch and cherry-picked my commits + merged with Ben's changes to fix MiniKDC tests to run on Virtual Box. That change was conflicting with my changes, where I was copying MiniKDC files with new scp method, and temp file was created inside that method. To merge Ben's changes, I added two optional parameters to scp(): 'pattern' and 'subst' to optionally substitute string while spp'ing files, which is needed for krb5.conf file.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apovzner/kafka kafka-2825

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/570.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #570
    
----
commit 44f7a6ea9fbfc11e6eebc44b5bfab36431469522
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-11T22:43:49Z

    Extended existing ducktape replication tests to include controller failover

commit 447e330c840e9a556aff2d05d54f873aee8641a5
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-12T04:12:43Z

    Using random dir under /temp for local kdc files to avoid conflicts.

commit a364a3f289277579f17973829a3d51a97749875c
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-12T21:40:32Z

    Fixed usage of random dir under local /tmp for miniKdc files

commit 5b2c048d1c0922b8d36286d96c90bae53c01a671
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-12T21:48:47Z

    Fix to make sure that temp dir for mini kcd files is removed after test finishes.

commit 18c8670e18cfdb641efe14df3c2479ba7e340e0d
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-17T19:17:05Z

    KAFKA-2825 Moved query zookeeper method from KafkaService to ZookeeperService

commit d70d1eb17fa1ee54dadd52d55ba3e89a81b37c0d
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-17T21:54:04Z

    KAFKA-2851 Added scp method to remote_account utils to scp between two remote nodes through unique local temp file

commit 34830cd67e00f7b28d1afcffe9653fc543bca9d0
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-17T21:59:02Z

    KAFKA-2851: minor fix to format string in utils.remote_coount.scp method

commit e5a28e3e2718e057138043a7d6cbc9d42c17d84e
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-18T02:56:57Z

    KAFKA-2851: clean up temp file even if scp fails

commit bed5a2f1f75462d857fc44382322544e6adc2bb2
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-18T18:17:09Z

    KAFKA-2825 Using only PLAINTEXT and SASL_SSL security protocols for controller failover tests

commit 84d21b6ac1324f7ac2bbc0f908d7a218e95501d5
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-20T21:49:49Z

    Merged with Ben's changes to make MiniKFC tests to run on Virtual Box

commit f0d630907f4eaf14e7274e9075022d949e5b2752
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-20T21:53:43Z

    Very minor changes: typo in output string and some white spaces.

----
","20/Nov/15 22:06;githubbot;Github user apovzner closed the pull request at:

    https://github.com/apache/kafka/pull/518
","02/Dec/15 22:45;githubbot;GitHub user apovzner opened a pull request:

    https://github.com/apache/kafka/pull/618

    KAFKA-2825: Add controller failover to existing replication tests

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apovzner/kafka kafka_2825_01

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/618.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #618
    
----
commit fa0b4156d209522b1fe7656f73bb2792d8c932b3
Author: Anna Povzner <anna@confluent.io>
Date:   2015-12-02T22:38:20Z

    KAFKA-2825: Add controller failover to existing replication tests

----
","03/Dec/15 18:44;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/618
","25/Apr/16 23:25;githubbot;Github user apovzner closed the pull request at:

    https://github.com/apache/kafka/pull/570
",,,,,,,,,,,,,,,
Add consumer system tests for compressed topics,KAFKA-3214,12936925,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,apovzner,hachikuji,hachikuji,05/Feb/16 01:07,26/Feb/16 21:41,12/Jan/21 11:54,26/Feb/16 21:41,,,,,0.10.0.0,,,,,consumer,,,,0,,,,"As far as I can tell, we don't have any ducktape tests which verify correctness when compression is enabled. If we did, we might have caught KAFKA-3179 earlier.",,githubbot,gwenshap,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-02-16 04:32:27.873,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 26 21:41:12 UTC 2016,,,,,,,"0|i2sggv:",9223372036854775807,,,,,,,,,,,,,,,,"16/Feb/16 04:32;githubbot;GitHub user hachikuji opened a pull request:

    https://github.com/apache/kafka/pull/919

    KAFKA-3214 [WIP]: Add Connect status tracking API

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/hachikuji/kafka KAFKA-3214

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/919.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #919
    
----
commit 1a7e3adf19d230792baedd68d2609e58ec7a137d
Author: Jason Gustafson <jason@confluent.io>
Date:   2016-02-10T23:25:25Z

    KAFKA-3214 [WIP]: Add status tracking API

----
","16/Feb/16 04:36;hachikuji;Disregard PR. Put the wrong JIRA by mistake.","23/Feb/16 22:27;githubbot;GitHub user apovzner opened a pull request:

    https://github.com/apache/kafka/pull/958

    KAFKA-3214: Added system tests for compressed topics

    Added the following tests:
    1. Extended TestVerifiableProducer (sanity check test) to test Trunk with snappy compression (one producer/one topic).
    2. Added CompressionTest that tests 3 producers: 2a) each uses a different compression; 2b) each either uses snappy compression or no compression.
    
    Enabled VerifiableProducer to run producers with different compression types (passed in the constructor).
    


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apovzner/kafka kafka-3214

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/958.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #958
    
----
commit 588d4caf3f8830dfcc185da30dfdb40de04cd7cd
Author: Anna Povzner <anna@confluent.io>
Date:   2016-02-23T22:22:34Z

    KAFKA-3214: Added system tests for compressed topics

----
","26/Feb/16 21:41;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/958
","26/Feb/16 21:41;gwenshap;Issue resolved by pull request 958
[https://github.com/apache/kafka/pull/958]",,,,,,,,,,,,,,,,,
Add System (ducktape) Test that asserts strict partition ordering despite node failure,KAFKA-3223,12937818,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,benstopford,benstopford,09/Feb/16 10:12,09/Feb/16 10:15,12/Jan/21 11:54,,,,,,,,,,,,,,,0,,,,"Kafka provides strong ordering when max.inflight.requests.per.connection = 1. There currently exists a bug (patched but not released) against this feature. It's an important feature for many customers so we should add a test to assert that the contract is honoured.

Suggest a similar format to ReassignPartitionsTest (reassign_partitions_test.py) or ReplicationTest (replication_test.py). It should be simply a case of asserting on the order of the messages (which are monotonically increasing numbers in these tests) in each partition with the inflight configuration whilst incurring repeated node failure.

Note that this jira is depended on the merge of KAFKA-3197 ",,benstopford,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2016-02-09 10:12:38.0,,,,,,,"0|i2slqv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Ducktape test for kafka-consumer-groups,KAFKA-2846,12913491,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,singhashish,singhashish,singhashish,16/Nov/15 23:12,24/Jan/16 01:00,12/Jan/21 11:54,24/Jan/16 00:59,,,,,0.10.0.0,,,,,,,,,0,,,,kafka-consumer-groups is a user facing tool. Having system tests will make sure that we are not changing its behavior unintentionally.,,ewencp,githubbot,singhashish,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-11-19 01:06:29.379,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 24 01:00:02 UTC 2016,,,,,,,"0|i2oh1b:",9223372036854775807,,,,,,,,,,,,,,,,"19/Nov/15 01:06;githubbot;GitHub user SinghAsDev opened a pull request:

    https://github.com/apache/kafka/pull/555

    KAFKA-2846: Add Ducktape test for kafka-consumer-groups

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/SinghAsDev/kafka KAFKA-2846

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/555.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #555
    
----
commit 623cffab6f6154242546607ee5acf680aef7c9d8
Author: Ashish Singh <asingh@cloudera.com>
Date:   2015-11-19T00:28:53Z

    KAFKA-2846: Add Ducktape test for kafka-consumer-groups

----
","24/Jan/16 00:59;ewencp;Issue resolved by pull request 555
[https://github.com/apache/kafka/pull/555]","24/Jan/16 01:00;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/555
",,,,,,,,,,,,,,,,,,,
Enable authorizer and ACLs in ducktape tests,KAFKA-2979,12920821,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,fpj,fpj,fpj,10/Dec/15 17:17,08/Jan/16 04:05,12/Jan/21 11:54,08/Jan/16 04:05,0.9.0.0,,,,0.10.0.0,,,,,system tests,,,,0,,,,Add some support to test ACLs with ducktape tests and enable some test cases to use it. ,,ewencp,fpj,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-16 13:59:32.986,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 08 04:05:07 UTC 2016,,,,,,,"0|i2pq7j:",9223372036854775807,,,,,,,,,,,,,,,,"16/Dec/15 13:59;githubbot;GitHub user fpj opened a pull request:

    https://github.com/apache/kafka/pull/683

    KAFKA-2979: Enable authorizer and ACLs in ducktape tests

    Patch by @fpj and @benstopford.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/fpj/kafka KAFKA-2979

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/683.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #683
    
----
commit 5586e3950442060e5c5dc19b89381e86c6d4a04f
Author: flavio junqueira <fpj@apache.org>
Date:   2015-11-27T20:14:35Z

    First cut of the ducktape test.

commit 15c23475ae440c0b77048510b12c69f4941950e1
Author: flavio junqueira <fpj@apache.org>
Date:   2015-11-28T00:00:05Z

    Fixes to references in zookeeper.py.

commit b0ff7f97fee552c6266cc3f5ce09f7e99db97d23
Author: flavio junqueira <fpj@apache.org>
Date:   2015-11-28T04:00:45Z

    Test case passes.

commit 885b42a7e1a8b42b00ed1c8cbb76ba2dc8930757
Author: flavio junqueira <fpj@apache.org>
Date:   2015-11-28T12:35:44Z

    KAFKA-2905: Make zookeeper replicated.

commit ff4e8f75845259d755cc0a0a11008115f5aff7e3
Author: flavio junqueira <fpj@apache.org>
Date:   2015-11-30T15:24:58Z

    KAFKA-2905: Clean up - moved config file, removed warns, moved jaas generation.

commit d78656e6faed82f2c8616f00c1ed1cfed97d2f3f
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-01T16:29:11Z

    KAFKA-2905: jaas reference and generation improvements.

commit 2628db2223818143c1509397aa6c384484525ff4
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-01T16:38:11Z

    KAFKA-2905: Changes to kafka.properties.

commit e78c9b4f3a5bd30bc8cd501076618f0642c5972a
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-01T16:39:18Z

    KAFKA-2905: Increased timeout for producer to get it to pass in my local machine.

commit abb09c007aaf3144853060efdb65cab74a0bd790
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-01T16:41:50Z

    Merge remote-tracking branch 'upstream/trunk' into KAFKA-2905

commit b9d3be240743c0541aaa9369d381562f5dd2969c
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-01T17:01:43Z

    KAFKA-2905: Adding plain_jaas.conf.

commit 85aa0713d86fb6783cbd29709834d2013aa61822
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-01T23:14:55Z

    KAFKA-2905: Removing commented code.

commit 70a21a4c10e474ae5f7996ee3badcfc448494917
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-02T00:14:43Z

    KAFKA-2905: Removed unnecessary sleep.

commit 21fb8ec5ce6711704dfe2217c47040fae7bad323
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-02T00:41:26Z

    KAFKA-2905: Removing PLAIN.

commit dcf76bf3d49680bbd2a07d102d7855d2b08ee6d1
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-02T01:45:52Z

    KAFKA-2905: Removed missing instance of PLAIN.

commit d66dae448a61bc6c12a7c61f9ae9bdf6b75057c2
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-02T09:14:15Z

    KAFKA-2905: Corrected the min isr configuration.

commit de068a2bbfe863ee4be3799fffdcfadff00ba67e
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-02T13:28:50Z

    KAFKA-2905: Changed to Kerberos auth.

commit 95bc8a938af8078fa907c64f1c1983402f19ad48
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-02T18:14:08Z

    KAFKA-2905: Moving system properties to zookeeper.py.

commit fc6ff2eb0578767ea278742a12f26c675b6cfc28
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-02T18:22:05Z

    KAFKA-2905: Remove changes to timeouts in produce_consume_validate.py.

commit 755959504cae1441046c131e5c921cc18c5d5b4b
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-03T00:06:24Z

    KAFKA-2905: Removed change in minikdc.py.

commit 548043593a2ff5711a631452f9e0732420a22dd6
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-03T00:09:04Z

    KAFKA-2905: Missing colon in zookeeper.py.

commit 0d200b7700924e059083e9d8d70d0f9ad7339bd1
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-03T00:13:46Z

    Merge remote-tracking branch 'upstream/trunk' into KAFKA-2905

commit a2b710c97d8bb50d4f7e4336bf855551296b7a08
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-03T01:19:12Z

    KAFKA-2905: Fixed bug in the generation of the principals string.

commit e820d0cd6ff34e7bbf481bcab4fe371f44110828
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-03T11:41:13Z

    KAFKA-2905: Increased zk connect time out and made zk jaas config conditional.

commit 6b279ba4202ad339bc9575c7e5c1a3f9e6085c8f
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-03T18:25:49Z

    KAFKA-2905: Adding test cases with different security protocols.

commit f70b6b2867f3e07b51e87821972d617a4c4395e6
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-03T18:52:49Z

    Merge remote-tracking branch 'upstream/trunk' into KAFKA-2905

commit bae1f9fb1268ee673463b726f8e89657b222f71f
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-03T20:11:03Z

    KAFKA-2905: Added comment and removed dead code.

commit 346f4caf1949154ca4f737828e232c0a87e64a5b
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-10T17:23:45Z

    KAFKA-2979: Initial ACL support for ducktape tests.

commit ce63ee4cc5d88162fd4711e9a832d3fb6b9f918e
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-10T17:29:43Z

    Merge remote-tracking branch 'upstream/trunk' into KAFKA-2979

commit cee61843c58c234e57efb9083ef227e830fbb0f4
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-11T16:43:59Z

    KAFKA-2979: Resolving merge conflicts.

commit c41be92dd988e89d834226e222479c29d2abef88
Author: flavio junqueira <fpj@apache.org>
Date:   2015-12-14T16:19:25Z

    KAFKA-2979: Changes to add authorizer to kafka.properties template.

----
","08/Jan/16 04:05;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/683
","08/Jan/16 04:05;ewencp;Issue resolved by pull request 683
[https://github.com/apache/kafka/pull/683]",,,,,,,,,,,,,,,,,,,
Make EndToEndAuthorizationTest replicated,KAFKA-2949,12919448,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,fpj,fpj,fpj,05/Dec/15 14:40,03/Jan/16 23:26,12/Jan/21 11:54,03/Jan/16 23:26,,,,,0.10.0.0,,,,,,,,,0,,,,"The call to create a topic in the setup method is setting the degree of replication to 1, we should make it 3.",,fpj,githubbot,ijuma,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-05 15:00:23.242,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 03 23:26:23 UTC 2016,,,,,,,"0|i2phqv:",9223372036854775807,,,,,,,,,,,,,,,,"05/Dec/15 15:00;ijuma;Good point. Also, we should configure the producer so that it fails in case replication fails.","05/Dec/15 15:32;githubbot;GitHub user fpj opened a pull request:

    https://github.com/apache/kafka/pull/631

    KAFKA-2949: Make EndToEndAuthorizationTest replicated.

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/fpj/kafka KAFKA-2949

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/631.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #631
    
----
commit cc9757e347858b06fc4afe442adbc82fd0ce841a
Author: Flavio Junqueira <fpj@apache.org>
Date:   2015-12-05T15:31:06Z

    KAFKA-2949: Making topic replicated.

----
","03/Jan/16 23:26;junrao;Issue resolved by pull request 631
[https://github.com/apache/kafka/pull/631]","03/Jan/16 23:26;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/631
",,,,,,,,,,,,,,,,,,
Verify all partitions consumed after rebalancing in system tests,KAFKA-2989,12921775,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,14/Dec/15 21:34,23/Dec/15 00:16,12/Jan/21 11:54,23/Dec/15 00:16,,,,,0.10.0.0,,,,,,,,,0,,,,"In KAFKA-2978, we found a bug which prevented the consumer from fetching some assigned partitions. Our system tests didn't catch the bug because we only assert that some messages from any topic are consumed after rebalancing. We should strengthen these assertions to ensure that each partition is consumed.",,ewencp,githubbot,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-21 18:40:35.073,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 23 00:16:13 UTC 2015,,,,,,,"0|i2pvk7:",9223372036854775807,,,,,,,,,,,,,,,,"21/Dec/15 18:40;githubbot;GitHub user hachikuji opened a pull request:

    https://github.com/apache/kafka/pull/702

    KAFKA-2989: system tests should verify partitions consumed after rebalancing

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/hachikuji/kafka KAFKA-2989

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/702.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #702
    
----
commit f6640867eecd3106c635fc08a0abefa2c2cabc8e
Author: Jason Gustafson <jason@confluent.io>
Date:   2015-12-16T01:49:36Z

    KAFKA-2989: system tests should verify partitions consumed after rebalancing

----
","23/Dec/15 00:16;ewencp;Issue resolved by pull request 702
[https://github.com/apache/kafka/pull/702]","23/Dec/15 00:16;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/702
",,,,,,,,,,,,,,,,,,,
System test for rolling upgrade to enable ZooKeeper ACLs with SASL,KAFKA-2905,12916729,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,fpj,fpj,fpj,28/Nov/15 12:45,04/Dec/15 01:48,12/Jan/21 11:54,04/Dec/15 01:48,0.9.0.0,,,,0.10.0.0,,,,,,,,,0,,,,Write a ducktape test to verify the ability of performing a rolling upgrade to enable the use of secure ACLs and SASL with ZooKeeper.,,fpj,githubbot,guozhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-11-30 20:09:38.28,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 04 01:48:04 UTC 2015,,,,,,,"0|i2p0z3:",9223372036854775807,,,,,,,,,,,,,,,,"30/Nov/15 20:09;guozhang;Github PR: https://github.com/apache/kafka/pull/598","04/Dec/15 01:48;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/598
",,,,,,,,,,,,,,,,,,,,
"Add test cases with ZK Auth, SASL and SSL",KAFKA-2732,12910033,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,fpj,fpj,fpj,03/Nov/15 16:13,28/Nov/15 16:23,12/Jan/21 11:54,28/Nov/15 16:23,0.9.0.0,,,,0.10.0.0,,,,,security,,,,0,,,,Add test cases to verify the security functionality being added in 0.9. ,,fpj,githubbot,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-11-03 16:23:25.186,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 28 16:23:59 UTC 2015,,,,,,,"0|i2nvwn:",9223372036854775807,,,,,,,,,,,,,,,,"03/Nov/15 16:23;githubbot;GitHub user fpj opened a pull request:

    https://github.com/apache/kafka/pull/410

    KAFKA-2732: Add class for ZK Auth.

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/fpj/kafka KAFKA-2732

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/410.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #410
    
----
commit bfb191d08b871b5ce6d71e44af883370725d4164
Author: Flavio Junqueira <fpj@apache.org>
Date:   2015-11-03T16:22:33Z

    KAFKA-2732: Add class for ZK Auth.

----
","28/Nov/15 16:23;junrao;Issue resolved by pull request 410
[https://github.com/apache/kafka/pull/410]","28/Nov/15 16:23;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/410
",,,,,,,,,,,,,,,,,,,
Add certificate authority functionality to TestSslUtils,KAFKA-2895,12916312,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,fpj,fpj,25/Nov/15 23:43,25/Nov/15 23:43,12/Jan/21 11:54,,0.9.0.0,,,,,,,,,clients,security,,,0,,,,The certificates generated in TestSslUtils are currently self-signed. I suggest we simulate the presence of a certificate authority by using the same key to sign certificates. This way we won't have to worry about the order we create the ssl configuration for clients in integration tests as we do currently (see IntegrationTestHarness for an example).,,fpj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-11-25 23:43:12.0,,,,,,,"0|i2oyen:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enhance new consumer integration test coverage,KAFKA-2812,12912320,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,11/Nov/15 22:18,20/Nov/15 23:47,12/Jan/21 11:54,20/Nov/15 23:47,,,,,,,,,,,,,,0,,,,There are still some test cases that we didn't get to in KAFKA-2274 (including hard broker and client failures) as well as additional validation that can be added to existing test cases.,,githubbot,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-11-11 23:38:39.865,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 20 23:47:00 UTC 2015,,,,,,,"0|i2o9v3:",9223372036854775807,,,,,,,,,,,,,,,,"11/Nov/15 23:38;githubbot;GitHub user hachikuji opened a pull request:

    https://github.com/apache/kafka/pull/500

    KAFKA-2812: improve consumer integration tests

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/hachikuji/kafka KAFKA-2812

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/500.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #500
    
----
commit ff9a2dfba81e15170204a21dec12307206e9bd61
Author: Jason Gustafson <jason@confluent.io>
Date:   2015-11-10T23:24:04Z

    KAFKA-2812: improve consumer integration tests

----
","20/Nov/15 23:47;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/500
",,,,,,,,,,,,,,,,,,,,
Integration tests for multi-consumer assignment including session timeouts,KAFKA-2769,12911254,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,apovzner,apovzner,apovzner,07/Nov/15 00:16,10/Nov/15 01:02,12/Jan/21 11:54,10/Nov/15 01:02,,,,,0.9.0.0,,,,,,,,,0,,,,"We currently don't have integration tests for multi-consumer range assignment and session timeout expiration scenarios.
This card is to add multi-consumer integration tests for range assignment:
-- initial assignment 
-- adding a new consumer to an existing group
-- adding and removing a topic to/from subscription
-- session timeout expiration when consumer stops polling and on consumer close().
",,apovzner,githubbot,guozhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-11-09 22:02:59.043,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 10 01:02:02 UTC 2015,,,,,,,"0|i2o3a7:",9223372036854775807,,,,,,,,,,,,,,,,"09/Nov/15 22:02;githubbot;GitHub user apovzner opened a pull request:

    https://github.com/apache/kafka/pull/472

    KAFKA-2769:  Multi-consumer integration tests for consumer assignment incl. session timeouts and corresponding fixes

    -- Refactored multi-consumer integration group assignment validation tests for round-robin assignment
    -- Added multi-consumer integration tests for session timeout expiration:
       1. When a consumer stops polling
        2. When a consumer calls close()
    -- Fixes to issues found with session timeout expiration tests woth help from Jason Gustafson: Try to avoid  SendFailedException exception by cancelling the scheduled tasks and ensuring metadata update before sending group leave requests + send leave group request with retries.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apovzner/kafka cpkafka-81

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/472.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #472
    
----
commit 0dc57aa7bed768559da19707e672dfbb55c0460b
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-06T04:02:55Z

    KAFKA-2769: Multi-consumer integration tests for consumer group subsribe and timeouts and consumer timeout fixes.
    
    -- Refactored multi-consumer integration group assignment validation tests for round-robin assignment
    -- Added multi-consumer integration tests for session timeout expiration
    -- Fixes to issues found with session timeout expiration tests woth help from Jason Gustafson.
        1. shouldKeepMemberAlive(): when we are in the sync phase, we do want to expire members if we don't get any response
        2. Try to avoid  SendFailedException exception by cancelling the scheduled tasks and ensuring metadata update before sending group leave requests.

commit 87554675c0ad5e51602c4cc37a40a9bb273b6dd0
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-07T00:10:04Z

    KAFKA-2769: More reliable sending of leave group request in the consumer on consumer close().

commit 353ef1c079f39ad15a91ba26f64f95c541d517f1
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-09T19:56:36Z

    Reverted a change in shouldKeepMemberAlive() to check for sync callback set

commit 83f703d6cb8720ac12e25692ce273e15813e89f6
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-09T20:52:15Z

    fixed minor test issues

----
","10/Nov/15 01:01;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/472
","10/Nov/15 01:02;guozhang;Issue resolved by pull request 472
[https://github.com/apache/kafka/pull/472]",,,,,,,,,,,,,,,,,,,
Integration tests for round-robin assignment,KAFKA-2737,12910200,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,apovzner,apovzner,apovzner,04/Nov/15 00:21,04/Nov/15 18:06,12/Jan/21 11:54,04/Nov/15 18:06,,,,,0.9.0.0,,,,,,,,,0,,,,We currently don't have integration tests which use round-robin assignment. This card is to add basic integration tests with round-robin assignment for both single-consumer and multi-consumer cases.,,apovzner,githubbot,guozhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-11-04 00:32:31.347,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 04 18:06:18 UTC 2015,,,,,,,"0|i2nwxz:",9223372036854775807,,guozhang,,,,,,,,,,,,,,"04/Nov/15 00:32;githubbot;GitHub user apovzner opened a pull request:

    https://github.com/apache/kafka/pull/413

    KAFKA-2737: Added single- and multi-consumer integration tests for round-robin assignment

    Two tests:
    1. One consumer subscribes to 2 topics, each with 2 partitions; includes adding and removing a topic.
    2. Several consumers subscribe to 2 topics, several partition each; includes adding one more consumer after initial assignment is done and verified.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apovzner/kafka cpkafka-76

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/413.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #413
    
----
commit 6e3a74863b50162bed338e6719af0ddd13109268
Author: Anna Povzner <anna@confluent.io>
Date:   2015-11-04T00:28:25Z

    KAFKA-2737: Added single- and multi-consumer integration tests for round-robin assignment

----
","04/Nov/15 18:06;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/413
","04/Nov/15 18:06;guozhang;Issue resolved by pull request 413
[https://github.com/apache/kafka/pull/413]",,,,,,,,,,,,,,,,,,,
Add integration tests for exceptional cases in Fetching for new consumer,KAFKA-2714,12909330,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,apovzner,apovzner,apovzner,30/Oct/15 21:22,30/Oct/15 21:54,12/Jan/21 11:54,30/Oct/15 21:54,,,,,0.9.0.0,,,,,,,,,0,,,,"We currently don't have integration tests for exceptional cases in fetches for new consumer. This ticket is to create the following test scenarios:
1. When reset policy is NONE, verify that NoOffsetForPartitionException is thrown if no initial position is set.
2. When reset policy is NONE, verify that OffsetOutOfRange is thrown if you seek out of range.
3. Verify RecordTooLargeException is thrown if a message is too large for the configured fetch size.",,apovzner,githubbot,gwenshap,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-10-30 21:25:01.717,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 30 21:54:43 UTC 2015,,,,,,,"0|i2nrif:",9223372036854775807,,,,,,,,,,,,,,,,"30/Oct/15 21:25;githubbot;Github user apovzner closed the pull request at:

    https://github.com/apache/kafka/pull/384
","30/Oct/15 21:26;githubbot;GitHub user apovzner opened a pull request:

    https://github.com/apache/kafka/pull/393

    KAFKA-2714: Added integration tests for exceptional cases in fetching

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apovzner/kafka cpkafka-84

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/393.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #393
    
----
commit 4f175d813270ae2943dd4466c51bedbf11e819ea
Author: Anna Povzner <anna@confluent.io>
Date:   2015-10-29T22:21:01Z

    MINOR: Added integration tests for exceptional cases in fetching

commit 6aa6aaf4b4f3ebb4f101fc0a5088195d52a2a0ba
Author: Anna Povzner <anna@confluent.io>
Date:   2015-10-30T00:00:50Z

    MINOR: Checking correct values in exceptions thrown in integration tests for exceptional cases in fetching

----
","30/Oct/15 21:54;gwenshap;Issue resolved by pull request 393
[https://github.com/apache/kafka/pull/393]","30/Oct/15 21:54;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/393
",,,,,,,,,,,,,,,,,,
Remove static JAAS config file for ZK auth tests,KAFKA-2705,12908905,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,fpj,fpj,fpj,29/Oct/15 14:25,29/Oct/15 19:01,12/Jan/21 11:54,29/Oct/15 19:01,,,,,0.9.0.0,,,,,,,,,0,,,,"We have a static login config file in the resources folder, and it is better for testing to have that file created dynamically. This issue adds this functionality. ",,fpj,githubbot,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-10-29 14:33:52.777,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 29 19:01:12 UTC 2015,,,,,,,"0|i2novz:",9223372036854775807,,,,,,,,,,,,,,,,"29/Oct/15 14:33;githubbot;GitHub user fpj opened a pull request:

    https://github.com/apache/kafka/pull/380

    KAFKA-2705: Remove static JAAS config file for ZK auth tests

    Remove static login config file.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/fpj/kafka KAFKA-2705

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/380.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #380
    
----
commit 943f9e2e98e117709098255c28269d649200abba
Author: Flavio Junqueira <fpj@apache.org>
Date:   2015-10-29T14:26:12Z

    KAFKA-2705: Remove static JAAS config file for ZK auth tests

----
","29/Oct/15 19:01;junrao;Issue resolved by pull request 380
[https://github.com/apache/kafka/pull/380]","29/Oct/15 19:01;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/380
",,,,,,,,,,,,,,,,,,,
Add test to validate times in RequestMetrics,KAFKA-2699,12908446,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,aauradkar,aauradkar,aauradkar,28/Oct/15 04:26,28/Oct/15 04:26,12/Jan/21 11:54,,,,,,,,,,,,,,,0,,,,No tests exist to validate the reported times in RequestMetrics. ,,aauradkar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-10-28 04:26:37.0,,,,,,,"0|i2nm1z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add timeout to ConsoleConsumer running with new consumer,KAFKA-2603,12901987,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,01/Oct/15 20:34,14/Oct/15 20:31,12/Jan/21 11:54,14/Oct/15 20:31,0.9.0.0,,,,0.9.0.0,,,,,,,,,0,,,,"ConsoleConsumer exits when no messages are received for a timeout period when run with the old consumer since the old consumer had a timeout parameter. This behaviour is not available with the new consumer and hence ducktape tests which rely on the timeout cannot be run with the new consumer. 

[~granders] has suggested a solution in KAFKA-2581.",,githubbot,guozhang,ijuma,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-10-04 16:21:27.897,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 14 20:31:29 UTC 2015,,,,,,,"0|i2mimf:",9223372036854775807,,guozhang,,,,,,,,,,,,,,"04/Oct/15 16:21;githubbot;GitHub user rajinisivaram opened a pull request:

    https://github.com/apache/kafka/pull/274

    KAFKA-2603: Add timeout arg to ConsoleConsumer 

    Added --timeout-ms argument to ConsoleConsumer that works with both old and new consumer. Also modified ducktape ConsoleConsumer service to use this arg instead of consumer.timeout.ms config that works only with the old consumer.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rajinisivaram/kafka KAFKA-2603

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/274.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #274
    
----
commit f76a27d6c7580bd38f1fe6792f8f4754485b3787
Author: Rajini Sivaram <rajinisivaram@googlemail.com>
Date:   2015-10-04T16:08:25Z

    KAFKA-2603: Add timeout arg to ConsoleConsumer that works with old and new consumer, use arg in ducktape service

----
","04/Oct/15 16:24;rsivaram;The PR adds a new argument -timeout-ms with the same semantics as consumer.timeout.ms in the old consumer. The new arg is supported with both old and new consumer. 

The ducktape ConsoleConsumer service uses the new arg instead of setting timeout in the consumer config file so that it can be used with both the old and new consumer.
","12/Oct/15 16:05;ijuma;Setting [~guozhang] as the reviewer. This change is needed in order to complete KAFKA-2581.","14/Oct/15 20:31;guozhang;Issue resolved by pull request 274
[https://github.com/apache/kafka/pull/274]","14/Oct/15 20:31;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/274
",,,,,,,,,,,,,,,,,
Add Ducktape based tests for KafkaLog4jAppender,KAFKA-2531,12863124,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,singhashish,singhashish,singhashish,10/Sep/15 17:04,27/Sep/15 01:33,12/Jan/21 11:54,27/Sep/15 01:33,,,,,0.9.0.0,,,,,,,,,0,,,,Add ducktape based tests for KafkaLog4jAppender.,,githubbot,gwenshap,singhashish,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-09-23 06:04:22.067,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 27 01:33:11 UTC 2015,,,,,,,"0|i2k0bz:",9223372036854775807,,,,,,,,,,,,,,,,"23/Sep/15 06:04;githubbot;GitHub user SinghAsDev opened a pull request:

    https://github.com/apache/kafka/pull/235

    KAFKA-2531: Add Ducktape based tests for KafkaLog4jAppender

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/SinghAsDev/kafka KAFKA-2531

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/235.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #235
    
----
commit a0880587b94a20c97475de4bc27db3d0fe14221b
Author: Ashish Singh <asingh@cloudera.com>
Date:   2015-09-22T23:21:27Z

    KAFKA-2531: Add Ducktape based tests for KafkaLog4jAppender

----
","27/Sep/15 01:33;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/235
","27/Sep/15 01:33;gwenshap;Issue resolved by pull request 235
[https://github.com/apache/kafka/pull/235]",,,,,,,,,,,,,,,,,,,
Add additional unit tests for new consumer Fetcher,KAFKA-2340,12845453,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,16/Jul/15 00:39,06/Aug/15 22:52,12/Jan/21 11:54,06/Aug/15 22:52,,,,,0.9.0.0,,,,,,,,,0,,,,"There are a number of cases in Fetcher which have no corresponding unit tests. To name a few:

- list offset with partition leader unknown
- list offset disconnect
- fetch disconnect

Additionally, updateFetchPosition (which was moved from KafkaConsumer) has no tests.",,aravind.selvan,githubbot,guozhang,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-08-05 00:42:09.427,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 06 22:52:09 UTC 2015,,,,,,,"0|i2hbcf:",9223372036854775807,,,,,,,,,,,,,,,,"05/Aug/15 00:42;githubbot;GitHub user hachikuji opened a pull request:

    https://github.com/apache/kafka/pull/112

    KAFKA-2340; improve KafkaConsumer Fetcher test coverage

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/hachikuji/kafka KAFKA-2340

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/112.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #112
    
----
commit d6424e566f20d521f535d0116af5bcd50ff1f249
Author: Jason Gustafson <jason@confluent.io>
Date:   2015-08-05T00:40:25Z

    KAFKA-2340; improve KafkaConsumer Fetcher test coverage

----
","06/Aug/15 22:51;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/112
","06/Aug/15 22:52;guozhang;Issue resolved by pull request 112
[https://github.com/apache/kafka/pull/112]",,,,,,,,,,,,,,,,,,,
TEST JIRA for KAFKA-1536,KAFKA-1538,12726976,Test,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Invalid,guozhang,guozhang,guozhang,13/Jul/14 01:40,17/Jul/14 16:59,12/Jan/21 11:54,17/Jul/14 16:59,,,,,,,,,,,,,,0,,,,,,guozhang,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/14 01:41;guozhang;KAFKA-1538.patch;https://issues.apache.org/jira/secure/attachment/12655431/KAFKA-1538.patch",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2014-07-17 16:59:45.747,,,false,,,,,,,,,,,,,,,,,405083,,,Thu Jul 17 16:59:45 UTC 2014,,,,,,,"0|i1xppz:",405119,,,,,,,,,,,,,,,,"13/Jul/14 01:41;guozhang;Created reviewboard https://reviews.apache.org/r/23445/
 against branch origin/trunk","17/Jul/14 16:59;nehanarkhede;Closing this as the test succeeded. ",,,,,,,,,,,,,,,,,,,,
remove unused test cases,KAFKA-1411,12709642,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,21/Apr/14 23:21,22/Apr/14 20:56,12/Jan/21 11:54,22/Apr/14 20:56,0.8.2.0,,,,0.8.2.0,,,,,core,,,,0,,,,"We have some redundant tests for producer.
1. Tests in LazyInitProducerTest are redundant from those in PrimitiveApiTest.
2. AsyncProducerTest.testBrokerListAndAsync() just returns immediately. There are other tests that cover the same functionality.
3. PrimitiveApiTest: The compression property wasn't set properly. Also, there is no need to test compression with other combinations such as encoder/multi-fetch.
",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/14 23:34;junrao;KAFKA-1411.patch;https://issues.apache.org/jira/secure/attachment/12641146/KAFKA-1411.patch",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,387964,,,Tue Apr 22 20:56:29 UTC 2014,,,,,,,"0|i1utz3:",388223,,,,,,,,,,,,,,,,"21/Apr/14 23:34;junrao;Created reviewboard https://reviews.apache.org/r/20541/
 against branch origin/trunk","22/Apr/14 20:56;junrao;Thanks for the review. Committed to trunk.",,,,,,,,,,,,,,,,,,,,
enable log4j in unit test,KAFKA-1285,12697988,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,28/Feb/14 18:43,28/Feb/14 21:54,12/Jan/21 11:54,28/Feb/14 21:54,0.8.2.0,,,,0.8.2.0,,,,,,,,,0,,,,,,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/14 18:45;junrao;KAFKA-1285.patch;https://issues.apache.org/jira/secure/attachment/12631809/KAFKA-1285.patch",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,376462,,,Fri Feb 28 21:54:56 UTC 2014,,,,,,,"0|i1svdz:",376758,,,,,,,,,,,,,,,,"28/Feb/14 18:45;junrao;Created reviewboard https://reviews.apache.org/r/18636/
 against branch origin/trunk","28/Feb/14 21:54;junrao;Thanks for the review. Committed to trunk.",,,,,,,,,,,,,,,,,,,,
Add Broker Failure Test for a Single Host,KAFKA-227,12535506,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jfung,jfung,jfung,16/Dec/11 21:06,06/Jan/12 00:45,12/Jan/21 11:54,06/Jan/12 00:45,0.7.1,,,,0.7.1,,,,,clients,,,,0,,,,"Add this test suite under ""system_test"" folder. It performs broker failure test with a mirroring setup in a single machine.

The setup contains a source brokers cluster and a mirror brokers cluster. The brokers in both clusters will be bounced alternately.

Message checksum is printed to the corresponding producer and consumer log files. The checksum will be validated at the end of the test. A simple logic will be used to determine how many messages are lost or duplicated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/12 19:37;jfung;KAFKA-227-v3.patch;https://issues.apache.org/jira/secure/attachment/12509598/KAFKA-227-v3.patch","16/Dec/11 21:07;jfung;KAFKA-227.patch;https://issues.apache.org/jira/secure/attachment/12507725/KAFKA-227.patch",,,,,,2.0,,,,,,,,,,,,,,,,,,,,2012-01-05 20:15:20.775,,,false,,,,,,,,,,,,,,,,,221190,,,Fri Jan 06 00:45:15 UTC 2012,,,,,,,"0|i0kqo7:",119138,,,,,,,,,,,,,,,,"05/Jan/12 19:37;jfung;KAFKA-227-v3.patch has the following changes:

1. Terminating processes cleanly
2. Restarted Kafka broker log messages are appended to the log file of a previously terminated broker (instead of overwritten to)
3. Report producer and source / mirror consumer message count for duplicate and missing messages
4. Producer is running in the background
5. Calling ConsumerOffsetChecker to wait for zero consumer offset lagging.","05/Jan/12 20:15;nehanarkhede;This is a great patch ! Just one comment. The *sh scripts in the system_test/broker_failure need to be given execute permissions. Apart from that, it works well.","06/Jan/12 00:45;nehanarkhede;I made the svn propset changes and committed this patch. This system test has been instrumental in testing KAFKA-228 and I see how this can be useful in testing more Kafka bug fixes and features",,,,,,,,,,,,,,,,,,,
A new unit test for ByteBufferMessageSet iterator,KAFKA-108,12519153,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,17/Aug/11 17:39,12/Sep/11 16:43,12/Jan/21 11:54,12/Sep/11 16:43,,,,,0.7,,,,,,,,,0,,,,"We need to add a unit test that iterates ByteBufferMessageSet with compression and without. Also, test that ByteBufferMessageSet can be iterated multiple times.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/11 17:41;junrao;kafka-108.patch;https://issues.apache.org/jira/secure/attachment/12490675/kafka-108.patch","17/Aug/11 22:56;junrao;kafka-108.patch.v2;https://issues.apache.org/jira/secure/attachment/12490709/kafka-108.patch.v2",,,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-08-19 00:29:50.51,,,false,,,,,,,,,,,,,,,,,60852,,,Fri Aug 19 00:29:50 UTC 2011,,,,,,,"0|i15yyf:",242963,,,,,,,,,,,,,,,,"17/Aug/11 17:41;junrao;Patch attached.","17/Aug/11 22:56;junrao;Attaching patch v2. The unit test fails and exposes a bug on handling compressed empty ByteBufferMessageSet.","19/Aug/11 00:29;nehanarkhede;+1. All tests look good.",,,,,,,,,,,,,,,,,,,
Revisit and improve the tests of MirrorMaker 2,KAFKA-10304,13319019,Test,Patch Available,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,yangguo1220,yangguo1220,yangguo1220,23/Jul/20 17:54,16/Dec/20 16:05,12/Jan/21 11:54,,,,,,2.8.0,,,,,KafkaConnect,mirrormaker,,,0,,,,"In a different MM2 change, [some concerns|https://github.com/apache/kafka/pull/9029#issuecomment-663094946] on tests were raised. It may be a good time to revisit and refactor the tests, possibly in the following way:

(1) are 100 messages good enough for integration tests?
 (2) what about the broker failure in the middle of integration tests?
 (3) other validations to check (e.g. topic config sync....)",,yangguo1220,,,,,,,,,,,,,,,,,,,KAFKA-10339,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 09 06:32:10 UTC 2020,,,,,,,"0|z0h54o:",9223372036854775807,,,,,,,,,,,,,,,,"09/Sep/20 06:21;yangguo1220;https://github.com/apache/kafka/pull/9224","09/Sep/20 06:32;yangguo1220;Hi [~mimaison] [~ryannedolan] this is mostly a refactoring pr on the MM2 integration tests. The purpose of doing that: (1) address the concern in [a previous PR|https://github.com/apache/kafka/pull/9029#issuecomment-663094946], (2) prepare for the future development (e.g. extract common functions). I think the current PR ([https://github.com/apache/kafka/pull/9224]) is just a starting point, and I am very appreciated for your feedback on what to test additionally and how to get close to the real scenario.",,,,,,,,,,,,,,,,,,,,
fix flaky MetricsTest.testGeneralBrokerTopicMetricsAreGreedilyRegistered,KAFKA-9786,13294934,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Won't Fix,chia7712,chia7712,chia7712,30/Mar/20 08:43,11/Dec/20 03:14,12/Jan/21 11:54,11/Dec/20 03:14,,,,,,,,,,,,,,0,flaky-test,,,"{code:java}
java.lang.AssertionError: expected:<18> but was:<23>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:633)
	at kafka.metrics.MetricsTest.testGeneralBrokerTopicMetricsAreGreedilyRegistered(MetricsTest.scala:108)
{code}


As gradle may use same JVM to run multiples test (see https://docs.gradle.org/current/dsl/org.gradle.api.tasks.testing.Test.html#org.gradle.api.tasks.testing.Test:forkEvery), the metrics from other tests can break MetricsTest.testGeneralBrokerTopicMetricsAreGreedilyRegistered. 

{code:scala}
  private def topicMetrics(topic: Option[String]): Set[String] = {
    val metricNames = KafkaYammerMetrics.defaultRegistry.allMetrics().keySet.asScala.map(_.getMBeanName)
    filterByTopicMetricRegex(metricNames, topic)
  }
{code}

MetricsTest.testGeneralBrokerTopicMetricsAreGreedilyRegistered is not captured by QA since the test which leaves orphan metrics in JVM is ReplicaManagerTest and it belongs to integrationTest. By contrast, MetricsTest is a part of unitTest. Hence, they are NOT executed by same JVM (since they are NOT in the same task). MetricsTest.testGeneralBrokerTopicMetricsAreGreedilyRegistered fails frequently on my jenkins because my jenkins verify kafka by running ""./gradlew clean core:test"".",,chia7712,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-03-30 09:30:31.357,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 30 09:30:31 UTC 2020,,,,,,,"0|z0d2bs:",9223372036854775807,,,,,,,,,,,,,,,,"30/Mar/20 09:30;githubbot;chia7712 commented on pull request #8385: #9786 KAFKA-9786 fix flaky MetricsTest.testGeneralBrokerTopicMetricsA…
URL: https://github.com/apache/kafka/pull/8385
 
 
   ```
   java.lang.AssertionError: expected:<18> but was:<23>
   	at org.junit.Assert.fail(Assert.java:89)
   	at org.junit.Assert.failNotEquals(Assert.java:835)
   	at org.junit.Assert.assertEquals(Assert.java:647)
   	at org.junit.Assert.assertEquals(Assert.java:633)
   	at kafka.metrics.MetricsTest.testGeneralBrokerTopicMetricsAreGreedilyRegistered(MetricsTest.scala:108)
   ```
   
   As gradle may use same JVM to run multiples test (see https://docs.gradle.org/current/dsl/org.gradle.api.tasks.testing.Test.html#org.gradle.api.tasks.testing.Test:forkEvery), the metrics from other tests can break ```MetricsTest.testGeneralBrokerTopicMetricsAreGreedilyRegistered```. 
   
   ```
     private def topicMetrics(topic: Option[String]): Set[String] = {
       val metricNames = KafkaYammerMetrics.defaultRegistry.allMetrics().keySet.asScala.map(_.getMBeanName)
       filterByTopicMetricRegex(metricNames, topic)
     }
   ```
   
   ```MetricsTest.testGeneralBrokerTopicMetricsAreGreedilyRegistered``` is not captured by QA since the guy who leaves orphan metrics in JVM is ```ReplicaManagerTest``` and it belongs to ```integrationTest```. By contrast, ```MetricsTest``` is a part of ```unitTest```. Hence, they are NOT executed by same JVM (since they are NOT in the same task). ```MetricsTest.testGeneralBrokerTopicMetricsAreGreedilyRegistered``` fails frequently on my jenkins because my jenkins verify kafka by running ""./gradlew clean core:test"".
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,
Build / JDK 8 / org.apache.kafka.common.security.authenticator.SaslAuthenticatorTest.testValidSaslPlainOverSsl failed,KAFKA-10797,13343792,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,monty,monty,monty,02/Dec/20 14:41,02/Dec/20 15:01,12/Jan/21 11:54,,,,,,,,,,,build,,,,0,,,,"There is one testcase running failed on jdk 8 version when submmit the pr:[https://github.com/apache/kafka/pull/9675.] but seems that no related among them.  

And there is no problem when build on jdk 11&15version.

[https://github.com/apache/kafka/pull/9675/checks?check_run_id=1485855276]

 

!image-2020-12-02-23-01-00-412.png!

 ",jdk 8 ,monty,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/20 15:01;monty;image-2020-12-02-23-01-00-412.png;https://issues.apache.org/jira/secure/attachment/13016366/image-2020-12-02-23-01-00-412.png","02/Dec/20 14:50;monty;截屏2020-12-02 下午10.35.10.png;https://issues.apache.org/jira/secure/attachment/13016364/%E6%88%AA%E5%B1%8F2020-12-02+%E4%B8%8B%E5%8D%8810.35.10.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-12-02 14:41:54.0,,,,,,,"0|z0l4rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integration tests for each CLI command to make sure it continues working with existing minimal authorizations,KAFKA-10610,13335444,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,kindkid,kindkid,14/Oct/20 16:28,14/Oct/20 16:28,12/Jan/21 11:54,,,,,,,,,,,tools,,,,0,,,,"It would be nice to have test coverage of all CLI commands (kafka-topics, kafka-acls, kafka-configs, kafka-console-consumer, etc) to ensure that they work for a user given the minimal permissions expected for each command.

This will help to catch regressions where a change to an existing command's functionality unwittingly requires expanded permissions.

An example regression these kinds of tests would have caught: https://issues.apache.org/jira/browse/KAFKA-10212",,kindkid,wsun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-10-14 16:28:18.0,,,,,,,"0|z0jpc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add missing tests for IOExceptions for GlobalStateManagerImpl,KAFKA-10351,13320767,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,showuon,mjsax,mjsax,04/Aug/20 02:11,01/Oct/20 01:27,12/Jan/21 11:54,30/Sep/20 18:27,,,,,2.7.0,,,,,streams,unit tests,,,0,beginner,newbie,,"As mentioned in [https://github.com/apache/kafka/pull/9047#pullrequestreview-456899096] we should improve test coverage for IOExceptions, ie, we don't die and verify the WARN log (using `LogCaptureAppender`)",,ableegoldman,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-08-04 02:11:31.0,,,,,,,"0|z0hfw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consider providing standard set of users in system tests,KAFKA-10443,13324900,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,rndgstn,rndgstn,27/Aug/20 19:40,27/Aug/20 19:40,12/Jan/21 11:54,,,,,,,,,,,system tests,,,,0,,,,"As part of the KIP-554 implementation we decided to exercise the AdminClient interface for creating SCRAM credentials within the system tests.  So instead of bootstrapping both the broker and the user credentials via ZooKeeper (`kafka-configs.sh --alter --zookeeper`) before the broker starts, we bootstrapped just the broker credential via ZooKeeper and then we started the brokers and created the user credential afterwards via the AdminClient (`kafka-configs.sh --alter --bootstrap-server`).  We did this by configuring the admin client to log in as the broker.  This works fine, but it feels like we should have a separate ""admin"" user available to do this rather than having to authenticate the admin client as the broker.  Furthermore, this feels like it might be a good pattern to consider everywhere -- whenever we create a broker user we should also create an admin user for tests that want/need to leverage it.",,rndgstn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-08-27 19:40:45.0,,,,,,,"0|z0i5a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connect system tests should wait for workers to join group,KAFKA-10286,13317525,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,gharris1727,gharris1727,gharris1727,18/Jul/20 06:59,22/Jul/20 18:10,12/Jan/21 11:54,20/Jul/20 14:07,2.6.0,,,,2.3.2,2.4.2,2.5.1,2.6.0,2.7.0,KafkaConnect,,,,0,,,,"There are a few flakey test failures for {{connect_distributed_test}} in which one of the workers does not join the group quickly, and the test fails in the following manner:
 # The test starts each of the connect workers, and waits for their REST APIs to become available
 # All workers start up, complete plugin scanning, and start their REST API
 # At least one worker kicks off an asynchronous job to join the group that hangs for a yet unknown reason (30s timeout)
 # The test continues without all of the members joined
 # The test makes a call to the REST api that it expects to succeed, and gets an error
 # The test fails without the worker ever joining the group

Instead of allowing the test to fail in this manner, we could wait for each worker to join the group with the existing 60s startup timeout. This change would go into effect for all system tests using the {{ConnectDistributedService}}, currently just {{connect_distributed_test}} and {{connect_rest_test}}. 

Alternatively we could retry the operation that failed, or ensure that we use a known-good worker to continue the test, but these would require more involved code changes. The existing wait-for-startup logic is the most natural place to fix this issue.",,gharris1727,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-07-18 06:59:42.0,,,,,,,"0|z0gvx4:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConnectDistributedTest.test_bounce should wait for graceful stop,KAFKA-10295,13318163,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,gharris1727,gharris1727,gharris1727,20/Jul/20 05:53,20/Jul/20 14:07,12/Jan/21 11:54,20/Jul/20 14:07,2.3.1,2.4.1,2.5.0,2.6.0,2.3.2,2.4.2,2.5.1,2.6.0,2.7.0,KafkaConnect,,,,0,,,,"In ConnectDistributedTest.test_bounce, there are flakey failures that appear to follow this pattern:
 # The test is parameterized for hard bounces, and with Incremental Cooperative Rebalancing enabled (does not appear for protocol=eager)
 # A source task is on a worker that will experience a hard bounce
 # The source task has written records which it has not yet committed in source offsets
 # The worker is hard-bounced, and the source task is lost
 # Incremental Cooperative Rebalance starts it's scheduled.rebalance.max.delay.ms delay before recovering the task
 # The test ends, connectors and Connect are stopped
 # The test verifies that the sink connector has only written records that have been committed by the source connector
 # This verification fails because the source offsets are stale, and there are un-committed records in the topic, and the sink connector has written at least one of them.

This can be addressed by ensuring that the test waits for the rebalance delay to expire, and for the lost task to recover and commit offsets past the progress it made before the bounce.",,gharris1727,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10296,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-07-20 05:53:20.0,,,,,,,"0|z0gzuw:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky HighAvailabilityTaskAssignorIntegrationTest.shouldScaleOutWithWarmupTasksAndPersistentStores,KAFKA-10184,13312273,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,vvcephei,guozhang,guozhang,18/Jun/20 16:32,22/Jun/20 17:44,12/Jan/21 11:54,,,,,,,,,,,streams,unit tests,,,0,,,,"{code}
Stacktrace
java.lang.AssertionError: Condition not met within timeout 120000. Input records haven't all been written to the changelog: 442
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:26)
	at org.apache.kafka.test.TestUtils.lambda$waitForCondition$6(TestUtils.java:401)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:449)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:417)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:398)
	at org.apache.kafka.streams.integration.HighAvailabilityTaskAssignorIntegrationTest.shouldScaleOutWithWarmupTasks(HighAvailabilityTaskAssignorIntegrationTest.java:149)
	at org.apache.kafka.streams.integration.HighAvailabilityTaskAssignorIntegrationTest.shouldScaleOutWithWarmupTasksAndPersistentStores(HighAvailabilityTaskAssignorIntegrationTest.java:91)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:119)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:414)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.lang.Thread.run(Thread.java:748)
{code}

I've only seen this once from PR builds, likely just a flaky test itself. cc [~ableegoldman][~vvcephei]
",,ableegoldman,guozhang,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-06-18 16:47:35.083,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 19 01:19:59 UTC 2020,,,,,,,"0|z0fzqw:",9223372036854775807,,,,,,,,,,,,,,,,"18/Jun/20 16:47;ableegoldman;Yeah, it's failing on the setup and hasn't even gotten to the real test at all cc/ [~vvcephei] seems like ""500"" was still too high :/","18/Jun/20 16:49;ableegoldman;[~vvcephei] Could we maybe do something like ""write as many records as you can in the 120000ms timeout""? Since as you said the whole point is just to make sure we have enough records to represent a ""reasonably large"" number, if it takes 2 minutes to write only 100 records then those 100 records represent a heavy load (apparently...)","18/Jun/20 16:51;ableegoldman;Or instead (or in addition to the above) maybe we should wait for the streams to be in RUNNING before we start the timeout for writing records","19/Jun/20 01:16;vvcephei;What in the world... How can we not have processed even 500 records in two minutes?

I agree waiting for start up first would probably help. Do we have any logs that could confirm the hypothesis that the startup phase is eating up a bunch of our timeout?

We should probably decrease the size of the records down from a whopping 1kB. My intent was to bridge the integration and system test worlds by creating “realistic” data here, but maybe that was expecting too much of the CIT infrastructure. ","19/Jun/20 01:19;ableegoldman;This is all pretty pathetic lol. Good 'ol Jenkins

Decreasing the message size might help though. I don't have any real evidence to back up the claim that startup might be occupying a large portion of this time, it just seems like good practice. Who knows what Jenkins is doing – maybe it's taking so long just to create topics?",,,,,,,,,,,,,,,,,
QueryableStateIntegrationTest may fail with JDK 7,KAFKA-5842,13100005,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,yuzhihong@gmail.com,yuzhihong@gmail.com,06/Sep/17 01:52,31/Mar/20 23:41,12/Jan/21 11:54,31/Mar/20 23:41,,,,,,,,,,unit tests,,,,0,,,,"Found the following when running test suite for 0.11.0.1 RC0 :
{code}
org.apache.kafka.streams.integration.QueryableStateIntegrationTest > concurrentAccesses FAILED
    java.lang.AssertionError: Key not found one
        at org.junit.Assert.fail(Assert.java:88)
        at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.verifyGreaterOrEqual(QueryableStateIntegrationTest.java:893)
        at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.concurrentAccesses(QueryableStateIntegrationTest.java:399)
{code}",,ewencp,guozhang,mjsax,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-10-24 00:34:39.581,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 31 23:41:15 UTC 2020,,,,,,,"0|i3jonz:",9223372036854775807,,,,,,,,,,,,,,,,"24/Oct/17 00:34;mjsax;Happened again: https://builds.apache.org/blue/organizations/jenkins/kafka-0.11.0-jdk7/detail/kafka-0.11.0-jdk7/323/tests","21/Nov/17 22:49;ewencp;I've also now seen this in a JDK8 job too on the 0.11.0 branch: https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/9576/

{code}
java.lang.AssertionError: Key not found forest
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.verifyGreaterOrEqual(QueryableStateIntegrationTest.java:886)
	at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.concurrentAccesses(QueryableStateIntegrationTest.java:399)
{code}","31/Mar/20 23:41;guozhang;Hopefully it has fixed via https://github.com/apache/kafka/pull/8370",,,,,,,,,,,,,,,,,,,
Improve tests to detect post-processing failures in Jetty request handling.,KAFKA-9622,13288155,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,rigelbm,rigelbm,27/Feb/20 21:57,28/Feb/20 16:13,12/Jan/21 11:54,,,,,,,,,,,,,,,0,,,,"There was a recent jetty-server version bump to [9.4.26.v20200117|https://mvnrepository.com/artifact/org.eclipse.jetty/jetty-server/9.4.26.v20200117], that caused errors in request post-processing. Jetty version [9.4.25.v20191220|https://mvnrepository.com/artifact/org.eclipse.jetty/jetty-server/9.4.25.v20191220] deleted a method which was still required by the Jersey version in use. 

That particular error did not surface in any tests because it only happens on request post-processing, i.e. after the response has already been sent to the client. From the test's point of view, the request behaved as expected. Internally on Jetty though, post-processing crashes and is aborted. That would include server bookkeeping, like freeing resources, etc. 

It would be ideal to have a way to verify that request handling completed successfully in the tests, after the response is validated.",,ijuma,kkonstantine,rigelbm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-02-27 22:06:27.047,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 28 16:13:41 UTC 2020,,,,,,,"0|z0bz8w:",9223372036854775807,,,,,,,,,,,,,,,,"27/Feb/20 22:06;ijuma;[~kkonstantine] Since Connect is the main user of Jetty in Kafka, any thoughts?","28/Feb/20 08:09;kkonstantine;Indeed, Connect uses Jetty for its REST API. 
Maybe a test that would verify proper shutdown of the Jetty server wouldn't be a bad idea, as long as it runs dependably.

Do you have any logs or evidence on how this error would surface with Jetty in Connect [~rigelbm]? It would be good to know whether a test case fits better with our integration tests or the system tests. Also, if you'd be interested to submit a PR with such a test I'd be happy to assign this Jira ticket to you. ","28/Feb/20 16:13;rigelbm;See [https://github.com/confluentinc/rest-utils/pull/173] for an example.",,,,,,,,,,,,,,,,,,,
Transient failure in KafkaAdminClientTest.testListOffsets,KAFKA-9463,13280890,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,yuzhihong@gmail.com,yuzhihong@gmail.com,22/Jan/20 02:55,22/Jan/20 02:55,12/Jan/21 11:54,,,,,,,,,,,,,,,0,,,,"When running tests with Java 11, I got the following test failure:
{code}
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment.
	at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)
	at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)
	at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)
	at org.apache.kafka.clients.admin.KafkaAdminClientTest.testListOffsets(KafkaAdminClientTest.java:2336)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:288)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:282)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment.
{code}
KafkaAdminClientTest.testListOffsets passes when it is run alone.",,mimaison,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-01-22 02:55:47.0,,,,,,,"0|z0aqxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add more unit tests for Materialized class,KAFKA-9334,13276379,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,sainath.batthala,sainath.batthala,sainath.batthala,25/Dec/19 19:28,31/Dec/19 20:30,12/Jan/21 11:54,31/Dec/19 20:30,,,,,2.5.0,,,,,unit tests,,,,0,newbie,,,"Add more unit tests for org.apache.kafka.streams.kstream.Materialized class.
For example:
There is unit test case for max allowed store length validation
There is no unit test case for negative retention ",,githubbot,mjsax,sainath.batthala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-12-26 02:39:04.36,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 31 20:29:45 UTC 2019,,,,,,,"0|z09zts:",9223372036854775807,,,,,,,,,,,,,,,,"26/Dec/19 02:39;githubbot;SainathB commented on pull request #7871: KAFKA-9334: Added more unit tests for Materialized class
URL: https://github.com/apache/kafka/pull/7871
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","26/Dec/19 20:08;mjsax;Thanks for the ticket and PR. I added you to the list of contributors and assigned the ticket to you. You can now also self-assign tickets.","31/Dec/19 20:29;githubbot;mjsax commented on pull request #7871: KAFKA-9334: Added more unit tests for Materialized class
URL: https://github.com/apache/kafka/pull/7871
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,
Using Failify for e2e testing of Kafka,KAFKA-8192,13226281,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,arminbalalaie,arminbalalaie,05/Apr/19 00:06,05/Apr/19 00:06,12/Jan/21 11:54,,,,,,,,,,,,,,,0,test-framework,,,"Hi,

 

I am the author of Failify, a test framework for end-to-end testing of distributed systems. Failify can be used to deterministically inject failures during a normal test case execution. Currently, node failure, network partition, network delay, network packet loss, and clock drift is supported. For a few supported languages (right now, Java and Scala), it is possible to enforce a specific order between nodes in order to reproduce a specific time-sensitive scenario and inject failures before or after a specific method is called when a specific stack trace is present. You can find more information in [https://failify.io|https://failify.io/].

 

My reasons for Failify being useful to Kafka are:
 * It is Docker-based and less messy and you can run the test cases in a single node and in parallel (there are plans for implementing the ability of deploying the same test case on a K8S or a Swarm cluster).
 * It is Docker-based so you can easily have test cases that run on different OSes. Also, you can define the services you depend on e.g. ZK as another node in your deployment definition.
 * The failure kinds supported are a superset of what is being supported now by Trogdor (in particular, Network delay and loss, clock drift and a more sophisticated network partitioning)
 * There will be more control over when a failure should be introduced in a test case.
 * You can write your test cases in Java or Scala or any other language that can be run on JVM and can use Java libraries.
 * It can be easily integrated into your build pipeline as you will be writing your regular JUnit test cases.
 * The API is compact and intuitive and there is a good documentation for the tool

 

Please let me know if you want to give it a try. ",,arminbalalaie,ivanyu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-04-05 00:06:25.0,,,,,,,"0|z01hi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReassignPartitionsClusterTest#shouldMoveSubsetOfPartitions is flaky,KAFKA-6736,13149396,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Cannot Reproduce,,yuzhihong@gmail.com,yuzhihong@gmail.com,02/Apr/18 02:58,22/Feb/19 03:08,12/Jan/21 11:54,07/May/18 19:18,,,,,,,,,,,,,,0,,,,"Saw this from https://builds.apache.org/job/kafka-trunk-jdk8/2518/testReport/junit/kafka.admin/ReassignPartitionsClusterTest/shouldMoveSubsetOfPartitions/ :

{code}
kafka.common.AdminCommandFailedException: Partition reassignment currently in progress for Map(topic1-0 -> Buffer(100, 102), topic1-2 -> Buffer(100, 102), topic2-1 -> Buffer(101, 100), topic2-2 -> Buffer(100, 102)). Aborting operation
	at kafka.admin.ReassignPartitionsCommand.reassignPartitions(ReassignPartitionsCommand.scala:612)
	at kafka.admin.ReassignPartitionsCommand$.executeAssignment(ReassignPartitionsCommand.scala:215)
	at kafka.admin.ReassignPartitionsClusterTest.shouldMoveSubsetOfPartitions(ReassignPartitionsClusterTest.scala:242)
{code}",,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-7977,KAFKA-6410,KAFKA-5183,KAFKA-6078,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-04-02 02:58:25.0,,,,,,,"0|i3s167:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make the time out for connect service to be dynamic,KAFKA-7594,13196312,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,mageshn,mageshn,mageshn,05/Nov/18 18:14,05/Nov/18 18:14,12/Jan/21 11:54,,,,,,,,,,,,,,,0,,,,"Currently, the timeout for Connect service to be started in the ducktape tests are hardcoded at 60 sec. The proposal is to make it configurable via the service init. The default would still be 60 seconds. This will allow downstream connectors to pass their own timeout values.",,mageshn,rayokota,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-11-05 18:14:48.0,,,,,,,"0|s004y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce dependency on mock in connector tests,KAFKA-5943,13103637,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,yuzhihong@gmail.com,yuzhihong@gmail.com,20/Sep/17 15:08,20/Aug/18 22:16,12/Jan/21 11:54,,,,,,,,,,,unit tests,,,,0,connector,mock,,"Currently connector tests make heavy use of mock (easymock, power mock).

This may hide the real logic behind operations and makes finding bugs difficult.


We should reduce the use of mocks so that developers can debug connector code using unit tests.
This would shorten the development cycle for connector.",,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-09-20 15:08:40.0,,,,,,,"0|i3kaxz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InternalTopicIntegrationTest sometimes fails,KAFKA-7049,13165644,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Cannot Reproduce,,yuzhihong@gmail.com,yuzhihong@gmail.com,12/Jun/18 18:30,02/Aug/18 21:38,12/Jan/21 11:54,02/Aug/18 21:36,,,,,,,,,,streams,unit tests,,,0,,,,"Saw the following based on commit fa1d0383902260576132e09bdf9efcc2784b55b4 :
{code}
org.apache.kafka.streams.integration.InternalTopicIntegrationTest > shouldCompactTopicsForKeyValueStoreChangelogs FAILED
    java.lang.RuntimeException: Timed out waiting for completion. lagMetrics=[0/2] totalLag=[0.0]
        at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitForCompletion(IntegrationTestUtils.java:227)
        at org.apache.kafka.streams.integration.InternalTopicIntegrationTest.shouldCompactTopicsForKeyValueStoreChangelogs(InternalTopicIntegrationTest.java:164)
{code}",,guozhang,mjsax,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-08-02 21:38:46.94,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 02 21:38:46 UTC 2018,,,,,,,"0|i3us7r:",9223372036854775807,,,,,,,,,,,,,,,,"02/Aug/18 01:48;yuzhihong@gmail.com;Haven't seen this error for a little while.","02/Aug/18 21:38;guozhang;It maybe fixed as part of https://github.com/apache/kafka/commit/96c53e96b8834c550e6db54b2748c08e545f9150 or https://github.com/apache/kafka/commit/d9d0d79287eeec0a1c3dcc2203288421284b5ca1.",,,,,,,,,,,,,,,,,,,,
Fix broken links in Dockerfile,KAFKA-6564,13138768,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,asorokhtey,asorokhtey,15/Feb/18 13:16,19/Jun/18 14:35,12/Jan/21 11:54,19/Jun/18 14:35,,,,,1.0.2,,,,,,,,,0,,,,"https://github.com/apache/kafka/blob/1.0.0/tests/docker/Dockerfile
{noformat}
# Install binary test dependencies.
ENV MIRROR=""http://mirrors.ocf.berkeley.edu/apache/""
RUN mkdir -p ""/opt/kafka-0.8.2.2"" && curl -s ""${MIRROR}kafka/0.8.2.2/kafka_2.10-0.8.2.2.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.8.2.2""
RUN mkdir -p ""/opt/kafka-0.9.0.1"" && curl -s ""${MIRROR}kafka/0.9.0.1/kafka_2.11-0.9.0.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.9.0.1""
RUN mkdir -p ""/opt/kafka-0.10.0.1"" && curl -s ""${MIRROR}kafka/0.10.0.1/kafka_2.11-0.10.0.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.10.0.1""
RUN mkdir -p ""/opt/kafka-0.10.1.1"" && curl -s ""${MIRROR}kafka/0.10.1.1/kafka_2.11-0.10.1.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.10.1.1""
RUN mkdir -p ""/opt/kafka-0.10.2.1"" && curl -s ""${MIRROR}kafka/0.10.2.1/kafka_2.11-0.10.2.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.10.2.1""
RUN mkdir -p ""/opt/kafka-0.11.0.0"" && curl -s ""${MIRROR}kafka/0.11.0.0/kafka_2.11-0.11.0.0.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.11.0.0""
{noformat}
This links seems to be broken and automated tests executed on docker fails with error:
{noformat}
log: /bin/sh -c mkdir -p ""/opt/kafka-0.8.2.2"" && curl -s ""${MIRROR}kafka/0.8.2.2/kafka_2.10-0.8.2.2.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.8.2.2""' returned a non-zero code: 2
{noformat}

 ",,asorokhtey,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-02-15 13:25:58.759,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 19 14:35:07 UTC 2018,,,,,,,"0|i3q867:",9223372036854775807,,,,,,,,,,,,,,,,"15/Feb/18 13:25;omkreddy;This was fixed in -KAFKA-6247- ","15/Feb/18 13:51;asorokhtey;Hi [~omkreddy],

Thanks for quick response,

I see that it's fixed for versions higher than 1.0.0.

Version  1.0.0 still not fixed.

Thanks, Andriy","19/Jun/18 14:35;omkreddy;This will get fixed in upcoming 1.0.2 release.",,,,,,,,,,,,,,,,,,,
"A question about broker down , the server is doing partition master election,the client producer may send msg fail . How the producer deal with the situation ??",KAFKA-4870,13049488,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Information Provided,,zhaoziyan,zhaoziyan,09/Mar/17 03:36,18/Jun/18 18:38,12/Jan/21 11:54,18/Jun/18 18:38,,,,,,,,,,clients,,,,0,,,,"the broker down . The kafka cluster is doing partion  master election , the producer send order msg or nomal msg ,the producer may send msg fail .How client update metadata and deal with the msg send fail ?? ",java client ,cmccabe,omkreddy,zhaoziyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-03-21 20:32:31.793,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 18 18:38:11 UTC 2018,,,,,,,"0|i3b5on:",9223372036854775807,,,,,,,,,,,,,,,,"21/Mar/17 20:32;cmccabe;This seems like a question, rather than a bug.  Did you ask about this on the mailing list?","22/Mar/17 03:26;zhaoziyan;thanks ","18/Jun/18 18:38;omkreddy;If the produce request fails, the producer automatically retry based on retries config for any retry exceptions. Also Producer updates the metadata for any exceptions or if any partitions does not have leader etc..

Post these kind of queries to [users@kafka.apache.org|mailto:users@kafka.apache.org] mailing list ([http://kafka.apache.org/contact]) for  quicker responses.",,,,,,,,,,,,,,,,,,,
Rewrite KStreamPeekTest at processor level avoiding driver usage,KAFKA-5739,13095019,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,ppatierno,ppatierno,ppatierno,16/Aug/17 13:27,04/Jun/18 05:44,12/Jan/21 11:54,04/Jun/18 05:44,,,,,2.0.0,,,,,streams,,,,0,,,,"Hi,
as already done for the {{KStreamPrintTest}} we could remove the usage of {{KStreamTestDriver}} even in the {{KStreamPeekTest}} and testing it at processor level not at stream level.
My proposal is to :

* create the {{KStreamPeek}} instance providing the action which fill a collection as already happens today
* testing for both {{forwardDownStream}} values true and false
* using the {{MockProcessorContext}} class for overriding the {{forward}} method filling a streamObserved collection as happens today {{forwardDownStream}} is true; checking that the {{forward}} isn't called when {{forwardDownStream}} is false (so the test fails)

Thanks,
Paolo ",,guozhang,mjsax,ppatierno,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-06-04 05:44:32.65,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 04 05:44:32 UTC 2018,,,,,,,"0|i3iunb:",9223372036854775807,,,,,,,,,,,,,,,,"16/Aug/17 13:27;ppatierno;[~damianguy] what do you think about that ?","04/Jun/18 05:44;guozhang;This is already fixed in trunk, coming in 2.0.0",,,,,,,,,,,,,,,,,,,,
ConsumerBounceTest#testClose sometimes fails,KAFKA-6698,13146751,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Cannot Reproduce,,yuzhihong@gmail.com,yuzhihong@gmail.com,21/Mar/18 08:05,29/May/18 03:17,12/Jan/21 11:54,29/May/18 03:17,,,,,,,,,,,,,,0,,,,"Saw the following in https://builds.apache.org/job/kafka-1.1-jdk7/94/testReport/junit/kafka.api/ConsumerBounceTest/testClose/ :
{code}
org.apache.kafka.common.errors.TimeoutException: The consumer group command timed out while waiting for group to initialize: 
Caused by: org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
{code}",,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 21 08:06:44 UTC 2018,,,,,,,"0|i3rkx3:",9223372036854775807,,,,,,,,,,,,,,,,"21/Mar/18 08:06;yuzhihong@gmail.com;Test output consisted of repeated occurrence of:

{code}
[2018-03-21 07:02:01,683] ERROR ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes (org.apache.zookeeper.server.ZooKeeperServer:472)
[2018-03-21 07:02:02,693] WARN Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1162)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-03-21 07:02:03,794] WARN Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1162)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
{code}",,,,,,,,,,,,,,,,,,,,,
EosIntegrationTest#shouldNotViolateEosIfOneTaskFails is flaky,KAFKA-6875,13157683,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Cannot Reproduce,,yuzhihong@gmail.com,yuzhihong@gmail.com,07/May/18 19:21,22/May/18 23:57,12/Jan/21 11:54,22/May/18 23:57,,,,,,,,,,streams,unit tests,,,0,newbie++,,,"From https://builds.apache.org/job/kafka-trunk-jdk10/81/testReport/junit/org.apache.kafka.streams.integration/EosIntegrationTest/shouldNotViolateEosIfOneTaskFails/ :
{code}
java.lang.AssertionError: Condition not met within timeout 60000. SteamsTasks did not request commit.
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:276)
	at org.apache.kafka.streams.integration.EosIntegrationTest.shouldNotViolateEosIfOneTaskFails(EosIntegrationTest.java:339)
{code}
From test output:
{code}
[2018-05-07 19:04:18,236] ERROR [Controller id=2 epoch=3] Controller 2 epoch 3 failed to change state for partition __transaction_state-34 from OnlinePartition to OnlinePartition (state.change.logger:76)
kafka.common.StateChangeFailedException: Failed to elect leader for partition __transaction_state-34 under strategy ControlledShutdownPartitionLeaderElectionStrategy
	at kafka.controller.PartitionStateMachine.$anonfun$doElectLeaderForPartitions$9(PartitionStateMachine.scala:328)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at kafka.controller.PartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:326)
	at kafka.controller.PartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:254)
	at kafka.controller.PartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:175)
	at kafka.controller.PartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:116)
	at kafka.controller.KafkaController$ControlledShutdown.doControlledShutdown(KafkaController.scala:1055)
	at kafka.controller.KafkaController$ControlledShutdown.$anonfun$process$1(KafkaController.scala:1031)
	at scala.util.Try$.apply(Try.scala:209)
	at kafka.controller.KafkaController$ControlledShutdown.process(KafkaController.scala:1031)
	at kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:69)
{code}",,mjsax,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/18 19:23;yuzhihong@gmail.com;EosIntegrationTest.out;https://issues.apache.org/jira/secure/attachment/12922315/EosIntegrationTest.out",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2018-05-07 20:13:19.693,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 07 21:49:24 UTC 2018,,,,,,,"0|i3tfmf:",9223372036854775807,,,,,,,,,,,,,,,,"07/May/18 19:24;yuzhihong@gmail.com;Test output was truncated:
{code}
[2018-05-07 19:01:22,039] WARN [Producer clientId=producer-14] Co
...[truncated 3848723 chars]...
Try.scala:209)
{code}","07/May/18 20:13;mjsax;Did you see this error:

{noformat}
org.apache.kafka.streams.errors.StreamsException: stream-thread [appId-1-d0047b36-c57d-4a10-a22e-5aee4ea3d449-StreamThread-39] Failed to rebalance. at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:840) at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:788) at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:749) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:718) Caused by: org.apache.kafka.streams.errors.ProcessorStateException: task directory [/tmp/kafka-17851027973441391293/appDir/appId-1/0_1] doesn't exist and couldn't be created at org.apache.kafka.streams.processor.internals.StateDirectory.directoryForTask(StateDirectory.java:98) at org.apache.kafka.streams.processor.internals.ProcessorStateManager.<init>(ProcessorStateManager.java:70) at org.apache.kafka.streams.processor.internals.AbstractTask.<init>(AbstractTask.java:90) at org.apache.kafka.streams.processor.internals.StreamTask.<init>(StreamTask.java:161) at org.apache.kafka.streams.processor.internals.StreamTask.<init>(StreamTask.java:146) at org.apache.kafka.streams.processor.internals.StreamThread$TaskCreator.createTask(StreamThread.java:429) at org.apache.kafka.streams.processor.internals.StreamThread$TaskCreator.createTask(StreamThread.java:381) at org.apache.kafka.streams.processor.internals.StreamThread$AbstractTaskCreator.createTasks(StreamThread.java:366) at org.apache.kafka.streams.processor.internals.TaskManager.addStreamTasks(TaskManager.java:148) at org.apache.kafka.streams.processor.internals.TaskManager.createTasks(TaskManager.java:107) at org.apache.kafka.streams.processor.internals.StreamThread$RebalanceListener.onPartitionsAssigned(StreamThread.java:268) at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:259) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:367) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:316) at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:290) at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1148) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1114) at org.apache.kafka.streams.processor.internals.ConsumerUtils.poll(ConsumerUtils.java:33) at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:831) ... 3 more
{noformat}

Might be an environmental issue? How often did it fail so far?","07/May/18 20:19;yuzhihong@gmail.com;From Jenkins:

Failed 1 times in the last 10 runs. Flakiness: 11%, Stability: 90%","07/May/18 21:39;yuzhihong@gmail.com;{code}
        if (!taskDir.exists() && !taskDir.mkdir()) {
            throw new ProcessorStateException(
                String.format(""task directory [%s] doesn't exist and couldn't be created"", taskDir.getPath()));
{code}
It seems the code can be more robust by checking whether mkdir() returned false due to non-existent parent directory (or other condition).","07/May/18 21:49;yuzhihong@gmail.com;The above error happened at the end of the failed test.
I doubt that was the cause for the timed out test.",,,,,,,,,,,,,,,,,
TopicMetadataTest is flaky,KAFKA-6734,13149392,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Cannot Reproduce,,yuzhihong@gmail.com,yuzhihong@gmail.com,02/Apr/18 01:53,05/May/18 19:19,12/Jan/21 11:54,05/May/18 19:19,,,,,,,,,,,,,,0,,,,"I got two different test failures in two runs of test suite:
{code}
kafka.integration.TopicMetadataTest > testAutoCreateTopic FAILED
    kafka.common.KafkaException: fetching topic metadata for topics [Set(testAutoCreateTopic)] from broker [List(BrokerEndPoint(0,,41557))] failed
        at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:77)
        at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:98)
        at kafka.integration.TopicMetadataTest.testAutoCreateTopic(TopicMetadataTest.scala:105)

        Caused by:
        java.net.SocketTimeoutException
            at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:211)
            at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
            at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385)
            at org.apache.kafka.common.network.NetworkReceive.readFromReadableChannel(NetworkReceive.java:122)
            at kafka.network.BlockingChannel.readCompletely(BlockingChannel.scala:131)
            at kafka.network.BlockingChannel.receive(BlockingChannel.scala:122)
            at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:82)
            at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:79)
            at kafka.producer.SyncProducer.send(SyncProducer.scala:124)
            at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:63)
            ... 2 more
{code}
{code}
kafka.integration.TopicMetadataTest > testIsrAfterBrokerShutDownAndJoinsBack FAILED
    java.lang.AssertionError: Topic metadata is not correctly updated for broker kafka.server.KafkaServer@4c45dc9f.
    Expected ISR: List(BrokerEndPoint(0,localhost,40822), BrokerEndPoint(1,localhost,39030))
    Actual ISR  : Vector(BrokerEndPoint(0,localhost,40822))
        at kafka.utils.TestUtils$.fail(TestUtils.scala:355)
        at kafka.utils.TestUtils$.waitUntilTrue(TestUtils.scala:865)
        at kafka.integration.TopicMetadataTest$$anonfun$checkIsr$1.apply(TopicMetadataTest.scala:191)
        at kafka.integration.TopicMetadataTest$$anonfun$checkIsr$1.apply(TopicMetadataTest.scala:189)
        at scala.collection.immutable.List.foreach(List.scala:392)
        at kafka.integration.TopicMetadataTest.checkIsr(TopicMetadataTest.scala:189)
        at kafka.integration.TopicMetadataTest.testIsrAfterBrokerShutDownAndJoinsBack(TopicMetadataTest.scala:231)
{code}",,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-04-02 01:53:56.0,,,,,,,"0|i3s15b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
discardChannel should be released in MockSelector#completeSend,KAFKA-6716,13148001,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,huxi_2b,yuzhihong@gmail.com,yuzhihong@gmail.com,26/Mar/18 18:23,25/Apr/18 09:42,12/Jan/21 11:54,03/Apr/18 06:44,,,,,2.0.0,,,,,,,,,0,,,,"{code}
    private void completeSend(Send send) throws IOException {
        // Consume the send so that we will be able to send more requests to the destination
        ByteBufferChannel discardChannel = new ByteBufferChannel(send.size());
        while (!send.completed()) {
            send.writeTo(discardChannel);
        }
        completedSends.add(send);
    }
{code}
The {{discardChannel}} should be closed before returning from the method",,githubbot,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-03-27 23:04:02.746,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 28 14:03:42 UTC 2018,,,,,,,"0|i3rsl3:",9223372036854775807,,,,,,,,,,,,,,,,"27/Mar/18 23:04;githubbot;huxihx opened a new pull request #4783: KAFKA-6716: Should close the `discardChannel` in completeSend
URL: https://github.com/apache/kafka/pull/4783
 
 
   KAFKA-6716: Should close the `discardChannel` in completeSend
   https://issues.apache.org/jira/browse/KAFKA-6716
   
   Should close the `discardChannel` in MockSelector#completeSend
   
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","28/Mar/18 14:03;githubbot;rajinisivaram closed pull request #4783: KAFKA-6716: Should close the `discardChannel` in completeSend
URL: https://github.com/apache/kafka/pull/4783
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/clients/src/test/java/org/apache/kafka/test/MockSelector.java b/clients/src/test/java/org/apache/kafka/test/MockSelector.java
index bd27d5c6d99..200d5111517 100644
--- a/clients/src/test/java/org/apache/kafka/test/MockSelector.java
+++ b/clients/src/test/java/org/apache/kafka/test/MockSelector.java
@@ -124,11 +124,12 @@ private void completeInitiatedSends() throws IOException {
 
     private void completeSend(Send send) throws IOException {
         // Consume the send so that we will be able to send more requests to the destination
-        ByteBufferChannel discardChannel = new ByteBufferChannel(send.size());
-        while (!send.completed()) {
-            send.writeTo(discardChannel);
+        try (ByteBufferChannel discardChannel = new ByteBufferChannel(send.size())) {
+            while (!send.completed()) {
+                send.writeTo(discardChannel);
+            }
+            completedSends.add(send);
         }
-        completedSends.add(send);
     }
 
     private void completeDelayedReceives() {


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,
SocketServerTest#closingChannelException fails sometimes,KAFKA-6531,13136153,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Cannot Reproduce,,yuzhihong@gmail.com,yuzhihong@gmail.com,04/Feb/18 18:59,16/Apr/18 01:46,12/Jan/21 11:54,16/Apr/18 01:46,,,,,,,,,,core,,,,0,,,,"From https://builds.apache.org/job/kafka-trunk-jdk9/361/testReport/junit/kafka.network/SocketServerTest/closingChannelException/ :

{code}
java.lang.AssertionError: Channels not removed
	at kafka.utils.TestUtils$.fail(TestUtils.scala:355)
	at kafka.utils.TestUtils$.waitUntilTrue(TestUtils.scala:865)
	at kafka.network.SocketServerTest.assertProcessorHealthy(SocketServerTest.scala:914)
	at kafka.network.SocketServerTest.$anonfun$closingChannelException$1(SocketServerTest.scala:763)
	at kafka.network.SocketServerTest.$anonfun$closingChannelException$1$adapted(SocketServerTest.scala:747)
{code}
Among the test output, I saw:
{code}
[2018-02-04 18:51:15,995] ERROR Processor 0 closed connection from /127.0.0.1:48261 (kafka.network.SocketServerTest$$anon$5$$anon$1:73)
java.lang.IllegalStateException: There is already a connection for id 127.0.0.1:1-127.0.0.1:2-0
	at org.apache.kafka.common.network.Selector.ensureNotRegistered(Selector.java:260)
	at org.apache.kafka.common.network.Selector.register(Selector.java:254)
	at kafka.network.SocketServerTest$TestableSelector.super$register(SocketServerTest.scala:1043)
	at kafka.network.SocketServerTest$TestableSelector.$anonfun$register$2(SocketServerTest.scala:1043)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.network.SocketServerTest$TestableSelector.runOp(SocketServerTest.scala:1037)
	at kafka.network.SocketServerTest$TestableSelector.register(SocketServerTest.scala:1043)
	at kafka.network.Processor.configureNewConnections(SocketServer.scala:723)
	at kafka.network.Processor.run(SocketServer.scala:532)
{code}",,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-02-04 18:59:10.0,,,,,,,"0|i3ps1r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document how to skip findbugs / checkstyle when running unit test,KAFKA-6735,13149393,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,ewencp,yuzhihong@gmail.com,yuzhihong@gmail.com,02/Apr/18 01:57,10/Apr/18 04:05,12/Jan/21 11:54,,,,,,,,,,,,,,,0,,,,"Even when running single unit test, findbugs dependency would result in some time spent before the test is actually run.

We should document how findbugs dependency can be skipped in such scenario:

{code}
-x findbugsMain -x findbugsTest -x checkStyleMain -x checkStyleTest
{code}",,ewencp,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-04-10 03:57:58.189,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 10 04:05:01 UTC 2018,,,,,,,"0|i3s15j:",9223372036854775807,,,,,,,,,,,,,,,,"10/Apr/18 03:57;ewencp;Why should we document outs for these? They should never result in merging PRs more quickly, as seems to be the goal here, since any failure mentioned here should result in CI failures that a committer would never merge since the CI builds failed.

 ","10/Apr/18 04:05;yuzhihong@gmail.com;findbugs may take some time. Skipping findbugs is not for CI. It is for running tests locally.",,,,,,,,,,,,,,,,,,,,
java.lang.NoClassDefFoundError: org/apache/kafka/common/network/LoginType,KAFKA-6338,13123886,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Not A Problem,,Ronald van de Kuil,Ronald van de Kuil,09/Dec/17 07:58,17/Mar/18 16:03,12/Jan/21 11:54,17/Mar/18 16:03,1.0.0,,,,,,,,,,,,,0,,,,"I have just setup a kerberized Kafa cluster with Ranger 0.7.1 and Kafka 1.0.0. 

It all seems to work fine as I see that authorisation policies are enforced and auditlogging is present.

On startup of a kafka server I see a stack trace but it does not seem to matter.

My wish is to keep the logs tidy and free of false alerts.

I wonder whether I have an issue somewhere.",,omkreddy,Ronald van de Kuil,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-01-05 12:07:09.667,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 17 16:03:00 UTC 2018,,,,,,,"0|i3nq0v:",9223372036854775807,,,,,,,,,,,,,,,,"09/Dec/17 07:58;Ronald van de Kuil;[2017-12-09 06:54:22,233] ERROR Error getting principal. (org.apache.ranger.authorization.kafka.authorizer.RangerKafkaAuthorizer)
java.lang.NoClassDefFoundError: org/apache/kafka/common/network/LoginType
        at org.apache.ranger.authorization.kafka.authorizer.RangerKafkaAuthorizer.configure(RangerKafkaAuthorizer.java:82)
        at org.apache.ranger.authorization.kafka.authorizer.RangerKafkaAuthorizer.configure(RangerKafkaAuthorizer.java:94)
        at kafka.server.KafkaServer.$anonfun$startup$4(KafkaServer.scala:254)
        at scala.Option.map(Option.scala:146)
        at kafka.server.KafkaServer.startup(KafkaServer.scala:252)
        at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:38)
        at kafka.Kafka$.main(Kafka.scala:92)
        at kafka.Kafka.main(Kafka.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.kafka.common.network.LoginType
        at java.lang.ClassLoader.findClass(ClassLoader.java:530)
        at org.apache.ranger.plugin.classloader.RangerPluginClassLoader$MyClassLoader.findClass(RangerPluginClassLoader.java:272)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at org.apache.ranger.plugin.classloader.RangerPluginClassLoader.loadClass(RangerPluginClassLoader.java:125)
        ... 8 more
","05/Jan/18 12:07;omkreddy;Some of the code/api got changed in Kafka 0.11. This needs to be fixed/updated in Ranger. maybe you can raise an issue on Ranger JIRA board.","29/Jan/18 13:56;Ronald van de Kuil;Oka, I raised RANGER-1964","17/Mar/18 16:03;omkreddy;Closing this  in-favor of RANGER-1964",,,,,,,,,,,,,,,,,,
Intermittent test failure in FetchRequestTest.testDownConversionWithConnectionFailure,KAFKA-6228,13119269,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Cannot Reproduce,,yuzhihong@gmail.com,yuzhihong@gmail.com,17/Nov/17 17:22,17/Feb/18 01:31,12/Jan/21 11:54,17/Feb/18 01:31,,,,,,,,,,,,,,0,,,,"From https://builds.apache.org/job/kafka-trunk-jdk8/2219/testReport/junit/kafka.server/FetchRequestTest/testDownConversionWithConnectionFailure/ :
{code}
java.lang.AssertionError: Fetch size too small 42, broker may have run out of memory
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at kafka.server.FetchRequestTest.kafka$server$FetchRequestTest$$fetch$1(FetchRequestTest.scala:214)
	at kafka.server.FetchRequestTest$$anonfun$testDownConversionWithConnectionFailure$2.apply(FetchRequestTest.scala:226)
	at kafka.server.FetchRequestTest$$anonfun$testDownConversionWithConnectionFailure$2.apply(FetchRequestTest.scala:226)
	at scala.collection.immutable.Range.foreach(Range.scala:160)
	at kafka.server.FetchRequestTest.testDownConversionWithConnectionFailure(FetchRequestTest.scala:226)
{code}
I ran FetchRequestTest locally which passed.
{code}
          assertTrue(s""Fetch size too small $size, broker may have run out of memory"",
              size > maxPartitionBytes - batchSize)
{code}
The assertion message should include maxPartitionBytes and batchSize which would give us more information.",,ijuma,klafferty,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-11-17 17:25:58.241,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 17 17:25:58 UTC 2017,,,,,,,"0|i3mxnz:",9223372036854775807,,,,,,,,,,,,,,,,"17/Nov/17 17:25;ijuma;cc [~rsivaram]",,,,,,,,,,,,,,,,,,,,,
SelectorTest may fail with ConcurrentModificationException,KAFKA-6300,13122387,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,yuzhihong@gmail.com,yuzhihong@gmail.com,03/Dec/17 00:02,06/Dec/17 19:44,12/Jan/21 11:54,06/Dec/17 19:43,,,,,1.0.1,1.1.0,,,,,,,,0,,,,"From https://builds.apache.org/job/kafka-trunk-jdk8/2255/testReport/junit/org.apache.kafka.common.network/SelectorTest/testImmediatelyConnectedCleaned/ :
{code}
java.util.ConcurrentModificationException
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909)
	at java.util.ArrayList$Itr.next(ArrayList.java:859)
	at org.apache.kafka.common.network.EchoServer.closeConnections(EchoServer.java:115)
	at org.apache.kafka.common.network.EchoServer.close(EchoServer.java:121)
	at org.apache.kafka.common.network.SelectorTest.tearDown(SelectorTest.java:95)
{code}
It seems sockets ArrayList was modified during closing.",,githubbot,rsivaram,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-12-03 00:15:07.677,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 06 19:44:03 UTC 2017,,,,,,,"0|i3ngun:",9223372036854775807,,,,,,,,,,,,,,,,"03/Dec/17 00:15;githubbot;GitHub user tedyu opened a pull request:

    https://github.com/apache/kafka/pull/4288

    KAFKA-6300 SelectorTest may fail with ConcurrentModificationException

    Synchronization is added w.r.t. sockets ArrayList to avoid ConcurrentModificationException
    
    ### Committer Checklist (excluded from commit message)
    - [ ] Verify design and implementation 
    - [ ] Verify test coverage and CI build status
    - [ ] Verify documentation (including upgrade notes)


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/tedyu/kafka trunk

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/4288.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #4288
    
----
commit 8943cc9a39f297dacf091e8e02c09b44f6b9a6fe
Author: tedyu <yuzhihong@gmail.com>
Date:   2017-12-03T00:13:37Z

    KAFKA-6300 SelectorTest may fail with ConcurrentModificationException

----
","06/Dec/17 17:49;githubbot;GitHub user tedyu opened a pull request:

    https://github.com/apache/kafka/pull/4299

    KAFKA-6300 SelectorTest may fail with ConcurrentModificationException

    Synchronization is added w.r.t. sockets ArrayList to avoid ConcurrentModificationException
    
    ### Committer Checklist (excluded from commit message)
    - [ ] Verify design and implementation 
    - [ ] Verify test coverage and CI build status
    - [ ] Verify documentation (including upgrade notes)


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/tedyu/kafka trunk

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/4299.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #4299
    
----
commit 6ae5d0856ba3243f77157507b3cf554bfec4ac56
Author: tedyu <yuzhihong@gmail.com>
Date:   2017-12-06T17:48:43Z

    KAFKA-6300 SelectorTest may fail with ConcurrentModificationException

----
","06/Dec/17 17:51;githubbot;Github user tedyu closed the pull request at:

    https://github.com/apache/kafka/pull/4288
","06/Dec/17 19:43;rsivaram;Issue resolved by pull request 4299
[https://github.com/apache/kafka/pull/4299]","06/Dec/17 19:44;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/4299
",,,,,,,,,,,,,,,,,
kerberos login fails,KAFKA-6198,13117555,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,Ronald van de Kuil,Ronald van de Kuil,10/Nov/17 11:56,10/Nov/17 13:10,12/Jan/21 11:54,10/Nov/17 13:10,0.11.0.1,,,,,,,,,clients,,,,0,,,,"I got very far with setting up kerberos on the raspberry pi as part of self study. 

I believe that the kafka server is happy with kerberos:

[2017-11-10 12:17:51,659] INFO Successfully authenticated client: authenticationID=kafka/pi99.dev.ibm.com@DEV.IBM.COM; authorizationID=kafka/pi99.dev.ibm.com@DEV.IBM.COM. (org.apache.kafka.common.security.authenticator.SaslServerCallbackHandler)
[2017-11-10 12:17:51,661] INFO Setting authorizedID: kafka (org.apache.kafka.common.security.authenticator.SaslServerCallbackHandler)

I have setup the kafka.security.auth.SimpleAclAuthorizer

And granted the following access:

Current ACLs for resource `Topic:kerberos-topic`: 
	User:producer has Allow permission for operations: Describe from hosts: *
	User:producer has Allow permission for operations: Write from hosts: *
	User:producer@DEV.IBM.COM has Allow permission for operations: Describe from hosts: *
	User:producer@DEV.IBM.COM has Allow permission for operations: Write from hosts: * 

When I start the client, then I see it getting the kerberos ticket:

[main] INFO org.apache.kafka.common.security.authenticator.AbstractLogin - Successfully logged in.
[kafka-kerberos-refresh-thread-producer@DEV.IBM.COM] INFO org.apache.kafka.common.security.kerberos.KerberosLogin - [Principal=producer@DEV.IBM.COM]: TGT refresh thread started.
[kafka-kerberos-refresh-thread-producer@DEV.IBM.COM] INFO org.apache.kafka.common.security.kerberos.KerberosLogin - [Principal=producer@DEV.IBM.COM]: TGT valid starting at: Fri Nov 10 12:50:11 CET 2017
[kafka-kerberos-refresh-thread-producer@DEV.IBM.COM] INFO org.apache.kafka.common.security.kerberos.KerberosLogin - [Principal=producer@DEV.IBM.COM]: TGT expires: Fri Nov 10 22:50:11 CET 2017
[kafka-kerberos-refresh-thread-producer@DEV.IBM.COM] INFO org.apache.kafka.common.security.kerberos.KerberosLogin - [Principal=producer@DEV.IBM.COM]: TGT refresh sleeping until: Fri Nov 10 21:13:37 CET 2017

But the client fails to login:

[kafka-producer-network-thread | producer-1] WARN org.apache.kafka.clients.NetworkClient - Connection to node -1 terminated during authentication. This may indicate that authentication failed due to invalid credentials.

I do not see any warnings in the logs, so I do not have much to go on.

What can I do to get my finger behind this issue?

Thank you,

Ronald - the NOOB",raspberrypi,Ronald van de Kuil,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 10 13:09:59 UTC 2017,,,,,,,"0|i3mn3z:",9223372036854775807,,,,,,,,,,,,,,,,"10/Nov/17 12:44;Ronald van de Kuil;Hmm, I found something using -Dsun.security.krb5.debug=true

>>>KRBError:
         cTime is Wed Mar 17 02:32:44 CET 1976 195874364000
         sTime is Fri Nov 10 13:39:42 CET 2017 1510317582000
         suSec is 983774
         error code is 7
         error Message is Server not found in Kerberos database
         cname is producer@DEV.IBM.COM
         sname is kafka/localhost@DEV.IBM.COM
         msgType is 30
KrbException: Server not found in Kerberos database (7) - LOOKING_UP_SERVER
","10/Nov/17 12:48;Ronald van de Kuil;Hmm, updating producer.properties does not work, sasl.kerberos.service.name=kafka/pi99.dev.ibm.com

>>>KRBError:
         cTime is Wed Aug 21 02:31:50 CEST 2024 1724200310000
         sTime is Fri Nov 10 13:46:44 CET 2017 1510318004000
         suSec is 372867
         error code is 7
         error Message is Server not found in Kerberos database
         cname is producer@DEV.IBM.COM
         sname is kafka/pi99.dev.ibm.com/localhost@DEV.IBM.COM
         msgType is 30
","10/Nov/17 13:09;Ronald van de Kuil;Sent recordset number 0 to topic >kerberos-topic<
[main] INFO eu.bde.sc4pilot.kafka.SASLProducer - RecordMetadata.offset: 3

Ha! Fixed!

bootstrap.servers=pi99.dev.ibm.com:9092",,,,,,,,,,,,,,,,,,,
RestoreIntegrationTest sometimes fails with assertion error,KAFKA-6137,13112627,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Cannot Reproduce,,yuzhihong@gmail.com,yuzhihong@gmail.com,27/Oct/17 15:29,02/Nov/17 16:56,12/Jan/21 11:54,02/Nov/17 16:56,,,,,,,,,,streams,unit tests,,,0,flaky-test,,,"From https://builds.apache.org/job/kafka-1.0-jdk7/62 :
{code}
org.apache.kafka.streams.integration.RestoreIntegrationTest > shouldSuccessfullyStartWhenLoggingDisabled FAILED
    java.lang.AssertionError
        at org.junit.Assert.fail(Assert.java:86)
        at org.junit.Assert.assertTrue(Assert.java:41)
        at org.junit.Assert.assertTrue(Assert.java:52)
        at org.apache.kafka.streams.integration.RestoreIntegrationTest.shouldSuccessfullyStartWhenLoggingDisabled(RestoreIntegrationTest.java:195)
{code}",,guozhang,mjsax,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-10-27 16:07:55.258,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 27 16:14:44 UTC 2017,,,,,,,"0|i3lsrj:",9223372036854775807,,,,,,,,,,,,,,,,"27/Oct/17 16:07;guozhang;This seems be caused by this:

{code}
Standard Error

Exception in thread ""restore-test-b1ec6d17-cabc-4e1f-b02e-e116d8f654dc-StreamThread-166"" org.apache.kafka.streams.errors.StreamsException: stream-thread [restore-test-b1ec6d17-cabc-4e1f-b02e-e116d8f654dc-StreamThread-166] Failed to rebalance.
	at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:860)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:808)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:774)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:744)
Caused by: org.apache.kafka.streams.errors.ProcessorStateException: task directory [/tmp/restore-test6134854033128612495/restore-test/0_0] doesn't exist and couldn't be created
	at org.apache.kafka.streams.processor.internals.StateDirectory.directoryForTask(StateDirectory.java:93)
	at org.apache.kafka.streams.processor.internals.ProcessorStateManager.<init>(ProcessorStateManager.java:95)
	at org.apache.kafka.streams.processor.internals.AbstractTask.<init>(AbstractTask.java:85)
	at org.apache.kafka.streams.processor.internals.StreamTask.<init>(StreamTask.java:117)
	at org.apache.kafka.streams.processor.internals.StreamThread$TaskCreator.createTask(StreamThread.java:404)
	at org.apache.kafka.streams.processor.internals.StreamThread$TaskCreator.createTask(StreamThread.java:365)
	at org.apache.kafka.streams.processor.internals.StreamThread$AbstractTaskCreator.createTasks(StreamThread.java:350)
	at org.apache.kafka.streams.processor.internals.TaskManager.addStreamTasks(TaskManager.java:137)
	at org.apache.kafka.streams.processor.internals.TaskManager.createTasks(TaskManager.java:88)
	at org.apache.kafka.streams.processor.internals.StreamThread$RebalanceListener.onPartitionsAssigned(StreamThread.java:259)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:264)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:367)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:316)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:295)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1138)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1103)
	at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:851)
{code}","27/Oct/17 16:14;guozhang;It sounds like an environment issue to me as the following condition is not expected to pass:

{code}
        if (!taskDir.exists() && !taskDir.mkdir()) {
            throw new ProcessorStateException(
                String.format(""task directory [%s] doesn't exist and couldn't be created"", taskDir.getPath()));
        }
{code}

I'd suggest leaving this ticket open for some time and see if we observe similar issues again. If not we can close as cannot reproduce.",,,,,,,,,,,,,,,,,,,,
ResetIntegrationTest may fail due to IllegalArgumentException,KAFKA-6109,13111536,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Cannot Reproduce,,yuzhihong@gmail.com,yuzhihong@gmail.com,23/Oct/17 21:12,27/Oct/17 15:30,12/Jan/21 11:54,27/Oct/17 15:30,,,,,,,,,,,,,,0,,,,"From https://builds.apache.org/job/kafka-trunk-jdk7/2918 :
{code}
org.apache.kafka.streams.integration.ResetIntegrationTest > testReprocessingFromScratchAfterResetWithIntermediateUserTopic FAILED
    java.lang.IllegalArgumentException: Setting the time to 1508791687000 while current time 1508791687475 is newer; this is not allowed
        at org.apache.kafka.common.utils.MockTime.setCurrentTimeMs(MockTime.java:81)
        at org.apache.kafka.streams.integration.AbstractResetIntegrationTest.beforePrepareTest(AbstractResetIntegrationTest.java:114)
        at org.apache.kafka.streams.integration.ResetIntegrationTest.before(ResetIntegrationTest.java:55)
{code}",,guozhang,mjsax,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-10-23 21:22:53.645,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 27 15:24:17 UTC 2017,,,,,,,"0|i3lm27:",9223372036854775807,,,,,,,,,,,,,,,,"23/Oct/17 21:22;guozhang;It is due to this PR https://github.com/apache/kafka/pull/4096. Will be fixed along with https://github.com/apache/kafka/pull/4096","23/Oct/17 21:25;guozhang;[~tedyu] Please take a look at https://github.com/apache/kafka/pull/4096/commits/bad05511683aa111fc9dccc37dbfa7b64d022753","27/Oct/17 15:24;mjsax;[~tedyu] this Jira is for {{ResetIntegrationTest}} but the failure you report is for {{RestoreIntegertionTest}}. Can you please open a new Jira instead. Thx.",,,,,,,,,,,,,,,,,,,
API changes review for Kafka,KAFKA-5984,13105661,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,aponomarenko,aponomarenko,28/Sep/17 05:51,28/Sep/17 05:51,12/Jan/21 11:54,,,,,,,,,,,,,,,0,,,,"The review of API changes for the Kafka library since 0.8.2.0 version: https://abi-laboratory.pro/java/tracker/timeline/kafka-clients/

The report is updated three times a week. Hope it will be helpful for users and maintainers of the library.

The report is generated by https://github.com/lvc/japi-tracker

Thank you.

!Kafka-Report-1.png|API changes review!
!Kafka-Report-2.png|API symbols timeline!",,aponomarenko,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/17 05:51;aponomarenko;Kafka-Report-1.png;https://issues.apache.org/jira/secure/attachment/12889434/Kafka-Report-1.png","28/Sep/17 05:51;aponomarenko;Kafka-Report-2.png;https://issues.apache.org/jira/secure/attachment/12889433/Kafka-Report-2.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-09-28 05:51:26.0,,,,,,,"0|i3kndj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Intermittent test failure in SaslPlainSslEndToEndAuthorizationTest.testAcls,KAFKA-5821,13099271,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Cannot Reproduce,,yuzhihong@gmail.com,yuzhihong@gmail.com,01/Sep/17 16:54,22/Sep/17 02:39,12/Jan/21 11:54,22/Sep/17 02:39,,,,,,,,,,,,,,0,,,,"From https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/7245/testReport/junit/kafka.api/SaslPlainSslEndToEndAuthorizationTest/testAcls/ :
{code}
java.lang.SecurityException: zookeeper.set.acl is true, but the verification of the JAAS login file failed.
	at kafka.server.KafkaServer.initZk(KafkaServer.scala:329)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:192)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:134)
	at kafka.integration.KafkaServerTestHarness$$anonfun$setUp$1.apply(KafkaServerTestHarness.scala:94)
	at kafka.integration.KafkaServerTestHarness$$anonfun$setUp$1.apply(KafkaServerTestHarness.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.integration.KafkaServerTestHarness.setUp(KafkaServerTestHarness.scala:93)
	at kafka.api.IntegrationTestHarness.setUp(IntegrationTestHarness.scala:66)
	at kafka.api.EndToEndAuthorizationTest.setUp(EndToEndAuthorizationTest.scala:158)
	at kafka.api.SaslEndToEndAuthorizationTest.setUp(SaslEndToEndAuthorizationTest.scala:48)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.runTestClass(JUnitTestClassExecuter.java:114)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.execute(JUnitTestClassExecuter.java:57)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassProcessor.processTestClass(JUnitTestClassProcessor.java:66)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93)
	at com.sun.proxy.$Proxy1.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:109)
	at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:146)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:128)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:404)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)
	at java.lang.Thread.run(Thread.java:745)
Standard Output

[2017-09-01 16:07:42,416] WARN SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/kafka646456529477143007.tmp'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it. (org.apache.zookeeper.ClientCnxn:1014)
[2017-09-01 16:07:42,416] ERROR Error while calling watcher  (org.apache.zookeeper.ClientCnxn:532)
java.lang.NullPointerException
	at kafka.controller.ZookeeperClient$ZookeeperClientWatcher$.process(ZookeeperClient.scala:215)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:530)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:505)
[2017-09-01 16:07:42,416] WARN Session 0x15e3e23457d0001 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1162)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2017-09-01 16:07:42,419] ERROR ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes (org.apache.zookeeper.server.ZooKeeperServer:472)
[2017-09-01 16:07:42,421] WARN SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/kafka646456529477143007.tmp'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it. (org.apache.zookeeper.ClientCnxn:1014)
[2017-09-01 16:07:42,579] WARN SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/kafka646456529477143007.tmp'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it. (org.apache.zookeeper.ClientCnxn:1014)
[2017-09-01 16:07:42,579] ERROR Error while calling watcher  (org.apache.zookeeper.ClientCnxn:532)
java.lang.NullPointerException
	at kafka.controller.ZookeeperClient$ZookeeperClientWatcher$.process(ZookeeperClient.scala:215)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:530)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:505)
[2017-09-01 16:07:42,579] WARN Session 0x15e3e23499c0001 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1162)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2017-09-01 16:07:42,604] WARN SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/kafka646456529477143007.tmp'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it. (org.apache.zookeeper.ClientCnxn:1014)
[2017-09-01 16:07:42,605] ERROR Error while calling watcher  (org.apache.zookeeper.ClientCnxn:532)
java.lang.NullPointerException
	at kafka.controller.ZookeeperClient$ZookeeperClientWatcher$.process(ZookeeperClient.scala:215)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:530)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:505)
[2017-09-01 16:07:42,605] WARN Session 0x15e3e23453b0001 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1162)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
{code}",,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-09-01 16:54:35.0,,,,,,,"0|i3jk53:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Warning from kafka mirror maker about ssl properties not valid,KAFKA-3327,12946612,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Cannot Reproduce,,munkhan,munkhan,03/Mar/16 17:20,19/Aug/17 15:19,12/Jan/21 11:54,19/Aug/17 15:19,0.9.0.1,,,,,,,,,config,,,,0,kafka,mirror-maker,ssl,"I am trying to run Mirror maker  over SSL. I have configured my broker following the procedure described in this document http://kafka.apache.org/documentation.html#security_overview 

I get the following warning when I start the mirror maker:

[root@munkhan-kafka1 kafka_2.10-0.9.0.1]# bin/kafka-run-class.sh kafka.tools.MirrorMaker --consumer.config config/datapush-consumer-ssl.properties --producer.config config/datapush-producer-ssl.properties --num.streams 2 --whitelist test1&
[1] 4701
[root@munkhan-kafka1 kafka_2.10-0.9.0.1]# [2016-03-03 10:24:35,348] WARN block.on.buffer.full config is deprecated and will be removed soon. Please use max.block.ms (org.apache.kafka.clients.producer.KafkaProducer)
[2016-03-03 10:24:35,523] WARN The configuration producer.type = sync was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2016-03-03 10:24:35,523] WARN The configuration ssl.keypassword = test1234 was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2016-03-03 10:24:35,523] WARN The configuration compression.codec = none was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2016-03-03 10:24:35,523] WARN The configuration serializer.class = kafka.serializer.DefaultEncoder was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2016-03-03 10:24:35,617] WARN Property security.protocol is not valid (kafka.utils.VerifiableProperties)
[2016-03-03 10:24:35,617] WARN Property ssl.keypassword is not valid (kafka.utils.VerifiableProperties)
[2016-03-03 10:24:35,617] WARN Property ssl.keystore.location is not valid (kafka.utils.VerifiableProperties)
[2016-03-03 10:24:35,618] WARN Property ssl.keystore.password is not valid (kafka.utils.VerifiableProperties)
[2016-03-03 10:24:35,618] WARN Property ssl.truststore.location is not valid (kafka.utils.VerifiableProperties)
[2016-03-03 10:24:35,618] WARN Property ssl.truststore.password is not valid (kafka.utils.VerifiableProperties)
[2016-03-03 10:24:35,752] WARN Property security.protocol is not valid (kafka.utils.VerifiableProperties)
[2016-03-03 10:24:35,752] WARN Property ssl.keypassword is not valid (kafka.utils.VerifiableProperties)
[2016-03-03 10:24:35,752] WARN Property ssl.keystore.location is not valid (kafka.utils.VerifiableProperties)
[2016-03-03 10:24:35,752] WARN Property ssl.keystore.password is not valid (kafka.utils.VerifiableProperties)
[2016-03-03 10:24:35,752] WARN Property ssl.truststore.location is not valid (kafka.utils.VerifiableProperties)
[2016-03-03 10:24:35,753] WARN Property ssl.truststore.password is not valid (kafka.utils.VerifiableProperties)
[2016-03-03 10:24:36,251] WARN No broker partitions consumed by consumer thread test-consumer-group_munkhan-kafka1.cisco.com-1457018675755-b9bb4c75-0 for topic test1 (kafka.consumer.RangeAssignor)
[2016-03-03 10:24:36,251] WARN No broker partitions consumed by consumer thread test-consumer-group_munkhan-kafka1.cisco.com-1457018675755-b9bb4c75-0 for topic test1 (kafka.consumer.RangeAssignor)

However the Mirror maker is able to mirror data . If I remove the configurations related to the warning messages from my producer  mirror maker does not work . So it seems despite the warning shown above the ssl.configuration properties are used somehow. 

My question is these are those warnings harmless in this context ?

",CentOS release 6.5,munkhan,omkreddy,qwertymaniac,richardatcloudera,umesh9794@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-08-19 15:19:11.357,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 19 15:19:11 UTC 2017,,,,,,,"0|i2u3yn:",9223372036854775807,,,,,,,,,,,,,,,,"19/Aug/17 15:19;omkreddy;mostly related to config issue.  Pl reopen if you think the issue still exists
",,,,,,,,,,,,,,,,,,,,,
Using Hamcrest for easy intent expression in tests,KAFKA-4555,13029011,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,rekhajoshm,rekhajoshm,rekhajoshm,18/Dec/16 21:06,06/Feb/17 18:32,12/Jan/21 11:54,,0.9.0.1,,,,,,,,,unit tests,,,,0,,,,Using Hamcrest for easy intent expression in tests,,githubbot,rekhajoshm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-12-18 21:12:07.151,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 06 18:32:40 UTC 2017,,,,,,,"0|i37qz3:",9223372036854775807,,,,,,,,,,,,,,,,"18/Dec/16 21:12;githubbot;GitHub user rekhajoshm opened a pull request:

    https://github.com/apache/kafka/pull/2273

    KAFKA-4555: Using Hamcrest for expressive intent in tests

    - Adding hamcrest in gradle files
    - Using hamcrest in couple of tests - SourceTaskOffsetCommitterTest, MetadataTest

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rekhajoshm/kafka KAFKA-4555

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2273.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2273
    
----
commit c9a66992b1095616f87c5748f210b973ebc7eb01
Author: Rekha Joshi <rekhajoshm@gmail.com>
Date:   2016-05-26T17:48:37Z

    Merge pull request #2 from apache/trunk
    
    Apache Kafka trunk pull

commit 8d7fb005cb132440e7768a5b74257d2598642e0f
Author: Rekha Joshi <rekhajoshm@gmail.com>
Date:   2016-05-30T21:37:43Z

    Merge pull request #3 from apache/trunk
    
    Apache Kafka trunk pull

commit fbef9a8fb1411282fbadec46955691c3e7ba2578
Author: Rekha Joshi <rekhajoshm@gmail.com>
Date:   2016-06-04T23:58:02Z

    Merge pull request #4 from apache/trunk
    
    Apache Kafka trunk pull

commit 172db701bf9affda1304b684921260d1cd36ae9e
Author: Rekha Joshi <rekhajoshm@gmail.com>
Date:   2016-06-06T22:10:31Z

    Merge pull request #6 from apache/trunk
    
    Apache Kafka trunk pull

commit 9d18d93745cf2bc9b0ab4bb9b25d9a31196ef918
Author: Rekha Joshi <rekhajoshm@gmail.com>
Date:   2016-06-07T19:36:45Z

    Merge pull request #7 from apache/trunk
    
    Apache trunk pull

commit 882faea01f28aef1977f4ced6567833bcf736840
Author: Rekha Joshi <rekhajoshm@gmail.com>
Date:   2016-06-13T20:01:43Z

    Merge pull request #8 from confluentinc/trunk
    
    Apache kafka trunk pull

commit 851315d39c0c308d79b9575546822aa932c46a09
Author: Rekha Joshi <rekhajoshm@gmail.com>
Date:   2016-06-27T17:34:54Z

    Merge pull request #9 from apache/trunk
    
    Merge Apache kafka trunk

commit 613f07c2b4193302c82a5d6eaa1e53e4b87bfbc1
Author: Rekha Joshi <rekhajoshm@gmail.com>
Date:   2016-07-09T17:03:45Z

    Merge pull request #11 from apache/trunk
    
    Merge Apache kafka trunk

commit 150e46e462cc192fb869e633f6d9ab681e7b83f9
Author: Rekha Joshi <rekhajoshm@gmail.com>
Date:   2016-08-02T19:44:09Z

    Merge pull request #12 from apache/trunk
    
    Apache Kafka trunk pull

commit 46b4868faed40ec83f87a089cf0db83b31bb2d8a
Author: Rekha Joshi <rekhajoshm@gmail.com>
Date:   2016-12-07T05:19:35Z

    Merge pull request #13 from apache/trunk
    
    Apache Kafka trunk pull

commit c2e44b8a935fb6e0ad9df399cdc5c5ee3e1b287a
Author: Joshi <rekhajoshm@gmail.com>
Date:   2016-12-18T21:09:00Z

    KAFKA-4555: Using Hamcrest in tests

commit 5101c1ec065c6ec210b3aa87b3e2122d529e323a
Author: Joshi <rekhajoshm@gmail.com>
Date:   2016-12-18T21:11:16Z

    KAFKA-4555: Using Hamcrest in tests

----
","06/Feb/17 18:32;githubbot;Github user rekhajoshm closed the pull request at:

    https://github.com/apache/kafka/pull/2273
",,,,,,,,,,,,,,,,,,,,
test submit jira issue,KAFKA-3295,12944812,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Invalid,,andy-hnhy,andy-hnhy,26/Feb/16 08:08,08/Mar/16 15:48,12/Jan/21 11:54,08/Mar/16 15:48,0.9.0.1,,,,,,,,,,,,,0,,,,test submit,,andy-hnhy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2016-02-26 08:08:11.0,,,,,,,"0|i2tsv3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add an unit test to validate the deletion of a partition marked as deleted,KAFKA-2355,12846734,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,eribeiro,eribeiro,eribeiro,22/Jul/15 01:27,23/Jul/15 16:28,12/Jan/21 11:54,23/Jul/15 16:19,0.8.2.1,,,,0.9.0.0,,,,,,,,,0,,,,Trying to delete a partition marked as deleted throws {{TopicAlreadyMarkedForDeletionException}} so this ticket add a unit test to validate this behaviour. ,,eribeiro,gwenshap,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/15 01:46;eribeiro;KAFKA-2355.patch;https://issues.apache.org/jira/secure/attachment/12746456/KAFKA-2355.patch","23/Jul/15 00:38;eribeiro;KAFKA-2355_2015-07-22_21:37:51.patch;https://issues.apache.org/jira/secure/attachment/12746681/KAFKA-2355_2015-07-22_21%3A37%3A51.patch","23/Jul/15 01:23;eribeiro;KAFKA-2355_2015-07-22_22:23:13.patch;https://issues.apache.org/jira/secure/attachment/12746692/KAFKA-2355_2015-07-22_22%3A23%3A13.patch",,,,,3.0,,,,,,,,,,,,,,,,,,,,2015-07-23 14:29:03.623,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 23 16:28:37 UTC 2015,,,,,,,"0|i2hj13:",9223372036854775807,,,,,,,,,,,,,,,,"22/Jul/15 01:46;eribeiro;Created reviewboard https://reviews.apache.org/r/36670/diff/
 against branch origin/trunk","22/Jul/15 01:50;eribeiro;Hi [~singhashish] and [~gwenshap]. I hope you don't get annoyed, but I saw that KAFKA-2345 was lacking a unit test to validate the new exception being thrown by KAFKA-2345. As this issue was already closed I created a new ticket, and added the unit test. Please, let me know what you think. Cheers! :)","23/Jul/15 00:38;eribeiro;Updated reviewboard https://reviews.apache.org/r/36670/diff/
 against branch origin/trunk","23/Jul/15 00:44;eribeiro;Hi [~ijuma] and [~granthenke], I have updated the patch as requested in the latest review. Let me know if something is missing or I misinterpreted, please. :)

ps: could one of you assign this little ticket to me? I asked for inclusion into colaborators' list so that I could assign issues to myself yesterday I guess, but it was not granted yet. :-P Cheers!","23/Jul/15 01:23;eribeiro;Updated reviewboard https://reviews.apache.org/r/36670/diff/
 against branch origin/trunk","23/Jul/15 14:29;ijuma;[~eribeiro], change looks good. I was unable to assign the issue to you. Maybe that also requires you to be in the contributors group.","23/Jul/15 15:36;eribeiro;Hi [~ijuma], no problem, mate. If it looks good to you then you can assign it to you and commit, please? Cheers. :)","23/Jul/15 15:40;ijuma;I am not a committer, so someone else must merge it, sorry.","23/Jul/15 16:19;gwenshap;+1 and pushed to trunk.
Thanks for the contribution [~eribeiro] and for the reviews [~ijuma], [~singhashish] and [~granthenke].

[~eribeiro] - I also added you to contributor list, so you can assign Jiras to yourself.","23/Jul/15 16:28;eribeiro;Oh, thanks [~gwenshap]! Sorry for any inconvenience or misstep, if any.",,,,,,,,,,,,
Add unit tests for BrokerTopicMetrics set through ReplicaManager,KAFKA-1917,12772234,Test,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,aauradkar,aauradkar,aauradkar,04/Feb/15 02:42,04/Feb/15 02:42,12/Jan/21 11:54,,,,,,,,,,,,,,,0,,,,"The BrokerTopicMetrics are gather for all individual topics and for AllTopics. There are no tests validating that these metrics are correctly set by the ReplicaManager. Since these metrics likely feed alerting systems, we should have proper test coverage.",,aauradkar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-02-04 02:42:33.0,,,,,,,"0|i255q7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Liars in PrimitiveApiTest that promise to test api in compression mode, but don't do this actually",KAFKA-1079,12672852,Test,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,omnomnom,omnomnom,omnomnom,08/Oct/13 17:52,19/Jun/14 05:09,12/Jan/21 11:54,17/Feb/14 19:46,0.8.0,,,,0.8.2.0,,,,,core,,,,0,newbie,test,,"Long time ago (0.7) we had ByteBufferMessageSet as a part of api and it's allowed us to control compression. Times goes on and now PrimitiveApiTest have methods that promise to test api with compression enabled, but in fact they don't. Moreover this methods almost entirely copy their counterparts without compression. In particular I'm talking about `testProduceAndMultiFetch` / `testProduceAndMultiFetchWithCompression` and `testMultiProduce`/`testMultiProduceWithCompression` pairs. 

The fix could be super-easy and soundness -- just parameterize methods with producer of each type (with/without compression). Sadly but it isn't feasible for junit3, so straightforward solution is to do the same ugly thing as `testDefaultEncoderProducerAndFetchWithCompression` method does -- forget about class-wide producer and roll-out it's own. I will attach path if that is a problem indeed. ",,guozhang,junrao,omnomnom,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/13 21:30;omnomnom;testing-with-compression-producer.patch;https://issues.apache.org/jira/secure/attachment/12609927/testing-with-compression-producer.patch","11/Dec/13 11:59;omnomnom;testing-with-compression-v2.patch;https://issues.apache.org/jira/secure/attachment/12618223/testing-with-compression-v2.patch",,,,,,2.0,,,,,,,,,,,,,,,,,,,,2013-10-09 03:23:44.262,,,false,,,,,,,,,,,,,,,,,352475,,,Mon Feb 17 19:46:47 UTC 2014,,,,,,,"0|i1orr3:",352762,,,,,,,,,,,,,,,,"09/Oct/13 03:23;junrao;Thanks for filing the jira. Yes, these two tests are supposed to enable compression, but they are not. I agree that using parameterized methods for unit tests will be ideal. I can't remember why we used junit3. So, it may not be necessary. Look forward to your patch.","23/Oct/13 21:30;omnomnom;Late patch for an issue. I also did a shallow cleanup: named parameters where they making code clear, removed unused parameters and variables, etc. AdminTools.deleteTopic is used to cleanup topics created by other test methods.","24/Oct/13 01:17;guozhang;Thanks for the patch Kostya, a couple of comments:

1. Is there a particular reason you wanted to use

val port = TestUtils.choosePort()

instead of

val port = TestUtils.choosePort?

2. I think it is better to keep

val topics = List((""test4"", 0), (""test1"", 0), (""test2"", 0), (""test3"", 0));

in testMultiProduce as it specifies the topic/partition more clear.","24/Oct/13 09:21;omnomnom;1. It is common convention in Scala to distinguish pure no arg functions from side effecting ones. Since current implementation of choosePort creates socket(s) and moreover can yield different results on different runs, I guess it is pretty reasonable to put braces next to the method call. 
2. Okay, I will revert this change. ","24/Oct/13 16:43;junrao;For 1, the convention we have been following is that we will only omit the braces if the function has no side effect. In this particular case, choosePort() does have side effect. So, we should include the braces.","10/Dec/13 17:00;omnomnom;From 7845a5af42ee44f9786542178c0789d93c66429f Mon Sep 17 00:00:00 2001
From: Kostya Golikov <johnysilver7@gmail.com>
Date: Thu, 24 Oct 2013 00:48:56 +0400
Subject: [PATCH] Fixed liars in primary api test: now checking is done not
 only against no-compression producer, but against with-compression producer
 as well.

---
 .../unit/kafka/integration/PrimitiveApiTest.scala  | 141 ++++++---------------
 1 file changed, 40 insertions(+), 101 deletions(-)

diff --git a/core/src/test/scala/unit/kafka/integration/PrimitiveApiTest.scala b/core/src/test/scala/unit/kafka/integration/PrimitiveApiTest.scala
index 5f331d2..c001b4e 100644
--- a/core/src/test/scala/unit/kafka/integration/PrimitiveApiTest.scala
+++ b/core/src/test/scala/unit/kafka/integration/PrimitiveApiTest.scala
@@ -35,12 +35,12 @@ import kafka.utils.{TestUtils, Utils}
  * End to end tests of the primitive apis against a local server
  */
 class PrimitiveApiTest extends JUnit3Suite with ProducerConsumerTestHarness with ZooKeeperTestHarness {
+  val requestHandlerLogger = Logger.getLogger(classOf[KafkaRequestHandler])
 
-  val port = TestUtils.choosePort
+  val port = TestUtils.choosePort()
   val props = TestUtils.createBrokerConfig(0, port)
   val config = new KafkaConfig(props)
   val configs = List(config)
-  val requestHandlerLogger = Logger.getLogger(classOf[KafkaRequestHandler])
 
   def testFetchRequestCanProperlySerialize() {
     val request = new FetchRequestBuilder()
@@ -100,7 +100,7 @@ class PrimitiveApiTest extends JUnit3Suite with ProducerConsumerTestHarness with
     val stringProducer1 = new Producer[String, String](config)
     stringProducer1.send(new KeyedMessage[String, String](topic, ""test-message""))
 
-    var fetched = consumer.fetch(new FetchRequestBuilder().addFetch(topic, 0, 0, 10000).build())
+    val fetched = consumer.fetch(new FetchRequestBuilder().addFetch(topic, 0, 0, 10000).build())
     val messageSet = fetched.messageSet(topic, 0)
     assertTrue(messageSet.iterator.hasNext)
 
@@ -108,8 +108,8 @@ class PrimitiveApiTest extends JUnit3Suite with ProducerConsumerTestHarness with
     assertEquals(""test-message"", Utils.readString(fetchedMessageAndOffset.message.payload, ""UTF-8""))
   }
 
-  def testProduceAndMultiFetch() {
-    createSimpleTopicsAndAwaitLeader(zkClient, List(""test1"", ""test2"", ""test3"", ""test4""), config.brokerId)
+  private def produceAndMultiFetch(producer: Producer[String, String]) {
+    createSimpleTopicsAndAwaitLeader(zkClient, List(""test1"", ""test2"", ""test3"", ""test4""))
 
     // send some messages
     val topics = List((""test4"", 0), (""test1"", 0), (""test2"", 0), (""test3"", 0));
@@ -171,117 +171,56 @@ class PrimitiveApiTest extends JUnit3Suite with ProducerConsumerTestHarness with
     requestHandlerLogger.setLevel(Level.ERROR)
   }
 
-  def testProduceAndMultiFetchWithCompression() {
-    createSimpleTopicsAndAwaitLeader(zkClient, List(""test1"", ""test2"", ""test3"", ""test4""), config.brokerId)
-
-    // send some messages
-    val topics = List((""test4"", 0), (""test1"", 0), (""test2"", 0), (""test3"", 0));
-    {
-      val messages = new mutable.HashMap[String, Seq[String]]
-      val builder = new FetchRequestBuilder()
-      for( (topic, partition) <- topics) {
-        val messageList = List(""a_"" + topic, ""b_"" + topic)
-        val producerData = messageList.map(new KeyedMessage[String, String](topic, topic, _))
-        messages += topic -> messageList
-        producer.send(producerData:_*)
-        builder.addFetch(topic, partition, 0, 10000)
-      }
-
-      // wait a bit for produced message to be available
-      val request = builder.build()
-      val response = consumer.fetch(request)
-      for( (topic, partition) <- topics) {
-        val fetched = response.messageSet(topic, partition)
-        assertEquals(messages(topic), fetched.map(messageAndOffset => Utils.readString(messageAndOffset.message.payload)))
-      }
-    }
-
-    // temporarily set request handler logger to a higher level
-    requestHandlerLogger.setLevel(Level.FATAL)
-
-    {
-      // send some invalid offsets
-      val builder = new FetchRequestBuilder()
-      for( (topic, partition) <- topics)
-        builder.addFetch(topic, partition, -1, 10000)
-
-      try {
-        val request = builder.build()
-        val response = consumer.fetch(request)
-        response.data.values.foreach(pdata => ErrorMapping.maybeThrowException(pdata.error))
-        fail(""Expected exception when fetching message with invalid offset"")
-      } catch {
-        case e: OffsetOutOfRangeException => ""this is good""
-      }
-    }
-
-    {
-      // send some invalid partitions
-      val builder = new FetchRequestBuilder()
-      for( (topic, _) <- topics)
-        builder.addFetch(topic, -1, 0, 10000)
-
-      try {
-        val request = builder.build()
-        val response = consumer.fetch(request)
-        response.data.values.foreach(pdata => ErrorMapping.maybeThrowException(pdata.error))
-        fail(""Expected exception when fetching message with invalid partition"")
-      } catch {
-        case e: UnknownTopicOrPartitionException => ""this is good""
-      }
-    }
+  def testProduceAndMultiFetch() {
+    val props = producer.config.props.props
+    val config = new ProducerConfig(props)
+    val noCompressionProducer = new Producer[String, String](config)
+    produceAndMultiFetch(noCompressionProducer)
+  }
 
-    // restore set request handler logger to a higher level
-    requestHandlerLogger.setLevel(Level.ERROR)
+  def testProduceAndMultiFetchWithCompression() {
+    val props = producer.config.props.props
+    props.put(""compression"", ""true"")
+    val config = new ProducerConfig(props)
+    val producerWithCompression = new Producer[String, String](config)
+    produceAndMultiFetch(producerWithCompression)
   }
 
-  def testMultiProduce() {
-    createSimpleTopicsAndAwaitLeader(zkClient, List(""test1"", ""test2"", ""test3"", ""test4""), config.brokerId)
+  private def multiProduce(producer: Producer[String, String]) {
+    val topics = Map(""test4"" -> 0, ""test1"" -> 0, ""test2"" -> 0, ""test3"" -> 0)
+    createSimpleTopicsAndAwaitLeader(zkClient, topics.keys)
 
-    // send some messages
-    val topics = List((""test4"", 0), (""test1"", 0), (""test2"", 0), (""test3"", 0));
     val messages = new mutable.HashMap[String, Seq[String]]
     val builder = new FetchRequestBuilder()
-    var produceList: List[KeyedMessage[String, String]] = Nil
-    for( (topic, partition) <- topics) {
+    for((topic, partition) <- topics) {
       val messageList = List(""a_"" + topic, ""b_"" + topic)
       val producerData = messageList.map(new KeyedMessage[String, String](topic, topic, _))
       messages += topic -> messageList
       producer.send(producerData:_*)
       builder.addFetch(topic, partition, 0, 10000)
     }
-    producer.send(produceList: _*)
 
     val request = builder.build()
     val response = consumer.fetch(request)
-    for( (topic, partition) <- topics) {
+    for((topic, partition) <- topics) {
       val fetched = response.messageSet(topic, partition)
       assertEquals(messages(topic), fetched.map(messageAndOffset => Utils.readString(messageAndOffset.message.payload)))
     }
   }
 
-  def testMultiProduceWithCompression() {
-    // send some messages
-    val topics = List((""test4"", 0), (""test1"", 0), (""test2"", 0), (""test3"", 0));
-    val messages = new mutable.HashMap[String, Seq[String]]
-    val builder = new FetchRequestBuilder()
-    var produceList: List[KeyedMessage[String, String]] = Nil
-    for( (topic, partition) <- topics) {
-      val messageList = List(""a_"" + topic, ""b_"" + topic)
-      val producerData = messageList.map(new KeyedMessage[String, String](topic, topic, _))
-      messages += topic -> messageList
-      producer.send(producerData:_*)
-      builder.addFetch(topic, partition, 0, 10000)
-    }
-    producer.send(produceList: _*)
+  def testMultiProduce() {
+    val props = producer.config.props.props
+    val config = new ProducerConfig(props)
+    val noCompressionProducer = new Producer[String, String](config)
+    multiProduce(noCompressionProducer)
+  }
 
-    // wait a bit for produced message to be available
-    val request = builder.build()
-    val response = consumer.fetch(request)
-    for( (topic, partition) <- topics) {
-      val fetched = response.messageSet(topic, 0)
-      assertEquals(messages(topic), fetched.map(messageAndOffset => Utils.readString(messageAndOffset.message.payload)))
-    }
+  def testMultiProduceWithCompression() {
+    val props = producer.config.props.props
+    props.put(""compression"", ""true"")
+    val config = new ProducerConfig(props)
+    val producerWithCompression = new Producer[String, String](config)
+    multiProduce(producerWithCompression)
   }
 
   def testConsumerEmptyTopic() {
@@ -294,16 +233,15 @@ class PrimitiveApiTest extends JUnit3Suite with ProducerConsumerTestHarness with
   }
 
   def testPipelinedProduceRequests() {
-    createSimpleTopicsAndAwaitLeader(zkClient, List(""test1"", ""test2"", ""test3"", ""test4""), config.brokerId)
+    val topics = Map(""test4"" -> 0, ""test1"" -> 0, ""test2"" -> 0, ""test3"" -> 0)
+    createSimpleTopicsAndAwaitLeader(zkClient, topics.keys)
     val props = producer.config.props.props
     props.put(""request.required.acks"", ""0"")
     val pipelinedProducer: Producer[String, String] = new Producer(new ProducerConfig(props))
 
     // send some messages
-    val topics = List((""test4"", 0), (""test1"", 0), (""test2"", 0), (""test3"", 0));
     val messages = new mutable.HashMap[String, Seq[String]]
     val builder = new FetchRequestBuilder()
-    var produceList: List[KeyedMessage[String, String]] = Nil
     for( (topic, partition) <- topics) {
       val messageList = List(""a_"" + topic, ""b_"" + topic)
       val producerData = messageList.map(new KeyedMessage[String, String](topic, topic, _))
@@ -338,10 +276,11 @@ class PrimitiveApiTest extends JUnit3Suite with ProducerConsumerTestHarness with
    * For testing purposes, just create these topics each with one partition and one replica for
    * which the provided broker should the leader for.  Create and wait for broker to lead.  Simple.
    */
-  def createSimpleTopicsAndAwaitLeader(zkClient: ZkClient, topics: Seq[String], brokerId: Int) {
+  private def createSimpleTopicsAndAwaitLeader(zkClient: ZkClient, topics: Iterable[String]) {
     for( topic <- topics ) {
-      AdminUtils.createTopic(zkClient, topic, 1, 1)
-      TestUtils.waitUntilLeaderIsElectedOrChanged(zkClient, topic, 0, 500)
+      AdminUtils.deleteTopic(zkClient, topic)
+      AdminUtils.createTopic(zkClient, topic, partitions = 1, replicationFactor = 1)
+      TestUtils.waitUntilLeaderIsElectedOrChanged(zkClient, topic, partition = 0, timeoutMs = 500)
     }
   }
 }
-- 
1.8.5.1
","11/Dec/13 05:09;junrao;There seems to be some weird characters in the pasted text. Could you attach the patch to this jira? Thanks,","11/Dec/13 11:59;omnomnom;+= New version of path with comments addressed (partition directly in a list of topics).","20/Dec/13 00:09;guozhang;+1 of version 2.","17/Feb/14 19:46;junrao;Thanks for the patch. Committed to trunk with the following minor change.

Removed the code to delete a topic before creating one since deleting is async and there is no guarantee that the deletion will complete before the creation.",,,,,,,,,,,,
0.7.1 seems to show less performance than 0.7.0,KAFKA-399,12598301,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Won't Fix,,miniway,miniway,11/Jul/12 00:55,11/Jul/13 22:26,12/Jan/21 11:54,11/Jul/13 22:26,0.7,0.7.1,,,,,,,,core,,,,0,,,,"On a test, 0.7.1 seems to show less performance than 0.7.0.

Producer is on a machine A, and Broker is on machine B.

Machine's spec is
- 8 core Intel(R) Xeon(R) CPU           E5405  @ 2.00GHz
- 16G RAM

Broker's configuration is
- num.threads=8
- socket.send.buffer=1048576
- socket.receive.buffer=1048576
- max.socket.request.bytes=104857600
- log.flush.interval=10000
- log.default.flush.interval.ms=1000
- log.default.flush.scheduler.interval.ms=1000
- log.file.size=536870912
- enable.zookeeper=true

Additional note
- no compression


Attached dstat result and used producer code",,jkreps,miniway,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/12 00:56;miniway;DirSource.java;https://issues.apache.org/jira/secure/attachment/12535961/DirSource.java","11/Jul/12 00:56;miniway;dstat.txt;https://issues.apache.org/jira/secure/attachment/12535962/dstat.txt",,,,,,2.0,,,,,,,,,,,,,,,,,,,,2013-07-11 22:26:31.479,,,false,,,,,,,,,,,,,,,,,299109,,,Thu Jul 11 22:26:31 UTC 2013,,,,,,,"0|i15znj:",243076,,,,,,,,,,,,,,,,"11/Jul/13 22:26;jkreps;0.8 is different enough that I think it is safe to mark this ""won't fix"".",,,,,,,,,,,,,,,,,,,,,
Add unit tests for ClusterConnectionStates,KAFKA-6104,13111257,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,sliebau,sliebau,sliebau,22/Oct/17 20:11,23/Oct/17 13:23,12/Jan/21 11:54,23/Oct/17 13:23,0.11.0.0,,,,1.0.0,1.1.0,,,,unit tests,,,,0,,,,"There are no unit tests for the ClusterConnectionStates object.

I've created tests to:
* Cycle through connection states and check correct properties during the process
* Check authentication exception is correctly stored
* Check that max reconnect backoff is not exceeded during reconnects
* Check that removed connections are correctly reset

There is currently no test that checks whether the reconnect times are correctly increased, as that is still being fixed in KAFKA-6101 .",,githubbot,sliebau,,,,,,,,,,,,,,,,,,,,,,,KAFKA-6101,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-10-22 20:17:39.096,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 23 13:16:58 UTC 2017,,,,,,,"0|i3lkcn:",9223372036854775807,,,,,,,,,,,,,,,,"22/Oct/17 20:17;githubbot;GitHub user soenkeliebau opened a pull request:

    https://github.com/apache/kafka/pull/4113

    KAFKA-6104: Added unit tests for ClusterConnectionStates.

    Added a few unit tests for ClusterConnectionStates.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/soenkeliebau/kafka KAFKA-6104

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/4113.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #4113
    
----
commit c42986e2084e19173719e9e554a84c33a460f60f
Author: Soenke Liebau <soenke.liebau@opencore.com>
Date:   2017-10-22T20:16:25Z

    KAFKA-6104: Added unit tests for ClusterConnectionStates.

----
","23/Oct/17 13:16;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/4113
",,,,,,,,,,,,,,,,,,,,
"Extra unit tests for NetworkClient.connectionDelay(Node node, long now)",KAFKA-3255,12940803,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,,frankscholten,frankscholten,20/Feb/16 17:54,22/Feb/16 18:13,12/Jan/21 11:54,22/Feb/16 18:13,0.9.0.1,,,,0.10.0.0,,,,,core,,,,0,test,,,I am exploring the Kafka codebase and noticed that this method was not covered so I added some tests. Also saw that the method isConnecting is not used anywhere in the code. ,,ewencp,frankscholten,githubbot,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/16 17:55;frankscholten;KAFKA-3255.patch;https://issues.apache.org/jira/secure/attachment/12788856/KAFKA-3255.patch",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2016-02-20 18:01:44.173,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 22 18:13:48 UTC 2016,,,,,,,"0|i2t44f:",9223372036854775807,,,,,,,,,,,,,,,,"20/Feb/16 18:01;ijuma;Thanks for the patch. Would you consider submitting a pull request as per the following?

https://cwiki.apache.org/confluence/display/KAFKA/Contributing+Code+Changes","20/Feb/16 18:19;githubbot;GitHub user frankscholten opened a pull request:

    https://github.com/apache/kafka/pull/941

    KAFKA-3255 Added unit tests for NetworkClient.connectionDelay(Node node, long now)

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/frankscholten/kafka tests/cluster-connection-states

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/941.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #941
    
----

----
","22/Feb/16 18:13;ewencp;Issue resolved by pull request 941
[https://github.com/apache/kafka/pull/941]","22/Feb/16 18:13;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/941
",,,,,,,,,,,,,,,,,,
benchmark test for the purgatory,KAFKA-2013,12780911,Test,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,yasuhiro.matsuda,yasuhiro.matsuda,yasuhiro.matsuda,10/Mar/15 16:38,01/Apr/15 23:17,12/Jan/21 11:54,01/Apr/15 23:17,,,,,0.9.0.0,,,,,purgatory,,,,0,,,,We need a micro benchmark test for measuring the purgatory performance.,,junrao,yasuhiro.matsuda,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/15 16:41;yasuhiro.matsuda;KAFKA-2013.patch;https://issues.apache.org/jira/secure/attachment/12703682/KAFKA-2013.patch","16/Mar/15 20:23;yasuhiro.matsuda;KAFKA-2013_2015-03-16_13:23:38.patch;https://issues.apache.org/jira/secure/attachment/12704864/KAFKA-2013_2015-03-16_13%3A23%3A38.patch","16/Mar/15 21:13;yasuhiro.matsuda;KAFKA-2013_2015-03-16_14:13:20.patch;https://issues.apache.org/jira/secure/attachment/12704877/KAFKA-2013_2015-03-16_14%3A13%3A20.patch","16/Mar/15 21:39;yasuhiro.matsuda;KAFKA-2013_2015-03-16_14:39:07.patch;https://issues.apache.org/jira/secure/attachment/12704887/KAFKA-2013_2015-03-16_14%3A39%3A07.patch","19/Mar/15 23:31;yasuhiro.matsuda;KAFKA-2013_2015-03-19_16:30:52.patch;https://issues.apache.org/jira/secure/attachment/12705748/KAFKA-2013_2015-03-19_16%3A30%3A52.patch","01/Apr/15 00:31;yasuhiro.matsuda;KAFKA-2013_2015-03-31_17:30:56.patch;https://issues.apache.org/jira/secure/attachment/12708568/KAFKA-2013_2015-03-31_17%3A30%3A56.patch",,6.0,,,,,,,,,,,,,,,,,,,,2015-04-01 23:17:17.979,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 01 23:17:17 UTC 2015,,,,,,,"0|i26l8f:",9223372036854775807,,,,,,,,,,,,,,,,"10/Mar/15 16:41;yasuhiro.matsuda;Created reviewboard https://reviews.apache.org/r/31893/diff/
 against branch origin/trunk","10/Mar/15 16:52;yasuhiro.matsuda;This benchmark test measures the enqueue rate.

Parameters
- the number of requests
- the target enqueue rate (request per second)
  A request interval follows an exponential distribution. 
- the timeout (milliseconds)
- a distribution of time of request completion (75th percentile and 50th percentile)
  It follows a log-normal distribution
- a data size per request

Each request has three keys. All requests have the identical set of keys.

After a run it shows the actual target rate and the actual enqueue rate.
","16/Mar/15 20:23;yasuhiro.matsuda;Updated reviewboard https://reviews.apache.org/r/31893/diff/
 against branch origin/trunk","16/Mar/15 21:13;yasuhiro.matsuda;Updated reviewboard https://reviews.apache.org/r/31893/diff/
 against branch origin/trunk","16/Mar/15 21:39;yasuhiro.matsuda;Updated reviewboard https://reviews.apache.org/r/31893/diff/
 against branch origin/trunk","19/Mar/15 23:31;yasuhiro.matsuda;Updated reviewboard https://reviews.apache.org/r/31893/diff/
 against branch origin/trunk","01/Apr/15 00:31;yasuhiro.matsuda;Updated reviewboard https://reviews.apache.org/r/31893/diff/
 against branch origin/trunk","01/Apr/15 23:17;junrao;Thanks for the latest patch. +1. Committed to trunk after making the following minor changes.

1. Removed unused var stopped
2. Removed the unnecessary awaitShutdown() call.
3. Configured the ShutdownableThread to be non-interruptible,",,,,,,,,,,,,,,
Sorry it's Just Test,KAFKA-1556,12729633,Test,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Incomplete,,bongster,bongster,25/Jul/14 00:59,07/Aug/14 14:27,12/Jan/21 11:54,07/Aug/14 14:27,,,,,,,,,,core,,,,0,newbie,,,"Sorry this is Test for me.
I Want to Commiter so i'm Test now

Sorry.",,bongster,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407707,,,2014-07-25 00:59:37.0,,,,,,,"0|i1y5m7:",407717,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
